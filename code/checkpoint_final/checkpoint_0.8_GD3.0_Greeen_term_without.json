[
    {
        "title": "CONCEPT BOTTLENECK GENERATIVE MODELS",
        "abstract": "We introduce a generative model with an intrinsically interpretable layer\u2014a concept bottleneck layer\u2014that constrains the model to encode human-understandable concepts. The concept bottleneck layer partitions the generative model into three parts: the pre-concept bottleneck portion, the CB layer, and the post-concept bottleneck portion. To train CB generative models, we complement the traditional task-based loss function for training generative models with a concept loss and an orthogonality loss. The CB layer and these loss terms are model agnostic, which we demonstrate by applying the CB layer to three different families of generative models: generative adversarial networks, variational autoencoders, and diffusion models. On multiple datasets across different types of generative models, steering a generative model, with the CB layer, outperforms all baselines\u2014in some cases, it is 10 times more effective. In addition, we show how the CB layer can be used to interpret the output of the generative model and debug the model during or post training.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces concept bottleneck generative models (CBGMs), a novel approach to generative modeling that incorporates an intrinsically interpretable layer, the concept bottleneck (CB) layer. This layer is designed to constrain the model to encode human-understandable concepts, enabling the model to be steered, interpreted, and debugged. The CB layer is model-agnostic, meaning it can be applied to various generative model families, including variational autoencoders (VAEs), generative adversarial networks (GANs), and diffusion models. The authors demonstrate the effectiveness of CBGMs in steering generative models, interpreting their output, and debugging them during and post-training.\n\nStrengths:\n\n1. The paper presents a novel and innovative approach to generative modeling, addressing a significant limitation of current generative models, which are largely inscrutable and difficult to steer.\n2. The concept bottleneck layer is model-agnostic, allowing it to be applied to various generative model families, increasing its versatility and potential impact.\n3. The authors provide a comprehensive evaluation of CBGMs, including steering, interpretation, and debugging experiments, which demonstrate their effectiveness and robustness.\n4. The paper discusses the limitations of current approaches to interpreting and steering generative models, highlighting the need for more interpretable and controllable models.\n\nWeaknesses:\n\n1. The paper requires a significant amount of background knowledge in generative modeling, which may make it challenging for readers without a strong foundation in the field.\n2. The authors rely heavily on existing work, which may make it difficult to distinguish the novelty and contributions of the paper.\n3. The paper does not provide a clear explanation of how the concept bottleneck layer is trained, which is a critical aspect of the method.\n4. The authors do not discuss the potential scalability issues of the method, particularly when dealing with large and complex datasets.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the concept bottleneck layer, including its training procedure and how it is integrated into the generative model.\n2. Include a more comprehensive discussion of the limitations of the method, including potential scalability issues and the need for large amounts of annotated data.\n3. Consider including more qualitative and quantitative evaluations of the method, such as user studies or expert evaluations, to provide a more comprehensive understanding of its effectiveness.\n4. Provide a clear explanation of how the method can be applied to other domains, such as text or audio, and discuss the potential challenges and opportunities of doing so.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to generative modeling, addressing a significant limitation of current generative models. The concept bottleneck layer is model-agnostic, allowing it to be applied to various generative model families, increasing its versatility and potential impact. However, the paper requires a significant amount of background knowledge in generative modeling, and the authors rely heavily on existing work. Despite these limitations, the paper is well-written, and the authors provide a comprehensive evaluation of the method. I recommend this paper for readers interested in generative modeling and interpretable AI.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes a novel generative model architecture, called concept bottleneck generative models (CBGMs), which integrates a concept bottleneck (CB) layer to enable interpretability, steerability, and debugging of generative models. The CB layer is a model-agnostic component that can be applied to different generative model families, including VAEs, GANs, and diffusion models. The authors show that the CB layer can effectively steer the output of generative models, improve their interpretability, and facilitate debugging during and post-training. The proposed framework is evaluated on multiple datasets and generative models, demonstrating its effectiveness and versatility.\n\nStrengths:\n\n1. The paper proposes a novel and innovative approach to generative models, addressing a significant challenge in the field.\n2. The authors provide a comprehensive framework for concept bottleneck generative models, including a clear description of the architecture, loss functions, and training procedure.\n3. The paper presents extensive experimental results, demonstrating the effectiveness of the proposed method across different datasets and generative models.\n4. The authors provide a thorough analysis of the limitations and challenges of their proposed method, acknowledging potential issues and discussing potential future directions.\n\nWeaknesses:\n\n1. The paper assumes that pre-defined human-understandable features are available, which may not always be the case in practice.\n2. The authors do not provide a clear explanation of how to determine the number of concepts and their specific definitions.\n3. The paper does not explore the potential limitations of the CB layer in terms of its ability to capture complex relationships between concepts.\n4. The authors do not provide a detailed analysis of the computational costs associated with training and deploying CBGMs.\n\nSuggestions for Improvement:\n\n1. Provide more detailed guidance on how to determine the number of concepts and their specific definitions.\n2. Explore alternative approaches for dealing with complex relationships between concepts.\n3. Conduct a more thorough analysis of the computational costs associated with training and deploying CBGMs.\n4. Discuss potential applications and extensions of the proposed method, such as its use in text-to-image models or other generative models.\n\nRecommendation:\n\nThis paper presents a novel and innovative approach to generative models, addressing a significant challenge in the field. The proposed framework is well-motivated, well-designed, and effectively evaluated. The authors provide a thorough analysis of the limitations and challenges of their proposed method, acknowledging potential issues and discussing potential future directions. Overall, this paper is a significant contribution to the field of generative models and has the potential to inspire new research directions and applications.\n\nRating: 8.5/10\n\nRecommendation for Publication: Yes",
        "gr_score_with": 0.8942857142857142,
        "gr_score_without": 0.817258883248731,
        "output_without": {
            "num_tokens_scored": 394,
            "num_green_tokens": 322,
            "green_fraction": 0.817258883248731,
            "z_score": 0.856747135696705,
            "p_value": 0.19579232362621968
        },
        "output_with": {
            "num_tokens_scored": 350,
            "num_green_tokens": 313,
            "green_fraction": 0.8942857142857142,
            "z_score": 4.410050648782306,
            "p_value": 5.167322227182673e-06
        }
    },
    {
        "title": "SWAP: SPARSE ENTROPIC WASSERSTEIN REGRESSION FOR ROBUST NETWORK PRUNING",
        "abstract": "This study addresses the challenge of inaccurate gradients in computing the empirical Fisher Information Matrix during neural network pruning. We introduce SWAP, a formulation of Entropic Wasserstein regression (EWR) for pruning, capitalizing on the geometric properties of the optimal transport problem. The \u201cswap\u201d of the commonly used linear regression with the EWR in optimization is analytically demonstrated to offer noise mitigation effects by incorporating neighborhood interpolation across data points with only marginal additional computational cost. The unique strength of SWAP is its intrinsic ability to balance noise reduction and covariance information preservation effectively. Extensive experiments performed on various networks and datasets show comparable performance of SWAP with state-of-the-art (SoTA) network pruning algorithms. Our proposed method outperforms the SoTA when the network size or the target sparsity is large, the gain is even larger with the existence of noisy gradients, possibly from noisy data, analog memory, or adversarial attacks. Notably, our proposed method achieves a gain of 6% improvement in accuracy and 8% improvement in testing loss for MobileNetV1 with less than one-fourth of the network parameters remaining.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method for neural network pruning called Sparse Entropic Wasserstein Regression Pruning (SWAP), which leverages the Entropic Wasserstein regression (EWR) to mitigate the effect of noisy gradients in computing the empirical Fisher Information Matrix (FIM). The proposed method, SWAP, is shown to outperform state-of-the-art (SoTA) network pruning algorithms, especially in the presence of noisy gradients.\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of the network pruning problem and its relation to the optimization problem, which is a significant contribution to the field.\n2. The proposed method, SWAP, is shown to be robust to noisy gradients, which is a critical aspect of neural network pruning.\n3. The paper provides a thorough comparison of SWAP with other SoTA methods, including magnitude pruning, WoodFisher, and Combinatorial Brain Surgeon.\n4. The authors provide a clear and concise algorithmic framework for SWAP, which is easy to implement and understand.\n\nWeaknesses:\n\n1. The paper assumes that the neural network is pre-trained, which may not be the case in practice.\n2. The proposed method requires the computation of the Wasserstein distance, which can be computationally expensive for large datasets.\n3. The paper does not provide a thorough analysis of the computational complexity of SWAP.\n4. The authors do not provide a clear explanation of how to choose the regularization parameter \u03bb and the entropic regularization term \u03b5.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the computational complexity of SWAP and its scalability to large datasets.\n2. Investigate the effect of different regularization parameters \u03bb and entropic regularization terms \u03b5 on the performance of SWAP.\n3. Provide a more comprehensive comparison of SWAP with other SoTA methods, including a more detailed analysis of their strengths and weaknesses.\n4. Investigate the applicability of SWAP to other machine learning tasks, such as clustering and dimensionality reduction.\n\nRecommendation:\n\nThe paper provides a novel and robust method for neural network pruning, which has the potential to improve the performance of neural networks in the presence of noisy gradients. However, the paper requires further investigation and improvement in terms of computational complexity, scalability, and applicability to other machine learning tasks. Therefore, I recommend that the authors provide a more detailed analysis of the computational complexity of SWAP and its scalability to large datasets, and investigate the effect of different regularization parameters \u03bb and entropic regularization terms \u03b5 on the performance of SWAP.\n\nOverall, I believe that the paper provides a significant contribution to the field of neural network pruning and has the potential to improve the performance of neural networks in the presence of noisy gradients.\n\nRating: 7.5/10\n\nRecommendation: Accept with minor revisions.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method called SWAP (Sparse Entropic Wasserstein Regression) for neural network pruning. The method aims to address the challenge of inaccurate gradients in computing the empirical Fisher Information Matrix during neural network pruning. The authors introduce a formulation of Entropic Wasserstein regression (EWR) for pruning, which capitalizes on the geometric properties of the optimal transport problem. The proposed method, SWAP, is demonstrated to be more robust to noisy gradients and achieves comparable performance with state-of-the-art (SoTA) network pruning algorithms. The authors also provide a theoretical analysis of the method, showing that it strikes a balance between gradient noise reduction and covariance capturing.\n\nStrengths:\n\n* The proposed method, SWAP, is novel and addresses a significant challenge in neural network pruning.\n* The authors provide a thorough theoretical analysis of the method, including a discussion on its robustness to noisy gradients.\n* The experimental results demonstrate that SWAP outperforms or matches state-of-the-art methods in various scenarios.\n* The authors provide a clear and concise presentation of the method and its results.\n\nWeaknesses:\n\n* The paper assumes that the training data is i.i.d., which may not be the case in practice.\n* The authors do not provide a detailed analysis of the computational complexity of the proposed method.\n* The experimental results are based on a limited set of datasets and networks, and it is not clear whether the method generalizes to other scenarios.\n* The authors do not provide a clear comparison with existing methods in terms of computational efficiency.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the computational complexity of the proposed method.\n* The experimental results should be extended to a wider range of datasets and networks to demonstrate the generalizability of the method.\n* The authors should provide a clear comparison with existing methods in terms of computational efficiency.\n* The authors should investigate the robustness of the proposed method to non-i.i.d. training data.\n\nRecommendation:\n\nThe proposed method, SWAP, is a novel and promising approach to neural network pruning. The authors provide a thorough theoretical analysis and experimental results that demonstrate its effectiveness. However, the method has some limitations, and further research is needed to address these issues. Overall, I recommend this paper for publication in a top-tier conference, but with the expectation that the authors will address the weaknesses and suggestions for improvement mentioned above.",
        "gr_score_with": 0.8392857142857143,
        "gr_score_without": 0.8117977528089888,
        "output_without": {
            "num_tokens_scored": 356,
            "num_green_tokens": 289,
            "green_fraction": 0.8117977528089888,
            "z_score": 0.5567865987532468,
            "p_value": 0.2888366205684008
        },
        "output_with": {
            "num_tokens_scored": 336,
            "num_green_tokens": 282,
            "green_fraction": 0.8392857142857143,
            "z_score": 1.8005623758674256,
            "p_value": 0.03588594191629517
        }
    },
    {
        "title": "BEYOND IMITATION: LEVERAGING FINE-GRAINED QUALITY SIGNALS FOR ALIGNMENT",
        "abstract": "Alignment with human preference is a desired property of large language models (LLMs). Currently, the main alignment approach is based on reinforcement learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is intricate to implement and train, thus recent studies explore how to develop alternative alignment approaches based on supervised fine-tuning (SFT). A major limitation of SFT is that it essentially does imitation learning, which cannot fully understand what are the expected behaviors. To address this issue, we propose an improved alignment approach named FIGA. Different from prior methods, we incorporate fine-grained (i.e., token or phrase level) quality signals that are derived by contrasting good and bad responses. Our approach has made two major contributions. Firstly, we curate a refined alignment dataset that pairs initial responses and the corresponding revised ones. Secondly, we devise a new loss function can leverage fine-grained quality signals to instruct the learning of LLMs for alignment. Extensive experiments have demonstrated the effectiveness of our approaches by comparing a number of competitive baselines. We release all the above-mentioned resources at https://github.com/RUCAIBox/FIGA.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach called FIGA (Fine-grained Quality-Aware Alignment) for aligning language models with human preferences. The approach leverages fine-grained quality signals derived by contrasting good and bad responses to instruct the learning of language models for alignment. The authors curate a refined alignment dataset called SPA (SubPar Alignment) that pairs initial responses with revised responses on queries where a language model performs less well. They also design a new loss function that incorporates fine-grained quality signals to encourage the model to learn good behaviors and penalize bad behaviors. The approach is evaluated on several benchmarks, showing superior performance compared to existing alignment methods.\n\nStrengths:\n\n1. The paper presents a novel approach for aligning language models with human preferences, which is a critical problem in natural language processing.\n2. The approach leverages fine-grained quality signals, which provides more detailed information about what makes a response good or bad.\n3. The authors curate a refined alignment dataset called SPA, which is a significant contribution to the field.\n4. The approach is evaluated on several benchmarks, showing superior performance compared to existing alignment methods.\n\nWeaknesses:\n\n1. The paper assumes the availability of a reward model, which may not always be the case in real-world scenarios.\n2. The approach relies on the quality of the revised responses, which may not always be available or reliable.\n3. The paper does not provide a detailed analysis of the Levenshtein distance metric used to quantify the similarity between initial and revised responses.\n4. The approach may not generalize well to out-of-distribution scenarios, where the reward model may not perform well.\n\nSuggestions for Improvement:\n\n1. Investigate alternative methods for deriving fine-grained quality signals, such as using other metrics or incorporating additional information.\n2. Evaluate the approach on more diverse and challenging datasets to assess its generalizability.\n3. Provide more detailed analysis of the Levenshtein distance metric and its limitations.\n4. Investigate the use of other reward models or alternative methods for aligning language models with human preferences.\n\nRecommendation:\n\nThe paper presents a novel and promising approach for aligning language models with human preferences. While there are some limitations and areas for improvement, the approach shows superior performance compared to existing alignment methods. I recommend this paper for publication in a top-tier conference, with the suggestion to address the weaknesses and improve the approach as mentioned above.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$ \nPaper Summary:\n\nThe paper proposes an alignment approach named FIGA to improve the alignment of large language models (LLMs) with human preferences. FIGA incorporates fine-grained quality signals derived by contrasting good and bad responses to instruct the learning of LLMs for alignment. The authors curate a refined alignment dataset called SPA that pairs initial responses and the corresponding revised ones. They also devise a new loss function that can leverage fine-grained quality signals to capture the underlying effect to generate high-quality responses.\n\nStrengths:\n\n* The paper proposes an innovative approach to alignment by leveraging fine-grained quality signals.\n* The authors curate a refined alignment dataset that pairs initial responses and the corresponding revised ones.\n* The proposed loss function can capture the underlying effect to generate high-quality responses.\n* The authors conduct extensive experiments to demonstrate the effectiveness of their approach.\n\nWeaknesses:\n\n* The paper assumes that a reward model for assessing the alignment level is available, which may not always be the case.\n* The authors do not provide a detailed analysis of the distribution shift between the LLM to be aligned and the ground-truth demonstrations.\n* The proposed loss function may not be robust to out-of-distribution scenarios.\n\nSuggestions for Improvement:\n\n* Provide more details about the reward model and its training process.\n* Conduct a more thorough analysis of the distribution shift between the LLM to be aligned and the ground-truth demonstrations.\n* Investigate the robustness of the proposed loss function to out-of-distribution scenarios.\n\nRecommendation:\n\nThe paper proposes an innovative approach to alignment by leveraging fine-grained quality signals. The authors conduct extensive experiments to demonstrate the effectiveness of their approach. However, the paper assumes that a reward model for assessing the alignment level is available, and the proposed loss function may not be robust to out-of-distribution scenarios. With some modifications to address these limitations, the paper has the potential to make a significant contribution to the field of alignment.",
        "gr_score_with": 0.8530612244897959,
        "gr_score_without": 0.8096590909090909,
        "output_without": {
            "num_tokens_scored": 352,
            "num_green_tokens": 285,
            "green_fraction": 0.8096590909090909,
            "z_score": 0.45333878623509694,
            "p_value": 0.3251524038781556
        },
        "output_with": {
            "num_tokens_scored": 245,
            "num_green_tokens": 209,
            "green_fraction": 0.8530612244897959,
            "z_score": 2.0765686321995616,
            "p_value": 0.018920695263948403
        }
    },
    {
        "title": "GUAGE MODEL POWERED DIALOGUE AGENTS",
        "abstract": "Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play simulation. In this manner, the LLM-powered dialogue agent can not only be generalized to different cases after the training, but also be applicable to different applications by just substituting the learned plug-in. In addition, we propose to evaluate the policy planning capability of dialogue systems under the interactive setting. Experimental results demonstrate that PPDPP consistently and substantially outperforms existing approaches on three different proactive dialogue applications, including negotiation, emotional support, and tutoring dialogues.1",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel paradigm for strategizing large language models (LLMs) powered dialogue agents, called Plug-and-Play Dialogue Policy Planner (PPDPP). PPDPP is designed to tackle the challenges of proactive dialogue problems, where the dialogue agent needs to strategically take the initiative to steer the conversation towards an anticipated goal. The proposed framework consists of a tunable language model plug-in that can be fine-tuned with available human-annotated data and reinforced learning from goal-oriented AI feedback. The authors demonstrate the effectiveness of PPDPP on three different proactive dialogue problems, including negotiation, emotional support, and tutoring dialogues.\n\nStrengths:\n1. The paper proposes a novel and effective approach to tackle the challenges of proactive dialogue problems.\n2. The framework is designed to be modular and flexible, allowing for easy adaptation to different applications and domains.\n3. The authors provide a thorough evaluation of PPDPP on three different datasets, demonstrating its superiority over existing approaches.\n4. The paper highlights the potential of tunable plug-ins to address specific shortcomings in LLMs and integrate with multiple plug-ins to tackle more complex dialogue challenges.\n\nWeaknesses:\n1. The paper assumes the availability of human-annotated data, which may not be feasible in all scenarios.\n2. The framework relies on the quality of the goal-oriented AI feedback, which may not always be available or reliable.\n3. The paper does not provide a detailed analysis of the computational resources required for training and deploying PPDPP.\n4. The authors do not discuss the potential limitations and biases of using LLMs in dialogue systems.\n\nSuggestions for Improvement:\n1. Provide more detailed information on the data requirements and preprocessing steps for training PPDPP.\n2. Discuss the potential limitations and biases of using LLMs in dialogue systems and propose potential solutions.\n3. Provide a more comprehensive analysis of the computational resources required for training and deploying PPDPP.\n4. Consider exploring other types of goal-oriented AI feedback, such as multi-modal feedback, to improve the robustness of PPDPP.\n\nRecommendation:\nThe paper proposes a novel and effective approach to tackle the challenges of proactive dialogue problems. While there are some limitations and areas for improvement, the framework shows promise for a wide range of applications. I recommend the paper for publication, with the understanding that the authors will address the suggested improvements in future work.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel dialogue policy planning paradigm, Plug-and-Play Dialogue Policy Planner (PPDPP), to strategize large language models (LLMs) for proactive dialogue problems. PPDPP leverages a tunable language model plug-in to predict the dialogue strategy at the next turn, which can be fine-tuned with available human-annotated corpora and reinforcement learning from goal-oriented AI feedback. The framework enables the LLM-powered dialogue agent to generalize to different cases and exhibit versatility across diverse applications.\n\nStrengths:\n1. The paper introduces a novel dialogue policy planning paradigm, PPDPP, which addresses the limitations of existing approaches in handling proactive dialogue problems.\n2. The framework leverages reinforcement learning from goal-oriented AI feedback, which enables the LLM-powered dialogue agent to learn from dynamic interactions and improve its policy planning capability.\n3. The paper proposes an LLM-based interactive evaluation approach, which assesses the policy planning capability of dialogue systems under the interactive setting.\n\nWeaknesses:\n1. The paper relies heavily on the quality of the human-annotated corpora, which may not be available for all dialogue problems.\n2. The framework requires significant computational resources and expertise in reinforcement learning, which may be a barrier for adoption in practical applications.\n3. The paper does not provide a thorough comparison of the PPDPP framework with existing LLM-based dialogue systems, which makes it difficult to evaluate its effectiveness.\n\nSuggestions for Improvement:\n1. The authors should provide more detailed information about the human-annotated corpora used for training and evaluation, including their size, quality, and annotation process.\n2. The paper should include a more comprehensive comparison of the PPDPP framework with existing LLM-based dialogue systems, including their strengths and limitations.\n3. The authors should provide more information about the computational resources and expertise required to implement and train the PPDPP framework.\n\nRecommendation:\nThe paper proposes an innovative dialogue policy planning paradigm, PPDPP, which has the potential to improve the performance of LLM-powered dialogue agents in proactive dialogue problems. While the paper has some limitations, it provides valuable insights into the importance of reinforcement learning from goal-oriented AI feedback and the need for more interactive evaluation protocols. With further development and refinement, the PPDPP framework could become a valuable tool for improving the effectiveness and efficiency of dialogue systems.\n\nRating: 7/10\n$$$",
        "gr_score_with": 0.8176470588235294,
        "gr_score_without": 0.766016713091922,
        "output_without": {
            "num_tokens_scored": 359,
            "num_green_tokens": 275,
            "green_fraction": 0.766016713091922,
            "z_score": -1.6094141496670091,
            "p_value": 0.9462370933314552
        },
        "output_with": {
            "num_tokens_scored": 340,
            "num_green_tokens": 278,
            "green_fraction": 0.8176470588235294,
            "z_score": 0.8137672353672707,
            "p_value": 0.20788915421013876
        }
    },
    {
        "title": "THE EFFECTIVENESS OF RANDOM FORGETTING FOR ROBUST GENERALIZATION",
        "abstract": "Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy. Adversarial Training (AT) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of AT is robust overfitting, where the network\u2019s robust performance on test data deteriorates with further training, thus hindering generalization. Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called \u201cForget to Mitigate Overfitting (FOMO)\". FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model\u2019s information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. Our experiments on benchmark datasets and adversarial attacks show that FOMO alleviates robust overfitting by significantly reducing the gap between the best and last robust test accuracy while improving the state-of-the-art robustness. Furthermore, FOMO provides a better trade-off between standard and robust accuracy, outperforming baseline adversarial methods. Finally, our framework is robust to AutoAttacks and increases generalization in many real-world scenarios.1",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel adversarial training paradigm called Forget to Mitigate Overfitting (FOMO) to address the problem of robust overfitting in deep neural networks (DNNs). FOMO alternates between the forgetting phase, where a subset of weights is randomly forgotten through reinitialization, and the relearning phase, where the network emphasizes learning generalizable features. The authors demonstrate the efficacy of FOMO on benchmark datasets and adversarial attacks, showing that it alleviates robust overfitting, improves robustness, and provides a better trade-off between standard and robust accuracy.\n\nStrengths:\n\n1. The paper provides a comprehensive overview of the problem of robust overfitting in adversarial training and its significance in real-world applications.\n2. The authors propose a novel and innovative solution, FOMO, which is grounded in the concept of active forgetting and relearning, inspired by the human brain's learning mechanisms.\n3. The experimental results demonstrate the effectiveness of FOMO in alleviating robust overfitting, improving robustness, and providing a better trade-off between standard and robust accuracy.\n4. The authors conduct extensive experiments on multiple datasets, architectures, and perturbation types, showcasing the scalability and robustness of FOMO.\n\nWeaknesses:\n\n1. The paper assumes that the forgetting phase is beneficial for generalization, but it is unclear whether this assumption holds in all cases.\n2. The authors do not provide a thorough analysis of the convergence properties of FOMO, which is crucial for understanding its robustness and generalization capabilities.\n3. The paper relies heavily on empirical results, and more theoretical analysis is needed to understand the underlying mechanisms of FOMO.\n4. The authors do not discuss the computational costs and complexity of implementing FOMO, which may be a limitation in practice.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the forgetting phase and its impact on generalization.\n2. Investigate the convergence properties of FOMO and its robustness to different perturbation types.\n3. Conduct more theoretical analysis to understand the underlying mechanisms of FOMO and its relationship to the effectiveness of adversarial training.\n4. Discuss the computational costs and complexity of implementing FOMO and provide strategies for reducing these costs.\n\nRecommendation:\n\nThe paper makes a significant contribution to the field of adversarial training by proposing a novel and innovative solution, FOMO. The experimental results demonstrate the effectiveness of FOMO, and the authors provide a comprehensive overview of the problem and its significance. However, more theoretical analysis and discussion of the computational costs are needed to fully understand the potential of FOMO. With further development and refinement, FOMO has the potential to become a widely adopted technique for improving the robustness and generalization of deep neural networks.\n\nOverall, I recommend this paper for publication in a top-tier conference or journal, with the suggestion that the authors address the limitations and weaknesses mentioned above.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel adversarial training paradigm called Forget to Mitigate Overfitting (FOMO) to improve the performance and generalization of Deep Neural Networks (DNNs) in adversarial learning. FOMO alternates between a forgetting phase, where a random subset of weights is reinitialized, and a relearning phase, where the network emphasizes learning generalizable features. The authors demonstrate the effectiveness of FOMO in alleviating robust overfitting and achieving significant results across multiple architectures and datasets.\n\nStrengths:\n\n* The paper provides a thorough analysis of the problem of robust overfitting in adversarial training and proposes a novel solution that is based on the concept of active forgetting.\n* The authors provide a clear and concise explanation of the FOMO method and its components, including the forgetting phase, relearning phase, and consolidation phase.\n* The paper includes an extensive set of experiments that demonstrate the effectiveness of FOMO in improving both standard and robust performance and generalization across a wide range of architectures, datasets, and perturbation types.\n* The authors provide a comprehensive ablation study that evaluates the importance of each component of the FOMO method.\n\nWeaknesses:\n\n* The paper assumes that the forgetting phase is necessary to alleviate robust overfitting, but it does not provide a clear explanation of why this is the case. It would be helpful to provide more theoretical justification for this assumption.\n* The paper does not provide a clear comparison with existing methods that use early stopping to prevent robust overfitting. It would be helpful to provide a more detailed analysis of the trade-offs between FOMO and these existing methods.\n* The paper uses a relatively small set of datasets and architectures in the experiments. It would be helpful to extend the experiments to a larger set of datasets and architectures to demonstrate the generalizability of the FOMO method.\n\nSuggestions for Improvement:\n\n* Provide a more detailed theoretical analysis of the FOMO method, including a clear explanation of why the forgetting phase is necessary to alleviate robust overfitting.\n* Conduct a more comprehensive comparison with existing methods that use early stopping to prevent robust overfitting.\n* Extend the experiments to a larger set of datasets and architectures to demonstrate the generalizability of the FOMO method.\n\nRecommendation:\n\nThis paper presents a novel and effective solution to the problem of robust overfitting in adversarial training. The FOMO method is well-designed and easy to implement, and the authors provide a comprehensive set of experiments that demonstrate its effectiveness. However, the paper would benefit from a more detailed theoretical analysis and a more comprehensive comparison with existing methods. Overall, I would recommend this paper for publication in a top conference in the field of adversarial learning.\n\nRating: 8/10\n\n$$$",
        "gr_score_with": 0.9108635097493036,
        "gr_score_without": 0.8614609571788413,
        "output_without": {
            "num_tokens_scored": 397,
            "num_green_tokens": 342,
            "green_fraction": 0.8614609571788413,
            "z_score": 3.0617771372022524,
            "p_value": 0.0011001360240621098
        },
        "output_with": {
            "num_tokens_scored": 359,
            "num_green_tokens": 327,
            "green_fraction": 0.9108635097493036,
            "z_score": 5.251643190706128,
            "p_value": 7.537413669191202e-08
        }
    },
    {
        "title": "SYMPHONY: SYMMETRY-EQUIVARIANT POINT- CENTERED SPHERICAL HARMONICS FOR MOLECULE GENERATION",
        "abstract": "We present Symphony, an E(3)-equivariant autoregressive generative model for 3D molecular geometries that iteratively builds a molecule from molecular fragments. Existing autoregressive models such as G-SchNet (Gebauer et al., 2019) and G-SphereNet (Luo & Ji, 2022) for molecules utilize rotationally invariant features to respect the 3D symmetries of molecules. In contrast, Symphony uses message-passing with higher-degree E(3)-equivariant features. This allows a novel representation of probability distributions via spherical harmonic signals to efficiently model the 3D geometry of molecules. We show that Symphony is able to accurately generate small molecules from the QM9 dataset, outperforming existing autoregressive models and approaching the performance of diffusion models.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents Symphony, an E(3)-equivariant autoregressive generative model for 3D molecular geometries that iteratively builds a molecule from molecular fragments. Unlike existing autoregressive models that use rotationally invariant features, Symphony uses higher-degree E(3)-equivariant features and spherical harmonic projections to build molecules while respecting the E(3) symmetries of molecular fragments. The authors demonstrate that Symphony outperforms existing autoregressive models and approaches the performance of diffusion models on various metrics.\n\nStrengths:\n\n1. The paper presents a novel approach to autoregressive molecular generation using higher-degree E(3)-equivariant features and spherical harmonic projections.\n2. The authors demonstrate that Symphony outperforms existing autoregressive models and approaches the performance of diffusion models on various metrics.\n3. The paper proposes a comprehensive set of tests to evaluate the quality of generated 3D structures, including validity, capturing training set statistics, and generalization capabilities.\n4. The authors provide a detailed comparison between their approach and existing methods, highlighting the advantages of their approach.\n\nWeaknesses:\n\n1. The paper assumes that the training set molecules are representative of the underlying distribution, which may not always be the case.\n2. The authors use a lookup table to assign bond orders, which may not be accurate for all molecules.\n3. The paper does not provide a clear explanation of how the spherical harmonic projections are used to model the radial distribution.\n4. The authors do not provide a detailed comparison between their approach and existing diffusion models, such as EDM.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of how the spherical harmonic projections are used to model the radial distribution.\n2. Use a more accurate method to assign bond orders, such as using a neural network to predict bond orders.\n3. Provide a more detailed comparison between the approach and existing diffusion models, such as EDM.\n4. Investigate the use of normalizing flows to smoothly model the radial distribution without any discretization.\n\nRecommendation:\n\nThe paper presents a novel and promising approach to autoregressive molecular generation using higher-degree E(3)-equivariant features and spherical harmonic projections. While there are some limitations and areas for improvement, the paper provides a comprehensive evaluation of the approach and demonstrates its effectiveness on various metrics. I recommend accepting the paper for publication, subject to the suggested improvements.\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\nThe paper presents Symphony, an E(3)-equivariant autoregressive generative model for 3D molecular geometries that builds molecules from molecular fragments. Symphony uses higher-degree E(3)-equivariant features and spherical harmonic projections to predict the focus node, target atomic number, and target position of new atoms, while respecting the E(3) symmetries of molecular fragments. The authors show that Symphony outperforms existing autoregressive models and approaches the performance of diffusion models on the QM9 dataset.\n\nStrengths:\n1. The paper presents a novel approach to molecular generation using E(3)-equivariant features and spherical harmonic projections, which allows for a more accurate representation of 3D molecular geometries.\n2. The authors provide a comprehensive evaluation of Symphony's performance on various metrics, including validity, uniqueness, bispectrum, and generalization capabilities.\n3. The paper highlights the advantages of autoregressive models over diffusion models, including faster inference speeds and improved scalability.\n4. The authors provide a clear explanation of the methods used in the paper, including the EMBEDDER, MLP, and linear layers.\n\nWeaknesses:\n1. The paper assumes access to a large dataset of molecular structures, which may not be available for all researchers.\n2. The authors do not provide a thorough comparison of Symphony's performance on different types of molecules, such as small molecules, proteins, and complexes.\n3. The paper does not discuss the computational cost of training Symphony, which may be a significant limitation for large-scale molecular generation tasks.\n4. The authors do not provide a clear explanation of how Symphony can be used for molecular design and optimization tasks.\n\nSuggestions for Improvement:\n1. Provide a more thorough comparison of Symphony's performance on different types of molecules.\n2. Discuss the computational cost of training Symphony and explore methods to reduce it.\n3. Provide a clear explanation of how Symphony can be used for molecular design and optimization tasks.\n4. Consider exploring normalizing flows to smoothly model the radial distribution without any discretization.\n\nRecommendation:\nThe paper presents a novel approach to molecular generation using E(3)-equivariant features and spherical harmonic projections, which shows promising results on molecular generation and completion. However, the paper assumes access to a large dataset of molecular structures, and the computational cost of training Symphony may be a significant limitation. Further exploration of normalizing flows and placing entire local environment motifs at once could improve the performance and scalability of Symphony.",
        "gr_score_with": 0.8899082568807339,
        "gr_score_without": 0.814935064935065,
        "output_without": {
            "num_tokens_scored": 308,
            "num_green_tokens": 251,
            "green_fraction": 0.814935064935065,
            "z_score": 0.6555393182961124,
            "p_value": 0.25606029014945914
        },
        "output_with": {
            "num_tokens_scored": 327,
            "num_green_tokens": 291,
            "green_fraction": 0.8899082568807339,
            "z_score": 4.064793734190551,
            "p_value": 2.4037479553718794e-05
        }
    },
    {
        "title": "ONE-HOT GENERALIZED LINEAR MODEL FOR SWITCHING BRAIN STATE DISCOVERY",
        "abstract": "Exposing meaningful and interpretable neural interactions is critical to understanding neural circuits. Inferred neural interactions from neural signals primarily reflect functional connectivity. In a long experiment, subject animals may experience different stages defined by the experiment, stimuli, or behavioral states, and hence functional connectivity can change over time. To model dynamically changing functional connectivity, prior work employs state-switching generalized linear models with hidden Markov models (i.e., HMM-GLMs). However, we argue they lack biological plausibility, as functional connectivities are shaped and confined by the underlying anatomical connectome. Here, we propose two novel priorinformed state-switching GLMs, called Gaussian HMM-GLM (Gaussian prior) and one-hot HMM-GLM (Gumbel-Softmax one-hot prior). We show that the learned prior should capture the state-invariant interaction, shedding light on the underlying anatomical connectome and revealing more likely physical neuron interactions. The state-dependent interaction modeled by each GLM offers traceability to capture functional variations across multiple brain states. Our methods effectively recover true interaction structures in simulated data, achieve the highest predictive likelihood, and enhance the interpretability of interaction patterns and hidden states when applied to real neural data. The code is available at https://github.com/JerrySoybean/onehot-hmmglm.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach for estimating time-varying functional connectivity in multi-state neural systems, called One-Hot Generalized Linear Model for Switching Brain State Discovery. The model, referred to as One-Hot HMM-GLM (OHG), decomposes the traditional weight matrix in GLMs into a discrete adjacency matrix representing the connection type and a positive-valued strength matrix. The authors argue that the regulated adjacency matrices with their shared prior should inform us about the underlying anatomical connectome, while the less restricted strength matrices can provide flexibility to capture functional variations across multiple brain states. The model is compared to several baseline methods on simulated and real neural datasets, showing superior performance in terms of log-likelihood, state accuracy, weight error, adjacency accuracy, and adjacency prior accuracy.\n\nStrengths:\n\n1. The paper proposes a novel and biologically plausible approach for estimating time-varying functional connectivity in multi-state neural systems.\n2. The model is compared to several baseline methods on simulated and real neural datasets, showing superior performance.\n3. The authors provide a clear and concise explanation of the model and its components.\n4. The paper includes a detailed experimental evaluation, including quantitative results and visualizations of the learned models.\n\nWeaknesses:\n\n1. The paper assumes that the underlying anatomical connectome is static, which may not be the case in reality.\n2. The model requires a large number of hyperparameters to be pre-defined, which can be challenging in practice.\n3. The paper does not provide a clear explanation of how the Gumbel-Softmax prior is used to enforce the adjacency matrices to learn shared information.\n4. The experimental evaluation is limited to two real neural datasets, and it would be beneficial to include more datasets to demonstrate the robustness of the model.\n\nSuggestions for Improvement:\n\n1. Consider incorporating a more dynamic model of the underlying anatomical connectome to better capture the complexity of neural systems.\n2. Provide a more detailed explanation of the Gumbel-Softmax prior and its role in enforcing shared information among adjacency matrices.\n3. Include more real neural datasets in the experimental evaluation to demonstrate the robustness of the model.\n4. Consider using more advanced techniques, such as Bayesian inference or deep learning, to improve the accuracy and robustness of the model.\n\nRecommendation:\n\nThe paper presents a novel and biologically plausible approach for estimating time-varying functional connectivity in multi-state neural systems. While there are some limitations and areas for improvement, the results demonstrate the potential of the model to improve our understanding of neural systems. I recommend this paper for publication in a reputable journal, with the hope that the authors will address the limitations and suggestions for improvement.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel one-hot generalized linear model (OHG) for switching brain state discovery. The authors aim to improve upon previous state-switching generalized linear models (GLMs) with hidden Markov models (HMMs) by incorporating a learnable prior that constrains the search space for interaction weights. The proposed OHG model decomposes the weight matrix into a discrete adjacency matrix and a positive-valued strength matrix, with a common Gumbel-Softmax prior imposed on the adjacency matrices. The authors demonstrate that the regulated adjacency matrices inform us about the underlying anatomical connectome, while the less restricted strength matrices capture functional variations across multiple brain states.\n\nStrengths:\n\n1. The authors provide a clear and concise explanation of the proposed OHG model, making it easy to follow for readers with a background in neuroscience and machine learning.\n2. The paper presents a well-structured and organized structure, with each section clearly outlining the contributions and limitations of the proposed model.\n3. The authors provide a comprehensive comparison of the proposed OHG model with state-of-the-art baseline methods, including GLM, HMM Corr, HMM Bern, HG, and GHG.\n4. The experimental evaluation is extensive, with the authors applying the proposed model to both simulated data and real neural datasets, including the prefrontal cortex (PFC) and barrel cortex datasets.\n5. The paper presents a clear and interpretable representation of the inferred hidden states and neural connectivities, making it easy to visualize and understand the results.\n\nWeaknesses:\n\n1. The paper assumes a specific form for the Gumbel-Softmax prior, which may not be biologically plausible in all cases.\n2. The authors do not provide a comprehensive analysis of the sensitivity of the proposed model to hyperparameters, which may affect the performance of the model.\n3. The paper assumes that the number of hidden states is known, which may not be the case in real-world scenarios.\n4. The authors do not provide a clear explanation of how the proposed model can be scaled to larger datasets.\n5. The paper assumes that the proposed model can be applied to any neural dataset, which may not be the case in all scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more comprehensive analysis of the sensitivity of the proposed model to hyperparameters, including the Gumbel-Softmax temperature and the strength of the shared prior.\n2. The paper should include a more detailed discussion of the assumptions made in the proposed model, including the specific form of the Gumbel-Softmax prior and the assumption that the number of hidden states is known.\n3. The authors should provide a clear explanation of how the proposed model can be scaled to larger datasets, including strategies for reducing the computational complexity of the model.\n4. The paper should include a more comprehensive evaluation of the proposed model, including a comparison with other state-of-the-art methods and a discussion of the limitations of the proposed model.\n5. The authors should provide a clear and concise summary of the contributions and limitations of the proposed model, making it easy for readers to understand the significance of the work.\n\nRecommendation:\n\nThe proposed OHG model is a significant contribution to the field of neural decoding and brain state discovery. The authors provide a clear and concise explanation of the proposed model, and the experimental evaluation is extensive and well-organized. However, the paper assumes a specific form for the Gumbel-Softmax prior, which may not be biologically plausible in all cases. Additionally, the authors do not provide a comprehensive analysis of the sensitivity of the proposed model to hyperparameters, which may affect the performance of the model. Therefore, I recommend that the authors address these limitations in future work, including a more comprehensive analysis of the sensitivity of the proposed model to hyperparameters and a more detailed discussion of the assumptions made in the proposed model.\n\nOverall, the proposed OHG model is a significant contribution to the field of neural decoding and brain state discovery, and the authors should be commended for their innovative work.\n\n$$$",
        "gr_score_with": 0.9246119733924612,
        "gr_score_without": 0.8972431077694235,
        "output_without": {
            "num_tokens_scored": 399,
            "num_green_tokens": 358,
            "green_fraction": 0.8972431077694235,
            "z_score": 4.856328581494316,
            "p_value": 5.979110402624432e-07
        },
        "output_with": {
            "num_tokens_scored": 451,
            "num_green_tokens": 417,
            "green_fraction": 0.9246119733924612,
            "z_score": 6.6161403965174115,
            "p_value": 1.8434911254334276e-11
        }
    },
    {
        "title": "DRM: MASTERING VISUAL REINFORCEMENT LEARN-",
        "abstract": "Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents\u2019 inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio (Sokar et al., 2023) as a metric to measure inactivity in the RL agent\u2019s network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent\u2019s activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM , a method that uses three core mechanisms to guide agents\u2019 exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations. 1 0% 20% 40% 60% 80% 100% Training Progress 0 100 200 300 400 500 600 Ep iso de R ew ar d Deepmind Control Suite (8 Hard Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e MetaWorld (8 Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e Adroit (3 Tasks) DrM DrQ-v2 ALIX TACO Figure 1: Success rate and episode reward as a function of training progress for each of the three domains that we consider (Deepmind Control Suite, MetaWorld, Adroit). All results are averaged over 4 random seeds, and the shaded region stands for standard deviation across different random seeds.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method, DrM, for visual reinforcement learning (RL) that aims to improve sample efficiency and asymptotic performance. The authors identify a major shortcoming in existing visual RL methods, where agents often exhibit sustained inactivity during early training, limiting their ability to explore effectively. They introduce three simple mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio, a metric that measures inactivity in the RL agent's network. The proposed method, DrM, is evaluated across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit, and demonstrates significant improvements in sample efficiency and asymptotic performance compared to existing state-of-the-art model-free algorithms.\n\nStrengths:\n\n1. The paper identifies a crucial shortcoming in existing visual RL methods and proposes a novel solution to address this issue.\n2. The authors introduce a new metric, dormant ratio, which provides valuable insights into agent behavior and can be used to balance exploration and exploitation.\n3. The proposed method, DrM, is evaluated extensively across three benchmark environments and demonstrates significant improvements in sample efficiency and asymptotic performance.\n4. The authors provide a comprehensive analysis of the dormant ratio and its relationship with agent behavior, which is a valuable contribution to the field of visual RL.\n\nWeaknesses:\n\n1. The paper assumes a fixed network capacity, which may not be realistic in many practical scenarios.\n2. The authors do not provide a detailed analysis of the computational efficiency of the proposed method.\n3. The paper focuses primarily on continuous control tasks and does not explore the applicability of the proposed method to discrete action tasks.\n4. The authors do not provide a clear explanation of why the dormant ratio is a better metric for balancing exploration and exploitation compared to other metrics.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational efficiency of the proposed method and explore ways to optimize its performance.\n2. The paper should be extended to include a more comprehensive evaluation of the proposed method on a wider range of tasks and environments.\n3. The authors should provide a clear explanation of why the dormant ratio is a better metric for balancing exploration and exploitation compared to other metrics.\n4. The paper should be revised to include a more detailed discussion of the potential limitations and challenges of the proposed method.\n\nRecommendation:\n\nThe paper proposes a novel method for visual RL that addresses a significant shortcoming in existing methods. The proposed method, DrM, demonstrates significant improvements in sample efficiency and asymptotic performance compared to existing state-of-the-art model-free algorithms. However, the paper assumes a fixed network capacity, does not provide a detailed analysis of computational efficiency, and focuses primarily on continuous control tasks. Despite these limitations, the paper makes a valuable contribution to the field of visual RL and provides a promising direction for future research.\n\nRating: 7/10\n\nRecommendation for Publication: Yes\n\nRecommendation for Future Research: Yes\n\nRecommendation for Further Development: Yes\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper introduces a novel online reinforcement learning algorithm, DrM (Dormant Ratio Minimization), which addresses the challenges of visual continuous control tasks. The authors identify a critical issue in existing visual RL methods: agents often exhibit sustained inactivity during early training, limiting their ability to explore effectively. They propose three simple yet effective mechanisms in DrM to balance exploration and exploitation while lowering the dormant ratio: a periodical neural network weight perturbation mechanism, a dormant-ratio-based exploration scheduler, and a dormant-ratio-based exploitation mechanism. The authors empirically verify that DrM achieves significant improvements in sample efficiency and asymptotic performance across three continuous control benchmark environments: DeepMind Control Suite, MetaWorld, and Adroit. Most notably, DrM is the first documented model-free algorithm that reliably solves complex tasks such as Dog and Manipulator in the DeepMind Control Suite and Adroit dexterous hand manipulation tasks without demonstrations from pixels.\n\nStrengths:\n\n1. The paper provides a clear and concise introduction to the problem of visual continuous control tasks and the challenges faced by existing visual RL methods.\n2. The authors propose a novel and effective approach to addressing the issue of sustained inactivity during early training, which is a critical problem in visual RL.\n3. The three mechanisms introduced in DrM are simple yet effective and can be easily implemented and computed efficiently.\n4. The authors provide extensive empirical evidence that DrM achieves significant improvements in sample efficiency and asymptotic performance across three benchmark environments.\n5. The paper provides a detailed analysis and comparison of the dormant ratio changes during the training process of DrM and three baseline algorithms, which demonstrates the effectiveness of DrM in minimizing the dormant ratio.\n\nWeaknesses:\n\n1. The paper assumes that the reader is familiar with the basics of reinforcement learning and visual RL, which may limit its accessibility to a broader audience.\n2. The authors do not provide a detailed explanation of the mathematical derivations behind the three mechanisms introduced in DrM, which may make it difficult for readers to fully understand the approach.\n3. The paper relies heavily on empirical evidence to demonstrate the effectiveness of DrM, which may not be sufficient to establish its theoretical foundations.\n4. The authors do not provide a clear explanation of how the dormant ratio is calculated and how it is used to guide the exploration and exploitation trade-off in DrM.\n5. The paper does not discuss potential limitations and challenges of the approach, such as the potential for the agent to get stuck in a sub-optimal policy if the dormant ratio is not properly managed.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the mathematical derivations behind the three mechanisms introduced in DrM to make it easier for readers to understand the approach.\n2. Include a more detailed discussion of the theoretical foundations of DrM and its relationship to existing work in visual RL.\n3. Provide a clearer explanation of how the dormant ratio is calculated and how it is used to guide the exploration and exploitation trade-off in DrM.\n4. Discuss potential limitations and challenges of the approach, such as the potential for the agent to get stuck in a sub-optimal policy if the dormant ratio is not properly managed.\n5. Consider including more experiments and comparisons with other state-of-the-art visual RL algorithms to further establish the effectiveness of DrM.\n\nRecommendation:\n\nThe paper provides a novel and effective approach to addressing the challenges of visual continuous control tasks. While there are some limitations and areas for improvement, the paper is well-written and provides extensive empirical evidence to support its claims. I recommend accepting the paper for publication in a top-tier conference in the field of reinforcement learning and artificial intelligence.",
        "gr_score_with": 0.8135593220338984,
        "gr_score_without": 0.7158176943699732,
        "output_without": {
            "num_tokens_scored": 373,
            "num_green_tokens": 267,
            "green_fraction": 0.7158176943699732,
            "z_score": -4.064227230300033,
            "p_value": 0.9999759040700673
        },
        "output_with": {
            "num_tokens_scored": 472,
            "num_green_tokens": 384,
            "green_fraction": 0.8135593220338984,
            "z_score": 0.7367898617581852,
            "p_value": 0.23062507472399507
        }
    },
    {
        "title": "ING VIA TASK-DRIVEN FEATURE SELECTION",
        "abstract": "This paper presents a data-driven, task-specific paradigm for experimental design, to shorten acquisition time, reduce costs, and accelerate the deployment of imaging devices. Current approaches in experimental design focus on model-parameter estimation and require specification of a particular model, whereas in imaging, other tasks may drive the design. Furthermore, such approaches often lead to intractable optimization problems in real-world imaging applications. Here we present a new paradigm for experimental design that simultaneously optimizes the design (set of image channels) and trains a machine-learning model to execute a userspecified image-analysis task. The approach obtains data densely-sampled over the measurement space (many image channels) for a small number of acquisitions, then identifies a subset of channels of prespecified size that best supports the task. We propose a method: TADRED for TAsk-DRiven Experimental Design in imaging, to identify the most informative channel-subset whilst simultaneously training a network to execute the task given the subset. Experiments demonstrate the potential of TADRED in diverse imaging applications: several clinicallyrelevant tasks in magnetic resonance imaging; and remote sensing and physiological applications of hyperspectral imaging. Results show substantial improvement over classical experimental design, two recent application-specific methods within the new paradigm, and state-of-the-art approaches in supervised feature selection. We anticipate further applications of our approach. Code is available: Code Link.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to experimental design in imaging applications, specifically for multi-channel imaging. The proposed method, TADRED, is a task-driven experimental design paradigm that simultaneously optimizes the design and trains a machine-learning model to execute a user-specified image-analysis task. TADRED uses a dual-network architecture, consisting of a scoring network and a task network, to identify the most informative channel-subset and train a high-performing network for the task. The paper demonstrates the effectiveness of TADRED in various imaging applications, including clinically relevant tasks in magnetic resonance imaging and remote sensing and physiological applications of hyperspectral imaging.\n\nStrengths:\n\n1. The paper presents a novel and innovative approach to experimental design in imaging applications, which is a significant contribution to the field.\n2. The proposed method, TADRED, is well-motivated and has a clear and logical structure.\n3. The paper provides a thorough evaluation of TADRED using various benchmark datasets and tasks, demonstrating its effectiveness in different imaging applications.\n4. The authors provide a comprehensive analysis of the strengths and limitations of TADRED, as well as comparisons with other state-of-the-art methods.\n\nWeaknesses:\n\n1. The paper assumes a priori knowledge of the task, which may not always be the case in real-world applications.\n2. The proposed method requires a large amount of training data, which may not be feasible in all scenarios.\n3. The paper does not provide a detailed explanation of the hyperparameter tuning process, which is crucial for the success of the method.\n4. The comparison with other methods is limited to a few benchmark datasets, and it would be beneficial to evaluate TADRED on a broader range of datasets and tasks.\n\nSuggestions for Improvement:\n\n1. Investigate the possibility of learning the task from the data, rather than relying on a priori knowledge.\n2. Explore alternative methods for hyperparameter tuning, such as Bayesian optimization or evolutionary algorithms.\n3. Provide a more detailed explanation of the hyperparameter tuning process and its impact on the performance of TADRED.\n4. Evaluate TADRED on a broader range of datasets and tasks to demonstrate its robustness and generalizability.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to experimental design in imaging applications, which has the potential to significantly impact the field. However, there are some limitations and areas for improvement that need to be addressed. With further refinement and evaluation, TADRED has the potential to become a widely used and effective method for experimental design in imaging applications.\n\n$$$\nNote: The review is based on the provided paper and does not include any external knowledge or information.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach for experimental design in imaging applications, dubbed TADRED, which stands for Task-Driven Experimental Design. The proposed method leverages a new paradigm that simultaneously optimizes the design of a set of image channels and trains a machine learning model to execute a user-specified image analysis task. TADRED is a two-stage approach that first trains a scoring network to score individual elements of the densely-sampled design, and then trains a task network to perform the task driving the design. The method is evaluated on six datasets and tasks in both clinically-relevant MRI and remote sensing and physiological applications in hyperspectral imaging, demonstrating substantial improvements over classical experimental design, recent application-specific methods, and state-of-the-art approaches in supervised feature selection.\n\nStrengths:\n\n* The paper presents a novel and innovative approach for experimental design in imaging applications, which is a significant contribution to the field.\n* The proposed method is able to outperform classical experimental design, recent application-specific methods, and state-of-the-art approaches in supervised feature selection on a range of datasets and tasks.\n* The paper provides a thorough evaluation of the proposed method, including comparisons with existing approaches and an ablation study to investigate the importance of different components of the method.\n* The code for the proposed method is made available, allowing for reproducibility and further development of the approach.\n\nWeaknesses:\n\n* The paper assumes a priori model specification, which may not be feasible in all imaging applications.\n* The proposed method requires a large amount of training data, which may not be available in all cases.\n* The paper does not provide a detailed analysis of the computational cost of the proposed method, which may be a limitation for large-scale applications.\n* The proposed method is evaluated on a limited range of datasets and tasks, and it is unclear whether the approach will generalize to other imaging applications.\n\nSuggestions for Improvement:\n\n* Consider developing a method that does not require a priori model specification, such as using a more general machine learning approach.\n* Investigate methods for reducing the amount of training data required, such as using transfer learning or semi-supervised learning.\n* Provide a more detailed analysis of the computational cost of the proposed method, including comparisons with existing approaches.\n* Evaluate the proposed method on a broader range of datasets and tasks, including those that are not clinically-relevant MRI or remote sensing and physiological applications in hyperspectral imaging.\n\nRecommendation:\n\nThe proposed method, TADRED, is a novel and innovative approach for experimental design in imaging applications that outperforms classical experimental design, recent application-specific methods, and state-of-the-art approaches in supervised feature selection. However, there are some limitations and areas for improvement that need to be addressed before the method can be widely adopted. Overall, the paper presents a significant contribution to the field of imaging and machine learning, and it is worth considering for publication in a top-tier conference or journal.",
        "gr_score_with": 0.8641025641025641,
        "gr_score_without": 0.8274111675126904,
        "output_without": {
            "num_tokens_scored": 394,
            "num_green_tokens": 326,
            "green_fraction": 0.8274111675126904,
            "z_score": 1.360533965684311,
            "p_value": 0.08683050601201397
        },
        "output_with": {
            "num_tokens_scored": 390,
            "num_green_tokens": 337,
            "green_fraction": 0.8641025641025641,
            "z_score": 3.1650814590925793,
            "p_value": 0.0007751976950576293
        }
    },
    {
        "title": "DYST: TOWARDS DYNAMIC NEURAL SCENE REPRESENTATIONS ON REAL-WORLD VIDEOS",
        "abstract": "Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, called DyST, which admits separate control over the camera and the content of the scene. The model is trained using a synthetic dataset, DySO, and real-world videos, and it learns to disentangle camera pose and scene dynamics from monocular real-world videos. The authors demonstrate the effectiveness of DyST in view synthesis and scene control capabilities on real-world videos of dynamic scenes.\n\nStrengths:\n\n1. The paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, which is a challenging task.\n2. The authors demonstrate the effectiveness of DyST in view synthesis and scene control capabilities on real-world videos of dynamic scenes.\n3. The paper presents a detailed analysis of the model and its learned latent representations for scene dynamics and camera pose.\n4. The authors propose a novel synthetic dataset, DySO, which is used for co-training DyST.\n\nWeaknesses:\n\n1. The paper assumes that the scene dynamics are known, which may not be the case in real-world scenarios.\n2. The authors use a simple set of camera and object motions in the synthetic dataset, which may not be sufficient to capture the complexity of real-world scenes.\n3. The paper does not provide a detailed analysis of the model's performance on more complex types of videos, such as videos with several independent moving objects or changing lighting conditions.\n4. The authors use a L2 loss for training the model, which may not be suitable for all types of scenes.\n\nSuggestions for Improvement:\n\n1. The authors could explore more complex types of videos and analyze the model's performance on these scenarios.\n2. The paper could benefit from a more detailed analysis of the model's performance on videos with changing lighting conditions.\n3. The authors could explore the use of more advanced loss functions, such as a adversarial loss, to improve the model's performance.\n4. The paper could benefit from a more detailed analysis of the model's learned latent representations and their relationship to the scene dynamics and camera pose.\n\nRecommendation:\n\nThe paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, which is a challenging task. The authors demonstrate the effectiveness of DyST in view synthesis and scene control capabilities on real-world videos of dynamic scenes. While there are some limitations to the paper, it is a significant contribution to the field and has the potential to be improved upon. Therefore, I recommend accepting the paper for publication.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, called DyST, which learns to separate the scene into global content and per-view camera pose and scene dynamics. The model is trained on a synthetic dataset, DySO, and a real-world dataset, Something-Something v2, through a novel co-training scheme that leverages sim-to-real transfer. The authors demonstrate the capabilities of DyST in view synthesis and scene control on real-world videos of dynamic scenes.\n\nStrengths:\n\n* The paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, which is an important and challenging task in computer vision.\n* The authors demonstrate the capabilities of DyST in view synthesis and scene control on real-world videos of dynamic scenes, which is a significant achievement.\n* The paper provides a thorough evaluation of the model, including quantitative and qualitative results, as well as an ablation study to investigate the importance of the latent control swap.\n* The authors provide a detailed description of the model architecture, training procedure, and evaluation metrics, making it easy to follow and reproduce the results.\n\nWeaknesses:\n\n* The paper assumes that the camera pose and scene dynamics are separable, which may not always be the case in real-world scenarios.\n* The authors do not provide a thorough analysis of the limitations of the model, such as its ability to handle complex scenes or long camera trajectories.\n* The paper does not provide a comparison with other state-of-the-art models for generative modeling of dynamic 3D visual scenes.\n* The authors do not provide a discussion on the potential applications of DyST, such as in virtual reality, robotics, or autonomous driving.\n\nSuggestions for Improvement:\n\n* Provide a more thorough analysis of the limitations of the model and discuss potential future work to address these limitations.\n* Compare the performance of DyST with other state-of-the-art models for generative modeling of dynamic 3D visual scenes.\n* Discuss the potential applications of DyST and provide examples of how it can be used in real-world scenarios.\n* Provide a more detailed description of the synthetic dataset, DySO, and how it was generated.\n\nRecommendation:\n\nThe paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, which is an important and challenging task in computer vision. The authors demonstrate the capabilities of DyST in view synthesis and scene control on real-world videos of dynamic scenes, which is a significant achievement. However, the paper assumes that the camera pose and scene dynamics are separable, which may not always be the case in real-world scenarios. Therefore, I recommend that the authors provide a more thorough analysis of the limitations of the model and discuss potential future work to address these limitations.\n\nOverall, the paper is well-written and provides a thorough evaluation of the model. However, it requires further work to address the limitations and provide a more comprehensive analysis of the model's capabilities.\n\nRating: 7/10\n\nRecommendation: Accept",
        "gr_score_with": 0.8723404255319149,
        "gr_score_without": 0.8586206896551725,
        "output_without": {
            "num_tokens_scored": 290,
            "num_green_tokens": 249,
            "green_fraction": 0.8586206896551725,
            "z_score": 2.4959222957595104,
            "p_value": 0.006281505911113371
        },
        "output_with": {
            "num_tokens_scored": 329,
            "num_green_tokens": 287,
            "green_fraction": 0.8723404255319149,
            "z_score": 3.2805856653321843,
            "p_value": 0.0005179590036557874
        }
    },
    {
        "title": "LEARNING MULTI-FACETED PROTOTYPICAL USER INTERESTS",
        "abstract": "We seek to uncover the latent interest units from behavioral data to better learn user preferences under the VAE framework. Existing practices tend to ignore the multiple facets of item characteristics, which may not capture it at appropriate granularity. Moreover, current studies equate the granularity of item space to that of user interests, which we postulate is not ideal as user interests would likely map to a small subset of item space. In addition, the compositionality of user interests has received inadequate attention, preventing the modeling of interactions between explanatory factors driving a user\u2019s decision. To resolve this, we propose to align user interests with multi-faceted item characteristics. First, we involve prototype-based representation learning to discover item characteristics along multiple facets. Second, we compose user interests from uncovered item characteristics via binding mechanism, separating the granularity of user preferences from that of item space. Third, we design a dedicated bi-directional binding block, aiding the derivation of compositional user interests. On real-world datasets, the experimental results demonstrate the strong performance of our proposed method compared to a series of baselines.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method called FACETVAE for learning multi-faceted prototypical user interests. The method aims to address the shortcomings of existing VAE-based disentangled recommendation models, including inadequate item space discovering, same level of granularity between user interests and item space assumption, and improper user interest complexity handling. FACETVAE is characterized by three main innovations: disentangling item space under a multi-faceted manner, binding compositional user interests from low-level ones discovered from item space, and effectively binding user interests via a bi-directional binding block. The authors demonstrate the effectiveness of FACETVAE on three real-world datasets, achieving state-of-the-art performance compared to several baselines.\n\nStrengths:\n\n1. The paper addresses a significant problem in recommendation systems, which is learning multi-faceted user interests.\n2. The proposed method, FACETVAE, is well-motivated and has a clear theoretical foundation.\n3. The authors provide a thorough evaluation of their method on three real-world datasets, demonstrating its effectiveness compared to several baselines.\n4. The paper is well-organized and easy to follow, with clear explanations of the methodology and experimental results.\n\nWeaknesses:\n\n1. The paper assumes that the number of facets and the number of clusters per facet are known, which may not be the case in practice.\n2. The authors do not provide a clear explanation of how to determine the number of facets and the number of clusters per facet in a data-driven manner.\n3. The paper assumes that the user interests are represented as a set of K vectors, which may not be the case in practice.\n4. The authors do not provide a clear explanation of how to handle the case where the user interests are not well-represented by a set of K vectors.\n\nSuggestions for Improvement:\n\n1. Provide a clear explanation of how to determine the number of facets and the number of clusters per facet in a data-driven manner.\n2. Discuss the limitations of the paper, including the assumption that the number of facets and the number of clusters per facet are known.\n3. Provide a clear explanation of how to handle the case where the user interests are not well-represented by a set of K vectors.\n4. Consider adding more experimental results to demonstrate the effectiveness of FACETVAE in different scenarios.\n\nRecommendation:\n\nThe paper is well-written and provides a thorough evaluation of the proposed method, FACETVAE. However, the authors should address the limitations of the paper and provide a clear explanation of how to handle the case where the user interests are not well-represented by a set of K vectors. With these improvements, the paper has the potential to make a significant contribution to the field of recommendation systems.\n\n$$$\nNote: The review is based on the provided paper and is intended to provide constructive feedback to the authors.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method called FACETVAE for learning multi-faceted prototypical user interests. The method aims to resolve the shortcomings of existing VAE-based disentangled recommendation models by disentangling item space under a multi-faceted manner, binding compositional user interests from low-level ones discovered from item space, and effectively binding user interests via a bi-directional binding block. The experimental results demonstrate the strong performance of FACETVAE compared to a series of baselines on three real-world datasets.\n\nStrengths:\n\n1. The paper proposes a novel method that addresses the shortcomings of existing VAE-based disentangled recommendation models.\n2. The method disentangles item space under a multi-faceted manner, which is a unique innovation in the field of recommendation systems.\n3. The bi-directional binding block is a novel component that effectively binds user interests from low-level ones discovered from item space.\n4. The experimental results demonstrate the strong performance of FACETVAE compared to a series of baselines.\n\nWeaknesses:\n\n1. The paper assumes that the number of facets and the number of clusters per facet are known in advance, which may not always be the case in real-world scenarios.\n2. The method requires a large number of hyperparameters to be tuned, which can be time-consuming and may not always lead to the optimal results.\n3. The paper does not provide a clear explanation of how the bi-directional binding block works and how it affects the performance of the model.\n4. The experimental results are limited to three real-world datasets, and it would be beneficial to conduct more experiments on other datasets to demonstrate the robustness of the method.\n\nSuggestions for Improvement:\n\n1. Investigate methods for automatically discovering the number of facets and the number of clusters per facet in a data-driven manner.\n2. Provide a clear explanation of how the bi-directional binding block works and how it affects the performance of the model.\n3. Conduct more experiments on other datasets to demonstrate the robustness of the method.\n4. Investigate methods for reducing the number of hyperparameters that need to be tuned.\n\nRecommendation:\n\nThe paper proposes a novel method that addresses the shortcomings of existing VAE-based disentangled recommendation models. The experimental results demonstrate the strong performance of FACETVAE compared to a series of baselines. However, there are some limitations and suggestions for improvement that need to be addressed. Overall, the paper is well-written and contributes to the field of recommendation systems. I recommend it for publication in a reputable conference or journal.\n\n$$$",
        "gr_score_with": 0.8443708609271523,
        "gr_score_without": 0.7896253602305475,
        "output_without": {
            "num_tokens_scored": 347,
            "num_green_tokens": 274,
            "green_fraction": 0.7896253602305475,
            "z_score": -0.4828491868434438,
            "p_value": 0.6853985883532724
        },
        "output_with": {
            "num_tokens_scored": 302,
            "num_green_tokens": 255,
            "green_fraction": 0.8443708609271523,
            "z_score": 1.927956824907589,
            "p_value": 0.026930248340548354
        }
    },
    {
        "title": "WIN-WIN: TRAINING HIGH-RESOLUTION VISION TRANSFORMERS",
        "abstract": "Transformers have become the standard in state-of-the-art vision architectures, achieving impressive performance on both image-level and dense pixelwise tasks. However, training vision transformers for high-resolution pixelwise tasks has a prohibitive cost. Typical solutions boil down to hierarchical architectures, fast and approximate attention, or training on low-resolution crops. This latter solution does not constrain architectural choices, but it leads to a clear performance drop when testing at resolutions significantly higher than that used for training, thus requiring ad-hoc and slow post-processing schemes. In this paper, we propose a novel strategy for efficient training and inference of high-resolution vision transformers: the key principle is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. We show that this strategy is effective when using relative positional embedding such as rotary embeddings. It is 4 times faster to train than a full-resolution network, and it is straightforward to use at test time compared to existing approaches. We apply this strategy to two dense prediction tasks with high resolution data. First, we show on the task of semantic segmentation that a simple setting with 2 windows performs best, hence the name of our method: Win-Win. To demonstrate the generality of our contribution, we further extend it to the binocular task of optical flow, reaching state-of-the-art performance on the Spring benchmark that contains Full-HD images with an inference time an order of magnitude faster than the best competitor. 0 10 20 30 40 50 60 Training time (hours) 45 50 55 60 m ea n Io U Semantic segmentation on BDD100k Win-Win (31GB) Full-res. (58GB) Full-res ViTDet (32GB) 0 25 50 75 100 125 150 175 Training time (hours) 1.6 2.0 2.4 2.8 3.2 En dPo in t E rro r ( EP E) Optical flow estimation on MPI-Sintel-clean Win-Win (14GB) Full-res. (28GB) Full-res ViT-Det (21GB) 80 100 120 140 160 180 Training time (hours) 0.0 0.5 1.0 1.5 In fe re nc e tim e (s ec on ds ) Optical flow estimation on MPI-Sintel-clean Full-res ViT Full-res ViT-Det Win-Win ViT + Tiling Figure 1: Validation performance vs. training time on semantic segmentation (left) and optical flow (middle). We compare our two-window training (Win-Win) to a standard full-resolution training as well as a sparsification of the attention following ViT-Det (Li et al., 2022a). We indicate the memory usage in parenthesis in the legend. Compared to full-resolution training, Win-Win allows to reduce the training time by a factor 3-4 and to half the memory usage while reaching a similar performance. Training and inference times on optical flow, for Win-Win vs. other strategies (right). ViT+Tiling corresponds to a setup similar to CroCo-Flow (Weinzaepfel et al., 2023) where the model is trained on random crops, but requires a tiling strategy at inference. While Win-Win is as fast to train as the latter, it can directly process full-resolution inputs at test time.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel strategy for efficient training and inference of high-resolution vision transformers, called Win-Win. The key principle of Win-Win is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. The authors show that this strategy is effective when using relative positional embedding such as rotary embeddings, and is 4 times faster to train than a full-resolution network.\n\nStrengths:\n\n* The paper proposes a novel and efficient strategy for training high-resolution vision transformers.\n* The Win-Win approach allows the model to learn local and global interactions, which is key for dense prediction tasks.\n* The paper shows that the Win-Win approach can achieve state-of-the-art performance on two dense prediction tasks: semantic segmentation and optical flow estimation.\n* The authors provide a thorough experimental evaluation of the Win-Win approach, including comparisons to other baselines and ablation studies.\n\nWeaknesses:\n\n* The paper assumes that the input images are already preprocessed, which may not be the case in real-world scenarios.\n* The authors do not provide a detailed analysis of the computational cost of the Win-Win approach, which may be a concern for large-scale applications.\n* The paper does not provide a clear explanation of why the Win-Win approach works, which may make it difficult for readers to understand and replicate the results.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the computational cost of the Win-Win approach, including a discussion of the trade-offs between training speed and inference speed.\n* Provide a more thorough evaluation of the Win-Win approach on other tasks and datasets, to demonstrate its generality and robustness.\n* Provide a more detailed explanation of why the Win-Win approach works, including a discussion of the theoretical foundations and the empirical evidence.\n\nRecommendation:\n\nThe paper proposes a novel and efficient strategy for training high-resolution vision transformers, and provides a thorough experimental evaluation of the approach. While there are some limitations and weaknesses, the paper is well-written and well-organized, and the results are impressive. I recommend this paper for publication in a top-tier conference.\n\nGrade: B+\n\nReviewer's Comments:\n\nThe paper is well-written and well-organized, and the results are impressive. However, there are some limitations and weaknesses that need to be addressed. The paper assumes that the input images are already preprocessed, which may not be the case in real-world scenarios. The authors do not provide a detailed analysis of the computational cost of the Win-Win approach, which may be a concern for large-scale applications. The paper does not provide a clear explanation of why the Win-Win approach works, which may make it difficult for readers to understand and replicate the results.\n\nOverall, I believe that the paper has the potential to make a significant contribution to the field, and I recommend it for publication in a top-tier conference. However, the authors need to address the limitations and weaknesses mentioned above to make the paper more robust and convincing.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel strategy, called Win-Win, for efficient training and inference of high-resolution vision transformers. The key principle of Win-Win is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. The authors demonstrate the effectiveness of Win-Win on two dense prediction tasks: semantic segmentation and optical flow estimation.\n\nStrengths:\n1. The paper proposes a novel and effective approach for efficient training and inference of high-resolution vision transformers.\n2. The Win-Win strategy is easy to implement and can be used with any vision transformer architecture.\n3. The paper provides a thorough experimental evaluation of Win-Win on two dense prediction tasks, including semantic segmentation and optical flow estimation.\n4. The results show that Win-Win achieves state-of-the-art performance on the Spring benchmark and outperforms other methods on the MPI-Sintel benchmark.\n\nWeaknesses:\n1. The paper assumes that the input image is divided into non-overlapping windows, which may not be the case in practice.\n2. The Win-Win strategy may not work well for tasks that require large receptive fields or global interactions.\n3. The paper does not provide a thorough analysis of the computational cost of Win-Win compared to other methods.\n4. The paper assumes that the training data is available at full resolution, which may not be the case in practice.\n\nSuggestions for Improvement:\n1. Investigate the effect of overlapping windows on the performance of Win-Win.\n2. Analyze the computational cost of Win-Win compared to other methods and provide a detailed comparison.\n3. Evaluate the performance of Win-Win on tasks that require large receptive fields or global interactions.\n4. Investigate the effect of different window sampling strategies on the performance of Win-Win.\n\nRecommendation:\nThe paper proposes a novel and effective approach for efficient training and inference of high-resolution vision transformers. The Win-Win strategy is easy to implement and can be used with any vision transformer architecture. The paper provides a thorough experimental evaluation of Win-Win on two dense prediction tasks, including semantic segmentation and optical flow estimation. However, the paper assumes that the input image is divided into non-overlapping windows, which may not be the case in practice. Therefore, I recommend that the authors investigate the effect of overlapping windows on the performance of Win-Win and provide a detailed comparison of the computational cost of Win-Win compared to other methods.\n\nOverall, I believe that the paper makes a significant contribution to the field of computer vision and deserves to be published in a top conference.\n\nRecommendation: Accept.",
        "gr_score_with": 0.8789808917197452,
        "gr_score_without": 0.837696335078534,
        "output_without": {
            "num_tokens_scored": 382,
            "num_green_tokens": 320,
            "green_fraction": 0.837696335078534,
            "z_score": 1.8422034697269045,
            "p_value": 0.03272269655539201
        },
        "output_with": {
            "num_tokens_scored": 314,
            "num_green_tokens": 276,
            "green_fraction": 0.8789808917197452,
            "z_score": 3.4990978180131065,
            "p_value": 0.00023341764187875927
        }
    },
    {
        "title": "FAKE IT TILL MAKE IT: FEDERATED LEARNING WITH CONSENSUS-ORIENTED GENERATION",
        "abstract": "In federated learning (FL), data heterogeneity is one key bottleneck that causes model divergence and limits performance. Addressing this, existing methods often regard data heterogeneity as an inherent property and propose to mitigate its adverse effects by correcting models. In this paper, we seek to break this inherent property by generating data to complement the original dataset to fundamentally mitigate heterogeneity level. As a novel attempt from the perspective of data, we propose federated learning with consensus-oriented generation (FedCOG). FedCOG consists of two key components at the client side: complementary data generation, which generates data extracted from the shared global model to complement the original dataset, and knowledge-distillation-based model training, which distills knowledge from global model to local model based on the generated data to mitigate over-fitting the original heterogeneous dataset. FedCOG has two critical advantages: 1) it can be a plug-and-play module to further improve the performance of most existing FL methods, and 2) it is naturally compatible with standard FL protocols such as Secure Aggregation since it makes no modification in communication process. Extensive experiments on classical and real-world FL datasets show that FedCOG consistently outperforms state-of-the-art methods .",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel federated learning algorithm, called FedCOG, to tackle the issue of data heterogeneity in FL. FedCOG consists of two key components: complementary data generation and knowledge-distillation-based model training. The complementary data generation module generates data from the shared global model to complement the original dataset, reducing the heterogeneity level. The knowledge-distillation-based model training module distills knowledge from the global model to the local model based on the generated data, mitigating the effects of heterogeneity. FedCOG is plug-and-play, compatible with standard FL protocols, and does not compromise on communication cost and privacy. The authors conduct extensive experiments on classical and real-world FL datasets, showing that FedCOG consistently outperforms state-of-the-art methods.\n\nStrengths:\n\n1. The paper addresses a critical issue in FL, data heterogeneity, from a novel perspective of modifying local dataset.\n2. FedCOG is a well-designed algorithm that consists of two key components, complementary data generation and knowledge-distillation-based model training, which are both orthogonal to most existing FL methods.\n3. The authors provide extensive experimental results on classical and real-world FL datasets, demonstrating the effectiveness of FedCOG.\n4. FedCOG is plug-and-play, compatible with standard FL protocols, and does not compromise on communication cost and privacy.\n\nWeaknesses:\n\n1. The paper does not provide a thorough analysis of the theoretical guarantees of FedCOG.\n2. The authors do not provide a detailed comparison with other state-of-the-art methods in terms of computational cost and scalability.\n3. The paper assumes a centralized setting, where the global model is shared among all clients, which may not be realistic in many FL scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the theoretical guarantees of FedCOG, including convergence rates and optimality guarantees.\n2. The authors should conduct a more thorough comparison with other state-of-the-art methods in terms of computational cost and scalability.\n3. The authors should consider a decentralized setting, where the global model is not shared among all clients, and evaluate the performance of FedCOG in such a setting.\n\nRecommendation:\n\nThe paper is well-written, and the proposed algorithm, FedCOG, is innovative and effective. However, the paper lacks a thorough theoretical analysis and a detailed comparison with other state-of-the-art methods. With some improvements, the paper can be considered for publication in a top-tier conference.",
        "peer_review_with_watermark": "$$\nPaper Summary:\n\nThe paper proposes a novel federated learning algorithm, called FedCOG, to tackle the issue of data heterogeneity in FL from the perspective of modifying local dataset. FedCOG consists of two key components: complementary data generation to reduce heterogeneity level and knowledge-distillation-based model training to mitigate the effects of heterogeneity. The algorithm generates data to complement the original dataset for each client, which is achieved by inversely optimizing inputs given the global and local model. The generated dataset is then utilized for the process of local model training, with the optimization objective defined as minimizing the task-driven loss on the original dataset and knowledge distillation loss on the generated dataset. The paper conducts extensive experiments on classical and real-world FL datasets, showing that FedCOG consistently outperforms state-of-the-art methods.\n\nStrengths:\n\n* The paper proposes a novel approach to tackle data heterogeneity in FL from the perspective of modifying local dataset.\n* The algorithm FedCOG is plug-and-play in most existing FL methods and is compatible with standard FL protocol such as Secure Aggregation.\n* The paper conducts extensive experiments on classical and real-world FL datasets, showing that FedCOG consistently outperforms state-of-the-art methods.\n* The paper provides a thorough analysis of the algorithm, including the effects of disagreement loss and knowledge distillation loss on the performance of FedCOG.\n\nWeaknesses:\n\n* The paper does not provide a thorough comparison with existing methods that address data heterogeneity in FL, such as SCAFFOLD and FedDyn.\n* The paper does not provide a detailed analysis of the computational cost of FedCOG, which is an important aspect of FL.\n* The paper assumes that the global model is known to all clients, which may not be the case in practice.\n* The paper does not provide a detailed analysis of the privacy implications of FedCOG.\n\nSuggestions for Improvement:\n\n* Provide a thorough comparison with existing methods that address data heterogeneity in FL.\n* Provide a detailed analysis of the computational cost of FedCOG.\n* Consider the scenario where the global model is not known to all clients.\n* Provide a detailed analysis of the privacy implications of FedCOG.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to tackle data heterogeneity in FL from the perspective of modifying local dataset. The algorithm FedCOG is plug-and-play in most existing FL methods and is compatible with standard FL protocol such as Secure Aggregation. The paper conducts extensive experiments on classical and real-world FL datasets, showing that FedCOG consistently outperforms state-of-the-art methods. However, the paper has some limitations, such as not providing a thorough comparison with existing methods and not providing a detailed analysis of the computational cost and privacy implications of FedCOG. Therefore, I recommend this paper for publication after addressing these limitations.\n\n$$",
        "gr_score_with": 0.8366013071895425,
        "gr_score_without": 0.7446153846153846,
        "output_without": {
            "num_tokens_scored": 325,
            "num_green_tokens": 242,
            "green_fraction": 0.7446153846153846,
            "z_score": -2.4958405722849752,
            "p_value": 0.9937170468020037
        },
        "output_with": {
            "num_tokens_scored": 306,
            "num_green_tokens": 256,
            "green_fraction": 0.8366013071895425,
            "z_score": 1.600907518755053,
            "p_value": 0.05469870202286078
        }
    },
    {
        "title": "GIM: LEARNING GENERALIZABLE IMAGE MATCHER",
        "abstract": "Image matching is a fundamental computer vision problem. While learning-based methods achieve state-of-the-art performance on existing benchmarks, they generalize poorly to in-the-wild images. Such methods typically need to train separate models for different scene types (e.g., indoor vs. outdoor) and are impractical when the scene type is unknown in advance. One of the underlying problems is the limited scalability of existing data construction pipelines, which limits the diversity of standard image matching datasets. To address this problem, we propose GIM, a self-training framework for learning a single generalizable model based on any image matching architecture using internet videos, an abundant and diverse data source. Given an architecture, GIM first trains it on standard domain-specific datasets and then combines it with complementary matching methods to create dense labels on nearby frames of novel videos. These labels are filtered by robust fitting, and then enhanced by propagating them to distant frames. The final model is trained on propagated data with strong augmentations. Not relying on complex 3D reconstruction makes GIM much more efficient and less likely to fail than standard SfM-and-MVS based frameworks. We also propose ZEB, the first zero-shot evaluation benchmark for image matching. By mixing data from diverse domains, ZEB can thoroughly assess the cross-domain generalization performance of different methods. Experiments demonstrate the effectiveness and generality of GIM. Applying GIM consistently improves the zero-shot performance of 3 state-of-theart image matching architectures as the number of downloaded videos increases (Fig. 1 (a)); with 50 hours of YouTube videos, the relative zero-shot performance improves by 8.4% \u2212 18.1%. GIM also enables generalization to extreme crossdomain data such as Bird Eye View (BEV) images of projected 3D point clouds (Fig. 1 (c)). More importantly, our single zero-shot model consistently outperforms domain-specific baselines when evaluated on downstream tasks inherent to their respective domains. The code will be released upon acceptance.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel framework, GIM, for learning generalizable image matching models using self-training on diverse internet videos. GIM first trains a model on standard domain-specific datasets and then uses the trained model to generate dense labels on nearby frames of downloaded videos. The final model is trained on propagated data with strong augmentations. The authors also introduce a new zero-shot evaluation benchmark, ZEB, consisting of data from 8 real-world and 4 simulated domains. Experiments demonstrate the effectiveness and generality of GIM for both image matching and various downstream tasks.\n\nStrengths:\n1. The paper proposes a novel and effective approach for learning generalizable image matching models using self-training on diverse internet videos.\n2. The authors introduce a new zero-shot evaluation benchmark, ZEB, which allows for thorough evaluation of an image matching model in in-the-wild environments.\n3. The paper demonstrates the effectiveness of GIM on various state-of-the-art architectures and downstream tasks.\n4. The authors provide a detailed analysis of the GIM framework and its components, including the self-training process, label propagation, and strong data augmentation.\n\nWeaknesses:\n1. The paper assumes that internet videos are a suitable source of diverse and large-scale training data, which may not be the case for all scenarios.\n2. The authors do not provide a thorough analysis of the limitations and potential failures of the GIM framework.\n3. The paper does not discuss the potential issues with data quality and consistency across different internet videos.\n4. The authors do not provide a clear comparison of the computational efficiency of GIM with other existing methods.\n\nSuggestions for Improvement:\n1. The authors should provide a more thorough analysis of the limitations and potential failures of the GIM framework.\n2. The paper should discuss the potential issues with data quality and consistency across different internet videos.\n3. The authors should provide a clear comparison of the computational efficiency of GIM with other existing methods.\n4. The paper should provide more details on the implementation of the GIM framework, including the specific architecture and hyperparameters used.\n\nRecommendation:\nThe paper proposes a novel and effective approach for learning generalizable image matching models using self-training on diverse internet videos. The authors introduce a new zero-shot evaluation benchmark, ZEB, which allows for thorough evaluation of an image matching model in in-the-wild environments. The paper demonstrates the effectiveness of GIM on various state-of-the-art architectures and downstream tasks. However, the paper assumes that internet videos are a suitable source of diverse and large-scale training data, which may not be the case for all scenarios. The authors should provide a more thorough analysis of the limitations and potential failures of the GIM framework. Despite these limitations, the paper is well-written and provides a clear contribution to the field of image matching.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes GIM, a self-training framework that learns generalizable image matching models using abundant and diverse internet videos. GIM first trains a base model on standard domain-specific datasets and then uses it to generate dense labels on nearby frames of downloaded videos through multi-method matching and label propagation. Strong data augmentations are applied to enhance the supervision signals. GIM also proposes ZEB, a novel zero-shot evaluation benchmark for image matching with diverse in-the-wild data, to thoroughly evaluate the generalization performance of different methods.\n\nStrengths:\n1. GIM proposes an innovative approach to learn generalizable image matching models using self-training and multi-method matching, which can efficiently generate reliable supervision signals on diverse internet videos.\n2. The paper presents a thorough evaluation of GIM's performance on both zero-shot and in-domain tasks, demonstrating its effectiveness and generality.\n3. ZEB, the proposed zero-shot evaluation benchmark, is a valuable contribution to the field of image matching, allowing for thorough evaluation of image matching models in in-the-wild environments.\n\nWeaknesses:\n1. The paper relies heavily on self-training, which may not always be feasible or desirable in certain applications.\n2. The proposed ZEB benchmark is limited to 12 real-world and 4 simulated datasets, which may not capture all possible in-the-wild scenarios.\n3. The paper does not provide a clear explanation of why strong data augmentations are necessary for GIM to work effectively.\n\nSuggestions for Improvement:\n1. Provide more details on why self-training is necessary for GIM and how it can be adapted to different applications.\n2. Consider expanding ZEB to include more real-world and simulated datasets to better capture in-the-wild scenarios.\n3. Provide more insight into why strong data augmentations are necessary for GIM to work effectively.\n\nRecommendation:\nThis paper presents an innovative approach to learning generalizable image matching models using self-training and multi-method matching. The proposed ZEB benchmark is a valuable contribution to the field of image matching, allowing for thorough evaluation of image matching models in in-the-wild environments. However, there are some limitations and areas for improvement that need to be addressed. With some modifications and further research, GIM and ZEB have the potential to become powerful tools for image matching and computer vision tasks.\n\nRating: 7.5/10\n$$$",
        "gr_score_with": 0.8096676737160121,
        "gr_score_without": 0.7175324675324676,
        "output_without": {
            "num_tokens_scored": 308,
            "num_green_tokens": 221,
            "green_fraction": 0.7175324675324676,
            "z_score": -3.6179323199659623,
            "p_value": 0.9998515169882269
        },
        "output_with": {
            "num_tokens_scored": 331,
            "num_green_tokens": 268,
            "green_fraction": 0.8096676737160121,
            "z_score": 0.4399983266387732,
            "p_value": 0.3299691596432879
        }
    },
    {
        "title": "VERA: VECTOR-BASED RANDOM MATRIX ADAPTATION",
        "abstract": "Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous peruser or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA)1, which significantly reduces the number of trainable parameters compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, image classification tasks, and show its application in instruction-tuning of 7B and 13B language models.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents Vector-based Random Matrix Adaptation (VeRA), a novel parameter-efficient finetuning method for large language models. VeRA reduces the number of trainable parameters compared to the state-of-the-art LoRA method, while maintaining comparable results. The method employs a single pair of frozen random matrices shared across all layers and learns small scaling vectors instead. VeRA achieves significant memory savings, making it suitable for scenarios that require frequent swapping of numerous finetuned models, such as cloud-based AI services personalized for individual users.\n\nStrengths:\n\n1. VeRA significantly reduces the number of trainable parameters compared to LoRA, making it more memory-efficient.\n2. The method achieves comparable results to LoRA on various benchmarks, including GLUE, E2E, and image classification tasks.\n3. VeRA is well-suited for scenarios that require frequent swapping of numerous finetuned models, such as cloud-based AI services personalized for individual users.\n4. The paper provides a thorough analysis of the method's components, including the impact of different initializations and the magnitude of adaptation.\n\nWeaknesses:\n\n1. The paper assumes that the frozen random matrices can be regenerated from a random number generator (RNG) seed, which may not be feasible in all scenarios.\n2. The method's performance may benefit from additional refinements, such as dynamic parameter budget allocation, or different initialization and regularization techniques.\n3. The paper focuses on language and vision models with Transformer architecture, and its applicability to other architectures and domains remains an area for future research.\n\nSuggestions for Improvement:\n\n1. Investigate the feasibility of regenerating frozen random matrices from an RNG seed in various scenarios.\n2. Explore additional refinements, such as dynamic parameter budget allocation, or different initialization and regularization techniques, to improve the method's performance.\n3. Conduct experiments on other architectures and domains to evaluate VeRA's applicability and performance.\n\nRecommendation:\n\nVeRA is a promising parameter-efficient finetuning method that achieves comparable results to LoRA while reducing the number of trainable parameters. The method's memory savings make it suitable for scenarios that require frequent swapping of numerous finetuned models. However, further research is needed to explore the method's limitations and potential areas for improvement. I recommend accepting this paper for publication in a reputable conference, with the suggestion that the authors address the limitations and areas for improvement mentioned above.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents Vector-based Random Matrix Adaptation (VeRA), a novel parameter-efficient finetuning method for large language models. VeRA reduces the number of trainable parameters compared to the state-of-the-art LoRA method while maintaining comparable results. The authors demonstrate the effectiveness of VeRA on various downstream tasks, including natural language understanding and generation, instruction-tuning, and image classification.\n\nStrengths:\n\n1. VeRA achieves a significant reduction in trainable parameters compared to LoRA, making it more efficient for large language models.\n2. The authors demonstrate the effectiveness of VeRA on various downstream tasks, including GLUE, E2E, instruction-tuning, and image classification.\n3. The method is particularly well-suited for scenarios that require frequent swapping of numerous finetuned models, such as cloud-based AI services personalized for individual users.\n4. The authors provide a thorough analysis of the method's performance and trade-offs.\n\nWeaknesses:\n\n1. The paper focuses primarily on language models and Transformers architecture, and it is not clear how VeRA would perform on other architectures or domains.\n2. The authors do not provide a detailed analysis of the computational costs and memory requirements of VeRA compared to LoRA.\n3. The paper relies heavily on LoRA as a baseline, and it would be beneficial to compare VeRA with other parameter-efficient finetuning methods.\n4. The authors do not provide a clear explanation of why the d scaling vector is more expressive than the b vector.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational costs and memory requirements of VeRA compared to LoRA.\n2. The authors should compare VeRA with other parameter-efficient finetuning methods to demonstrate its competitiveness.\n3. The authors should provide a clearer explanation of why the d scaling vector is more expressive than the b vector.\n4. The authors should investigate the performance of VeRA on other architectures and domains.\n\nRecommendation:\n\nOverall, the paper presents a novel and efficient finetuning method for large language models. The authors demonstrate the effectiveness of VeRA on various downstream tasks and provide a thorough analysis of the method's performance and trade-offs. However, the paper has some limitations and areas for improvement. With further investigation and refinement, VeRA has the potential to become a widely adopted finetuning method for large language models.\n\nRecommendation: Accept.\n\n$$$",
        "gr_score_with": 0.8327759197324415,
        "gr_score_without": 0.7982954545454546,
        "output_without": {
            "num_tokens_scored": 352,
            "num_green_tokens": 281,
            "green_fraction": 0.7982954545454546,
            "z_score": -0.07965677118064404,
            "p_value": 0.5317448791256855
        },
        "output_with": {
            "num_tokens_scored": 299,
            "num_green_tokens": 249,
            "green_fraction": 0.8327759197324415,
            "z_score": 1.4171246520273824,
            "p_value": 0.07822324310189008
        }
    },
    {
        "title": "FANTASTIC GENERALIZATION MEASURES",
        "abstract": "We study the notion of a generalization bound being uniformly tight, meaning that the difference between the bound and the population loss is small for all learning algorithms and all population distributions. Numerous generalization bounds have been proposed in the literature as potential explanations for the ability of neural networks to generalize in the overparameterized setting. However, in their paper \u201cFantastic Generalization Measures and Where to Find Them,\u201d Jiang et al. (2020) examine more than a dozen generalization bounds, and show empirically that none of them are uniformly tight. This raises the question of whether uniformly-tight generalization bounds are at all possible in the overparameterized setting. We consider two types of generalization bounds: (1) bounds that may depend on the training set and the learned hypothesis (e.g., margin bounds). We prove mathematically that no such bound can be uniformly tight in the overparameterized setting; (2) bounds that may in addition also depend on the learning algorithm (e.g., stability bounds). For these bounds, we show a trade-off between the algorithm\u2019s performance and the bound\u2019s tightness. Namely, if the algorithm achieves good accuracy on certain distributions, then no generalization bound can be uniformly tight for it in the overparameterized setting. We explain how these formal results can, in our view, inform research on generalization bounds for neural networks, while stressing that other interpretations of these results are also possible.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper studies the notion of a generalization bound being uniformly tight, meaning that the difference between the bound and the population loss is small for all learning algorithms and all population distributions. The authors examine two types of generalization bounds: (1) bounds that may depend on the training set and the learned hypothesis, and (2) bounds that may also depend on the learning algorithm. They prove mathematically that no generalization bound of the first type can be uniformly tight in the overparameterized setting, and establish a trade-off between learning performance and estimability for the second type of bounds.\n\nStrengths:\n\n* The paper provides a rigorous and formal analysis of the notion of uniformly tight generalization bounds.\n* The authors introduce a framework for understanding the limitations of generalization bounds in the overparameterized setting.\n* The paper presents several theorems and lemmas that provide a deeper understanding of the relationships between learning performance, estimability, and generalization bounds.\n\nWeaknesses:\n\n* The paper assumes a high level of mathematical background, which may make it challenging for readers without a strong background in machine learning and theoretical computer science.\n* The authors do not provide a comprehensive review of existing generalization bounds and their limitations, which may make it difficult for readers to understand the significance of their results.\n* The paper's conclusions and implications may be subject to interpretation, and the authors acknowledge that different scholarly positions exist on this matter.\n\nSuggestions for Improvement:\n\n* The authors could provide a more accessible introduction to the mathematical framework and notation used in the paper.\n* The paper could benefit from a more comprehensive review of existing generalization bounds and their limitations, which would provide context for the authors' results.\n* The authors could provide more concrete examples and illustrations to demonstrate the implications of their results and make the paper more engaging for readers.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the field of machine learning and theoretical computer science by shedding light on the limitations of generalization bounds in the overparameterized setting. The authors' formal analysis and rigorous proofs provide a solid foundation for understanding the relationships between learning performance, estimability, and generalization bounds. However, the paper's assumptions and conclusions may be subject to interpretation, and the authors could benefit from providing a more comprehensive review of existing generalization bounds and their limitations. Overall, the paper is well-written and well-structured, and it is a valuable contribution to the field.\n\nRecommendation for Publication: Yes\n\nRecommendation for Future Research:\n\n* Investigate the implications of the authors' results for existing generalization bounds and their limitations.\n* Explore the relationships between learning performance, estimability, and generalization bounds in more detail.\n* Develop new generalization bounds that take into account the limitations and trade-offs identified by the authors.\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper studies the concept of generalization bounds in the context of neural networks, specifically in the overparameterized setting. The authors investigate whether it is possible to obtain generalization bounds that are uniformly tight, meaning that they are tight for all learning algorithms and population distributions. The paper provides two main results: (1) a negative result showing that algorithm-independent generalization bounds cannot be uniformly tight in the overparameterized setting, and (2) a trade-off between learnability and estimability, which implies that algorithm-dependent generalization bounds are also limited. The authors discuss the implications of these results and suggest two options for obtaining uniformly tight generalization bounds: (1) specifying a set of 'nice' or 'natural' population distributions, or (2) making explicit assumptions about the learning algorithm.\n\nStrengths:\n\n1. The paper provides a clear and concise introduction to the problem of generalization bounds in the context of neural networks.\n2. The authors provide a rigorous mathematical analysis of the problem, using tools from learning theory and statistics.\n3. The paper presents two main results, which are well-motivated and have significant implications for the field of neural networks.\n4. The authors discuss the implications of their results and suggest two options for obtaining uniformly tight generalization bounds.\n\nWeaknesses:\n\n1. The paper assumes a high level of background knowledge in learning theory and statistics, which may make it difficult for non-experts to follow.\n2. The paper focuses primarily on the mathematical analysis of the problem, and may not provide sufficient empirical evidence to support the claims made.\n3. The authors' suggestion for obtaining uniformly tight generalization bounds (specifying a set of 'nice' or 'natural' population distributions or making explicit assumptions about the learning algorithm) may be seen as restrictive or limiting.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed introduction to the problem of generalization bounds, including a clear explanation of the concepts and terminology used.\n2. Consider providing more empirical evidence to support the claims made, such as experimental results or case studies.\n3. Provide more detail on the assumptions underlying the authors' suggestion for obtaining uniformly tight generalization bounds, and discuss potential limitations or drawbacks of these assumptions.\n\nRecommendation:\n\nThe paper provides a significant contribution to the field of neural networks, and the results presented are well-motivated and have significant implications. However, the paper assumes a high level of background knowledge and focuses primarily on the mathematical analysis of the problem. To make the paper more accessible and impactful, the authors should consider providing more empirical evidence and detail on the assumptions underlying their suggestion for obtaining uniformly tight generalization bounds. With these improvements, the paper has the potential to make a significant contribution to the field of neural networks and promote further research in this area.",
        "gr_score_with": 0.8579545454545454,
        "gr_score_without": 0.8093922651933702,
        "output_without": {
            "num_tokens_scored": 362,
            "num_green_tokens": 293,
            "green_fraction": 0.8093922651933702,
            "z_score": 0.44704154583174127,
            "p_value": 0.3274225315267928
        },
        "output_with": {
            "num_tokens_scored": 352,
            "num_green_tokens": 302,
            "green_fraction": 0.8579545454545454,
            "z_score": 2.718569905251996,
            "p_value": 0.0032782399878469952
        }
    },
    {
        "title": "STEVE-EYE: EQUIPPING LLM-BASED EMBOD-",
        "abstract": "Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to \u201ca blindfolded text-based game.\u201d Consequently, LLMbased agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model to address this limitation. Steve-Eye integrates the LLM with a visual encoder to process visual-text inputs and generate multimodal feedback. We adopt a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, enabling our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks, then carry out experiments from a wide range of perspectives to validate our model\u2019s capability to strategically act and plan. Codes and datasets will be released.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Steve-Eye, a large multimodal model that enables large language model (LLM)-based embodied agents to interact with the open world via visual-text interfaces. Steve-Eye is equipped with three essential functions: multimodal perception, foundational knowledge base, and skill prediction and planning. The authors construct an extensive dataset of 850K open-world instruction pairs to train Steve-Eye and develop three open-world evaluation benchmarks to validate the model's capability to strategically act and plan.\n\nStrengths:\n\n1. The paper presents a novel approach to multimodal interaction in open-world environments, addressing the limitations of text-only agents.\n2. The proposed model, Steve-Eye, demonstrates improved performance in multimodal perception, foundational knowledge question answering, and skill prediction and planning.\n3. The authors provide a comprehensive evaluation of the model's performance on three open-world benchmarks, including environmental visual captioning, foundational knowledge question answering, and skill prediction and planning.\n\nWeaknesses:\n\n1. The paper relies heavily on the use of pre-trained LLMs and visual encoders, which may limit the model's ability to generalize to new tasks and environments.\n2. The dataset collection process is labor-intensive and requires significant human annotation effort, which may not be scalable for larger datasets.\n3. The paper does not provide a thorough analysis of the model's limitations and failure cases, which may be essential for understanding its robustness and reliability.\n\nSuggestions for Improvement:\n\n1. Investigate the use of more robust and generalizable pre-trained models that can handle a wide range of tasks and environments.\n2. Explore more efficient and scalable methods for dataset collection, such as active learning or transfer learning.\n3. Conduct a more comprehensive analysis of the model's limitations and failure cases to better understand its robustness and reliability.\n\nRecommendation:\n\nThis paper presents a novel and promising approach to multimodal interaction in open-world environments. While there are some limitations and areas for improvement, the proposed model demonstrates improved performance on three open-world benchmarks. I recommend this paper for publication in a reputable conference, with the understanding that the authors will address the limitations and suggestions for improvement outlined above.\n\nRating: 7.5/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Steve-Eye, a large multimodal model that combines a pre-trained language model with a visual encoder to enable embodied agents to interact with the open world via visual-text interfaces. Steve-Eye is trained on an extensive dataset of 850K instruction pairs and evaluated on three open-world benchmarks: environmental visual captioning, foundational knowledge question answering, and skill prediction and planning.\n\nStrengths:\n\n1. The paper proposes a novel approach to enable embodied agents to interact with the open world by integrating a pre-trained language model with a visual encoder.\n2. The proposed Steve-Eye model is trained on a large dataset of 850K instruction pairs, which is a significant improvement over previous works.\n3. The paper evaluates Steve-Eye on three open-world benchmarks, providing a comprehensive assessment of its capabilities.\n4. The authors provide a detailed description of the training process and the architecture of the proposed model.\n\nWeaknesses:\n\n1. The paper assumes that the pre-trained language model is already available, which may not be the case for all researchers.\n2. The paper does not provide a clear explanation of how the visual encoder is trained and fine-tuned.\n3. The paper evaluates Steve-Eye on a specific dataset and benchmarks, which may not be representative of all open-world scenarios.\n4. The paper does not provide a detailed comparison with other state-of-the-art approaches.\n\nSuggestions for Improvement:\n\n1. Provide a clear explanation of the training process for the visual encoder and how it is fine-tuned.\n2. Evaluate Steve-Eye on a more diverse set of benchmarks and datasets to ensure its generalizability.\n3. Compare Steve-Eye with other state-of-the-art approaches to provide a more comprehensive assessment of its capabilities.\n4. Provide more details on the implementation of the proposed model, including the hyperparameters and training settings.\n\nRecommendation:\n\nThe paper proposes a novel approach to enable embodied agents to interact with the open world by integrating a pre-trained language model with a visual encoder. While there are some limitations and areas for improvement, the paper provides a comprehensive assessment of the proposed model and its capabilities. I recommend this paper for publication, but suggest that the authors address the mentioned weaknesses and provide more details on the implementation of the proposed model.\n\nRating: 7/10\n\n$$$",
        "gr_score_with": 0.8576271186440678,
        "gr_score_without": 0.7987987987987988,
        "output_without": {
            "num_tokens_scored": 333,
            "num_green_tokens": 266,
            "green_fraction": 0.7987987987987988,
            "z_score": -0.05451446450636297,
            "p_value": 0.5217373576416915
        },
        "output_with": {
            "num_tokens_scored": 295,
            "num_green_tokens": 253,
            "green_fraction": 0.8576271186440678,
            "z_score": 2.474684557061376,
            "p_value": 0.006667695030322897
        }
    },
    {
        "title": "PERCEPTUAL GROUP TOKENIZER: BUILDING PERCEPTION WITH ITERATIVE GROUPING",
        "abstract": "Human visual recognition system shows astonishing capability of compressing visual information into a set of tokens containing rich representations without label supervision. One critical driving principle behind it is perceptual grouping (Palmer, 2002; Wagemans et al., 2012; Herzog, 2018). Despite being widely used in computer vision in the early 2010s, it remains a mystery whether perceptual grouping can be leveraged to derive a neural visual recognition backbone that generates as powerful representations. In this paper, we propose the Perceptual Group Tokenizer, a model that entirely relies on grouping operations to extract visual features and perform self-supervised representation learning, where a series of grouping operations are used to iteratively hypothesize the context for pixels or superpixels to refine feature representations. We show that the proposed model can achieve competitive performance compared to state-of-the-art vision architectures, and inherits desirable properties including adaptive computation without re-training, and interpretability. Specifically, Perceptual Group Tokenizer achieves 80.3% on ImageNet-1K self-supervised learning benchmark with linear probe evaluation, establishing a new milestone for this paradigm.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel visual recognition architecture called Perceptual Group Tokenizer (PGT), which entirely relies on perceptual grouping principles to extract visual features and perform self-supervised representation learning. The model consists of a series of grouping operations that iteratively hypothesize the context for pixels or superpixels to refine feature representations. The authors show that PGT achieves competitive performance compared to state-of-the-art vision architectures on self-supervised learning benchmarks and inherits desirable properties such as adaptive computation and high model interpretability.\n\nStrengths:\n1. The paper proposes a novel and innovative approach to visual recognition, which is based on perceptual grouping principles.\n2. The model achieves competitive performance compared to state-of-the-art vision architectures on self-supervised learning benchmarks.\n3. The authors provide a thorough analysis of the model's properties, including adaptive computation and high model interpretability.\n4. The paper provides a clear and concise explanation of the model's architecture and training procedure.\n\nWeaknesses:\n1. The paper assumes that the reader is familiar with the concept of perceptual grouping and its applications in computer vision.\n2. The model's performance on other tasks, such as object detection and segmentation, is not evaluated.\n3. The paper does not provide a thorough comparison with other state-of-the-art models, such as ViT and ResNet.\n4. The model's computational cost is relatively high due to the iterative grouping processes.\n\nSuggestions for Improvement:\n1. Provide a more detailed explanation of the perceptual grouping principles and their applications in computer vision.\n2. Evaluate the model's performance on other tasks, such as object detection and segmentation.\n3. Compare the model's performance with other state-of-the-art models, such as ViT and ResNet.\n4. Explore ways to reduce the model's computational cost, such as using more efficient grouping operations.\n\nRecommendation:\nThe paper proposes a novel and innovative approach to visual recognition, which is based on perceptual grouping principles. The model achieves competitive performance compared to state-of-the-art vision architectures on self-supervised learning benchmarks and inherits desirable properties such as adaptive computation and high model interpretability. However, the paper assumes that the reader is familiar with the concept of perceptual grouping and its applications in computer vision, and the model's performance on other tasks is not evaluated. Therefore, I recommend that the authors provide a more detailed explanation of the perceptual grouping principles and their applications in computer vision, evaluate the model's performance on other tasks, and compare the model's performance with other state-of-the-art models.\n\nScore: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel visual recognition architecture called Perceptual Group Tokenizer (PGT) that relies entirely on perceptual grouping principles for feature extraction and self-supervised representation learning. The model iteratively hypothesizes contexts for pixels or superpixels using a series of grouping operations, which are inspired by the human visual recognition system. PGT achieves competitive performance on the ImageNet-1K benchmark, outperforming state-of-the-art vision architectures, and has desirable properties such as adaptive computation and high model interpretability.\n\nStrengths:\n1. The paper proposes a novel and innovative approach to visual recognition, leveraging perceptual grouping principles to extract features and perform self-supervised representation learning.\n2. The model achieves competitive performance on the ImageNet-1K benchmark, outperforming state-of-the-art vision architectures.\n3. PGT has desirable properties such as adaptive computation and high model interpretability, making it a promising direction for future research.\n4. The paper provides a clear and concise explanation of the model's architecture and training procedure, making it easy for readers to understand and replicate the results.\n\nWeaknesses:\n1. The paper does not provide a thorough comparison with other state-of-the-art vision architectures, and it is unclear how PGT performs on other benchmarks.\n2. The model's computational cost is relatively high due to the iterative grouping processes, which may limit its practical applications.\n3. The paper does not provide a clear explanation of how the model's grouping operation relates to self-attention mechanisms, and how it can be used to improve the performance of other vision architectures.\n4. The model's ability to generalize to other tasks and domains is not thoroughly evaluated, and it is unclear how PGT performs on tasks such as object detection and semantic segmentation.\n\nSuggestions for Improvement:\n1. Provide a more comprehensive comparison with other state-of-the-art vision architectures, and evaluate PGT's performance on other benchmarks.\n2. Investigate alternative grouping operations that can reduce the model's computational cost and improve its practical applications.\n3. Provide a more detailed explanation of how the model's grouping operation relates to self-attention mechanisms, and how it can be used to improve the performance of other vision architectures.\n4. Evaluate PGT's ability to generalize to other tasks and domains, and provide a clear explanation of its strengths and limitations in these areas.\n\nRecommendation:\nThe paper proposes a novel and innovative approach to visual recognition, leveraging perceptual grouping principles to extract features and perform self-supervised representation learning. While the model has desirable properties such as adaptive computation and high model interpretability, its computational cost is relatively high, and its ability to generalize to other tasks and domains is not thoroughly evaluated. Therefore, I recommend that the authors provide a more comprehensive comparison with other state-of-the-art vision architectures, investigate alternative grouping operations, and evaluate PGT's ability to generalize to other tasks and domains before publishing the paper.",
        "gr_score_with": 0.8209876543209876,
        "gr_score_without": 0.7607142857142857,
        "output_without": {
            "num_tokens_scored": 280,
            "num_green_tokens": 213,
            "green_fraction": 0.7607142857142857,
            "z_score": -1.6431591864203916,
            "p_value": 0.9498249991146497
        },
        "output_with": {
            "num_tokens_scored": 324,
            "num_green_tokens": 266,
            "green_fraction": 0.8209876543209876,
            "z_score": 0.9447140844760277,
            "p_value": 0.1724024322465823
        }
    },
    {
        "title": "FOSI: Hybrid First and Second Order Optimization",
        "abstract": "Popular machine learning approaches forgo second-order information due to the difficulty of computing curvature in high dimensions. We present FOSI, a novel meta-algorithm that improves the performance of any base first-order optimizer by efficiently incorporating second-order information during the optimization process. In each iteration, FOSI implicitly splits the function into two quadratic functions defined on orthogonal subspaces, then uses a second-order method to minimize the first, and the base optimizer to minimize the other. We formally analyze FOSI\u2019s convergence and the conditions under which it improves a base optimizer. Our empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of first-order methods such as Heavy-Ball and Adam, and outperforms second-order methods (K-FAC and L-BFGS).",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes FOSI, a hybrid optimization algorithm that combines a first-order base optimizer with Newton's method to improve the convergence of any base first-order optimizer. FOSI iteratively splits the optimization problem into two quadratic problems on orthogonal subspaces, uses Newton's method to minimize one, and the base optimizer to minimize the other. The algorithm is designed to efficiently incorporate second-order information without computing the entire Hessian, making it suitable for large-scale optimization problems. The paper formally analyzes FOSI's convergence and the conditions under which it improves a base optimizer. Empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of first-order methods and outperforms second-order methods on various deep neural network training tasks.\n\nStrengths:\n\n1. FOSI is a hybrid optimization algorithm that combines the benefits of first-order and second-order methods, making it a robust and efficient optimization technique.\n2. The algorithm is designed to efficiently incorporate second-order information without computing the entire Hessian, making it suitable for large-scale optimization problems.\n3. The paper provides a thorough analysis of FOSI's convergence and the conditions under which it improves a base optimizer.\n4. Empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of first-order methods and outperforms second-order methods on various deep neural network training tasks.\n\nWeaknesses:\n\n1. The algorithm requires the number of Lanczos iterations (m) to be set, which can be a hyperparameter that needs to be tuned.\n2. The paper assumes that the Hessian is symmetric and positive definite, which may not be the case in all optimization problems.\n3. The algorithm requires the use of a base first-order optimizer, which may not be the best choice for all optimization problems.\n4. The paper does not provide a thorough analysis of the computational complexity of FOSI.\n\nSuggestions for Improvement:\n\n1. Investigate methods for automatically tuning the number of Lanczos iterations (m) and other hyperparameters of FOSI.\n2. Consider relaxing the assumption that the Hessian is symmetric and positive definite to make FOSI more applicable to a wider range of optimization problems.\n3. Investigate the use of other base first-order optimizers, such as stochastic gradient descent, to see if they can be improved by FOSI.\n4. Provide a thorough analysis of the computational complexity of FOSI and compare it to other optimization algorithms.\n\nRecommendation:\n\nFOSI is a promising optimization algorithm that combines the benefits of first-order and second-order methods. While there are some limitations and areas for improvement, the algorithm shows promising results on various deep neural network training tasks. I recommend further investigation and development of FOSI to make it a more robust and efficient optimization technique.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes FOSI, a hybrid optimization algorithm that improves the performance of any base first-order optimizer by incorporating second-order information. FOSI iteratively splits the optimization problem into pairs of quadratic problems on orthogonal subspaces and uses a second-order method (Newton's method) to minimize one and the base optimizer to minimize the other. The paper provides a detailed description of the FOSI algorithm, a thorough spectral analysis of its preconditioner, and convergence guarantees in the stochastic setting. The empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of first-order methods and outperforms second-order methods on various deep neural networks training tasks.\n\nStrengths:\n\n1. The paper proposes a novel and innovative approach to optimization, which is a significant contribution to the field.\n2. The paper provides a thorough analysis of the FOSI algorithm, including its spectral properties and convergence guarantees.\n3. The empirical evaluation demonstrates the effectiveness of FOSI on various deep neural networks training tasks.\n4. The paper provides a detailed comparison with state-of-the-art second-order methods, which is helpful for understanding the advantages and limitations of FOSI.\n\nWeaknesses:\n\n1. The paper assumes that the base optimizer is a first-order method, which may limit the applicability of FOSI to other optimization algorithms.\n2. The paper does not provide a detailed analysis of the computational complexity and memory requirements of FOSI, which may be important for large-scale optimization problems.\n3. The paper does not explore the robustness and stability of FOSI in the presence of noisy gradients or non-convex objectives.\n4. The paper does not provide a detailed comparison with other hybrid optimization algorithms, which may be helpful for understanding the relative advantages and limitations of FOSI.\n\nSuggestions for Improvement:\n\n1. Investigate the applicability of FOSI to other optimization algorithms, such as second-order methods or stochastic gradient methods.\n2. Provide a detailed analysis of the computational complexity and memory requirements of FOSI.\n3. Explore the robustness and stability of FOSI in the presence of noisy gradients or non-convex objectives.\n4. Compare FOSI with other hybrid optimization algorithms, such as Kronecker-factored approximations or quasi-Newton methods.\n\nRecommendation:\n\nThe paper proposes a novel and innovative approach to optimization, which is a significant contribution to the field. The empirical evaluation demonstrates the effectiveness of FOSI on various deep neural networks training tasks. However, the paper assumes that the base optimizer is a first-order method, which may limit the applicability of FOSI to other optimization algorithms. Therefore, the paper requires some modifications and improvements before it can be considered a final version.",
        "gr_score_with": 0.8474025974025974,
        "gr_score_without": 0.8126888217522659,
        "output_without": {
            "num_tokens_scored": 331,
            "num_green_tokens": 269,
            "green_fraction": 0.8126888217522659,
            "z_score": 0.5774091470782914,
            "p_value": 0.28183154825638046
        },
        "output_with": {
            "num_tokens_scored": 308,
            "num_green_tokens": 261,
            "green_fraction": 0.8474025974025974,
            "z_score": 2.0800298643834707,
            "p_value": 0.018761396864707684
        }
    },
    {
        "title": "UNLEASHING LARGE-SCALE VIDEO GENERATIVE PRE-TRAINING FOR VISUAL ROBOT MANIPULATION",
        "abstract": "Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations. In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative pre-training. We introduce GR-1, a GPT-style model designed for multi-task language-conditioned visual robot manipulation. GR-1 takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states. It predicts robot actions as well as future images in an end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly finetuned on robot data after pre-trained on a large-scale video dataset. We perform extensive experiments on the challenging CALVIN benchmark and a real robot. On CALVIN benchmark, our method outperforms state-of-the-art baseline methods and improves the success rate from 88.9% to 94.9%. In the setting of zero-shot unseen scene generalization, GR-1 improves the success rate from 53.3% to 85.4%. In real robot experiments, GR-1 also outperforms baseline methods and shows strong potentials in generalization to unseen scenes and objects. We provide inaugural evidence that a unified GPT-style transformer, augmented with large-scale video generative pre-training, exhibits remarkable generalization to multi-task visual robot manipulation. Project page: https://GR1-Manipulation.github.io",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to visual robot manipulation by leveraging large-scale video generative pre-training. The authors introduce GR-1, a GPT-style transformer model that takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states, and predicts robot actions and future images in an end-to-end manner. GR-1 is first pre-trained on language-conditioned video prediction with a large-scale video dataset and then finetuned on robot data. The authors demonstrate the effectiveness of GR-1 on the challenging CALVIN benchmark and a real robot, outperforming state-of-the-art baseline methods in various settings, including multi-task learning, zero-shot unseen scene generalization, small dataset, and zero-shot unseen language generalization.\n\nStrengths:\n\n* The paper presents a novel approach to visual robot manipulation by leveraging large-scale video generative pre-training, which is a promising direction for future research.\n* The authors demonstrate the effectiveness of GR-1 on the challenging CALVIN benchmark and a real robot, outperforming state-of-the-art baseline methods in various settings.\n* The paper provides a comprehensive evaluation of GR-1, including multi-task learning, zero-shot unseen scene generalization, small dataset, and zero-shot unseen language generalization.\n\nWeaknesses:\n\n* The paper relies heavily on pre-training on a large-scale video dataset, which may not be feasible for all applications or domains.\n* The authors do not provide a thorough analysis of the trade-offs between pre-training and finetuning, which may be important for practical applications.\n* The paper assumes access to a large-scale video dataset, which may not be available for all researchers or applications.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the trade-offs between pre-training and finetuning, including the impact on performance, computational resources, and data requirements.\n* Explore alternative approaches to pre-training, such as using smaller datasets or different types of data, to make the method more widely applicable.\n* Provide more information about the scalability of GR-1, including the computational resources required for training and inference.\n\nRecommendation:\n\nThe paper presents a novel and promising approach to visual robot manipulation, and the authors demonstrate its effectiveness on challenging benchmarks. However, the paper relies heavily on pre-training on a large-scale video dataset, and the trade-offs between pre-training and finetuning are not thoroughly analyzed. To make the method more widely applicable and scalable, the authors should explore alternative approaches to pre-training and provide a more detailed analysis of the trade-offs between pre-training and finetuning. With these improvements, the paper has the potential to make a significant impact in the field of visual robot manipulation.\n\nRating: 7/10\n\nRecommendation for Publication: Accept with revisions.",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper proposes a novel approach to visual robot manipulation by leveraging large-scale video generative pre-training. The authors introduce GR-1, a GPT-style transformer model that takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states, and predicts robot actions and future images in an end-to-end manner. GR-1 is first pre-trained on language-conditioned video prediction with a large-scale video dataset and then finetuned on robot data. The authors demonstrate the effectiveness of GR-1 in various experiments, including CALVIN benchmark and real robot experiments, and show that it outperforms state-of-the-art baseline methods in settings of multi-task learning, zero-shot unseen scene generalization, small dataset, and zero-shot unseen language generalization.\n\nStrengths:\n\n1. The paper presents a novel approach to visual robot manipulation by leveraging large-scale video generative pre-training, which is a significant contribution to the field.\n2. The authors demonstrate the effectiveness of GR-1 in various experiments, including CALVIN benchmark and real robot experiments, which shows the potential of GR-1 in practical applications.\n3. The paper provides a detailed explanation of the GR-1 model and its architecture, which makes it easy to understand and replicate the results.\n4. The authors discuss the limitations of the current approach and propose future work, which shows a clear direction for further research.\n\nWeaknesses:\n\n1. The paper assumes that the robot data is scarce, which may not be true in all cases. In some scenarios, robot data may be abundant, and the approach may not be as effective.\n2. The authors do not provide a detailed analysis of the video generative pre-training phase, which may affect the performance of GR-1.\n3. The paper does not provide a clear explanation of how GR-1 generalizes to unseen scenes and objects, which is a critical aspect of visual robot manipulation.\n4. The authors do not provide a detailed comparison with other state-of-the-art methods, which may affect the credibility of the results.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the video generative pre-training phase and its impact on the performance of GR-1.\n2. The paper should provide a clear explanation of how GR-1 generalizes to unseen scenes and objects, which is a critical aspect of visual robot manipulation.\n3. The authors should provide a more detailed comparison with other state-of-the-art methods, which may affect the credibility of the results.\n4. The paper should discuss potential limitations and challenges of the approach, such as dealing with noisy or incomplete data.\n\nRecommendation:\n\nThe paper presents a novel approach to visual robot manipulation by leveraging large-scale video generative pre-training, which has the potential to improve the performance of visual robot manipulation tasks. However, there are some limitations and challenges that need to be addressed, such as providing a more detailed analysis of the video generative pre-training phase and explaining how GR-1 generalizes to unseen scenes and objects. Overall, the paper is well-written and presents interesting results, and with some improvements, it has the potential to make a significant contribution to the field.\n\n$$$",
        "gr_score_with": 0.8469945355191257,
        "gr_score_without": 0.7699115044247787,
        "output_without": {
            "num_tokens_scored": 339,
            "num_green_tokens": 261,
            "green_fraction": 0.7699115044247787,
            "z_score": -1.3846665823685607,
            "p_value": 0.9169227790624354
        },
        "output_with": {
            "num_tokens_scored": 366,
            "num_green_tokens": 310,
            "green_fraction": 0.8469945355191257,
            "z_score": 2.247918043418647,
            "p_value": 0.012290708200129261
        }
    },
    {
        "title": "DOUBLY ROBUST INSTANCE-REWEIGHTED ADVERSARIAL TRAINING",
        "abstract": "Assigning importance weights to adversarial data has achieved great success in training adversarially robust networks under limited model capacity. However, existing instance-reweighted adversarial training (AT) methods heavily depend on heuristics and/or geometric interpretations to determine those importance weights, making these algorithms lack rigorous theoretical justification/guarantee. Moreover, recent research has shown that adversarial training suffers from a severe non-uniform robust performance across the training distribution, e.g., data points belonging to some classes can be much more vulnerable to adversarial attacks than others. To address both issues, in this paper, we propose a novel doubly-robust instance reweighted AT framework, which allows to obtain the importance weights via exploring distributionally robust optimization (DRO) techniques, and at the same time boosts the robustness on the most vulnerable examples. In particular, our importance weights are obtained by optimizing the KL-divergence regularized loss function, which allows us to devise new algorithms with a theoretical convergence guarantee. Experiments on standard classification datasets demonstrate that our proposed approach outperforms related state-of-the-art baseline methods in terms of average robust performance, and at the same time improves the robustness against attacks on the weakest data points. Codes can be found in the Supplement.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel doubly robust instance reweighted adversarial training framework, which combines distributionally robust optimization (DRO) and bilevel optimization to determine instance weights for adversarial training in a theoretically grounded way. The framework addresses the non-uniform issues of traditional adversarial training by boosting the robust performance of the most vulnerable examples. The authors develop a novel algorithm based on implicit function theorem and tracking a running average of the outer level function values to solve the doubly robust optimization problem. The proposed framework leads to a new finite-sum compositional bilevel optimization problem, which can be solved by the developed algorithm with theoretical guarantee.\n\nStrengths:\n\n* The paper proposes a novel and theoretically grounded approach to instance reweighted adversarial training, which addresses the non-uniform issues of traditional adversarial training.\n* The authors develop a novel algorithm based on implicit function theorem and tracking a running average of the outer level function values to solve the doubly robust optimization problem.\n* The proposed framework leads to a new finite-sum compositional bilevel optimization problem, which can be solved by the developed algorithm with theoretical guarantee.\n* The experiments on standard benchmarks demonstrate the effectiveness of the proposed approach in improving average robust performance and robustness against attacks on the weakest data points.\n\nWeaknesses:\n\n* The paper assumes that the attack loss function \u2113\u2032(\u00b7; \u03b8) is twice differentiable, which may not be the case in practice.\n* The authors use a log-barrier penalty method to penalize the linear constraint into the attack objective, which may not be efficient for large-scale problems.\n* The proposed algorithm requires the computation of the implicit gradient \u2202\u03b4\u0302\u2217i(\u03b8)\u2202\u03b8, which can be computationally expensive.\n\nSuggestions for Improvement:\n\n* The authors should investigate the use of alternative penalty methods, such as the entropic penalty method, to penalize the linear constraint into the attack objective.\n* The authors should provide more details on the computational efficiency of the proposed algorithm and the implicit gradient computation.\n* The authors should investigate the use of more efficient algorithms, such as the stochastic compositional gradient descent (SCGD) algorithm, to solve the doubly robust optimization problem.\n\nRecommendation:\n\nThe paper proposes a novel and theoretically grounded approach to instance reweighted adversarial training, which addresses the non-uniform issues of traditional adversarial training. The authors develop a novel algorithm based on implicit function theorem and tracking a running average of the outer level function values to solve the doubly robust optimization problem. The proposed framework leads to a new finite-sum compositional bilevel optimization problem, which can be solved by the developed algorithm with theoretical guarantee. The experiments on standard benchmarks demonstrate the effectiveness of the proposed approach in improving average robust performance and robustness against attacks on the weakest data points. Therefore, I recommend this paper for publication in a top-tier conference.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel doubly robust instance reweighted adversarial training framework, which addresses the limitations of traditional adversarial training methods. The proposed framework uses distributionally robust optimization (DRO) and bilevel optimization to determine the instance weights in a theoretically grounded way, and also boosts the robustness on the most vulnerable examples. The authors provide a new finite-sum compositional bilevel optimization problem and develop a novel algorithm based on implicit function theorem and tracking a running average of the outer level function values. The experiments on standard benchmarks demonstrate that the proposed framework outperforms related state-of-the-art baseline approaches in average robust performance and also improves the robustness against attacks on the weakest data points.\n\nStrengths:\n\n* The paper proposes a novel doubly robust instance reweighted adversarial training framework, which addresses the limitations of traditional adversarial training methods.\n* The proposed framework uses distributionally robust optimization (DRO) and bilevel optimization to determine the instance weights in a theoretically grounded way.\n* The authors provide a new finite-sum compositional bilevel optimization problem and develop a novel algorithm based on implicit function theorem and tracking a running average of the outer level function values.\n* The experiments on standard benchmarks demonstrate that the proposed framework outperforms related state-of-the-art baseline approaches in average robust performance and also improves the robustness against attacks on the weakest data points.\n\nWeaknesses:\n\n* The paper assumes that the adversarial noise is bounded, which may not be realistic in some scenarios.\n* The proposed framework requires the use of DRO, which may not be applicable in some cases where the distribution of the data is unknown.\n* The authors do not provide a detailed analysis of the computational complexity of the proposed algorithm.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the computational complexity of the proposed algorithm.\n* The authors should provide more experimental results on different types of data and scenarios.\n* The authors should provide more details on how the proposed framework can be extended to other types of attacks and defense mechanisms.\n\nRecommendation:\n\nThe paper proposes a novel and interesting approach to adversarial training, and the experimental results demonstrate its effectiveness. However, the paper has some limitations, and the authors should provide more details and experimental results to make the paper more comprehensive. With some minor revisions, the paper can be considered for publication in a top-tier conference.\n\nRating: 8/10\n\n$$$",
        "gr_score_with": 0.8804347826086957,
        "gr_score_without": 0.8295819935691319,
        "output_without": {
            "num_tokens_scored": 311,
            "num_green_tokens": 258,
            "green_fraction": 0.8295819935691319,
            "z_score": 1.3044700912626621,
            "p_value": 0.0960366753295307
        },
        "output_with": {
            "num_tokens_scored": 276,
            "num_green_tokens": 243,
            "green_fraction": 0.8804347826086957,
            "z_score": 3.340927414286313,
            "p_value": 0.0004174952480443263
        }
    },
    {
        "title": "GENSIM: GENERATING ROBOTIC SIMULATION TASKS VIA LARGE LANGUAGE MODELS",
        "abstract": "Collecting large amounts of real-world interaction data to train general robotic policies is often prohibitively expensive, thus motivating the use of simulation data. However, existing methods for data generation have generally focused on scenelevel diversity (e.g., object instances and poses) rather than task-level diversity, due to the human effort required to come up with and verify novel tasks. This has made it challenging for policies trained on simulation data to demonstrate significant task-level generalization. In this paper, we propose to automatically generate rich simulation environments and expert demonstrations by exploiting a large language models\u2019 (LLM) grounding and coding ability. Our approach, dubbed GENSIM, has two modes: goal-directed generation, wherein a target task is given to the LLM and the LLM proposes a task curriculum to solve the target task, and exploratory generation, wherein the LLM bootstraps from previous tasks and iteratively proposes novel tasks that would be helpful in solving more complex tasks. We use GPT4 to expand the existing benchmark by ten times to over 100 tasks, on which we conduct supervised finetuning and evaluate several LLMs including finetuned GPTs and Code Llama on code generation for robotic simulation tasks. Furthermore, we observe that LLMs-generated simulation programs can enhance task-level generalization significantly when used for multitask policy training. We further find that with minimal sim-to-real adaptation, the multitask policies pretrained on GPT4-generated simulation tasks exhibit stronger transfer to unseen long-horizon tasks in the real world and outperform baselines by 25%. 1",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework, GenSim, that utilizes large language models (LLMs) to automatically generate rich simulation environments and expert demonstrations for robotic policy training. The framework operates in two distinct modes: goal-directed generation, where a target task is given to the LLM, and exploratory generation, where the LLM bootstraps from previous tasks to propose novel tasks. The authors demonstrate that LLMs can generate high-quality, achievable, and diverse tasks, which can enhance task-level generalization of robotic policies. They evaluate several LLMs, including finetuned GPTs and Code Llama, on code generation for robotic simulation tasks and show that pretraining on LLM-generated simulation tasks can improve real-world generalization capabilities by 25%.\n\nStrengths:\n\n* The paper presents a novel and scalable framework for generating simulation tasks using LLMs, which can augment diverse simulation tasks for robotic policy training.\n* The authors demonstrate the effectiveness of their framework in generating high-quality, achievable, and diverse tasks, which can enhance task-level generalization of robotic policies.\n* The paper provides a thorough evaluation of several LLMs on code generation for robotic simulation tasks and shows that pretraining on LLM-generated simulation tasks can improve real-world generalization capabilities.\n\nWeaknesses:\n\n* The paper relies heavily on a single LLM model, GPT4, and does not explore the robustness of their framework to different LLM models.\n* The evaluation of the framework is limited to a specific task domain (table-top pick-and-place) and does not consider more complex robotic tasks.\n* The paper does not provide a thorough analysis of the limitations of their framework, such as the potential for hallucinations and lack of grounding in physical and geometric details.\n\nSuggestions for Improvement:\n\n* To improve the robustness of the framework, the authors should explore the use of multiple LLM models and evaluate their framework on different task domains.\n* The authors should provide a more thorough analysis of the limitations of their framework, including the potential for hallucinations and lack of grounding in physical and geometric details.\n* The paper should include a more detailed discussion of the potential applications of their framework, such as generating dexterous and complex robotic tasks.\n\nRecommendation:\n\nThe paper presents a novel and scalable framework for generating simulation tasks using LLMs, which can enhance task-level generalization of robotic policies. While the paper has some limitations, it provides a thorough evaluation of their framework and demonstrates its effectiveness in generating high-quality, achievable, and diverse tasks. Therefore, I recommend this paper for publication in a top-tier conference.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework, GenSim, that utilizes large language models (LLMs) to generate diverse simulation tasks for robotic policy learning. The framework consists of three components: a task creator, a task library, and a language-conditioned multi-task policy training procedure. The task creator generates novel task descriptions and corresponding code implementations, while the task library stores previously generated high-quality tasks to aid in the creation of new tasks. The language-conditioned multi-task policy training procedure leverages the generated tasks to enhance task-level generalization. The authors demonstrate the effectiveness of GenSim by scaling up a human-curated benchmark from 10 to over 100 tasks and evaluating several LLM models, including GPT4 and Code Llama, on simulated manipulation task creations.\n\nStrengths:\n\n* The paper presents a novel and scalable framework for generating diverse simulation tasks, which has the potential to improve task-level generalization in robotic policy learning.\n* The authors demonstrate the effectiveness of GenSim by scaling up a human-curated benchmark and evaluating several LLM models on simulated manipulation task creations.\n* The paper highlights the potential of LLMs to generate high-quality, achievable, and diverse tasks, which can be used to train multitask policies.\n\nWeaknesses:\n\n* The paper relies heavily on the performance of LLMs, which may not be generalizable to all robotic tasks and environments.\n* The evaluation metrics used in the paper, such as syntax-correct, runtime-verified, and task completed, may not capture all aspects of task generation and policy learning.\n* The paper does not provide a thorough analysis of the limitations and challenges of using LLMs for task generation and policy learning.\n\nSuggestions for Improvement:\n\n* The authors should provide a more thorough analysis of the limitations and challenges of using LLMs for task generation and policy learning, such as hallucinations, syntax errors, and lack of grounding in physical and geometric details.\n* The paper should include a more detailed evaluation of the performance of GenSim on a variety of robotic tasks and environments, including complex and dexterous tasks.\n* The authors should explore the use of other evaluation metrics, such as task success rates and policy generalization, to provide a more comprehensive assessment of GenSim.\n\nRecommendation:\n\nThe paper presents a novel and scalable framework for generating diverse simulation tasks, which has the potential to improve task-level generalization in robotic policy learning. However, the paper relies heavily on the performance of LLMs, and the evaluation metrics used may not capture all aspects of task generation and policy learning. Therefore, I recommend that the authors address these limitations and challenges in future work and provide a more comprehensive evaluation of GenSim.\n\n$$$",
        "gr_score_with": 0.8126801152737753,
        "gr_score_without": 0.783625730994152,
        "output_without": {
            "num_tokens_scored": 342,
            "num_green_tokens": 268,
            "green_fraction": 0.783625730994152,
            "z_score": -0.7567360683076866,
            "p_value": 0.7753959992737018
        },
        "output_with": {
            "num_tokens_scored": 347,
            "num_green_tokens": 282,
            "green_fraction": 0.8126801152737753,
            "z_score": 0.590794510901771,
            "p_value": 0.2773290566530554
        }
    },
    {
        "title": "COT3DREF: CHAIN-OF-THOUGHTS DATA-EFFICIENT 3D VISUAL GROUNDING",
        "abstract": "3D visual grounding is the ability to localize objects in 3D scenes conditioned by utterances. Most existing methods devote the referring head to localize the referred object directly, causing failure in complex scenarios. In addition, it does not illustrate how and why the network reaches the final decision. In this paper, we address this question \u201cCan we design an interpretable 3D visual grounding framework that has the potential to mimic the human perception system?\u201d. To this end, we formulate the 3D visual grounding problem as a sequence-to-sequence (Seq2Seq) task by first predicting a chain of anchors and then the final target. Interpretability not only improves the overall performance but also helps us identify failure cases. Following the chain of thoughts approach enables us to decompose the referring task into interpretable intermediate steps, boosting the performance and making our framework extremely data-efficient. Moreover, our proposed framework can be easily integrated into any existing architecture. We validate our approach through comprehensive experiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent performance gains compared to existing methods without requiring manually annotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is significantly data-efficient, whereas on the Sr3D dataset, when trained only on 10% of the data, we match the SOTA performance that trained on the entire data. The code is available at https://cot3dref.github.io/.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel and interpretable framework, CoT3DRef, for 3D visual grounding. The framework formulates the problem of 3D visual grounding from a natural language instruction as a sequence-to-sequence task, predicting a chain of anchor objects that are subsequently utilized to localize the final target object. This sequential approach enhances interpretability and improves overall performance and data efficiency. The framework is data-efficient and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data. The authors also demonstrate the effectiveness of their approach by integrating it into four different baselines and achieving state-of-the-art results on three well-known 3D visual grounding benchmarks.\n\nStrengths:\n\n1. The paper proposes a novel and interpretable framework for 3D visual grounding, which is a significant contribution to the field.\n2. The framework is data-efficient and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data.\n3. The authors demonstrate the effectiveness of their approach by integrating it into four different baselines and achieving state-of-the-art results on three well-known 3D visual grounding benchmarks.\n4. The paper provides a comprehensive analysis of the framework's performance and limitations, including the use of pseudo-labels and the Pathway module.\n\nWeaknesses:\n\n1. The paper relies heavily on pseudo-labels, which may not be accurate and can lead to performance loss on certain datasets.\n2. The Pathway module has limitations, including not handling multi-path scenarios, which may lead to suboptimal performance in certain cases.\n3. The paper does not provide a thorough comparison with other state-of-the-art methods, which makes it difficult to evaluate the framework's performance in a broader context.\n4. The paper assumes that the input utterance is well-formed and does not handle cases where the utterance is ambiguous or contains negations.\n\nSuggestions for Improvement:\n\n1. The authors should investigate alternative methods for collecting pseudo-labels, such as using more advanced natural language processing techniques or incorporating human annotations.\n2. The Pathway module should be modified to handle multi-path scenarios, such as using a graph-based approach to reason about different possibilities.\n3. The paper should provide a more thorough comparison with other state-of-the-art methods, including a detailed analysis of the framework's performance and limitations.\n4. The paper should address the assumption that the input utterance is well-formed and provide a more robust solution for handling ambiguous or negated utterances.\n\nRecommendation:\n\nThe paper proposes a novel and interpretable framework for 3D visual grounding, which is a significant contribution to the field. However, the paper relies heavily on pseudo-labels, which may not be accurate, and the Pathway module has limitations. To improve the framework, the authors should investigate alternative methods for collecting pseudo-labels and modify the Pathway module to handle multi-path scenarios. Additionally, the paper should provide a more thorough comparison with other state-of-the-art methods and address the assumption that the input utterance is well-formed. Overall, the paper is well-written and provides a comprehensive analysis of the framework's performance and limitations.\n\nRecommendation: Accept after revisions.\n\n$$$",
        "peer_review_with_watermark": "$$\nPaper Summary:\n\nThe paper proposes a novel and interpretable framework for 3D visual grounding, termed CoT3DRef. The framework formulates the 3D visual grounding problem as a sequence-to-sequence task, predicting a chain of anchor objects that are subsequently used to localize the final target object. This sequential approach enhances interpretability and improves overall performance and data efficiency. The framework is data-efficient and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data.\n\nStrengths:\n\n*   The proposed framework is interpretable and provides a clear understanding of how the model arrives at its decisions.\n*   The framework is data-efficient and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data.\n*   The use of pseudo-labels eliminates the need for manual annotations, making the framework more efficient and scalable.\n\nWeaknesses:\n\n*   The accuracy of the pseudo-labels plays a vital role in the overall performance, and a significant accuracy gap is measured between automatically collected pseudo-labels and ground-truth data.\n*   The anchor localization block in the pseudo module is tailored on the ScanNet dataset and will need adaptations to be usable on other 3D scene datasets.\n*   The Pathway module does not handle the multi-path scenario, where multiple paths are valid.\n\nSuggestions for Improvement:\n\n*   To improve the performance on the Nr3D dataset, the pseudo-label module should be enhanced to reduce the accuracy gap between automatically collected pseudo-labels and ground-truth data.\n*   The anchor localization block in the pseudo module should be adapted to be usable on other 3D scene datasets.\n*   The Pathway module should be modified to handle the multi-path scenario.\n\nRecommendation:\n\nThe proposed framework, CoT3DRef, is a significant contribution to the field of 3D visual grounding. The framework's interpretability, data efficiency, and ability to eliminate the need for manual annotations make it a valuable tool for researchers and practitioners. However, to fully realize the potential of the framework, the accuracy gap between pseudo-labels and ground-truth data should be addressed, and the Pathway module should be modified to handle the multi-path scenario. With these improvements, the framework has the potential to become a state-of-the-art method for 3D visual grounding.\n\n$$",
        "gr_score_with": 0.8129032258064516,
        "gr_score_without": 0.7907608695652174,
        "output_without": {
            "num_tokens_scored": 368,
            "num_green_tokens": 291,
            "green_fraction": 0.7907608695652174,
            "z_score": -0.4427888101933491,
            "p_value": 0.6710407511084228
        },
        "output_with": {
            "num_tokens_scored": 310,
            "num_green_tokens": 252,
            "green_fraction": 0.8129032258064516,
            "z_score": 0.56822974610264,
            "p_value": 0.28493948785457573
        }
    },
    {
        "title": "REWARD DESIGN FOR JUSTIFIABLE SEQUENTIAL DECISION-MAKING",
        "abstract": "Equipping agents with the capacity to justify made decisions using supporting evidence represents a cornerstone of accountable decision-making. Furthermore, ensuring that justifications are in line with human expectations and societal norms is vital, especially in high-stakes situations such as healthcare. In this work, we propose the use of a debate-based reward model for reinforcement learning agents, where the outcome of a zero-sum debate game quantifies the justifiability of a decision in a particular state. This reward model is then used to train a justifiable policy, whose decisions can be more easily corroborated with supporting evidence. In the debate game, two argumentative agents take turns providing supporting evidence for two competing decisions. Given the proposed evidence, a proxy of a human judge evaluates which decision is better justified. We demonstrate the potential of our approach in learning policies for prescribing and justifying treatment decisions of septic patients. We show that augmenting the reward with the feedback signal generated by the debate-based reward model yields policies highly favored by the judge when compared to the policy obtained solely from the environment rewards, while hardly sacrificing any performance. Moreover, in terms of the overall performance and justifiability of trained policies, the debate-based feedback is comparable to the feedback obtained from an ideal judge proxy that evaluates decisions using the full information encoded in the state. This suggests that the debate game outputs key information contained in states that is most relevant for evaluating decisions, which in turn substantiates the practicality of combining our approach with human-in-the-loop evaluations. Lastly, we showcase that agents trained via multi-agent debate learn to propose evidence that is resilient to refutations and closely aligns with human preferences.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to designing rewards for reinforcement learning (RL) agents that can justify their decisions using supporting evidence. The authors introduce a debate-based reward model, where two argumentative agents take turns providing supporting evidence for two competing decisions. A proxy of a human judge evaluates which decision is better justified based on the proposed evidence. The authors demonstrate the effectiveness of their approach in learning policies for treating septic patients, showing that the debate-based feedback yields policies highly favored by the judge while hardly sacrificing any performance.\n\nStrengths:\n\n1. The paper proposes a novel and innovative approach to designing rewards for RL agents that can justify their decisions using supporting evidence.\n2. The authors provide a thorough and detailed explanation of their approach, including the formal setup, learning framework, and experiments.\n3. The paper presents a comprehensive evaluation of the effectiveness of the debate-based reward model, including quantitative and qualitative results.\n4. The authors provide a clear and concise description of the experimental setup, including the patient cohort, environment, and evaluation metrics.\n\nWeaknesses:\n\n1. The paper assumes a well-defined argument space, which may not be the case in all domains. The authors acknowledge this limitation and suggest potential alternative approaches.\n2. The paper relies on a proxy of a human judge, which may not accurately capture human preferences and biases.\n3. The authors do not provide a thorough discussion of the potential biases and limitations of the debate-based reward model.\n4. The paper assumes a fixed number of turns in the debate game, which may not be realistic in all scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed discussion of the potential biases and limitations of the debate-based reward model, including the impact of human biases and preferences.\n2. The paper should include a more thorough evaluation of the effectiveness of the debate-based reward model in different domains and scenarios.\n3. The authors should provide a more detailed explanation of the learning framework and the role of the proxy judge in the debate-based reward model.\n4. The paper should include a more comprehensive discussion of the potential applications and limitations of the debate-based reward model.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to designing rewards for RL agents that can justify their decisions using supporting evidence. While there are some limitations and weaknesses to the approach, the paper provides a thorough and detailed evaluation of the effectiveness of the debate-based reward model. I recommend acceptance of the paper for publication in a top-tier conference, with the suggestion that the authors address the limitations and weaknesses mentioned above.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$ \nPaper Summary:\n\nThe paper proposes a framework for designing rewards that incentivize reinforcement learning agents to make justifiable decisions. The framework uses a debate-based reward model, where the outcome of a zero-sum debate game quantifies the justifiability of a decision in a particular state. The debate game involves two argumentative agents that take turns providing supporting evidence for two competing decisions, and a proxy of a human judge evaluates which decision is better justified. The paper demonstrates the effectiveness of the framework in learning policies for prescribing and justifying treatment decisions for septic patients, and shows that the policies trained with the debate-based reward model are highly favored by the human judge.\n\nStrengths:\n\n1. The paper presents a novel and innovative approach to designing rewards for reinforcement learning agents, which is a critical problem in the field.\n2. The framework is well-motivated and grounded in the concept of justifiable decision-making, which is an important aspect of accountable decision-making.\n3. The paper provides a thorough analysis of the framework, including its technical components and the experiments conducted to evaluate its effectiveness.\n4. The paper demonstrates the applicability of the framework to a real-world problem, namely prescribing and justifying treatment decisions for septic patients.\n\nWeaknesses:\n\n1. The paper assumes that the human judge's preferences can be accurately captured and modeled, which may not always be the case.\n2. The paper does not provide a thorough analysis of the computational complexity of the framework, which may be a limitation in certain applications.\n3. The paper assumes that the debate-based reward model is effective in all cases, which may not be the case in certain scenarios.\n4. The paper does not provide a detailed comparison with other reward design methods, which would be useful for understanding the relative strengths and weaknesses of the framework.\n\nSuggestions for Improvement:\n\n1. The paper could provide a more thorough analysis of the computational complexity of the framework, including a discussion of the time and space complexity of the algorithm.\n2. The paper could provide a more detailed comparison with other reward design methods, including a discussion of the strengths and weaknesses of each method.\n3. The paper could provide a more thorough analysis of the limitations of the framework, including a discussion of scenarios in which the framework may not be effective.\n4. The paper could provide a more detailed discussion of the implications of the framework for real-world applications, including a discussion of the potential benefits and challenges of implementing the framework in practice.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to designing rewards for reinforcement learning agents, which has the potential to make a significant impact in the field. The framework is well-motivated and grounded in the concept of justifiable decision-making, and the paper provides a thorough analysis of the framework and its technical components. However, the paper assumes that the human judge's preferences can be accurately captured and modeled, which may not always be the case, and the paper does not provide a thorough analysis of the computational complexity of the framework. Therefore, I recommend that the paper be accepted for publication, but with the suggestion that the authors address these limitations in future work.",
        "gr_score_with": 0.8854748603351955,
        "gr_score_without": 0.8181818181818182,
        "output_without": {
            "num_tokens_scored": 319,
            "num_green_tokens": 261,
            "green_fraction": 0.8181818181818182,
            "z_score": 0.8121131574797527,
            "p_value": 0.20836335199937417
        },
        "output_with": {
            "num_tokens_scored": 358,
            "num_green_tokens": 317,
            "green_fraction": 0.8854748603351955,
            "z_score": 4.04339839329746,
            "p_value": 2.634099605296072e-05
        }
    },
    {
        "title": "ERAL GEOMETRY FOR KNOWLEDGE DISTILLATION",
        "abstract": "Knowledge distillation aims to train a compact student network using soft supervision from a larger teacher network and hard supervision from ground truths. However, determining an optimal knowledge fusion ratio that balances these supervisory signals remains challenging. Prior methods generally resort to a constant or heuristic-based fusion ratio, which often falls short of a proper balance. In this study, we introduce a novel adaptive method for learning a sample-wise knowledge fusion ratio, exploiting both the correctness of teacher and student, as well as how well the student mimics the teacher on each sample. Our method naturally leads to the intra-sample trilateral geometric relations among the student prediction (S), teacher prediction (T ), and ground truth (G). To counterbalance the impact of outliers, we further extend to the inter-sample relations, incorporating the teacher\u2019s global average prediction (T\u0304 ) for samples within the same class. A simple neural network then learns the implicit mapping from the intraand inter-sample relations to an adaptive, sample-wise knowledge fusion ratio in a bilevel-optimization manner. Our approach provides a simple, practical, and adaptable solution for knowledge distillation that can be employed across various architectures and model sizes. Extensive experiments demonstrate consistent improvements over other loss re-weighting methods on image classification, attack detection, and click-through rate prediction.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach named TGeo-KD for learning sample-wise knowledge fusion ratios during knowledge distillation (KD). The authors argue that existing methods for determining the knowledge fusion ratio often rely on heuristic rules or constant values, which may not capture the nuanced dynamics of the knowledge transfer process. To address this limitation, the authors introduce a novel adaptive method that learns a sample-wise knowledge fusion ratio by exploiting both the correctness of the teacher and student, as well as the discrepancy between their predictions. The proposed method, TGeo-KD, leverages the trilateral geometry among the signals from the student, teacher, and ground truth by modeling both intra- and inter-sample geometric relations.\n\nStrengths:\n\n1. The paper presents a well-structured and clear introduction to the problem of knowledge distillation and the limitations of existing methods.\n2. The proposed approach, TGeo-KD, is innovative and well-motivated, leveraging the trilateral geometry among the signals from the student, teacher, and ground truth.\n3. The authors provide extensive experiments across diverse domains, including image classification, attack detection, and click-through rate prediction, demonstrating the consistent superiority of TGeo-KD over other re-weighting methods.\n4. The paper provides a clear and concise explanation of the proposed method, including the bilevel optimization problem and the neural network architecture.\n\nWeaknesses:\n\n1. The paper assumes that the teacher network is pre-trained, which may not be the case in many real-world scenarios.\n2. The proposed method relies on the availability of ground truth labels, which may not be available in some applications.\n3. The paper does not provide a thorough comparison with other methods that do not use trilateral geometry, making it difficult to evaluate the effectiveness of the proposed approach.\n4. The authors do not provide a clear explanation of how the proposed method can be extended to more complex scenarios, such as multi-task learning or transfer learning.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed comparison with other methods that do not use trilateral geometry, including a discussion of the advantages and disadvantages of each approach.\n2. Explore the use of TGeo-KD in scenarios where the teacher network is not pre-trained, such as in self-distillation or multi-task learning.\n3. Investigate the use of TGeo-KD in applications where ground truth labels are not available, such as in unsupervised learning or semi-supervised learning.\n4. Provide a more detailed explanation of how the proposed method can be extended to more complex scenarios, including a discussion of the challenges and potential solutions.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to knowledge distillation that leverages the trilateral geometry among the signals from the student, teacher, and ground truth. The extensive experiments across diverse domains demonstrate the consistent superiority of TGeo-KD over other re-weighting methods. However, the paper assumes that the teacher network is pre-trained, and the proposed method relies on the availability of ground truth labels. Therefore, I recommend that the authors address these limitations in future work and provide a more detailed comparison with other methods that do not use trilateral geometry.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes an innovative method named TGeo-KD for learning sample-wise knowledge fusion ratios in knowledge distillation (KD). The authors argue that existing methods often use a uniform or heuristic-based fusion ratio, which may not balance the supervisory signals well. The proposed TGeo-KD method learns a sample-wise knowledge fusion ratio by exploiting the trilateral geometry among the signals from the student, teacher, and ground truth. The method captures both intra-sample and inter-sample geometric relations and adapts the fusion ratio using a neural network. The authors conduct extensive experiments on various tasks, including image classification, attack detection, and click-through rate prediction, and demonstrate the superiority of TGeo-KD over other re-weighting methods.\n\nStrengths:\n\n1. The paper proposes an innovative method for learning sample-wise knowledge fusion ratios in KD, which addresses the limitation of existing methods.\n2. The method captures both intra-sample and inter-sample geometric relations, providing a more comprehensive understanding of the knowledge fusion process.\n3. The authors conduct extensive experiments on various tasks, demonstrating the effectiveness of TGeo-KD in different domains.\n4. The paper provides a clear and concise explanation of the method and its underlying principles.\n\nWeaknesses:\n\n1. The paper relies heavily on the use of neural networks to learn the fusion ratio, which may not be suitable for all applications or domains.\n2. The method may not be effective in cases where the teacher model is not well-trained or has poor performance.\n3. The paper does not provide a clear explanation of how the method can be extended to other domains or tasks beyond image classification.\n\nSuggestions for Improvement:\n\n1. The authors should provide more detailed explanations of how the method can be extended to other domains or tasks beyond image classification.\n2. The paper should include more discussions on the limitations of the method and potential areas for future research.\n3. The authors should provide more quantitative evaluations of the method's performance in different scenarios.\n\nRecommendation:\n\nThe paper proposes an innovative method for learning sample-wise knowledge fusion ratios in KD, which addresses the limitation of existing methods. The method captures both intra-sample and inter-sample geometric relations, providing a more comprehensive understanding of the knowledge fusion process. The authors conduct extensive experiments on various tasks, demonstrating the effectiveness of TGeo-KD in different domains. While the paper has some limitations, it provides a clear and concise explanation of the method and its underlying principles. Therefore, I recommend this paper for publication in the conference.",
        "gr_score_with": 0.8278145695364238,
        "gr_score_without": 0.7989821882951654,
        "output_without": {
            "num_tokens_scored": 393,
            "num_green_tokens": 314,
            "green_fraction": 0.7989821882951654,
            "z_score": -0.05013360562382587,
            "p_value": 0.5199920400081065
        },
        "output_with": {
            "num_tokens_scored": 302,
            "num_green_tokens": 250,
            "green_fraction": 0.8278145695364238,
            "z_score": 1.2086710650157877,
            "p_value": 0.11339462114749693
        }
    },
    {
        "title": "TAPMO: SHAPE-AWARE MOTION GENERATION OF SKELETON-FREE CHARACTERS",
        "abstract": "Previous motion generation methods are limited to the pre-rigged 3D human model, hindering their applications in the animation of various non-rigged characters. In this work, we present TapMo, a Text-driven Animation Pipeline for synthesizing Motion in a broad spectrum of skeleton-free 3D characters. The pivotal innovation in TapMo is its use of shape deformation-aware features as a condition to guide the diffusion model, thereby enabling the generation of meshspecific motions for various characters. Specifically, TapMo comprises two main components Mesh Handle Predictor and Shape-aware Diffusion Module. Mesh Handle Predictor predicts the skinning weights and clusters mesh vertices into adaptive handles for deformation control, which eliminates the need for traditional skeletal rigging. Shape-aware Motion Diffusion synthesizes motion with mesh-specific adaptations. This module employs text-guided motions and mesh features extracted during the first stage, preserving the geometric integrity of the animations by accounting for the character\u2019s shape and deformation. Trained in a weakly-supervised manner, TapMo can accommodate a multitude of nonhuman meshes, both with and without associated text motions. We demonstrate the effectiveness and generalizability of TapMo through rigorous qualitative and quantitative experiments. Our results reveal that TapMo consistently outperforms existing auto-animation methods, delivering superior-quality animations for both seen or unseen heterogeneous 3D characters. The project page: https://semanticdh.github.io/TapMo.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel text-driven animation pipeline called TapMo, which enables the generation of realistic and anthropomorphic motion for a wide range of skeleton-free 3D characters. The pipeline consists of two key components: the Mesh Handle Predictor and the Shape-aware Motion Diffusion. The Mesh Handle Predictor predicts the skinning weights and clusters mesh vertices into adaptive handles for semantic control, while the Shape-aware Motion Diffusion generates text-guided motions considering the specific deformation properties of the mesh. The authors propose a weakly-supervised training strategy for adaptive mesh handle learning and mesh-specific motion learning to train TapMo with limited ground-truth data. The paper demonstrates the effectiveness and generalizability of TapMo through extensive experiments and shows that it achieves state-of-the-art performance compared to baseline methods.\n\nStrengths:\n\n1. The paper presents a novel and comprehensive solution for text-driven animation of skeleton-free 3D characters.\n2. The authors propose a weakly-supervised training strategy for adaptive mesh handle learning and mesh-specific motion learning, which is innovative and effective.\n3. The paper demonstrates the effectiveness and generalizability of TapMo through extensive experiments and shows that it achieves state-of-the-art performance compared to baseline methods.\n4. The authors provide a clear and concise explanation of the proposed method and its components.\n\nWeaknesses:\n\n1. The paper assumes that the input mesh is provided, which may not be the case in practice. The authors should provide more information on how to handle this issue.\n2. The paper does not provide a detailed comparison with other methods that can handle skeleton-free 3D characters.\n3. The authors should provide more information on the computational cost of the proposed method and how it compares to other methods.\n4. The paper assumes that the text description is provided, which may not be the case in practice. The authors should provide more information on how to handle this issue.\n\nSuggestions for Improvement:\n\n1. Provide more information on how to handle the case where the input mesh is not provided.\n2. Provide a detailed comparison with other methods that can handle skeleton-free 3D characters.\n3. Provide more information on the computational cost of the proposed method and how it compares to other methods.\n4. Provide more information on how to handle the case where the text description is not provided.\n5. Consider providing more information on the robustness of the proposed method to different types of meshes and text descriptions.\n\nRecommendation:\n\nThe paper presents a novel and comprehensive solution for text-driven animation of skeleton-free 3D characters. The authors propose a weakly-supervised training strategy for adaptive mesh handle learning and mesh-specific motion learning, which is innovative and effective. The paper demonstrates the effectiveness and generalizability of TapMo through extensive experiments and shows that it achieves state-of-the-art performance compared to baseline methods. However, the paper assumes that the input mesh is provided, which may not be the case in practice. The authors should provide more information on how to handle this issue. Overall, the paper is well-written and provides a clear and concise explanation of the proposed method and its components. I recommend this paper for publication in the conference.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel text-driven animation pipeline called TapMo, which enables the generation of realistic and anthropomorphic motion for a wide range of skeleton-free 3D characters. TapMo consists of two key components: the Mesh Handle Predictor and the Shape-aware Motion Diffusion. The Mesh Handle Predictor predicts the skinning weights and clusters mesh vertices into adaptive handles for semantic control, while the Shape-aware Motion Diffusion generates text-guided motions considering the specific deformation properties of the mesh. To train TapMo with limited ground-truth data, the authors propose a weakly-supervised training strategy for adaptive mesh handle learning and mesh-specific motion learning. The authors demonstrate the effectiveness of TapMo through extensive experiments and show that it achieves state-of-the-art performance compared to baseline methods.\n\nStrengths:\n\n1. Novelty: TapMo is a novel text-driven animation pipeline that enables the generation of realistic and anthropomorphic motion for a wide range of skeleton-free 3D characters.\n2. Effectiveness: The authors demonstrate the effectiveness of TapMo through extensive experiments and show that it achieves state-of-the-art performance compared to baseline methods.\n3. Weakly-supervised training strategy: The authors propose a weakly-supervised training strategy for adaptive mesh handle learning and mesh-specific motion learning, which is suitable for training TapMo with limited ground-truth data.\n\nWeaknesses:\n\n1. Complexity: TapMo is a complex system consisting of two key components, which may be difficult to implement and train.\n2. Limited training data: TapMo requires a large amount of training data, which may be difficult to obtain, especially for skeleton-free 3D characters.\n3. Evaluation metrics: The authors use a combination of metrics to evaluate the performance of TapMo, which may not be comprehensive and may not capture all aspects of the animation quality.\n\nSuggestions for Improvement:\n\n1. Simplify the implementation and training of TapMo by introducing more efficient algorithms and techniques.\n2. Collect more training data, especially for skeleton-free 3D characters, to improve the robustness and generalizability of TapMo.\n3. Evaluate the performance of TapMo using more comprehensive metrics, such as user studies and expert evaluations, to better understand its strengths and limitations.\n\nRecommendation:\n\nI recommend accepting this paper for publication in the conference. TapMo is a novel and effective text-driven animation pipeline that has the potential to revolutionize the field of computer animation. While there are some limitations and areas for improvement, the authors have made significant contributions to the field and have demonstrated the effectiveness of TapMo through extensive experiments. With some further refinement and improvement, TapMo has the potential to become a widely-used tool for animators and researchers.\n\n$$$",
        "gr_score_with": 0.8876712328767123,
        "gr_score_without": 0.8874172185430463,
        "output_without": {
            "num_tokens_scored": 302,
            "num_green_tokens": 268,
            "green_fraction": 0.8874172185430463,
            "z_score": 3.798099800626272,
            "p_value": 7.290480866874135e-05
        },
        "output_with": {
            "num_tokens_scored": 365,
            "num_green_tokens": 324,
            "green_fraction": 0.8876712328767123,
            "z_score": 4.187640324926219,
            "p_value": 1.409348727207542e-05
        }
    },
    {
        "title": "DiLu : A Knowledge-Driven Approach to Autonomous Driving with Large Language Models",
        "abstract": "Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models (LLMs) with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu\u2019s capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems. To the best of our knowledge, we are the first to leverage knowledge-driven capability in decision-making for autonomous vehicles. Through the proposed DiLu framework, LLM is strengthened to apply knowledge and to reason causally in the autonomous driving domain. Project page: https://pjlab-adg.github.io/DiLu/",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework called DiLu, which integrates an interactive environment, a driver agent, and a memory component to address the challenges of autonomous driving. DiLu leverages large language models (LLMs) to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. The framework consists of a Reasoning and a Reflection module, which enable the system to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. The paper presents extensive experiments that prove DiLu's capability to accumulate experience and generalize to different environments.\n\nStrengths:\n\n* The paper proposes a novel framework that addresses the challenges of autonomous driving by leveraging LLMs and a memory component.\n* The framework is designed to enable the system to perform decision-making based on common-sense knowledge and evolve continuously.\n* The paper presents extensive experiments that demonstrate the effectiveness of DiLu in accumulating experience and generalizing to different environments.\n* The paper highlights the potential of DiLu to be deployed on practical autonomous driving systems.\n\nWeaknesses:\n\n* The paper assumes that the LLMs used in the framework are pre-trained and available, which may not be the case in real-world scenarios.\n* The paper does not provide a thorough analysis of the decision-making latency of the DiLu framework, which is a critical aspect of autonomous driving systems.\n* The paper does not provide a comparison with other knowledge-driven approaches to autonomous driving.\n* The paper assumes that the memory component is updated continuously, which may not be feasible in real-world scenarios.\n\nSuggestions for Improvement:\n\n* Provide a more thorough analysis of the decision-making latency of the DiLu framework and propose methods to reduce it.\n* Compare DiLu with other knowledge-driven approaches to autonomous driving and highlight its advantages.\n* Provide a more detailed explanation of the memory component and its update process.\n* Discuss the potential limitations of using LLMs in the framework and propose methods to address them.\n\nRecommendation:\n\nThe paper proposes a novel and effective framework for autonomous driving that leverages LLMs and a memory component. While there are some weaknesses and limitations, the paper presents extensive experiments that demonstrate the effectiveness of DiLu. Therefore, I recommend that the paper be accepted for publication, with the suggestion that the authors address the weaknesses and limitations mentioned above.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes the DiLu framework, a novel knowledge-driven approach to autonomous driving using large language models (LLMs). The authors aim to instill human-level knowledge into autonomous driving systems, addressing the limitations of data-driven methods, such as dataset bias, overfitting, and uninterpretability. The DiLu framework integrates an interactive environment, a driver agent, and a memory component, leveraging LLMs for decision-making based on common-sense knowledge and continuous evolution. The authors conduct extensive experiments, demonstrating the framework's capability to accumulate experience, exhibit strong generalization ability, and acquire experiences from real-world datasets.\n\nStrengths:\n\n1. The paper presents a unique perspective on autonomous driving, highlighting the importance of human-level knowledge in decision-making.\n2. The authors propose a novel framework, DiLu, which integrates LLMs with a memory module, allowing for continuous experience accumulation and decision-making.\n3. The experimental results demonstrate the framework's strong generalization ability compared to state-of-the-art reinforcement learning-based methods.\n4. The authors provide a clear explanation of the framework's components, including the Reasoning, Reflection, and Memory modules.\n\nWeaknesses:\n\n1. The paper assumes that LLMs can be directly applied to autonomous driving tasks without extensive fine-tuning or adaptation, which might not be the case in real-world scenarios.\n2. The authors do not provide a comprehensive analysis of the potential risks associated with relying on LLMs in autonomous driving systems, such as hallucinations or biased decision-making.\n3. The framework's decision-making latency of 5-10 seconds might be a significant limitation in real-time autonomous driving applications.\n4. The authors do not provide a clear explanation of how the framework handles edge cases, such as unusual or unexpected scenarios.\n\nSuggestions for Improvement:\n\n1. Conduct more comprehensive experiments to evaluate the framework's performance in real-world scenarios, including edge cases and unusual situations.\n2. Provide a more detailed analysis of the potential risks associated with relying on LLMs in autonomous driving systems.\n3. Investigate methods to reduce the framework's decision-making latency, such as using more efficient LLMs or optimizing the framework's architecture.\n4. Consider incorporating additional components, such as sensor data or expert knowledge, to improve the framework's decision-making capabilities.\n\nRecommendation:\n\nThe DiLu framework presents an innovative approach to autonomous driving, leveraging LLMs for decision-making based on human-level knowledge. While the framework has demonstrated strong generalization ability, there are potential risks associated with relying on LLMs in autonomous driving systems. Further research is needed to address these limitations and ensure the framework's safe and reliable deployment in real-world scenarios.\n\nOverall, the paper presents a valuable contribution to the field of autonomous driving, highlighting the importance of human-level knowledge in decision-making. The authors provide a clear explanation of the framework's components and demonstrate its strong generalization ability. However, further research is needed to address the potential risks associated with relying on LLMs in autonomous driving systems.\n\n$$$",
        "gr_score_with": 0.8253164556962025,
        "gr_score_without": 0.7564935064935064,
        "output_without": {
            "num_tokens_scored": 308,
            "num_green_tokens": 233,
            "green_fraction": 0.7564935064935064,
            "z_score": -1.9085436646611325,
            "p_value": 0.9718395073897262
        },
        "output_with": {
            "num_tokens_scored": 395,
            "num_green_tokens": 326,
            "green_fraction": 0.8253164556962025,
            "z_score": 1.2581817198095835,
            "p_value": 0.10416302223004542
        }
    },
    {
        "title": "RTFS-NET: RECURRENT TIME-FREQUENCY MOD-",
        "abstract": "Audio-visual speech separation methods aim to integrate different modalities to generate high-quality separated speech, thereby enhancing the performance of downstream tasks such as speech recognition. Most existing state-of-the-art (SOTA) models operate in the time domain. However, their overly simplistic approach to modeling acoustic features often necessitates larger and more computationally intensive models in order to achieve SOTA performance. In this paper, we present a novel time-frequency domain audio-visual speech separation method: Recurrent Time-Frequency Separation Network (RTFS-Net), which applies its algorithms on the complex time-frequency bins yielded by the Short-Time Fourier Transform. We model and capture the time and frequency dimensions of the audio independently using a multi-layered RNN along each dimension. Furthermore, we introduce a unique attention-based fusion technique for the efficient integration of audio and visual information, and a new mask separation approach that takes advantage of the intrinsic spectral nature of the acoustic features for a clearer separation. RTFS-Net outperforms the prior SOTA method in both inference speed and separation quality while reducing the number of parameters by 90% and MACs by 83%. This is the first time-frequency domain audio-visual speech separation method to outperform all contemporary time-domain counterparts.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel audio-visual speech separation (AVSS) method called RTFS-Net, which operates in the time-frequency domain. The proposed method utilizes a multi-layered recurrent neural network (RNN) to model and capture the time and frequency dimensions of the audio independently. Additionally, it introduces a unique attention-based fusion technique for the efficient integration of audio and visual information, and a new mask separation approach that takes advantage of the intrinsic spectral nature of the acoustic features for a clearer separation. The experimental evaluations conducted on three widely used datasets demonstrate the superiority of RTFS-Net over state-of-the-art (SOTA) methods, achieving better separation quality and reducing computational complexity by 83% and parameter count by 90%.\n\nStrengths:\n\n1. The proposed method, RTFS-Net, is the first time-frequency domain AVSS method to outperform all contemporary T-domain counterparts.\n2. The method achieves remarkable performance improvements while maintaining a significantly reduced computational complexity and parameter count.\n3. The use of a multi-layered RNN to model and capture the time and frequency dimensions of the audio independently is a novel and effective approach.\n4. The attention-based fusion technique and the new mask separation approach are unique and efficient ways to integrate audio and visual information.\n5. The experimental evaluations conducted on multiple datasets demonstrate the superiority of RTFS-Net over SOTA methods.\n\nWeaknesses:\n\n1. The paper assumes that the audio and video inputs are aligned in time, which may not always be the case in real-world scenarios.\n2. The method relies on a pre-trained video encoder, which may not be available for all applications.\n3. The experimental evaluations were conducted on a limited number of datasets, and it would be beneficial to evaluate the method on a larger variety of datasets.\n4. The paper does not provide a detailed analysis of the computational complexity of the method, which may be a concern for large-scale applications.\n5. The code for RTFS-Net is not provided, which makes it difficult for readers to reproduce the results.\n\nSuggestions for Improvement:\n\n1. Investigate methods to align the audio and video inputs in time, even if they are not perfectly aligned.\n2. Provide a more detailed analysis of the computational complexity of the method and explore ways to reduce it further.\n3. Evaluate the method on a larger variety of datasets to demonstrate its robustness and generalizability.\n4. Provide a more detailed explanation of the attention-based fusion technique and the new mask separation approach.\n5. Provide a more detailed comparison with SOTA methods and highlight the advantages of RTFS-Net over these methods.\n\nRecommendation:\n\nThe paper presents a novel and effective AVSS method that outperforms SOTA methods. However, it has some limitations that need to be addressed. With some improvements, RTFS-Net has the potential to become a leading method in the field of AVSS. I recommend this paper for publication in a reputable conference, and I hope that the authors will address the limitations and suggestions for improvement.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel audio-visual speech separation (AVSS) method, Recurrent Time-Frequency Separation Network (RTFS-Net), which operates in the time-frequency domain. RTFS-Net employs a multi-layered RNN to model the time and frequency dimensions of audio independently, a unique attention-based fusion technique for integrating audio and visual information, and a new mask separation approach that takes advantage of the intrinsic spectral nature of acoustic features. The authors conduct comprehensive experiments on three widely used datasets, LRS2, LRS3, and VoxCeleb2, and demonstrate the superiority of RTFS-Net over contemporary T-domain methods in terms of separation quality, inference speed, and computational complexity.\n\nStrengths:\n\n1. RTFS-Net is the first TF-domain AVSS method to outperform all contemporary T-domain methods, achieving this while reducing computational complexity by 83% and parameter count by 90%.\n2. The paper presents a thorough analysis of the challenges faced by TF-domain AVSS methods, including the need to model both time and frequency dimensions independently, the importance of visual information, and the loss of amplitude and phase information during feature extraction.\n3. The authors propose a novel architecture, RTFS-Net, which addresses these challenges through a combination of RNNs, attention-based fusion, and a new mask separation approach.\n4. The paper includes a comprehensive evaluation of RTFS-Net on three widely used datasets, demonstrating its superiority over contemporary T-domain methods.\n\nWeaknesses:\n\n1. The paper assumes a relatively simple audio encoder, which may not be optimal for more complex audio scenarios.\n2. The authors do not provide a detailed analysis of the computational complexity of RTFS-Net, which may be important for large-scale deployments.\n3. While the paper presents a thorough evaluation of RTFS-Net, it would be beneficial to include additional experiments, such as comparisons with other TF-domain AVSS methods or investigations of the robustness of RTFS-Net to different types of noise or distortion.\n\nSuggestions for Improvement:\n\n1. Consider incorporating more complex audio encoders, such as those based on convolutional neural networks (CNNs) or recurrent neural networks (RNNs), to improve the performance of RTFS-Net in more complex audio scenarios.\n2. Provide a more detailed analysis of the computational complexity of RTFS-Net, including a comparison with contemporary T-domain methods, to demonstrate its efficiency in large-scale deployments.\n3. Include additional experiments to evaluate the robustness of RTFS-Net to different types of noise or distortion, such as background noise, music, or other extraneous audio sources.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to audio-visual speech separation, RTFS-Net, which outperforms contemporary T-domain methods in terms of separation quality, inference speed, and computational complexity. While there are some weaknesses in the paper, including the assumption of a simple audio encoder and the lack of a detailed analysis of computational complexity, the authors present a thorough evaluation of RTFS-Net and demonstrate its superiority over contemporary methods. Therefore, I recommend this paper for publication in the conference.\n\n$$$",
        "gr_score_with": 0.8238095238095238,
        "gr_score_without": 0.7643020594965675,
        "output_without": {
            "num_tokens_scored": 437,
            "num_green_tokens": 334,
            "green_fraction": 0.7643020594965675,
            "z_score": -1.8652752093549814,
            "p_value": 0.9689285809063096
        },
        "output_with": {
            "num_tokens_scored": 420,
            "num_green_tokens": 346,
            "green_fraction": 0.8238095238095238,
            "z_score": 1.2201803990246367,
            "p_value": 0.11119824780965593
        }
    },
    {
        "title": "SOHES: SELF-SUPERVISED OPEN-WORLD HIERARCHICAL ENTITY SEGMENTATION",
        "abstract": "Open-world entity segmentation, as an emerging computer vision task, aims at segmenting entities in images without being restricted by pre-defined classes, offering impressive generalization capabilities on unseen images and concepts. Despite its promise, existing entity segmentation methods like Segment Anything Model (SAM) rely heavily on costly expert annotators. This work presents Self-supervised Open-world Hierarchical Entity Segmentation (SOHES), a novel approach that sidesteps the need for human annotations. SOHES operates in three phases: self-exploration, self-instruction, and self-correction. Given a pretrained self-supervised representation, we produce abundant high-quality pseudolabels through visual feature clustering. Then, we train a segmentation model on the pseudo-labels, and rectify the noises in pseudo-labels via a teacher-student mutual-learning procedure. Beyond segmenting entities, SOHES also captures their constituent parts, providing a hierarchical understanding of visual entities. Using raw images as the sole training data, our method achieves unprecedented performance in self-supervised open-world segmentation, marking a significant milestone towards high-quality open-world entity segmentation in the absence of human-annotated masks.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel self-supervised approach, called Self-supervised Open-world Hierarchical Entity Segmentation (SOHES), for open-world entity segmentation. SOHES operates in three phases: self-exploration, self-instruction, and self-correction. In the self-exploration phase, the method generates initial pseudo-labels by clustering visual features from a pre-trained self-supervised representation. In the self-instruction phase, a segmentation model is trained on the initial pseudo-labels to refine the segmentation. Finally, in the self-correction phase, a teacher-student mutual-learning framework is employed to further improve the segmentation model. SOHES achieves state-of-the-art performance on various datasets, including MS-COCO, LVIS, ADE20K, EntitySeg, and SA-1B, and significantly closes the gap between self-supervised and supervised methods.\n\nStrengths:\n\n1. SOHES is a novel and effective approach for open-world entity segmentation, which has the potential to revolutionize the field of computer vision.\n2. The method is self-supervised, which means that it can learn from unlabeled data, making it more efficient and cost-effective than traditional supervised learning approaches.\n3. SOHES can learn hierarchical structures of visual entities, which is a unique feature of this approach.\n4. The method achieves state-of-the-art performance on various datasets, including those with complex scenes and diverse classes.\n5. The paper provides a thorough evaluation of the method, including ablation studies and comparisons with state-of-the-art methods.\n\nWeaknesses:\n\n1. The method requires a large amount of unlabeled data, which may not be readily available in all scenarios.\n2. The self-exploration phase can be computationally expensive, which may limit the scalability of the method.\n3. The teacher-student mutual-learning framework used in the self-correction phase may not be suitable for all types of data and may require additional hyperparameter tuning.\n4. The paper does not provide a detailed analysis of the robustness and generalizability of the method to different types of data and scenarios.\n5. The method may not be suitable for real-time applications due to its computational complexity.\n\nSuggestions for Improvement:\n\n1. Investigate the use of more efficient clustering algorithms in the self-exploration phase to reduce computational complexity.\n2. Explore the use of different teacher-student mutual-learning frameworks to improve the performance of the method.\n3. Provide a more detailed analysis of the robustness and generalizability of the method to different types of data and scenarios.\n4. Investigate the use of transfer learning to adapt the method to new domains and scenarios.\n5. Provide a more detailed evaluation of the method's performance on real-world datasets and scenarios.\n\nRecommendation:\n\nThe paper presents a novel and effective approach for open-world entity segmentation, which has the potential to revolutionize the field of computer vision. The method is self-supervised, which makes it more efficient and cost-effective than traditional supervised learning approaches. However, the method requires a large amount of unlabeled data and may not be suitable for real-time applications due to its computational complexity. Overall, the paper is well-written and provides a thorough evaluation of the method, but it requires additional work to address the weaknesses and limitations of the approach.\n\nRating: 8/10\n\nRecommendation for Publication: Yes\n\nRecommendation for Future Work: Yes\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel self-supervised approach to open-world entity segmentation, titled Self-Supervised Open-World Hierarchical Entity Segmentation (SOHES). The method consists of three phases: self-exploration, self-instruction, and self-correction. In the self-exploration phase, the model generates initial pseudo-labels by clustering visual features from a pre-trained self-supervised representation. The self-instruction phase trains a segmentation model on the initial pseudo-labels to refine the segmentation. In the self-correction phase, a teacher-student mutual-learning framework is employed to further improve the model's performance. The proposed method achieves state-of-the-art performance in self-supervised open-world segmentation and demonstrates its ability to segment entities and their constituent parts in an open world.\n\nStrengths:\n\n1. The paper proposes a novel self-supervised approach to open-world entity segmentation, which is a challenging task.\n2. The method consists of three phases, each of which is well-motivated and effectively addresses a different aspect of the task.\n3. The paper provides a thorough evaluation of the proposed method on various datasets and demonstrates its superiority over existing methods.\n4. The method is able to segment entities and their constituent parts in an open world, which is a significant advancement in the field of computer vision.\n5. The paper provides a clear and concise explanation of the method and its components, making it easy to follow and understand.\n\nWeaknesses:\n\n1. The paper assumes that the pre-trained self-supervised representation is available, which may not always be the case in practice.\n2. The method requires a large number of images for training, which may not be feasible in some scenarios.\n3. The paper does not provide a clear explanation of how the teacher-student mutual-learning framework is implemented and why it is effective.\n4. The paper does not provide a thorough analysis of the computational complexity of the method and its potential bottlenecks.\n5. The paper assumes that the ground-truth annotations are available for evaluation, which may not always be the case in practice.\n\nSuggestions for Improvement:\n\n1. Provide a clear explanation of how to obtain a pre-trained self-supervised representation and how to adapt it to the task of open-world entity segmentation.\n2. Provide a thorough analysis of the computational complexity of the method and its potential bottlenecks.\n3. Provide a clear explanation of how the teacher-student mutual-learning framework is implemented and why it is effective.\n4. Provide a thorough evaluation of the method on a wider range of datasets and scenarios.\n5. Provide a clear explanation of how to handle cases where the ground-truth annotations are not available.\n\nRecommendation:\n\nThe paper proposes a novel and effective self-supervised approach to open-world entity segmentation. The method consists of three phases and demonstrates its ability to segment entities and their constituent parts in an open world. The paper provides a thorough evaluation of the proposed method on various datasets and demonstrates its superiority over existing methods. However, there are some weaknesses and suggestions for improvement that need to be addressed. With some modifications and improvements, the proposed method has the potential to become a state-of-the-art approach to open-world entity segmentation.\n\nRecommendation: Accept with minor revisions.",
        "gr_score_with": 0.8140161725067385,
        "gr_score_without": 0.7605633802816901,
        "output_without": {
            "num_tokens_scored": 426,
            "num_green_tokens": 324,
            "green_fraction": 0.7605633802816901,
            "z_score": -2.0345610018720186,
            "p_value": 0.9790524676379269
        },
        "output_with": {
            "num_tokens_scored": 371,
            "num_green_tokens": 302,
            "green_fraction": 0.8140161725067385,
            "z_score": 0.6752188323863081,
            "p_value": 0.24976837173815514
        }
    },
    {
        "title": "PRE-TRAINING WITH SYNTHETIC DATA HELPS OFFLINE REINFORCEMENT LEARNING",
        "abstract": "Recently, it has been shown that for offline deep reinforcement learning (DRL), pre-training Decision Transformer with a large language corpus can improve downstream performance (Reid et al., 2022). A natural question to ask is whether this performance gain can only be achieved with language pre-training, or can be achieved with simpler pre-training schemes which do not involve language. In this paper, we first show that language is not essential for improved performance, and indeed pre-training with synthetic IID data for a small number of updates can match the performance gains from pre-training with a large language corpus; moreover, pre-training with data generated by a one-step Markov chain can further improve the performance. Inspired by these experimental results, we then consider pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm, which is Q-learning-based and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training with simple synthetic data for a small number of updates can also improve CQL, providing consistent performance improvement on D4RL Gym locomotion datasets. The results of this paper not only illustrate the importance of pre-training for offline DRL but also show that the pre-training data can be synthetic and generated with remarkably simple mechanisms.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper explores the effectiveness of pre-training offline deep reinforcement learning (DRL) models using synthetic data, rather than language data. The authors show that pre-training Decision Transformer (DT) with synthetic data generated from a one-step Markov Chain can match or even surpass the performance of pre-training with a large language corpus. They also demonstrate that pre-training Conservative Q-Learning (CQL) with synthetic Markov Decision Process (MDP) data can provide a significant performance boost. The results suggest that synthetic data can be a surprisingly effective approach to improve offline DRL performance for both transformer-based and Q-learning-based algorithms.\n\nStrengths:\n\n* The paper provides a clear and concise overview of the related work and the contributions of the paper.\n* The authors conduct extensive ablations to demonstrate the robustness of synthetic pre-training over different settings of the synthetic data.\n* The results are presented in a clear and organized manner, with tables and figures that facilitate easy comparison of the different methods.\n* The authors provide theoretical insights into why IID data can still achieve a good performance, which adds depth to the understanding of the results.\n\nWeaknesses:\n\n* The paper assumes that the reader is familiar with the basics of DRL and reinforcement learning, which may make it difficult for non-experts to understand the paper.\n* The authors do not provide a clear explanation of why pre-training with synthetic data is effective, which is a limitation of the paper.\n* The results are based on a limited set of experiments, which may not be representative of all possible scenarios.\n* The paper does not provide a detailed comparison with other pre-training methods, such as pre-training with generic image data or using offline DRL data itself to learn representations.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of why pre-training with synthetic data is effective, including a discussion of the potential benefits of using synthetic data for pre-training.\n* Conduct more extensive experiments to demonstrate the robustness of synthetic pre-training over different settings of the synthetic data.\n* Provide a more detailed comparison with other pre-training methods, including a discussion of the potential advantages and disadvantages of each method.\n* Consider using more diverse and challenging datasets to evaluate the performance of the pre-training methods.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the field of offline DRL by demonstrating the effectiveness of pre-training with synthetic data. However, the paper has some limitations, such as the lack of a clear explanation of why pre-training with synthetic data is effective and the limited scope of the experiments. Overall, I would recommend this paper for publication, but with the suggestion that the authors address the limitations mentioned above.\n\nGrade: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper explores the effectiveness of pre-training offline Deep Reinforcement Learning (DRL) models using synthetic data, specifically Markov Chain (MC) and Markov Decision Process (MDP) data. The authors demonstrate that pre-training Decision Transformer (DT) and Conservative Q-Learning (CQL) with synthetic data can significantly improve their performance compared to standard training without pre-training, and even outperform pre-training with a large language corpus like Wikipedia. The authors also show the robustness of synthetic pre-training over different settings of the synthetic data, including the degree of past dependence, MC state-space size, and the degree of randomness in the transitions.\n\nStrengths:\n\n1. The paper presents a simple yet effective synthetic pre-training scheme for DT and CQL, which challenges the previous view that language pre-training can provide unique benefits for DRL.\n2. The authors provide extensive ablation studies to show the robustness of synthetic pre-training over different settings of the synthetic data.\n3. The paper presents theoretical insights into why IID data can still achieve a good performance, showing that the forward dynamics objective is equivalent to finding the state centroids underlying the synthetic dataset.\n4. The authors demonstrate the efficiency and ease of use of synthetic pre-training, requiring significantly fewer pre-train updates and computation time compared to standard pre-training methods.\n\nWeaknesses:\n\n1. The paper assumes a fixed transition matrix for the MC and MDP data, which may not be realistic for many real-world scenarios where the transition dynamics are uncertain or changing over time.\n2. The authors do not investigate the impact of synthetic pre-training on the robustness and stability of the trained models, which is an important aspect of offline DRL.\n3. The paper focuses primarily on the performance of DT and CQL, and it is unclear how the synthetic pre-training scheme would generalize to other offline DRL algorithms.\n\nSuggestions for Improvement:\n\n1. Investigate the impact of synthetic pre-training on the robustness and stability of the trained models, and explore methods to improve their robustness and stability.\n2. Consider using more realistic transition matrices for the MC and MDP data, and explore the impact of uncertainty and changing transition dynamics on the performance of synthetic pre-training.\n3. Investigate the generalizability of synthetic pre-training to other offline DRL algorithms, and explore the impact of synthetic pre-training on their performance.\n\nRecommendation:\n\nThe paper presents a significant contribution to the field of offline DRL, demonstrating the effectiveness of synthetic pre-training for improving the performance of DT and CQL. The authors provide extensive ablation studies and theoretical insights, and demonstrate the efficiency and ease of use of synthetic pre-training. While there are some limitations and weaknesses in the paper, it is a well-written and well-structured paper that presents a compelling argument for the use of synthetic pre-training in offline DRL.\n\nOverall, I recommend accepting this paper for publication in a top-tier conference in the field of machine learning or reinforcement learning.\n\nScore: 8/10",
        "gr_score_with": 0.8617283950617284,
        "gr_score_without": 0.8381201044386423,
        "output_without": {
            "num_tokens_scored": 383,
            "num_green_tokens": 321,
            "green_fraction": 0.8381201044386423,
            "z_score": 1.8653462370067906,
            "p_value": 0.031066444027457674
        },
        "output_with": {
            "num_tokens_scored": 405,
            "num_green_tokens": 349,
            "green_fraction": 0.8617283950617284,
            "z_score": 3.1059274638226473,
            "p_value": 0.0009484161962238129
        }
    },
    {
        "title": "CROSSQ: BATCH NORMALIZATION",
        "abstract": "Sample efficiency is a crucial problem in deep reinforcement learning. Recent algorithms, such as REDQ and DroQ, found a way to improve the sample efficiency by increasing the update-to-data (UTD) ratio to 20 gradient update steps on the critic per environment sample. However, this comes at the expense of a greatly increased computational cost. To reduce this computational burden, we introduce CrossQ: A lightweight algorithm for continuous control tasks that makes careful use of Batch Normalization and removes target networks to surpass the current state-of-the-art in sample efficiency while maintaining a low UTD ratio of 1. Notably, CrossQ does not rely on advanced bias-reduction schemes used in current methods. CrossQ\u2019s contributions are threefold: (1) it matches or surpasses current state-of-the-art methods in terms of sample efficiency, (2) it substantially reduces the computational cost compared to REDQ and DroQ, (3) it is easy to implement, requiring just a few lines of code on top of SAC.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces CrossQ, a lightweight algorithm for continuous control tasks that achieves superior performance by removing much of the algorithmic design complexity that was added over the years. CrossQ uses Batch Normalization and removes target networks to surpass the current state-of-the-art in sample efficiency while maintaining a low update-to-data (UTD) ratio of 1. The authors present three design choices that constitute CrossQ: removing target networks, using Batch Normalization, and wider critic networks. They provide empirical investigations and hypotheses for CrossQ's success and show that CrossQ matches or surpasses the state-of-the-art algorithms in sample efficiency while being up to 4\u00d7 faster in terms of wallclock time.\n\nStrengths:\n\n* The paper presents a new algorithm, CrossQ, that achieves superior performance in continuous control tasks.\n* The authors provide a thorough analysis of the design choices that constitute CrossQ and their impact on the algorithm's performance.\n* The paper includes a comprehensive set of experiments that demonstrate the effectiveness of CrossQ in various environments.\n* The authors provide a clear and concise description of the algorithm and its components.\n\nWeaknesses:\n\n* The paper assumes a good understanding of deep reinforcement learning and its components, which may make it challenging for readers without a background in the field.\n* The authors do not provide a detailed theoretical analysis of the algorithm's performance, which may limit its applicability to other domains.\n* The paper focuses on continuous control tasks, and it is unclear whether CrossQ can be extended to other domains, such as discrete control tasks or multi-agent systems.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of the design choices that constitute CrossQ and their impact on the algorithm's performance.\n* Include a more comprehensive set of experiments that demonstrate the effectiveness of CrossQ in various environments.\n* Provide a detailed theoretical analysis of the algorithm's performance, including a discussion of its convergence properties and stability.\n* Consider extending CrossQ to other domains, such as discrete control tasks or multi-agent systems.\n\nRecommendation:\n\nThe paper presents a new algorithm, CrossQ, that achieves superior performance in continuous control tasks. The authors provide a thorough analysis of the design choices that constitute CrossQ and their impact on the algorithm's performance. While the paper has some limitations, it is a valuable contribution to the field of deep reinforcement learning. I recommend accepting this paper for publication in a top-tier conference or journal.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper introduces CrossQ, a lightweight algorithm for continuous control tasks that makes careful use of Batch Normalization and removes target networks to surpass the current state-of-the-art in sample efficiency while maintaining a low UTD ratio of 1. The authors claim that CrossQ is computationally more efficient than REDQ and DroQ, and matches or surpasses their performance in terms of sample efficiency. CrossQ's contributions are threefold: it matches or surpasses current state-of-the-art methods in terms of sample efficiency, it substantially reduces the computational cost compared to REDQ and DroQ, and it is easy to implement, requiring just a few lines of code on top of SAC.\n\nStrengths:\n\n1. The authors provide a clear and concise overview of the existing state-of-the-art methods, REDQ and DroQ, and highlight the limitations of these methods.\n2. The authors introduce a new algorithm, CrossQ, which is computationally more efficient and matches or surpasses the performance of REDQ and DroQ in terms of sample efficiency.\n3. The authors provide a thorough analysis of the design choices in CrossQ, including the removal of target networks, the use of Batch Normalization, and wider critic networks.\n4. The authors conduct a series of ablation studies to investigate the impact of different design choices on the performance of CrossQ.\n\nWeaknesses:\n\n1. The authors claim that CrossQ is computationally more efficient than REDQ and DroQ, but they do not provide a thorough analysis of the computational cost of CrossQ.\n2. The authors only evaluate CrossQ on a limited set of environments, and it is unclear whether CrossQ will perform well on more complex tasks.\n3. The authors do not provide a clear explanation of why Batch Normalization is effective in CrossQ, and it is unclear whether this is a generalizable result.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more thorough analysis of the computational cost of CrossQ, including a comparison with REDQ and DroQ.\n2. The authors should evaluate CrossQ on a more diverse set of environments to ensure that it generalizes well to different tasks.\n3. The authors should provide a more detailed explanation of why Batch Normalization is effective in CrossQ, and investigate whether this result is generalizable to other algorithms.\n\nRecommendation:\n\nThis paper is a significant contribution to the field of deep reinforcement learning, as it introduces a new algorithm, CrossQ, which is computationally more efficient and matches or surpasses the performance of existing state-of-the-art methods. However, to fully evaluate the impact of CrossQ, the authors should provide a more thorough analysis of the computational cost and evaluate CrossQ on a more diverse set of environments.",
        "gr_score_with": 0.8057324840764332,
        "gr_score_without": 0.7587301587301587,
        "output_without": {
            "num_tokens_scored": 315,
            "num_green_tokens": 239,
            "green_fraction": 0.7587301587301587,
            "z_score": -1.8308693768700388,
            "p_value": 0.9664399789525553
        },
        "output_with": {
            "num_tokens_scored": 314,
            "num_green_tokens": 253,
            "green_fraction": 0.8057324840764332,
            "z_score": 0.2542230414701789,
            "p_value": 0.3996616253548189
        }
    },
    {
        "title": "Never Train from Scratch: FAIR COMPARISON OF LONG- SEQUENCE MODELS REQUIRES DATA-DRIVEN PRIORS",
        "abstract": "Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using only the downstream task data, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 absolute points. Subsequently, we analyze the utility of previously-proposed structured parameterizations for SSMs and show they become mostly redundant in the presence of data-driven initialization obtained through pretraining. Our work shows that, when evaluating different architectures on supervised tasks, incorporation of data-driven priors via pretraining is essential for reliable performance estimation, and can be done efficiently.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to evaluate the performance of long-range sequence models by incorporating self-supervised pretraining on the task data itself. The authors argue that training models from scratch using random initialization can lead to gross overestimation of the differences between architectures, and that pretraining with standard denoising objectives using only the downstream task data can lead to dramatic gains across multiple architectures. The paper demonstrates the effectiveness of self-supervised pretraining on the Long Range Arena (LRA) benchmark, a standard benchmark for long sequence modeling, and shows that pretraining leads to large empirical gains over previously-reported performances on LRA across a range of architectures. The authors also analyze the utility of structured parameterizations for State Space Models (SSMs) and show that they become mostly redundant in the presence of data-driven initialization obtained through pretraining.\n\nStrengths:\n\n* The paper presents a novel and efficient approach to evaluate the performance of long-range sequence models.\n* The authors provide a comprehensive array of experiments to demonstrate the effectiveness of self-supervised pretraining on the LRA benchmark.\n* The paper highlights the importance of accounting for the pretraining stage while designing and evaluating novel architectures.\n* The authors provide a thorough analysis of the utility of structured parameterizations for SSMs and show that they become mostly redundant in the presence of data-driven initialization obtained through pretraining.\n\nWeaknesses:\n\n* The paper assumes that the downstream task data is available, which may not be the case in all scenarios.\n* The authors do not provide a detailed analysis of the computational overhead of self-supervised pretraining.\n* The paper does not provide a thorough comparison with other pretraining methods, such as supervised pretraining or transfer learning.\n* The authors do not provide a clear explanation of how to choose the pretraining objective and the number of pretraining iterations.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the computational overhead of self-supervised pretraining and discuss ways to reduce it.\n* Compare self-supervised pretraining with other pretraining methods, such as supervised pretraining or transfer learning, to demonstrate its effectiveness.\n* Provide a clear explanation of how to choose the pretraining objective and the number of pretraining iterations.\n* Discuss the potential limitations of self-supervised pretraining and how to address them.\n\nRecommendation:\n\nThis paper presents a novel and efficient approach to evaluate the performance of long-range sequence models. The authors provide a comprehensive array of experiments to demonstrate the effectiveness of self-supervised pretraining on the LRA benchmark. While there are some weaknesses and limitations to the paper, it makes a significant contribution to the field and has the potential to impact the design and evaluation of novel architectures. I recommend this paper for publication in a top-tier conference or journal.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper explores the importance of pretraining in evaluating the performance of long-sequence models on benchmarks such as Long Range Arena (LRA). The authors argue that training models from scratch, as is common practice, leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives using only the downstream task data leads to dramatic gains across multiple architectures. They demonstrate that vanilla Transformers match the performance of state-of-the-art S4 models on LRA when properly pretrained and that pretraining leads to significant improvements in the performance of State Space models (SSMs). The authors also show that the data-driven priors learned via pretraining become redundant in the presence of hand-crafted modeling biases used in current solutions.\n\nStrengths:\n\n* The paper makes a strong argument for the importance of pretraining in evaluating the performance of long-sequence models.\n* The authors provide a comprehensive array of experiments that demonstrate the effectiveness of self-supervised pretraining on the task data itself.\n* The paper highlights the need to account for the pretraining stage while designing and evaluating novel architectures in the future.\n\nWeaknesses:\n\n* The paper could benefit from a more detailed discussion of the potential limitations and pitfalls of self-supervised pretraining.\n* The authors could provide more insight into the mechanisms by which pretraining leads to improved performance, particularly in the context of long-sequence models.\n* The paper could benefit from a more nuanced discussion of the trade-offs between pretraining and fine-tuning.\n\nSuggestions for Improvement:\n\n* The authors could provide a more detailed discussion of the potential limitations and pitfalls of self-supervised pretraining, such as overfitting or the need for large amounts of data.\n* The authors could provide more insight into the mechanisms by which pretraining leads to improved performance, particularly in the context of long-sequence models.\n* The paper could benefit from a more nuanced discussion of the trade-offs between pretraining and fine-tuning, including the potential for pretraining to overfit the task data.\n\nRecommendation:\n\nOverall, the paper makes a strong case for the importance of pretraining in evaluating the performance of long-sequence models and provides a comprehensive array of experiments that demonstrate the effectiveness of self-supervised pretraining on the task data itself. However, the paper could benefit from a more detailed discussion of the potential limitations and pitfalls of self-supervised pretraining and a more nuanced discussion of the trade-offs between pretraining and fine-tuning.\n\nI recommend accepting the paper for publication, pending revisions to address the suggestions for improvement outlined above.\n\n$$$",
        "gr_score_with": 0.8677966101694915,
        "gr_score_without": 0.7976539589442815,
        "output_without": {
            "num_tokens_scored": 341,
            "num_green_tokens": 272,
            "green_fraction": 0.7976539589442815,
            "z_score": -0.10801685071037054,
            "p_value": 0.543008837258166
        },
        "output_with": {
            "num_tokens_scored": 295,
            "num_green_tokens": 256,
            "green_fraction": 0.8677966101694915,
            "z_score": 2.9113463325326676,
            "p_value": 0.0017993743848342036
        }
    },
    {
        "title": "LEMUR: INTEGRATING LARGE LANGUAGE MODELS",
        "abstract": "The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that demands high-level abstract reasoning about program properties that is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework, LEMUR, that integrates Large Language Models (LLMs) with automated reasoners for automated program verification. The framework combines the strengths of LLMs, which can perform high-level abstract reasoning, and automated reasoners, which can perform precise low-level reasoning. The authors present a formal calculus, LEMUR, that operates over a configuration, which is either a distinguished symbol or a tuple of a program, an assumption, and a list of properties. The calculus performs oracle calls to LLMs to propose new properties and revise them. The authors also present a sound and terminating algorithm based on LEMUR, which can be used to instantiate the calculus as an automated decision procedure.\n\nStrengths:\n\n1. The paper proposes a novel and innovative framework that combines the strengths of LLMs and automated reasoners for automated program verification.\n2. The authors present a formal calculus, LEMUR, that provides a theoretical foundation for the integration of LLMs and automated reasoners.\n3. The paper presents a sound and terminating algorithm based on LEMUR, which can be used to instantiate the calculus as an automated decision procedure.\n4. The authors demonstrate the practicality of LEMUR by instantiating it as a fully automated framework and evaluating its efficiency on standard benchmark sets.\n\nWeaknesses:\n\n1. The paper assumes that the LLMs can generate correct and useful properties, which may not always be the case.\n2. The authors do not provide a thorough evaluation of the performance of LEMUR on a wide range of benchmarks.\n3. The paper assumes that the automated reasoners can perform precise low-level reasoning, which may not always be the case.\n4. The authors do not provide a clear explanation of how the LEMUR calculus can be used to handle complex properties and large programs.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more thorough evaluation of the performance of LEMUR on a wide range of benchmarks.\n2. The paper should provide a clearer explanation of how the LEMUR calculus can be used to handle complex properties and large programs.\n3. The authors should provide a more detailed description of the LLMs used in the paper and their limitations.\n4. The paper should provide a more thorough discussion of the strengths and weaknesses of the LEMUR framework compared to other automated program verification approaches.\n\nRecommendation:\n\nThe paper proposes a novel and innovative framework that combines the strengths of LLMs and automated reasoners for automated program verification. The authors present a formal calculus, LEMUR, that provides a theoretical foundation for the integration of LLMs and automated reasoners. The paper also presents a sound and terminating algorithm based on LEMUR, which can be used to instantiate the calculus as an automated decision procedure. However, the paper assumes that the LLMs can generate correct and useful properties, which may not always be the case. Therefore, the paper should provide a more thorough evaluation of the performance of LEMUR on a wide range of benchmarks and provide a clearer explanation of how the LEMUR calculus can be used to handle complex properties and large programs.\n\nIn summary, the paper has the potential to make a significant contribution to the field of automated program verification, but it requires further evaluation and refinement to be considered a complete and reliable solution.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes a novel framework, LEMUR, that integrates Large Language Models (LLMs) with automated reasoners for automated program verification. The framework, LEMUR, is designed to leverage the high-level abstract reasoning capabilities of LLMs to propose new properties, which are then checked by automated reasoners. The authors present a formal calculus for LEMUR, prove its soundness, and describe an instantiation of the calculus as a sound automated verification procedure. They also present experimental results showing that LEMUR can solve hard benchmarks from the Software-Verification Competition 2023 that were beyond the reach of state-of-the-art conventional program verifiers.\n\nStrengths:\n\n* The paper presents a novel framework that integrates LLMs with automated reasoners for automated program verification, which is a significant contribution to the field.\n* The authors provide a formal calculus for LEMUR, which is an important aspect of the paper.\n* The experimental results show that LEMUR can solve hard benchmarks that were beyond the reach of state-of-the-art conventional program verifiers.\n* The paper highlights the importance of LLMs in augmenting formal reasoning and provides a clear understanding of the challenges and limitations of using LLMs in program verification.\n\nWeaknesses:\n\n* The paper assumes that the LLMs will always generate correct properties, which may not be the case in practice.\n* The paper does not provide a thorough analysis of the scalability of LEMUR to larger programs.\n* The paper relies heavily on the performance of the LLMs, which may not be reliable or consistent.\n* The paper does not provide a clear comparison with other state-of-the-art program verification tools.\n\nSuggestions for Improvement:\n\n* The authors should provide a more thorough analysis of the scalability of LEMUR to larger programs.\n* The authors should investigate the reliability and consistency of the LLMs and provide a more detailed analysis of their performance.\n* The authors should compare LEMUR with other state-of-the-art program verification tools to demonstrate its effectiveness.\n* The authors should provide more experimental results to demonstrate the robustness of LEMUR.\n\nRecommendation:\n\nThis paper presents a novel framework that integrates LLMs with automated reasoners for automated program verification, which is a significant contribution to the field. While the paper has some weaknesses, it provides a clear understanding of the challenges and limitations of using LLMs in program verification. I recommend this paper for publication, but with the suggestion that the authors address the weaknesses mentioned above.\n\nRating: 8/10\n\nReviewers will appreciate the novelty of the framework, the clarity of the presentation, and the experimental results. They may have some concerns about the scalability, reliability, and consistency of the LLMs, as well as the comparison with other state-of-the-art program verification tools. Overall, this is a well-written paper that makes a significant contribution to the field of program verification.",
        "gr_score_with": 0.8424068767908309,
        "gr_score_without": 0.7753164556962026,
        "output_without": {
            "num_tokens_scored": 316,
            "num_green_tokens": 245,
            "green_fraction": 0.7753164556962026,
            "z_score": -1.0966706759814477,
            "p_value": 0.863607310170764
        },
        "output_with": {
            "num_tokens_scored": 349,
            "num_green_tokens": 294,
            "green_fraction": 0.8424068767908309,
            "z_score": 1.9808327422988352,
            "p_value": 0.023805016818113353
        }
    },
    {
        "title": "MOVINGPARTS: MOTION-BASED 3D PART DISCOV-",
        "abstract": "We present MovingParts, a NeRF-based method for dynamic scene reconstruction and part discovery. We consider motion as an important cue for identifying parts, that all particles on the same part share the common motion pattern. From the perspective of fluid simulation, existing deformation-based methods for dynamic NeRF can be seen as parameterizing the scene motion under the Eulerian view, i.e., focusing on specific locations in space through which the fluid flows as time passes. However, it is intractable to extract the motion of constituting objects or parts using the Eulerian view representation. In this work, we introduce the dual Lagrangian view and enforce representations under the Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangian view, we parameterize the scene motion by tracking the trajectory of particles on objects. The Lagrangian view makes it convenient to discover parts by factorizing the scene motion as a composition of part-level rigid motions. Experimentally, our method can achieve fast and high-quality dynamic scene reconstruction from even a single moving camera, and the induced part-based representation allows direct applications of part tracking, animation, 3D scene editing, etc.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper presents MovingParts, a novel NeRF-based approach for dynamic scene reconstruction and part discovery. The method considers motion as an important cue for identifying parts and proposes a hybrid approach that learns motion under both the Eulerian and Lagrangian views. The paper introduces a novel motion grouping module that enables unsupervised object part discovery via motion grouping. The method achieves high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods and is the only one that simultaneously achieves part discovery.\n\nStrengths:\n1. The paper presents a novel approach for dynamic scene reconstruction and part discovery, which is a challenging problem in computer vision and graphics.\n2. The method considers motion as an important cue for identifying parts, which is a novel insight in the field.\n3. The paper introduces a hybrid approach that learns motion under both the Eulerian and Lagrangian views, which is a unique contribution.\n4. The method achieves high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods.\n5. The paper provides a thorough evaluation of the method on the D-NeRF 360\u25e6 synthetic dataset and a synthetic dataset with ground-truth motion masks.\n\nWeaknesses:\n1. The paper assumes that the scene is piece-wise rigid, which may not be the case in all scenarios.\n2. The method requires a large number of parameters, which may lead to overfitting.\n3. The paper does not provide a thorough analysis of the time complexity of the method.\n4. The method assumes that the motion is smooth and continuous, which may not be the case in all scenarios.\n5. The paper does not provide a clear explanation of how the method handles occlusions and self-occlusions.\n\nSuggestions for Improvement:\n1. The authors should provide a more thorough analysis of the time complexity of the method.\n2. The authors should provide a more detailed explanation of how the method handles occlusions and self-occlusions.\n3. The authors should provide a more thorough evaluation of the method on real-world datasets.\n4. The authors should provide a more detailed explanation of how the method can be applied to other domains, such as human-object interaction.\n5. The authors should provide a more thorough discussion of the limitations of the method and potential future work.\n\nRecommendation:\nThe paper presents a novel approach for dynamic scene reconstruction and part discovery, which is a challenging problem in computer vision and graphics. The method achieves high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods and is the only one that simultaneously achieves part discovery. However, the paper assumes that the scene is piece-wise rigid, which may not be the case in all scenarios, and the method requires a large number of parameters, which may lead to overfitting. Therefore, the paper is recommended for publication after addressing these limitations.\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents MovingParts, a novel NeRF-based method for dynamic scene reconstruction and part discovery. The authors introduce a hybrid approach that models scene motion under both the Eulerian and Lagrangian views, enabling high-quality dynamic scene reconstruction and automatic rigid part discovery. The method consists of three modules: a canonical module that models scene geometry and appearance, an Eulerian module that records particle motion, and a Lagrangian module that tracks particle trajectories. The authors also introduce a cycle-consistency loss to enforce reciprocity between the Eulerian and Lagrangian modules. The method is evaluated on the D-NeRF 360\u25e6 synthetic dataset and a synthetic dataset with ground-truth motion masks, demonstrating its ability to achieve high-quality reconstruction and part discovery.\n\nStrengths:\n\n* The paper presents a novel and effective method for dynamic scene reconstruction and part discovery.\n* The hybrid approach that models scene motion under both the Eulerian and Lagrangian views is a key contribution.\n* The use of a cycle-consistency loss to enforce reciprocity between the Eulerian and Lagrangian modules is a clever idea.\n* The method is evaluated on multiple datasets, including a synthetic dataset with ground-truth motion masks.\n* The paper demonstrates the ability of the method to achieve high-quality reconstruction and part discovery.\n\nWeaknesses:\n\n* The paper relies on a complex and computationally expensive model architecture.\n* The method requires a large amount of training data and computational resources.\n* The paper does not provide a clear explanation of how the method can be adapted to real-world scenarios.\n* The method is limited to reconstructing scenes with piece-wise rigid motion.\n* The paper does not provide a clear discussion of the limitations and potential biases of the method.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed explanation of the method's architecture and how it can be adapted to real-world scenarios.\n* The authors should provide more information about the computational resources required to train the model and the time complexity of the method.\n* The authors should provide a clearer discussion of the limitations and potential biases of the method.\n* The authors should provide more information about the datasets used in the evaluation and how they were created.\n* The authors should provide a more detailed explanation of the cycle-consistency loss and how it is used to enforce reciprocity between the Eulerian and Lagrangian modules.\n\nRecommendation:\n\nThe paper presents a novel and effective method for dynamic scene reconstruction and part discovery. However, the method relies on a complex and computationally expensive model architecture, and the paper does not provide a clear explanation of how the method can be adapted to real-world scenarios. Nevertheless, the method demonstrates the ability to achieve high-quality reconstruction and part discovery, and the paper provides a thorough evaluation of the method on multiple datasets. I recommend this paper for acceptance, but with the suggestion that the authors address the limitations and potential biases of the method.\n\n$$$",
        "gr_score_with": 0.8365384615384616,
        "gr_score_without": 0.7551724137931034,
        "output_without": {
            "num_tokens_scored": 290,
            "num_green_tokens": 219,
            "green_fraction": 0.7551724137931034,
            "z_score": -1.9081778439039765,
            "p_value": 0.9718158828834175
        },
        "output_with": {
            "num_tokens_scored": 312,
            "num_green_tokens": 261,
            "green_fraction": 0.8365384615384616,
            "z_score": 1.613751342204663,
            "p_value": 0.05329068628909476
        }
    },
    {
        "title": "EARLY NEURON ALIGNMENT IN TWO-LAYER RELU NETWORKS WITH SMALL INITIALIZATION",
        "abstract": "This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons\u2019 directional dynamics allows us to provide an O( logn \u221a\u03bc ) upper bound on the time it takes for all neurons to achieve good alignment with the input data, where n is the number of data points and \u03bc measures how well the data are separated. After the early alignment phase, the loss converges to zero at a O( 1t ) rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThis paper studies the problem of training a binary classifier via gradient flow on two-layer ReLU networks with small initialization. The authors consider a training dataset with well-separated input vectors and provide a careful analysis of the neurons' directional dynamics. They show that during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. The authors also provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data, which is O(logn\u221a\u00b5). After the early alignment phase, the loss converges to zero at a O(1/t) rate, and the weight matrix on the first layer is approximately low-rank. The authors validate their theoretical findings using a numerical experiment on classifying two digits from the MNIST dataset.\n\nStrengths:\n\n1. The paper provides a complete analysis of the dynamics of gradient flow for training a two-layer ReLU network on well-separated data with small initialization.\n2. The authors provide a formal analysis for the early alignment phase with a finite but small initialization scale \u03f5, which is a significant improvement over previous work.\n3. The paper provides a quantitative bound on the time it takes for good alignment to be achieved, which is O(logn\u221a\u00b5).\n4. The authors validate their theoretical findings using a numerical experiment on classifying two digits from the MNIST dataset.\n\nWeaknesses:\n\n1. The paper assumes that the training dataset has well-separated input vectors, which may not be a realistic assumption in practice.\n2. The authors use a balanced initialization scheme, which may not be a common initialization scheme in practice.\n3. The paper does not provide a detailed comparison with other training schemes, such as gradient descent only on the first-layer weights.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed comparison with other training schemes, such as gradient descent only on the first-layer weights.\n2. The paper should be extended to consider more realistic initialization schemes and training datasets.\n3. The authors should provide more numerical experiments to validate their theoretical findings.\n\nRecommendation:\n\nThis paper provides a significant contribution to the understanding of the dynamics of gradient flow for training two-layer ReLU networks with small initialization. The authors' analysis is thorough and provides a quantitative bound on the time it takes for good alignment to be achieved. However, the paper assumes a balanced initialization scheme and well-separated input vectors, which may not be realistic in practice. Therefore, I recommend that the authors extend their analysis to more realistic initialization schemes and training datasets.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThis paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. The authors consider a training dataset with well-separated input vectors and provide a complete analysis of the dynamics of gradient flow for training a two-layer ReLU network on well-separated data with small initialization. They show that during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. The authors also provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data and show that after the early alignment phase, the loss converges to zero at a O(1/t) rate and the weight matrix on the first layer is approximately low-rank.\n\nStrengths:\n1. The paper provides a complete analysis of the dynamics of gradient flow for training a two-layer ReLU network on well-separated data with small initialization.\n2. The authors provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data, which is a significant improvement over previous work.\n3. The paper shows that after the early alignment phase, the loss converges to zero at a O(1/t) rate and the weight matrix on the first layer is approximately low-rank, which is a desirable property for neural networks.\n4. The numerical experiment on classifying two digits from the MNIST dataset correlates with the theoretical findings, which provides strong evidence for the correctness of the analysis.\n\nWeaknesses:\n1. The paper assumes that the data are well-separated, which may not be the case in practice.\n2. The analysis relies on a number of technical assumptions, such as the balancedness condition and the assumption that the data points are not orthogonal to each other.\n3. The paper does not provide a clear explanation for why the alignment phase ends and the fitting phase begins.\n4. The numerical experiment is limited to a simple binary classification task and does not provide a comprehensive evaluation of the method.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed explanation for why the alignment phase ends and the fitting phase begins.\n2. The paper should be extended to include a more comprehensive evaluation of the method, including experiments on more complex datasets and tasks.\n3. The authors should provide a more detailed analysis of the implications of the technical assumptions used in the paper.\n4. The paper should be extended to include a more detailed discussion of the relationship between the alignment phase and the fitting phase.\n\nRecommendation:\nThis paper provides a significant contribution to the field of neural networks by providing a complete analysis of the dynamics of gradient flow for training a two-layer ReLU network on well-separated data with small initialization. The paper shows that the alignment phase ends and the fitting phase begins, which is a desirable property for neural networks. The numerical experiment provides strong evidence for the correctness of the analysis, and the paper provides a clear and concise presentation of the results. Therefore, I recommend this paper for publication in a top-tier conference or journal in the field of neural networks.",
        "gr_score_with": 0.8291316526610645,
        "gr_score_without": 0.7791044776119403,
        "output_without": {
            "num_tokens_scored": 335,
            "num_green_tokens": 261,
            "green_fraction": 0.7791044776119403,
            "z_score": -0.9558305460099322,
            "p_value": 0.8304210734329848
        },
        "output_with": {
            "num_tokens_scored": 357,
            "num_green_tokens": 296,
            "green_fraction": 0.8291316526610645,
            "z_score": 1.3763444622922496,
            "p_value": 0.08435750678500437
        }
    },
    {
        "title": "SIVE NEURAL NETWORK GENERATION",
        "abstract": "Spatio-temporal modeling is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPD, for spatio-temporal few-shot learning with urban knowledge transfer. Unlike conventional approaches that heavily rely on common feature extraction or intricate few-shot learning designs, our solution takes a novel approach by performing generative pre-training on a collection of neural network parameters optimized with data from source cities. We recast spatio-temporal few-shot learning as pre-training a generative diffusion model, which generates tailored neural networks guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPD employs a Transformer-based denoising diffusion model, which is model-agnostic to integrate with powerful spatio-temporal neural networks. By addressing challenges arising from data gaps and the complexity of generalizing knowledge across cities, our framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction. The implementation of our approach is available: https://github.com/tsinghua-fib-lab/GPD.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel generative pre-training framework, called GPD, for spatio-temporal few-shot learning with urban knowledge transfer. The framework leverages a Transformer-based diffusion model to generate tailored neural networks guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. The authors demonstrate that their framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction.\n\nStrengths:\n\n1. The paper presents a novel approach to address the challenges of spatio-temporal few-shot learning, which is a pressing problem in smart city applications.\n2. The authors provide a comprehensive overview of the existing literature on spatio-temporal few-shot learning and highlight the limitations of existing approaches.\n3. The proposed framework is model-agnostic, ensuring compatibility with state-of-the-art spatio-temporal prediction models.\n4. The authors conduct extensive experiments on multiple real-world datasets and demonstrate the effectiveness of their framework.\n\nWeaknesses:\n\n1. The paper assumes that the region prompts are available, which may not always be the case in real-world scenarios.\n2. The authors do not provide a detailed analysis of the hyperparameters and their impact on the performance of the framework.\n3. The paper does not discuss the scalability of the framework to large-scale datasets.\n4. The authors do not provide a detailed comparison of the computational cost of the proposed framework with existing approaches.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the hyperparameters and their impact on the performance of the framework.\n2. The paper should discuss the scalability of the framework to large-scale datasets and provide a detailed comparison of the computational cost with existing approaches.\n3. The authors should provide a more detailed explanation of the region prompts and how they are generated.\n4. The paper should discuss the potential applications of the proposed framework in real-world smart city scenarios.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to address the challenges of spatio-temporal few-shot learning. The proposed framework is model-agnostic, ensuring compatibility with state-of-the-art spatio-temporal prediction models. The authors conduct extensive experiments on multiple real-world datasets and demonstrate the effectiveness of their framework. However, the paper assumes that the region prompts are available, and the authors do not provide a detailed analysis of the hyperparameters and their impact on the performance of the framework. Therefore, I recommend that the authors address these limitations and provide a more detailed comparison of the computational cost with existing approaches.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper proposes a novel generative pre-training framework, called GPD, for spatio-temporal few-shot learning with urban knowledge transfer. The authors aim to address the challenges of data scarcity and heterogeneity in smart city applications. GPD leverages a Transformer-based diffusion model to generate tailored neural networks guided by prompts, enabling adaptability to diverse data distributions and city-specific characteristics. The authors demonstrate the effectiveness of GPD on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction, achieving superior performance compared to state-of-the-art baselines.\n\nStrengths:\n\n1. The paper presents a pioneering approach to address the challenges of data scarcity and heterogeneity in smart city applications.\n2. The authors propose a novel generative pre-training framework, GPD, which leverages a Transformer-based diffusion model to generate tailored neural networks guided by prompts.\n3. The paper provides extensive experiments on multiple real-world datasets, demonstrating the effectiveness of GPD in achieving superior performance compared to state-of-the-art baselines.\n4. The authors demonstrate the model-agnostic nature of GPD, ensuring compatibility with existing urban computing models.\n\nWeaknesses:\n\n1. The paper assumes access to rich data in source cities, which may not be feasible in real-world scenarios.\n2. The authors do not provide a thorough analysis of the prompt design and selection, which is crucial for the success of GPD.\n3. The paper focuses on two specific tasks, traffic speed prediction and crowd flow prediction, and it is unclear how GPD would perform on other spatio-temporal prediction tasks.\n4. The authors do not provide a thorough discussion on the potential limitations and challenges of GPD, such as the need for large-scale computational resources and the potential for overfitting.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more thorough analysis of the prompt design and selection, including the selection of features and the design of the prompt vector.\n2. The paper should provide more details on the experimental setup, including the specific datasets used and the evaluation metrics employed.\n3. The authors should investigate the generalizability of GPD to other spatio-temporal prediction tasks and provide more details on the potential limitations and challenges of the approach.\n4. The paper should provide more details on the computational resources required to train and deploy GPD, and discuss potential strategies for reducing the computational burden.\n\nRecommendation:\n\nI recommend acceptance of this paper for publication in the conference proceedings. The authors have presented a novel and effective approach to addressing the challenges of data scarcity and heterogeneity in smart city applications. The paper provides extensive experiments and demonstrates the effectiveness of GPD on multiple real-world datasets. While there are some limitations and areas for improvement, the paper represents a significant advancement in urban transfer learning and has the potential to revolutionize smart city applications in data-scarce environments.\n\n$$$",
        "gr_score_with": 0.8442622950819673,
        "gr_score_without": 0.7965517241379311,
        "output_without": {
            "num_tokens_scored": 290,
            "num_green_tokens": 231,
            "green_fraction": 0.7965517241379311,
            "z_score": -0.1465377880385817,
            "p_value": 0.5582515693362942
        },
        "output_with": {
            "num_tokens_scored": 366,
            "num_green_tokens": 309,
            "green_fraction": 0.8442622950819673,
            "z_score": 2.1172424783191763,
            "p_value": 0.017119636982646142
        }
    },
    {
        "title": "SPURIOUS FEATURES IN PROMPT DESIGN or: How I learned to start worrying about prompt formatting",
        "abstract": "As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FORMATSPREAD, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights1. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces FORMATSPREAD, an algorithm designed to efficiently search the space of plausible prompt formats under a user-specified computational budget. The authors analyze the sensitivity of widely used, open-source language models (LLMs) to prompt formatting choices, finding that performance spreads across tasks, models, and number of few-shot examples. They propose a grammar that defines the space of plausible prompt formats and semantic equivalence between formats, allowing for the efficient exploration of the space. The authors demonstrate that FORMATSPREAD can be used to estimate the performance spread across prompt formatting choices, and that this spread is large regardless of model choice, even when increasing model size, number of few-shots, or using instruction tuning.\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of the sensitivity of LLMs to prompt formatting choices, which is a crucial aspect of using these models in real-world applications.\n2. The authors propose a novel grammar that defines the space of plausible prompt formats and semantic equivalence between formats, which allows for the efficient exploration of the space.\n3. The paper introduces FORMATSPREAD, an algorithm that can be used to estimate the performance spread across prompt formatting choices, which is a valuable tool for researchers and practitioners.\n4. The authors provide a thorough evaluation of the algorithm on a large set of tasks and models, demonstrating its effectiveness in estimating performance spread.\n\nWeaknesses:\n\n1. The paper assumes that the grammar is manually constructed, which may not be practical for large-scale applications.\n2. The authors do not provide a detailed analysis of the computational cost of FORMATSPREAD, which may be a limitation for large-scale applications.\n3. The paper focuses on classification tasks and does not explore other types of tasks, such as open-ended text generation.\n4. The authors do not provide a detailed analysis of the robustness of FORMATSPREAD to different types of perturbations, such as adversarial attacks.\n\nSuggestions for Improvement:\n\n1. Develop a more automated method for constructing the grammar, which could be based on machine learning algorithms or other techniques.\n2. Provide a more detailed analysis of the computational cost of FORMATSPREAD and explore ways to reduce it.\n3. Explore the use of FORMATSPREAD on other types of tasks, such as open-ended text generation.\n4. Conduct a more thorough analysis of the robustness of FORMATSPREAD to different types of perturbations, such as adversarial attacks.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the field of natural language processing and language modeling, and the authors should be commended for their thorough analysis of the sensitivity of LLMs to prompt formatting choices. However, to make the paper more impactful, the authors should address the limitations mentioned above and provide a more detailed analysis of the algorithm's computational cost and robustness.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper introduces FORMATSPREAD, an algorithm designed to efficiently search the space of plausible prompt formats under a user-specified computational budget. The authors analyze the sensitivity of large language models (LLMs) to prompt formatting choices, finding significant performance spread across tasks, even with increased model size, number of few-shot examples, or instruction tuning. They propose a grammar to define the space of plausible prompt formats, which is used to quantify model sensitivity in terms of performance range in a target task across the space of equivalent prompt formats. The authors also present a suite of analyses to characterize model sensitivity to formatting, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of the sensitivity of LLMs to prompt formatting choices, which is a crucial aspect of effective model usage.\n2. The authors propose a well-defined grammar to describe the space of plausible prompt formats, which allows for a systematic analysis of model sensitivity.\n3. The study demonstrates the significance of considering performance spread across plausible formats, rather than relying on a single format.\n4. The authors present a suite of analyses to characterize model sensitivity to formatting, providing a deeper understanding of the underlying mechanisms.\n\nWeaknesses:\n\n1. The study is limited to a specific set of LLMs and tasks, which may not be representative of all possible scenarios.\n2. The authors rely on a manual construction of the grammar, which may not be feasible for more complex tasks or formats.\n3. The study assumes that the grammar is complete and accurate, which may not be the case in all situations.\n4. The authors do not provide a detailed analysis of the computational cost of FORMATSPREAD, which may be a limitation in certain scenarios.\n\nSuggestions for Improvement:\n\n1. Expand the study to include a broader range of LLMs, tasks, and formats to increase the generalizability of the findings.\n2. Develop a more automated method for constructing the grammar, which could reduce the reliance on manual construction.\n3. Provide a more detailed analysis of the computational cost of FORMATSPREAD, including the time and resources required to run the algorithm.\n4. Consider incorporating additional analysis methods, such as sensitivity analysis or uncertainty quantification, to provide a more comprehensive understanding of model sensitivity.\n\nRecommendation:\n\nThe paper provides a significant contribution to the field of LLMs, highlighting the importance of considering performance spread across plausible formats. While the study has some limitations, the proposed grammar and analysis methods are well-defined and effective. I recommend this paper for publication in a top-tier conference, with the understanding that the authors address the suggested improvements in future work.\n\nOverall, the paper is well-written, and the authors provide a clear and concise presentation of their findings. The study is well-structured, and the authors effectively communicate the significance of their results. However, the limitations of the study should be addressed in future work to increase the generalizability and robustness of the findings.",
        "gr_score_with": 0.8654292343387471,
        "gr_score_without": 0.8248587570621468,
        "output_without": {
            "num_tokens_scored": 354,
            "num_green_tokens": 292,
            "green_fraction": 0.8248587570621468,
            "z_score": 1.1695665251464868,
            "p_value": 0.12108772755073455
        },
        "output_with": {
            "num_tokens_scored": 431,
            "num_green_tokens": 373,
            "green_fraction": 0.8654292343387471,
            "z_score": 3.3961495259705092,
            "p_value": 0.0003417050751323205
        }
    },
    {
        "title": "ERROR-FREE DIFFERENTIABLE SWAP FUNCTIONS",
        "abstract": "Sorting is a fundamental operation of all computer systems, having been a longstanding significant research topic. Beyond the problem formulation of traditional sorting algorithms, we consider sorting problems for more abstract yet expressive inputs, e.g., multi-digit images and image fragments, through a neural sorting network. To learn a mapping from a high-dimensional input to an ordinal variable, the differentiability of sorting networks needs to be guaranteed. In this paper we define a softening error by a differentiable swap function, and develop an error-free swap function that holds a non-decreasing condition and differentiability. Furthermore, a permutation-equivariant Transformer network with multi-head attention is adopted to capture dependency between given inputs and also leverage its model capacity with self-attention. Experiments on diverse sorting benchmarks show that our methods perform better than or comparable to baseline methods.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel sorting network that learns to sort high-dimensional inputs, such as multi-digit images and image fragments, using a neural network-based sorting network with an error-free differentiable swap function (DSF). The proposed method is differentiable, which allows for effective training with a gradient-based learning scheme. The authors also propose an error-free DSF that resolves the error accumulation problem induced by a soft DSF. The paper demonstrates the effectiveness of the proposed method on diverse sorting benchmarks.\n\nStrengths:\n\n* The paper proposes a novel sorting network that learns to sort high-dimensional inputs.\n* The proposed method is differentiable, which allows for effective training with a gradient-based learning scheme.\n* The authors provide a detailed analysis of the error accumulation problem and propose an error-free DSF to resolve it.\n* The paper demonstrates the effectiveness of the proposed method on diverse sorting benchmarks.\n\nWeaknesses:\n\n* The paper assumes that the inputs are high-dimensional and unordered, which may not be the case in all real-world applications.\n* The proposed method may not be effective for sorting low-dimensional inputs or inputs with clear ordering information.\n* The paper does not provide a detailed analysis of the computational complexity of the proposed method.\n* The paper assumes that the supervision is provided in the form of permutation matrices, which may not be available in all cases.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the computational complexity of the proposed method.\n* The paper should discuss the limitations of the proposed method and provide examples of real-world applications where it may not be effective.\n* The authors should provide more experimental results to demonstrate the effectiveness of the proposed method on a wider range of benchmarks.\n* The paper should discuss the potential negative societal impact of the proposed method, such as the potential for bias in the sorting process.\n\nRecommendation:\n\nThe paper proposes a novel sorting network that learns to sort high-dimensional inputs using a neural network-based sorting network with an error-free DSF. The proposed method is differentiable, which allows for effective training with a gradient-based learning scheme. The paper demonstrates the effectiveness of the proposed method on diverse sorting benchmarks. However, the paper assumes that the inputs are high-dimensional and unordered, which may not be the case in all real-world applications. Therefore, I recommend that the authors provide a more detailed analysis of the computational complexity of the proposed method and discuss the limitations of the proposed method in the future.\n\nOverall, the paper is well-written and provides a clear and concise introduction to the problem of sorting high-dimensional inputs. The proposed method is novel and effective, and the paper provides a detailed analysis of the error accumulation problem and proposes an error-free DSF to resolve it. However, the paper assumes that the inputs are high-dimensional and unordered, which may not be the case in all real-world applications. Therefore, I recommend that the authors provide a more detailed analysis of the computational complexity of the proposed method and discuss the limitations of the proposed method in the future.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel sorting network that learns to sort high-dimensional inputs, such as multi-digit images and image fragments, using a neural network-based sorting network with an error-free differentiable swap function (DSF). The proposed method is differentiable, which allows for end-to-end training using gradient-based optimization. The authors also propose a permutation-equivariant Transformer network with multi-head attention to capture dependency between high-dimensional inputs. Experiments on diverse sorting benchmarks show that the proposed method performs better than or comparable to baseline methods.\n\nStrengths:\n\n1. The paper proposes a novel sorting network that learns to sort high-dimensional inputs using a neural network-based sorting network with an error-free DSF.\n2. The proposed method is differentiable, which allows for end-to-end training using gradient-based optimization.\n3. The authors also propose a permutation-equivariant Transformer network with multi-head attention to capture dependency between high-dimensional inputs.\n4. Experiments on diverse sorting benchmarks show that the proposed method performs better than or comparable to baseline methods.\n\nWeaknesses:\n\n1. The paper assumes that the input data is high-dimensional, which may not be the case for all sorting tasks.\n2. The proposed method may not be suitable for sorting tasks with large input sizes.\n3. The paper does not provide a detailed analysis of the computational complexity of the proposed method.\n4. The paper assumes that the DSF is differentiable, which may not be the case for all DSFs.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational complexity of the proposed method.\n2. The authors should experiment with different DSFs to see how they affect the performance of the proposed method.\n3. The authors should provide more details on how the permutation-equivariant Transformer network with multi-head attention is trained.\n4. The authors should provide a more detailed analysis of the performance of the proposed method on sorting tasks with large input sizes.\n\nRecommendation:\n\nThe paper proposes a novel sorting network that learns to sort high-dimensional inputs using a neural network-based sorting network with an error-free DSF. The proposed method is differentiable, which allows for end-to-end training using gradient-based optimization. Experiments on diverse sorting benchmarks show that the proposed method performs better than or comparable to baseline methods. However, the paper assumes that the input data is high-dimensional, which may not be the case for all sorting tasks. Therefore, the paper requires further experimentation to validate its performance on sorting tasks with different input sizes.\n\nOverall, the paper is well-written and provides a clear explanation of the proposed method. However, it requires further experimentation to validate its performance on sorting tasks with different input sizes.\n\nRecommendation: Accept\n\n$$$",
        "gr_score_with": 0.9033613445378151,
        "gr_score_without": 0.8178438661710037,
        "output_without": {
            "num_tokens_scored": 269,
            "num_green_tokens": 220,
            "green_fraction": 0.8178438661710037,
            "z_score": 0.7319001108630064,
            "p_value": 0.23211476925894653
        },
        "output_with": {
            "num_tokens_scored": 238,
            "num_green_tokens": 215,
            "green_fraction": 0.9033613445378151,
            "z_score": 3.986646844956435,
            "p_value": 3.350682915924868e-05
        }
    },
    {
        "title": "WEAKER MVI CONDITION: EXTRAGRADIENT METH-",
        "abstract": "This paper proposes a new framework of algorithms that is extended from the celebrated extragradient algorithm. The min-max problem has attracted increasing attention because of its applications in machine learning tasks such as generative adversarial networks (GANs) training. While there has been exhaustive research on convex-concave setting, problem of nonconvex-nonconcave setting faces many challenges, such as convergence to limit cycles. Given that general min-max optimization has been found to be intractable, recent research efforts have shifted towards tackling structured problems. One of these follows the weak Minty variational inequality (weak MVI), which is motivated by relaxing Minty variational inequality (MVI) without compromising convergence guarantee of extragradient algorithm. Existing extragradient-type algorithms involve one exploration step and one update step per iteration. We analyze the algorithms with multiple exploration steps and show that current assumption can be further relaxed when more exploration is introduced. Furthermore, we design an adaptive algorithm that explores until the optimal improvement is achieved. This process exploits information from the whole trajectory and effectively tackles cyclic behaviors.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThis paper proposes a new framework for extragradient-type algorithms, which extends the celebrated extragradient algorithm. The authors focus on the weak Minty variational inequality (weak MVI) problem, which is a relaxation of the Minty variational inequality without compromising convergence guarantees. They analyze the algorithms with multiple exploration steps and show that the current assumption can be further relaxed when more exploration is introduced. Furthermore, they design an adaptive algorithm that explores until the optimal improvement is achieved. The paper provides bound analysis on the framework of multi-step extrapolation EG+ algorithms, relaxes the condition \u03c1 > \u22121/2L to \u03c1 > \u2212(1\u22121/e)/L, and captures past algorithms as special cases.\n\nStrengths:\n\n1. The paper provides a new framework for extragradient-type algorithms, which extends the celebrated extragradient algorithm.\n2. The authors analyze the algorithms with multiple exploration steps and show that the current assumption can be further relaxed when more exploration is introduced.\n3. The paper designs an adaptive algorithm that explores until the optimal improvement is achieved.\n4. The paper provides bound analysis on the framework of multi-step extrapolation EG+ algorithms.\n5. The authors relax the condition \u03c1 > \u22121/2L to \u03c1 > \u2212(1\u22121/e)/L, which is a significant improvement.\n\nWeaknesses:\n\n1. The paper assumes that the function f(x,y) is smooth and possibly nonconvex-nonconcave, which may not be the case in practice.\n2. The authors use a tolerance \u03b52 to prevent the algorithm from incorrect convergence, which may not be necessary in some cases.\n3. The paper does not provide a detailed comparison with other algorithms, such as gradient descent ascent and stochastic extragradient.\n4. The authors do not provide a theoretical analysis of the convergence rate of the adaptive algorithm.\n5. The paper assumes that the weak MVI condition holds, which may not be the case in practice.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed comparison with other algorithms, such as gradient descent ascent and stochastic extragradient.\n2. Theoretically analyze the convergence rate of the adaptive algorithm.\n3. Relax the assumption that the function f(x,y) is smooth and possibly nonconvex-nonconcave.\n4. Provide a more detailed explanation of the tolerance \u03b52 and its role in preventing incorrect convergence.\n5. Provide more experimental results to demonstrate the effectiveness of the adaptive algorithm.\n\nRecommendation:\n\nThis paper provides a new framework for extragradient-type algorithms, which extends the celebrated extragradient algorithm. The authors analyze the algorithms with multiple exploration steps and show that the current assumption can be further relaxed when more exploration is introduced. The paper provides bound analysis on the framework of multi-step extrapolation EG+ algorithms, relaxes the condition \u03c1 > \u22121/2L to \u03c1 > \u2212(1\u22121/e)/L, and captures past algorithms as special cases. However, the paper assumes that the function f(x,y) is smooth and possibly nonconvex-nonconcave, and uses a tolerance \u03b52 to prevent the algorithm from incorrect convergence. Therefore, I recommend that the authors provide a more detailed comparison with other algorithms, theoretically analyze the convergence rate of the adaptive algorithm, relax the assumption that the function f(x,y) is smooth and possibly nonconvex-nonconcave, provide a more detailed explanation of the tolerance \u03b52, and provide more experimental results to demonstrate the effectiveness of the adaptive algorithm.\n\nOverall, this paper provides a significant contribution to the field of optimization and machine learning, and I recommend that it be published in a reputable journal.\n\nRating: 8/10\n\nRecommendation: Publish",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework for solving weak Minty variational inequality (MVI) type saddle point problems. The framework generalizes the extragradient (EG) algorithm to multi-step cases and provides a bound analysis for the convergence of the algorithm. The authors relax the condition \u03c1 > \u22121/2L to \u03c1 > \u2212(1\u22121/e)/L, which allows for a wider range of problem parameters. The paper also proposes an adaptive algorithm that explores aggressively and can handle problems with limit cycles.\n\nStrengths:\n\n1. The paper provides a novel framework for solving weak MVI type saddle point problems.\n2. The authors relax the condition \u03c1 > \u22121/2L to \ufffdho > \u2212(1\u22121/e)/L, which allows for a wider range of problem parameters.\n3. The paper proposes an adaptive algorithm that explores aggressively and can handle problems with limit cycles.\n4. The authors provide a bound analysis for the convergence of the algorithm.\n5. The paper shows that the proposed algorithm can outperform existing methods in certain cases.\n\nWeaknesses:\n\n1. The paper assumes that the problem parameters are known, which may not be the case in practice.\n2. The authors do not provide a clear explanation of how to select the parameters of the algorithm.\n3. The paper does not provide a detailed analysis of the computational complexity of the algorithm.\n4. The authors do not compare the proposed algorithm with other methods that have been designed for solving weak MVI type saddle point problems.\n5. The paper does not provide a clear explanation of how to handle cases where the problem parameters are not known.\n\nSuggestions for Improvement:\n\n1. Provide a clear explanation of how to select the parameters of the algorithm.\n2. Provide a detailed analysis of the computational complexity of the algorithm.\n3. Compare the proposed algorithm with other methods that have been designed for solving weak MVI type saddle point problems.\n4. Provide a clear explanation of how to handle cases where the problem parameters are not known.\n5. Provide more experimental results to demonstrate the effectiveness of the proposed algorithm.\n\nRecommendation:\n\nThe paper provides a novel framework for solving weak MVI type saddle point problems and proposes an adaptive algorithm that can handle problems with limit cycles. The authors relax the condition \u03c1 > \u22121/2L to \u03c1 > \u2212(1\u22121/e)/L, which allows for a wider range of problem parameters. The paper provides a bound analysis for the convergence of the algorithm. However, the authors do not provide a clear explanation of how to select the parameters of the algorithm, do not provide a detailed analysis of the computational complexity of the algorithm, do not compare the proposed algorithm with other methods that have been designed for solving weak MVI type saddle point problems, and do not provide a clear explanation of how to handle cases where the problem parameters are not known. Therefore, I recommend that the authors revise the paper to address these issues.\n\nOverall, the paper has the potential to be a high-quality contribution to the field of optimization, but it requires revisions to address the issues mentioned above.",
        "gr_score_with": 0.781021897810219,
        "gr_score_without": 0.7380952380952381,
        "output_without": {
            "num_tokens_scored": 336,
            "num_green_tokens": 248,
            "green_fraction": 0.7380952380952381,
            "z_score": -2.836513560259239,
            "p_value": 0.9977195479232526
        },
        "output_with": {
            "num_tokens_scored": 274,
            "num_green_tokens": 214,
            "green_fraction": 0.781021897810219,
            "z_score": -0.7850914160280386,
            "p_value": 0.7838000138806724
        }
    },
    {
        "title": "TAIL: TASK-SPECIFIC ADAPTERS FOR IMITATION LEARNING WITH LARGE PRETRAINED MODELS",
        "abstract": "The full potential of large pretrained models remains largely untapped in control domains like robotics. This is mainly due to data scarcity and computational challenges associated with training or fine-tuning large models for such applications. Prior work mainly emphasizes either effective pretraining of large models for decision-making or single-task adaptation. But real-world problems will require data-efficient, continual adaptation for new control tasks. Recognizing these constraints, we introduce TAIL (Task-specific Adapters for Imitation Learning), a framework for efficient adaptation to a stream of new control tasks. Inspired by recent advancements in parameter-efficient fine-tuning in language domains, we explore efficient fine-tuning techniques\u2014e.g., Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA)\u2014in TAIL to adapt large pretrained models for new tasks with limited demonstration data. Our extensive experiments comparing prevalent parameter-efficient fine-tuning techniques and adaptation baselines suggest that TAIL with LoRA can achieve the best post-adaptation performance with only 1% of the trainable parameters of full fine-tuning while avoiding catastrophic forgetting and preserving adaptation plasticity in continual learning settings.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Task-specific Adapters for Imitation Learning (TAIL), a framework for efficient adaptation of large pretrained models to new control tasks. The authors explore parameter-efficient fine-tuning (PEFT) techniques, specifically Low-Rank Adaptation (LoRA), and demonstrate their potential in enhancing adaptation efficiency, mitigating catastrophic forgetting, and ensuring robust performance across diverse tasks. The proposed framework is evaluated on the LIBERO robotic manipulation continual learning benchmark, showcasing its effectiveness in adapting to new tasks while preserving performance on previous tasks.\n\nStrengths:\n\n1. The paper proposes a novel framework for efficient adaptation of large pretrained models, addressing the challenges of catastrophic forgetting and overfitting in continual learning scenarios.\n2. The authors provide a comprehensive exploration of PEFT techniques, including LoRA, Bottleneck Adapter, and Prefix Tuning, and demonstrate their potential in enhancing adaptation efficiency and mitigating catastrophic forgetting.\n3. The paper presents extensive experimental results on the LIBERO benchmark, showcasing the effectiveness of TAIL in adapting to new tasks while preserving performance on previous tasks.\n4. The authors provide a thorough analysis of the strengths and limitations of each adaptation strategy, providing valuable insights for future research.\n\nWeaknesses:\n\n1. The paper assumes a fixed architecture for the pretrained model, which may not be optimal for all applications.\n2. The authors do not provide a detailed comparison of the computational efficiency of TAIL with other adaptation methods.\n3. The paper assumes a fixed dataset for pretraining, which may not be representative of all real-world scenarios.\n4. The authors do not provide a detailed analysis of the robustness of TAIL to noisy or incomplete data.\n\nSuggestions for Improvement:\n\n1. Investigate the impact of varying the architecture of the pretrained model on the performance of TAIL.\n2. Conduct a more detailed comparison of the computational efficiency of TAIL with other adaptation methods.\n3. Evaluate the robustness of TAIL to noisy or incomplete data.\n4. Explore the application of TAIL to other domains, such as reinforcement learning.\n\nRecommendation:\n\nThe paper presents a novel and effective framework for efficient adaptation of large pretrained models in continual learning scenarios. The authors provide a comprehensive exploration of PEFT techniques and demonstrate their potential in enhancing adaptation efficiency and mitigating catastrophic forgetting. While there are some limitations to the paper, the proposed framework has the potential to significantly impact the field of artificial intelligence and robotics. Therefore, I recommend acceptance of the paper.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Task-specific Adapters for Imitation Learning (TAIL), a framework for efficient adaptation of large pretrained decision-making models to a stream of new control tasks. TAIL incorporates lightweight adapter modules into pretrained decision-making models and comprehensively compares efficient adaptation techniques implemented in TAIL in a continual imitation learning setting. The authors explore parameter-efficient adaptation techniques such as Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA) to adapt large pretrained models for new tasks with limited demonstration data. The experiments demonstrate that TAIL with LoRA can achieve the best post-adaptation performance with only 1% of the trainable parameters of full fine-tuning while avoiding catastrophic forgetting and preserving adaptation plasticity.\n\nStrengths:\n\n* The paper proposes a novel and efficient framework for adapting large pretrained decision-making models to new control tasks.\n* The authors provide a comprehensive comparison of different efficient adaptation techniques, including Bottleneck Adapters, P-Tuning, and LoRA.\n* The experiments demonstrate the effectiveness of TAIL with LoRA in achieving state-of-the-art performance on the LIBERO benchmark.\n* The authors provide a thorough analysis of the advantages and limitations of each adaptation strategy.\n\nWeaknesses:\n\n* The paper assumes a specific pretraining architecture and frozen CLIP visual/language encoders, which may limit its generalizability to other architectures.\n* The experiments are conducted on a specific benchmark (LIBERO) and may not be representative of other tasks or environments.\n* The authors do not provide a detailed comparison of TAIL with other state-of-the-art adaptation methods.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed comparison of TAIL with other state-of-the-art adaptation methods to demonstrate its superiority.\n* The experiments should be conducted on a more diverse set of tasks and environments to demonstrate the generalizability of TAIL.\n* The authors should provide a more detailed analysis of the advantages and limitations of each adaptation strategy, including a discussion of the trade-offs between efficiency, accuracy, and robustness.\n\nRecommendation:\n\nThe paper proposes a novel and efficient framework for adapting large pretrained decision-making models to new control tasks. The experiments demonstrate the effectiveness of TAIL with LoRA in achieving state-of-the-art performance on the LIBERO benchmark. However, the paper assumes a specific pretraining architecture and frozen CLIP visual/language encoders, which may limit its generalizability to other architectures. The authors should provide a more detailed comparison of TAIL with other state-of-the-art adaptation methods and conduct experiments on a more diverse set of tasks and environments to demonstrate the generalizability of TAIL. With these improvements, the paper has the potential to make a significant contribution to the field of adaptation and learning.\n\n$$$",
        "gr_score_with": 0.8403614457831325,
        "gr_score_without": 0.8165137614678899,
        "output_without": {
            "num_tokens_scored": 327,
            "num_green_tokens": 267,
            "green_fraction": 0.8165137614678899,
            "z_score": 0.7468249567411807,
            "p_value": 0.2275846144467572
        },
        "output_with": {
            "num_tokens_scored": 332,
            "num_green_tokens": 279,
            "green_fraction": 0.8403614457831325,
            "z_score": 1.8388139823702403,
            "p_value": 0.032971275477981114
        }
    },
    {
        "title": "ZERO-MEAN REGULARIZED SPECTRAL CONTRASTIVE LEARNING: IMPLICITLY MITIGATING WRONG CON-",
        "abstract": "Contrastive learning has emerged as a popular paradigm of self-supervised learning that learns representations by encouraging representations of positive pairs to be similar while representations of negative pairs to be far apart. The spectral contrastive loss, in synergy with the notion of positive-pair graphs, offers valuable theoretical insights into the empirical successes of contrastive learning. In this paper, we propose incorporating an additive factor into the term of spectral contrastive loss involving negative pairs. This simple modification can be equivalently viewed as introducing a regularization term that enforces the mean of representations to be zero, which thus is referred to as zero-mean regularization. It intuitively relaxes the orthogonality of representations between negative pairs and implicitly alleviates the adverse effect of wrong connections in the positive-pair graph, leading to better performance and robustness. To clarify this, we thoroughly investigate the role of zero-mean regularized spectral contrastive loss in both unsupervised and supervised scenarios with respect to theoretical analysis and quantitative evaluation. These results highlight the potential of zero-mean regularized spectral contrastive learning to be a promising approach in various tasks.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel regularization technique, zero-mean regularization, for spectral contrastive learning (SpeCL). The authors argue that SpeCL has two limitations: (i) it requires the orthogonality of representations between negative pairs, and (ii) it does not account for wrong connections in the positive-pair graph. To address these limitations, they introduce an additive factor, \u03c4, into the term of SpeCL involving negative pairs. This modification is equivalent to regularizing the mean of representations to be zero, referred to as zero-mean regularization. The authors provide theoretical investigations in both unsupervised and supervised scenarios, demonstrating the effectiveness of zero-mean regularization in mitigating adverse effects of wrong connections.\n\nStrengths:\n\n1. The paper provides a clear and concise introduction to the problem and the proposed solution.\n2. The authors provide a thorough theoretical analysis of the benefits of zero-mean regularization in both unsupervised and supervised scenarios.\n3. The paper presents extensive experimental results to verify the effectiveness of zero-mean regularization on various benchmark datasets.\n4. The authors provide a clear explanation of the implications of zero-mean regularization on the structure of the learned representations.\n\nWeaknesses:\n\n1. The paper assumes a specific form of the positive-pair graph, which may not be realistic in all scenarios.\n2. The authors do not provide a clear explanation of how to choose the value of \u03c4 in practice.\n3. The paper does not discuss the computational complexity of the proposed method.\n4. The authors do not provide a comparison with other regularization techniques.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of how to choose the value of \u03c4 in practice.\n2. Discuss the computational complexity of the proposed method and provide a comparison with other regularization techniques.\n3. Provide more experimental results on different datasets and scenarios to further verify the effectiveness of zero-mean regularization.\n4. Discuss the implications of zero-mean regularization on the interpretability of the learned representations.\n\nRecommendation:\n\nThe paper presents a novel and effective regularization technique for spectral contrastive learning. The authors provide a thorough theoretical analysis and extensive experimental results to demonstrate the effectiveness of zero-mean regularization. The paper is well-written and easy to follow. However, the authors should address the limitations and suggestions for improvement mentioned above to make the paper more comprehensive and robust. I recommend this paper for publication in a top-tier conference in the field of machine learning.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper proposes zero-mean regularization as an additive factor to the spectral contrastive loss (SpeCL) involving negative pairs. The modification is equivalent to enforcing the mean of representations to be zero, which is referred to as zero-mean regularization. The authors argue that this modification relaxes the orthogonality of representations between negative pairs and alleviates the adverse effect of wrong connections in the positive-pair graph, leading to better performance and robustness. Theoretical investigations in both unsupervised and supervised scenarios demonstrate the effectiveness of zero-mean regularization in mitigating adverse effects of wrong connections. The authors also provide experiments on benchmark datasets to verify the effectiveness of zero-mean regularization in contrastive learning, supervised classification, unsupervised domain adaptation, and learning with noisy labels.\n\nStrengths:\n\n1. The paper provides a clear and concise introduction to the spectral contrastive loss (SpeCL) and its limitations.\n2. The authors propose a simple yet effective modification to SpeCL, which is equivalent to enforcing the mean of representations to be zero.\n3. Theoretical investigations in both unsupervised and supervised scenarios demonstrate the effectiveness of zero-mean regularization in mitigating adverse effects of wrong connections.\n4. The authors provide experiments on benchmark datasets to verify the effectiveness of zero-mean regularization in contrastive learning, supervised classification, unsupervised domain adaptation, and learning with noisy labels.\n5. The paper provides a clear and concise presentation of the main contributions, which makes it easy to follow and understand.\n\nWeaknesses:\n\n1. The paper assumes that the representations learned by SpeCL exhibit the Neural Collapse phenomenon, which may not be true in all cases.\n2. The authors do not provide a clear explanation of why zero-mean regularization is effective in mitigating label noise.\n3. The experiments on benchmark datasets are limited to a few datasets, and more experiments on different datasets would be helpful to verify the effectiveness of zero-mean regularization.\n4. The paper does not provide a clear comparison of zero-mean regularization with other state-of-the-art methods.\n5. The authors do not provide a clear discussion of the limitations of zero-mean regularization and potential future directions.\n\nSuggestions for Improvement:\n\n1. Provide more experiments on different datasets to verify the effectiveness of zero-mean regularization.\n2. Provide a clear comparison of zero-mean regularization with other state-of-the-art methods.\n3. Provide a clear discussion of the limitations of zero-mean regularization and potential future directions.\n4. Provide a clear explanation of why zero-mean regularization is effective in mitigating label noise.\n5. Provide more details on the theoretical investigations, such as the proof of Theorem 3.1 and Theorem 3.3.\n\nRecommendation:\n\nThe paper proposes a simple yet effective modification to SpeCL, which is equivalent to enforcing the mean of representations to be zero. Theoretical investigations in both unsupervised and supervised scenarios demonstrate the effectiveness of zero-mean regularization in mitigating adverse effects of wrong connections. The experiments on benchmark datasets verify the effectiveness of zero-mean regularization in contrastive learning, supervised classification, unsupervised domain adaptation, and learning with noisy labels. Therefore, I recommend accepting this paper for publication in the conference.",
        "gr_score_with": 0.7948717948717948,
        "gr_score_without": 0.7477477477477478,
        "output_without": {
            "num_tokens_scored": 333,
            "num_green_tokens": 249,
            "green_fraction": 0.7477477477477478,
            "z_score": -2.383472880435936,
            "p_value": 0.9914249259683259
        },
        "output_with": {
            "num_tokens_scored": 312,
            "num_green_tokens": 248,
            "green_fraction": 0.7948717948717948,
            "z_score": -0.22617732000308338,
            "p_value": 0.5894682452038101
        }
    },
    {
        "title": "RDESIGN: HIERARCHICAL DATA-EFFICIENT REPRE-",
        "abstract": "While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules\u2019 primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Moreover, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both clusterlevel and sample-level to fully leverage the limited data. By constraining data representations within a limited hyperspherical space, the intrinsic relationships between data points could be explicitly imposed. Moreover, we incorporated extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. Extensive experiments demonstrate the effectiveness of our proposed method, providing a reliable baseline for future RNA design tasks. The source code and benchmark dataset are available at github.com/A4Bio/RDesign.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel data-driven RNA design pipeline called RDesign, which aims to systematically address the challenge of designing RNA sequences that reliably fold into specified tertiary structures. The authors compile a large, well-curated benchmark dataset and design a comprehensive structural modeling approach to represent the complex RNA tertiary structure. They introduce a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster-level and sample-level. Additionally, they incorporate extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. The proposed method, RDesign, demonstrates effectiveness in designing RNA sequences with structures similar to the target structure.\n\nStrengths:\n\n1. The paper addresses a challenging problem in RNA design, which is to design RNA sequences that reliably fold into specified tertiary structures.\n2. The authors propose a novel hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster-level and sample-level.\n3. The incorporation of extracted secondary structures with base pairs as prior knowledge is a novel and effective approach to facilitate the RNA design process.\n4. The authors demonstrate the effectiveness of their proposed method, RDesign, through extensive experiments and visualization examples.\n\nWeaknesses:\n\n1. The paper relies heavily on the availability of a large, well-curated benchmark dataset, which may not be readily available for other researchers to reproduce the results.\n2. The authors do not provide a clear explanation of how the proposed method can be extended to design RNA sequences with complex tertiary structures.\n3. The paper does not provide a detailed analysis of the computational efficiency of the proposed method.\n4. The authors do not provide a clear evaluation of the generalizability of the proposed method to other RNA design tasks.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of how the proposed method can be extended to design RNA sequences with complex tertiary structures.\n2. The authors should provide a more detailed analysis of the computational efficiency of the proposed method.\n3. The authors should provide a clear evaluation of the generalizability of the proposed method to other RNA design tasks.\n4. The authors should provide a clear explanation of how the proposed method can be used to design RNA sequences with specific functional properties.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to RNA design, which demonstrates the potential to design RNA sequences with structures similar to the target structure. However, the paper has some limitations that need to be addressed, such as the availability of a large, well-curated benchmark dataset and the computational efficiency of the proposed method. Overall, the paper is well-written and provides a clear explanation of the proposed method and its effectiveness. I recommend this paper for publication in a top-tier conference in the field of bioinformatics and computational biology.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes a novel data-driven RNA design pipeline, RDESIGN, which tackles the challenging task of designing RNA sequences that fold into a specified tertiary structure. The authors argue that existing approaches, primarily developed for protein design, are not directly applicable to RNA due to its structural complexity and data scarcity. To address this, the authors design a comprehensive RNA tertiary structure modeling approach and a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster and sample levels. The proposed method incorporates extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. The authors demonstrate the effectiveness of RDESIGN through extensive experiments across standard RNA design benchmarks and generalization ability assessments, providing a reliable pipeline for future research in this field.\n\nStrengths:\n\n1. The paper tackles a challenging problem in RNA design, which is of significant interest due to the importance of RNA molecules in various cellular processes.\n2. The authors provide a comprehensive RNA tertiary structure modeling approach, which is essential for understanding RNA functional mechanisms and discovering RNA-targeted therapies.\n3. The proposed hierarchical data-efficient representation learning framework is novel and effective, leveraging contrastive learning to learn structural representations.\n4. The incorporation of secondary structure information as prior knowledge is a significant improvement over existing approaches.\n5. The authors provide extensive experimental results, demonstrating the effectiveness of RDESIGN across various RNA design benchmarks and generalization ability assessments.\n\nWeaknesses:\n\n1. The paper assumes that the tertiary structure of the RNA molecule is known, which is not always the case.\n2. The proposed method relies heavily on the availability of high-quality RNA tertiary structure data, which is scarce.\n3. The authors do not provide a clear explanation of how the proposed method can be scaled up to design longer RNA sequences.\n4. The paper does not discuss the potential limitations and challenges of applying the proposed method to real-world RNA design problems.\n5. The authors do not provide a thorough analysis of the computational cost and efficiency of the proposed method.\n\nSuggestions for Improvement:\n\n1. The authors should provide more details on how to obtain the tertiary structure of the RNA molecule, which is a critical component of the proposed method.\n2. The authors should discuss the potential limitations and challenges of applying the proposed method to real-world RNA design problems.\n3. The authors should provide a more thorough analysis of the computational cost and efficiency of the proposed method.\n4. The authors should consider incorporating more advanced machine learning techniques, such as transfer learning and ensemble methods, to improve the performance of the proposed method.\n5. The authors should provide more details on how to extend the proposed method to design longer RNA sequences.\n\nRecommendation:\n\nThis paper proposes a novel and effective approach to RNA design, which tackles a challenging problem of significant interest. The proposed method incorporates secondary structure information as prior knowledge and learns structural representations through contrastive learning. While there are some limitations and challenges associated with the proposed method, the authors provide extensive experimental results, demonstrating its effectiveness across various RNA design benchmarks and generalization ability assessments. I highly recommend this paper for publication in a top-tier conference or journal.",
        "gr_score_with": 0.8651399491094147,
        "gr_score_without": 0.8290322580645161,
        "output_without": {
            "num_tokens_scored": 310,
            "num_green_tokens": 257,
            "green_fraction": 0.8290322580645161,
            "z_score": 1.278173735996309,
            "p_value": 0.10059408725614749
        },
        "output_with": {
            "num_tokens_scored": 393,
            "num_green_tokens": 340,
            "green_fraction": 0.8651399491094147,
            "z_score": 3.2286443184977927,
            "p_value": 0.0006218924176961916
        }
    },
    {
        "title": "ONE-TO-MANY POLICY TRANSFER",
        "abstract": "We investigate the problem of transferring an expert policy from a source robot to multiple different robots. To solve this problem, we propose a method named Meta-Evolve that uses continuous robot evolution to efficiently transfer the policy to each target robot through a set of tree-structured evolutionary robot sequences. The robot evolution tree allows the robot evolution paths to be shared, so our approach can significantly outperform naive one-to-one policy transfer. We present a heuristic approach to determine an optimized robot evolution tree. Experiments have shown that our method is able to improve the efficiency of one-to-three transfer of manipulation policy by up to 3.2\u00d7 and one-to-six transfer of agile locomotion policy by 2.4\u00d7 in terms of simulation cost over the baseline of launching multiple independent one-to-one policy transfers. Supplementary videos available at the project website: https://sites.google.com/view/meta-evolve.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a method called Meta-Evolve for transferring an expert policy from a source robot to multiple target robots. The method uses continuous robot evolution to efficiently transfer the policy through a tree-structured evolutionary robot sequence. The paper presents a heuristic approach to determine an optimized robot evolution tree and shows that the method can significantly outperform one-to-one policy transfer baselines in terms of simulation cost and training iterations.\n\nStrengths:\n\n* The paper introduces a new research problem of transferring an expert policy from a source robot to multiple target robots, which is an important problem in robotics.\n* The method proposed, Meta-Evolve, is a novel and efficient approach to solve this problem.\n* The paper presents a heuristic approach to determine the robot evolution tree, which is a challenging problem.\n* The experiments conducted on Hand Manipulation Suite tasks and an agile locomotion task show that the method can significantly outperform one-to-one policy transfer baselines.\n\nWeaknesses:\n\n* The paper assumes that the source and target robots can be matched in their kinematic tree topology, which may not always be the case.\n* The method proposed relies on the assumption that the target robots are mutually similar in morphology and kinematics, which may not always be true.\n* The paper does not provide a formal proof of the convergence of the method, which is an important aspect of the method's theoretical validity.\n* The experiments conducted are limited to two tasks and it is unclear whether the method can generalize to other tasks.\n\nSuggestions for Improvement:\n\n* Provide a formal proof of the convergence of the method to establish its theoretical validity.\n* Investigate the assumption that the source and target robots can be matched in their kinematic tree topology and propose a method to relax this assumption.\n* Explore the possibility of using other machine learning techniques, such as meta-learning, to improve the performance of the method.\n* Conduct more extensive experiments on a variety of tasks to demonstrate the method's generalizability.\n\nRecommendation:\n\nThe paper proposes a novel and efficient method for transferring an expert policy from a source robot to multiple target robots. While the method has some limitations, it shows promising results in the experiments conducted. With further development and refinement, the method has the potential to make significant contributions to the field of robotics. Therefore, I recommend the paper for publication in a reputable conference.\n\nRating: 7/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a method called Meta-Evolve for efficiently transferring a well-trained policy from one source robot to multiple target robots. The method utilizes continuous robot evolution to gradually adapt the policy from the source robot to the target robots through a tree-structured evolutionary robot sequence. The authors present a heuristic approach to determine the optimal robot evolution tree that minimizes the total cost of training and exploration during policy transfer.\n\nStrengths:\n\n1. The paper introduces a new and important research problem of one-to-many policy transfer, which is a significant extension of the traditional one-to-one policy transfer problem.\n2. The proposed Meta-Evolve method is innovative and effective in reducing the cost of policy transfer by sharing evolution paths among multiple target robots.\n3. The authors provide a thorough analysis of the problem and the proposed solution, including a clear explanation of the heuristic approach for determining the robot evolution tree.\n4. The experimental results demonstrate the effectiveness of the Meta-Evolve method in reducing the cost of policy transfer compared to one-to-one policy transfer baselines.\n\nWeaknesses:\n\n1. The paper assumes that the source and target robots have similar kinematic tree topology, which may not always be the case in real-world scenarios.\n2. The proposed heuristic approach for determining the robot evolution tree may not be optimal and may require further refinement or optimization.\n3. The paper does not provide a thorough analysis of the computational complexity of the Meta-Evolve method and its scalability to large-scale problems.\n4. The experimental results are limited to two specific tasks and may not generalize to other tasks or robot morphologies.\n\nSuggestions for Improvement:\n\n1. Investigate alternative approaches for determining the robot evolution tree that may be more optimal or scalable.\n2. Analyze the computational complexity of the Meta-Evolve method and its scalability to large-scale problems.\n3. Provide more experimental results to demonstrate the effectiveness of the Meta-Evolve method on a wider range of tasks and robot morphologies.\n4. Investigate the possibility of learning or optimizing the evolution tree and meta robots to improve the performance of the Meta-Evolve method.\n\nRecommendation:\n\nThe paper is well-written and provides a thorough analysis of the problem and the proposed solution. However, the paper assumes that the source and target robots have similar kinematic tree topology, which may not always be the case in real-world scenarios. Therefore, I recommend that the authors investigate alternative approaches for determining the robot evolution tree that may be more optimal or scalable. Additionally, the authors should provide more experimental results to demonstrate the effectiveness of the Meta-Evolve method on a wider range of tasks and robot morphologies.\n\nRating: 7/10\n\nRecommendation for publication: Yes\n\n$$$",
        "gr_score_with": 0.8228228228228228,
        "gr_score_without": 0.7597597597597597,
        "output_without": {
            "num_tokens_scored": 333,
            "num_green_tokens": 253,
            "green_fraction": 0.7597597597597597,
            "z_score": -1.835482664923095,
            "p_value": 0.9667828973544828
        },
        "output_with": {
            "num_tokens_scored": 333,
            "num_green_tokens": 274,
            "green_fraction": 0.8228228228228228,
            "z_score": 1.0414659665193184,
            "p_value": 0.1488296702836906
        }
    },
    {
        "title": "WHAT\u2019S IN MY BIG DATA?",
        "abstract": "Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose WHAT\u2019S IN MY BIG DATA? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities\u2014count and search\u2014at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, we find that about 50% of the documents in RedPajama and LAION-2B-en are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE. We open-source WIMBD\u2019s code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes WHAT'S IN MY BIG DATA? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities\u2014count and search\u2014at scale, which allows us to analyze more than 35 terabytes on a standard compute node. The authors apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama, and uncover several surprising and previously undocumented findings about these corpora.\n\nStrengths:\n\n* The paper proposes a novel platform and set of analyses for understanding the contents of large text corpora.\n* The authors provide a comprehensive analysis of ten different corpora used to train popular language models.\n* The paper highlights the importance of understanding the contents of large text corpora and provides a framework for doing so.\n* The authors provide a clear and concise description of the WIMBD platform and its capabilities.\n* The paper includes a thorough analysis of the results, including both quantitative and qualitative findings.\n\nWeaknesses:\n\n* The paper assumes that the reader has a strong background in natural language processing and machine learning.\n* The authors do not provide a detailed description of the Elasticsearch search engine used in WIMBD.\n* The paper does not provide a clear explanation of the hash function used in the counting and searching algorithms.\n* The authors do not provide a detailed description of the evaluation metrics used to assess the performance of WIMBD.\n* The paper does not provide a clear explanation of the limitations of WIMBD and how they can be addressed.\n\nSuggestions for Improvement:\n\n* Provide a more detailed description of the Elasticsearch search engine used in WIMBD.\n* Provide a clear explanation of the hash function used in the counting and searching algorithms.\n* Provide a detailed description of the evaluation metrics used to assess the performance of WIMBD.\n* Provide a clear explanation of the limitations of WIMBD and how they can be addressed.\n* Consider including a more detailed analysis of the results, including a discussion of the implications of the findings.\n\nRecommendation:\n\nThis paper provides a novel and comprehensive framework for understanding the contents of large text corpora. The authors provide a clear and concise description of the WIMBD platform and its capabilities, and the paper includes a thorough analysis of the results. However, the paper assumes a strong background in natural language processing and machine learning, and some of the technical details are not clearly explained. Overall, this paper is a valuable contribution to the field of natural language processing and machine learning, and it has the potential to be a highly influential paper in the field.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a platform called WIMBD (WHAT'S IN MY BIG DATA?) to investigate the content of large text corpora. WIMBD builds on two basic capabilities: count and search, which allow for the analysis of large text corpora. The authors apply WIMBD to ten different corpora used to train language models, including C4, The Pile, and RedPajama. Their analysis reveals several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. The authors open-source WIMBD's code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them.\n\nStrengths:\n\n1. WIMBD is a comprehensive platform for analyzing large text corpora, providing a set of sixteen analyses across four aspects: high-level statistics, data quality, community- and society-relevant measurements, and cross-data analysis.\n2. The authors apply WIMBD to ten different corpora used to train language models, providing a broad understanding of the content of large text corpora.\n3. The analysis reveals several surprising and previously undocumented findings about the corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination.\n4. The authors open-source WIMBD's code and artifacts, providing a standard set of evaluations for new text-based corpora and encouraging more analyses and transparency around them.\n\nWeaknesses:\n\n1. The analysis is limited to ten corpora, and it is unclear whether the findings are generalizable to other corpora.\n2. The authors rely on Elasticsearch for search, which may not be scalable for extremely large corpora.\n3. The analysis does not account for the complexity of natural language, which may lead to inaccurate results.\n4. The authors do not provide a clear evaluation of the impact of the findings on language model performance.\n\nSuggestions for Improvement:\n\n1. Expand the analysis to more corpora to increase the generalizability of the findings.\n2. Investigate alternative search algorithms to improve scalability and accuracy.\n3. Consider the complexity of natural language in the analysis and develop more sophisticated methods to account for it.\n4. Evaluate the impact of the findings on language model performance to provide a more complete understanding of the relationship between corpus quality and language model performance.\n\nRecommendation:\n\nThe paper proposes a comprehensive platform for analyzing large text corpora, providing a set of sixteen analyses across four aspects. While the analysis reveals several surprising and previously undocumented findings about the corpora, the limitations of the analysis, such as the reliance on Elasticsearch and the lack of evaluation of the impact of the findings on language model performance, should be addressed. Therefore, I recommend this paper for publication in a top-tier conference, but with the expectation that the authors will address the limitations and provide a more comprehensive analysis in future work.\n\n$$$",
        "gr_score_with": 0.904891304347826,
        "gr_score_without": 0.8392282958199357,
        "output_without": {
            "num_tokens_scored": 311,
            "num_green_tokens": 261,
            "green_fraction": 0.8392282958199357,
            "z_score": 1.7297511003720634,
            "p_value": 0.04183737725792531
        },
        "output_with": {
            "num_tokens_scored": 368,
            "num_green_tokens": 333,
            "green_fraction": 0.904891304347826,
            "z_score": 5.030650546004888,
            "p_value": 2.4440919349606954e-07
        }
    },
    {
        "title": "COMPLEX PRIORS AND FLEXIBLE INFERENCE IN RECURRENT CIRCUITS WITH DENDRITIC NONLINEARITIES",
        "abstract": "Despite many successful examples in which probabilistic inference can account for perception, we have little understanding of how the brain represents and uses structured priors that capture the complexity of natural input statistics. Here we construct a recurrent circuit model that can implicitly represent priors over latent variables, and combine them with sensory and contextual sources of information to encode task-specific posteriors. Inspired by the recent success of diffusion models as means of learning and using priors over images, our model uses dendritic nonlinearities optimized for denoising, and stochastic somatic integration with the degree of noise modulated by an oscillating global signal. Combining these elements into a recurrent network yields a dynamical system that samples from the prior at a rate prescribed by the period of the global oscillator. Additional inputs reflecting sensory or top-down contextual information alter these dynamics to generate samples from the corresponding posterior, with different input gating patterns selecting different inference tasks. We demonstrate that this architecture can sample from low dimensional nonlinear manifolds and multimodal posteriors. Overall, the model provides a new framework for circuit-level representation of probabilistic information, in a format that facilitates flexible inference.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel circuit-level model for representing complex high-dimensional distributions, inspired by the recent success of diffusion models in machine learning. The model, called the recurrent circuit model, uses dendritic nonlinearities optimized for denoising and stochastic somatic integration with a modulated global signal to sample from the prior at a rate prescribed by the period of the global oscillator. The model can flexibly use sensory and contextual sources of information to encode task-specific posteriors. The authors demonstrate that this architecture can sample from low-dimensional nonlinear manifolds and multimodal posteriors.\n\nStrengths:\n\n1. The paper proposes a novel and biologically-inspired model for representing complex high-dimensional distributions.\n2. The model is flexible and can be used to encode task-specific posteriors using sensory and contextual sources of information.\n3. The authors demonstrate that the model can sample from low-dimensional nonlinear manifolds and multimodal posteriors.\n4. The paper provides a thorough analysis of the model's performance and compares it to traditional diffusion models.\n\nWeaknesses:\n\n1. The paper assumes a fixed architecture for the dendritic tree, which may not be biologically realistic.\n2. The model's performance is evaluated on a limited set of tasks and distributions, and it is unclear how well it generalizes to other scenarios.\n3. The paper does not provide a clear explanation of how the model's parameters are learned, which is a critical component of any machine learning model.\n4. The paper's conclusion is somewhat overstated, as the model's performance is not significantly better than traditional diffusion models in all cases.\n\nSuggestions for Improvement:\n\n1. Consider exploring more biologically realistic architectures for the dendritic tree, such as those with variable branching factors or depths.\n2. Evaluate the model's performance on a wider range of tasks and distributions to assess its generalizability.\n3. Provide a more detailed explanation of how the model's parameters are learned, including any optimization algorithms or techniques used.\n4. Be more cautious in the paper's conclusion, as the model's performance is not uniformly better than traditional diffusion models.\n\nRecommendation:\n\nI recommend accepting this paper for publication, subject to the above suggestions for improvement. The paper proposes a novel and biologically-inspired model for representing complex high-dimensional distributions, and the authors demonstrate its flexibility and performance on a range of tasks. However, the paper's assumptions and limitations should be carefully considered and addressed in future work.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel circuit-level model for representing complex high-dimensional distributions, inspired by the success of diffusion models in machine learning. The authors adapt several ideas from diffusion models to construct a recurrent circuit model that can implicitly represent sensory priors and combine them with other sources of information to encode task-specific posteriors. The proposed circuit uses dendritic nonlinearities, optimized for denoising, and stochastic somatic activity modulated by a global oscillation that determines the effective rate of sampling. The authors demonstrate that this architecture can sample from low-dimensional nonlinear manifolds and multimodal posteriors, and provide a new framework for circuit-level representation of probabilistic information.\n\nStrengths:\n\n1. The paper provides a novel and innovative approach to representing complex high-dimensional distributions, which is inspired by the success of diffusion models in machine learning.\n2. The authors propose a biologically-motivated circuit-level model that can implicitly represent sensory priors and combine them with other sources of information to encode task-specific posteriors.\n3. The proposed circuit uses dendritic nonlinearities, optimized for denoising, and stochastic somatic activity modulated by a global oscillation that determines the effective rate of sampling.\n4. The authors demonstrate that this architecture can sample from low-dimensional nonlinear manifolds and multimodal posteriors.\n5. The paper provides a new framework for circuit-level representation of probabilistic information, which has the potential to be useful for understanding the neural mechanisms of perception and action.\n\nWeaknesses:\n\n1. The paper assumes that the prior distribution is known, which may not be the case in real-world scenarios.\n2. The authors do not provide a detailed explanation of how the proposed circuit would be implemented in biological systems.\n3. The paper relies on simulations to demonstrate the performance of the proposed circuit, which may not be representative of real-world scenarios.\n4. The authors do not provide a clear explanation of how the proposed circuit would be trained and fine-tuned.\n5. The paper assumes that the global oscillation is a fixed parameter, which may not be the case in real-world scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of how the proposed circuit would be implemented in biological systems.\n2. The paper should include more experimental data to demonstrate the performance of the proposed circuit in real-world scenarios.\n3. The authors should provide a clear explanation of how the proposed circuit would be trained and fine-tuned.\n4. The paper should include more discussion on the potential applications and limitations of the proposed circuit.\n5. The authors should provide a more detailed explanation of how the proposed circuit would handle cases where the prior distribution is not known.\n\nRecommendation:\n\nI recommend that the authors revise the paper to address the weaknesses and suggestions for improvement. The proposed circuit has the potential to be a useful tool for understanding the neural mechanisms of perception and action, and with further development and refinement, it could be a valuable contribution to the field.\n\nRating:\n\nI would rate this paper as a 7 out of 10. The proposed circuit is innovative and has the potential to be a useful tool for understanding the neural mechanisms of perception and action. However, the paper assumes that the prior distribution is known, and the authors do not provide a detailed explanation of how the proposed circuit would be implemented in biological systems. With further development and refinement, I believe that this paper could be a valuable contribution to the field.\n\n$$$",
        "gr_score_with": 0.8424068767908309,
        "gr_score_without": 0.7916666666666666,
        "output_without": {
            "num_tokens_scored": 336,
            "num_green_tokens": 266,
            "green_fraction": 0.7916666666666666,
            "z_score": -0.38159100583924016,
            "p_value": 0.6486176216951209
        },
        "output_with": {
            "num_tokens_scored": 349,
            "num_green_tokens": 294,
            "green_fraction": 0.8424068767908309,
            "z_score": 1.9808327422988352,
            "p_value": 0.023805016818113353
        }
    },
    {
        "title": "DEMYSTIFYING LOCAL & GLOBAL FAIRNESS TRADE-OFFS IN FEDERATED LEARNING USING PARTIAL INFORMATION DECOMPOSITION",
        "abstract": "This work presents an information-theoretic perspective to group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender, race, etc. Existing works often focus on either global fairness (overall disparity of the model across all clients) or local fairness (disparity of the model at each client), without always considering their trade-offs. There is a lack of understanding of the interplay between global and local fairness in FL, particularly under data heterogeneity, and if and when one implies the other. To address this gap, we leverage a body of work in information theory called partial information decomposition (PID), which first identifies three sources of unfairness in FL, namely, Unique Disparity, Redundant Disparity, and Masked Disparity. We demonstrate how these three disparities contribute to global and local fairness using canonical examples. This decomposition helps us derive fundamental limits on the trade-off between global and local fairness, highlighting where they agree or disagree. We introduce the Accuracy & Global-Local Fairness Optimality Problem (AGLFOP), a convex optimization that defines the theoretical limits of accuracy and fairness trade-offs, identifying the best possible performance any FL strategy can attain given a dataset and client distribution. We also present experimental results on synthetic datasets and the ADULT dataset to support our theoretical findings.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper \"DEMYSTIFYING LOCAL & GLOBAL FAIRNESS TRADE-OFFS IN FEDERATED LEARNING USING PARTIAL INFORMATION DECOMPOSITION\" presents an information-theoretic perspective to group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender, race, etc. The authors leverage a body of work in information theory called partial information decomposition (PID) to identify three sources of unfairness in FL: Unique Disparity, Redundant Disparity, and Masked Disparity. They demonstrate how these three disparities contribute to global and local fairness using canonical examples and provide a framework for quantifying accuracy-fairness trade-offs in FL.\n\nStrengths:\n\n1. The paper provides a comprehensive understanding of group fairness trade-offs in FL using an information-theoretic perspective.\n2. The authors leverage PID to identify three sources of unfairness in FL, which is a novel contribution to the field.\n3. The paper provides a framework for quantifying accuracy-fairness trade-offs in FL, which is a significant contribution to the field.\n4. The authors provide experimental results on synthetic and Adult datasets to support their theoretical findings.\n\nWeaknesses:\n\n1. The paper assumes a binary sensitive attribute, which may not be applicable to all real-world scenarios.\n2. The authors do not provide a clear explanation of how to implement the PID decomposition in practice.\n3. The paper assumes a fixed client distribution, which may not be realistic in real-world scenarios.\n4. The authors do not provide a comparison with existing methods for fairness in FL.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of how to implement the PID decomposition in practice.\n2. The paper should be extended to include a comparison with existing methods for fairness in FL.\n3. The authors should provide a more comprehensive discussion of the limitations of the PID decomposition.\n4. The paper should be extended to include a discussion of how to handle non-binary sensitive attributes.\n\nRecommendation:\n\nThe paper provides a significant contribution to the field of fairness in FL and is well-written and well-organized. However, the authors should address the limitations mentioned above to make the paper more comprehensive and practical. I would recommend this paper for publication in a top-tier conference in the field of machine learning and fairness.",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper \"DEMYSTIFYING LOCAL & GLOBAL FAIRNESS TRADE-OFFS IN FEDERATED LEARNING USING PARTIAL INFORMATION DECOMPOSITION\" proposes an information-theoretic approach to understand the interplay between local and global fairness in Federated Learning (FL). The authors leverage partial information decomposition (PID) to identify three sources of unfairness in FL, namely, Unique Disparity, Redundant Disparity, and Masked Disparity. They demonstrate how these three disparities contribute to global and local fairness using canonical examples. The authors introduce the Accuracy & Global-Local Fairness Optimality Problem (AGLFOP), a convex optimization framework that defines the theoretical limits of accuracy and fairness trade-offs in FL. They also present experimental results on synthetic datasets and the Adult dataset to support their theoretical findings.\n\nStrengths:\n\n1. The paper provides a comprehensive understanding of the interplay between local and global fairness in FL using an information-theoretic approach.\n2. The authors introduce the PID framework, which decomposes global and local disparity into three non-negative components, providing insights into the sources of unfairness in FL.\n3. The AGLFOP framework is introduced, which defines the theoretical limits of accuracy and fairness trade-offs in FL, providing a convex optimization approach to exploring the trade-offs between accuracy and fairness.\n4. The paper presents experimental results on synthetic datasets and the Adult dataset, providing empirical evidence to support the theoretical findings.\n\nWeaknesses:\n\n1. The paper assumes that the data is binary, which may not be the case in real-world scenarios.\n2. The PID framework may not capture all the nuances of fairness in FL, as it relies on mutual information, which may not capture all the dependencies between the sensitive attribute, client label, and model predictions.\n3. The AGLFOP framework may not be scalable to large datasets, as it requires computing the mutual information between the sensitive attribute, client label, and model predictions.\n4. The paper does not discuss the practical implications of the AGLFOP framework, such as how to implement it in real-world scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should extend the PID framework to capture more nuanced dependencies between the sensitive attribute, client label, and model predictions.\n2. The authors should discuss the practical implications of the AGLFOP framework, such as how to implement it in real-world scenarios.\n3. The authors should investigate the scalability of the AGLFOP framework to large datasets.\n4. The authors should provide more experimental results on real-world datasets to demonstrate the effectiveness of the AGLFOP framework.\n\nRecommendation:\n\nThe paper provides a comprehensive understanding of the interplay between local and global fairness in FL using an information-theoretic approach. The authors introduce the PID framework, which decomposes global and local disparity into three non-negative components, providing insights into the sources of unfairness in FL. The AGLFOP framework is introduced, which defines the theoretical limits of accuracy and fairness trade-offs in FL, providing a convex optimization approach to exploring the trade-offs between accuracy and fairness. However, the paper assumes that the data is binary, which may not be the case in real-world scenarios, and the PID framework may not capture all the nuances of fairness in FL. The authors should extend the PID framework to capture more nuanced dependencies between the sensitive attribute, client label, and model predictions, discuss the practical implications of the AGLFOP framework, investigate the scalability of the AGLFOP framework to large datasets, and provide more experimental results on real-world datasets to demonstrate the effectiveness of the AGLFOP framework.\n\nIn conclusion, the paper provides a comprehensive understanding of the interplay between local and global fairness in FL using an information-theoretic approach, but it requires further development to capture more nuanced dependencies between the sensitive attribute, client label, and model predictions, discuss the practical implications of the AGLFOP framework, investigate the scalability of the AGLFOP framework to large datasets, and provide more experimental results on real-world datasets to demonstrate the effectiveness of the AGLFOP framework.",
        "gr_score_with": 0.8647887323943662,
        "gr_score_without": 0.802547770700637,
        "output_without": {
            "num_tokens_scored": 314,
            "num_green_tokens": 252,
            "green_fraction": 0.802547770700637,
            "z_score": 0.11314152944657332,
            "p_value": 0.4549591753097201
        },
        "output_with": {
            "num_tokens_scored": 355,
            "num_green_tokens": 307,
            "green_fraction": 0.8647887323943662,
            "z_score": 3.0520412472955996,
            "p_value": 0.0011364543276600927
        }
    },
    {
        "title": "INCREMENTAL RANDOMIZED SMOOTHING CERTIFICATION",
        "abstract": "Randomized smoothing-based certification is an effective approach for obtaining robustness certificates of deep neural networks (DNNs) against adversarial attacks. This method constructs a smoothed DNN model and certifies its robustness through statistical sampling, but it is computationally expensive, especially when certifying with a large number of samples. Furthermore, when the smoothed model is modified (e.g., quantized or pruned), certification guarantees may not hold for the modified DNN, and recertifying from scratch can be prohibitively expensive. We present the first approach for incremental robustness certification for randomized smoothing, IRS. We show how to reuse the certification guarantees for the original smoothed model to certify an approximated model with very few samples. IRS significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees. We experimentally demonstrate the effectiveness of our approach, showing up to 4.1x certification speedup over the certification that applies randomized smoothing of approximate model from scratch.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach for incremental robustness certification of deep neural networks (DNNs) using randomized smoothing (RS). The method, called Incremental Randomized Smoothing (IRS), reuses the certification guarantees of the original smoothed model to certify an approximated model with very few samples. This approach significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees.\n\nStrengths:\n\n1. The paper addresses a critical limitation of existing RS-based certification practices, which requires expensive certification to be re-run from scratch for each approximate network.\n2. The proposed IRS approach is the first incremental RS-based certification framework that efficiently computes the certified radius of the updated smoothed classifier.\n3. The paper presents an extensive evaluation of the performance of IRS, demonstrating speedups of up to 4.1x over the standard non-incremental RS baseline on state-of-the-art classification models.\n4. The authors provide a thorough analysis of the trade-offs involved in choosing the smoothing parameter \u03c3, which affects the efficiency of IRS.\n\nWeaknesses:\n\n1. The paper assumes that the approximate network is similar to the original network, which may not always be the case in real-world scenarios.\n2. The authors do not provide a detailed analysis of the computational cost of storing and retrieving the certification cache, which may become a bottleneck for large-scale applications.\n3. The paper assumes that the user has a larger sample budget, which may not always be the case in real-world scenarios.\n4. The authors do not provide a detailed comparison with other incremental certification methods, such as deterministic certification using formal logic.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the assumptions made in the paper, particularly with regards to the similarity between the approximate and original networks.\n2. The paper should include a more detailed analysis of the computational cost of storing and retrieving the certification cache, including the memory requirements and the time complexity of the operations involved.\n3. The authors should provide a more detailed comparison with other incremental certification methods, including deterministic certification using formal logic.\n4. The paper should include a discussion of the potential applications of IRS in real-world scenarios, including approximate tuning and shipping certification caches.\n\nRecommendation:\n\nThe paper presents a novel and efficient approach for incremental robustness certification of DNNs using randomized smoothing. While there are some limitations and weaknesses to the approach, the authors provide a thorough analysis of the trade-offs involved and demonstrate the effectiveness of IRS in practice. Therefore, I recommend accepting the paper for publication in a reputable conference or journal.\n\nRating: 8/10\n\nReview:\n\nThe paper is well-written and easy to follow, with a clear introduction to the problem and a thorough presentation of the proposed approach. The authors provide a detailed analysis of the trade-offs involved in choosing the smoothing parameter \u03c3 and demonstrate the effectiveness of IRS in practice. However, the paper assumes that the approximate network is similar to the original network, which may not always be the case in real-world scenarios. Additionally, the authors do not provide a detailed analysis of the computational cost of storing and retrieving the certification cache, which may become a bottleneck for large-scale applications.\n\nOverall, the paper presents a novel and efficient approach for incremental robustness certification of DNNs using randomized smoothing. While there are some limitations and weaknesses to the approach, the authors provide a thorough analysis of the trade-offs involved and demonstrate the effectiveness of IRS in practice. Therefore, I recommend accepting the paper for publication in a reputable conference or journal.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes Incremental Randomized Smoothing (IRS) certification, a novel approach to certify the robustness of deep neural networks (DNNs) with reduced computational cost. IRS leverages the certification guarantees obtained from the original smoothed model to certify a smoothed approximated model with very few samples. This approach significantly reduces the computational cost of certification while maintaining strong robustness guarantees. The authors extensively evaluate the performance of IRS on state-of-the-art DNNs and show that it achieves up to 4.1x certification speedup over the standard non-incremental RS baseline.\n\nStrengths:\n\n1. IRS provides a novel approach to incremental certification, which is essential for efficient certification of DNNs in real-world applications.\n2. The authors demonstrate the effectiveness of IRS through extensive experiments on state-of-the-art DNNs and show significant certification speedup over the standard non-incremental RS baseline.\n3. IRS maintains strong robustness guarantees, which is critical for reliable certification of DNNs.\n4. The approach is general and can be applied to various DNN approximations, including quantization and pruning.\n\nWeaknesses:\n\n1. The authors assume that the cache stores the top predicted class index, its lower confidence bound, and the seeds for Gaussian corruptions obtained from the RS execution of the original classifier. However, storing this additional information may require significant memory and may not be feasible for all applications.\n2. The choice of the smoothing parameter \u03c3 affects the efficiency of IRS, and larger values of \u03c3 generally lead to better results. However, this may come at the cost of reduced accuracy in the model.\n3. IRS targets fast certification while maintaining a sufficiently large radius, which may limit its applicability to scenarios where a larger radius is required.\n4. The authors do not provide a detailed analysis of the theoretical guarantees of IRS, which may be essential for understanding its robustness and reliability.\n\nSuggestions for Improvement:\n\n1. Investigate alternative approaches to store the cache, such as using more compact representations or leveraging distributed storage.\n2. Provide a more detailed analysis of the theoretical guarantees of IRS, including its robustness and reliability.\n3. Explore the applicability of IRS to scenarios where a larger radius is required, such as in scenarios with limited computational resources.\n4. Investigate the extension of IRS to other types of DNN approximations, such as weight sharing or knowledge distillation.\n\nRecommendation:\n\nThis paper presents a novel and effective approach to incremental certification of DNNs, which has significant implications for efficient and effective certification of DNNs in real-world applications. While there are some limitations and areas for improvement, the authors provide a thorough evaluation of the approach and demonstrate its effectiveness through extensive experiments. Therefore, I recommend this paper for publication in a reputable conference or journal.",
        "gr_score_with": 0.8481927710843373,
        "gr_score_without": 0.8493827160493828,
        "output_without": {
            "num_tokens_scored": 405,
            "num_green_tokens": 344,
            "green_fraction": 0.8493827160493828,
            "z_score": 2.4848047342081907,
            "p_value": 0.006481125659095672
        },
        "output_with": {
            "num_tokens_scored": 415,
            "num_green_tokens": 352,
            "green_fraction": 0.8481927710843373,
            "z_score": 2.454692430435087,
            "p_value": 0.007050258699899916
        }
    },
    {
        "title": "SELF-CONSUMING GENERATIVE MODELS GO MAD",
        "abstract": "Seismic advances in generative AI algorithms for imagery, text, and other data types have led to the temptation to use AI-synthesized data to train next-generation models. Repeating this process creates an autophagous (\u201cself-consuming\u201d) loop whose properties are poorly understood. We conduct a thorough analytical and empirical analysis using state-of-the-art generative image models of three families of autophagous loops that differ in how fixed or fresh real training data is available through the generations of training and whether the samples from previous-generation models have been biased to trade off data quality versus diversity. Our primary conclusion across all scenarios is that without enough fresh real data in each generation of an autophagous loop, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease. We term this condition Model Autophagy Disorder (MAD), by analogy to mad cow disease, and show that appreciable MADness arises in just a few generations. Generation t = 1 t = 3 t = 5 t = 7 t = 9 Figure 1: Training generative artificial intelligence (AI) models on synthetic data progressively amplifies artifacts. As AI-synthesized data proliferates in standard datasets and the Internet, future AI models will train on both real and synthetic data, forming autophagous (\u201cself-consuming\u201d) loops. Here we highlight a potential unintended consequence of autophagous training. We trained a sequence of StyleGAN2 (Karras et al., 2019a) models wherein the model at generation t \u2265 2 trains only on data synthesized by the model at generation t\u2212 1. This setup is a fully synthetic loop (Figure 3) without sampling bias (\u03bb = 1). Note how the cross-hatched artifacts (possibly an architectural fingerprint (Karras et al., 2021)) are progressively amplified at each generation. Appendix D has more samples.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper explores the concept of autophagous (self-consuming) loops in generative AI models, where AI-synthesized data is used to train future models, creating a loop of synthetic data being used to train subsequent models. The authors propose three families of autophagous loops: fully synthetic, synthetic augmentation, and fresh data loops. They conduct a thorough analytical and empirical analysis using state-of-the-art generative image models and demonstrate that without enough fresh real data in each generation, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease, a condition they term Model Autophagy Disorder (MAD).\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of autophagous loops, proposing three realistic models that mimic real-world generative model training scenarios.\n2. The authors demonstrate the negative impact of autophagous loops on the quality and diversity of synthetic data, providing empirical evidence of Model Autophagy Disorder (MAD).\n3. The paper highlights the importance of sampling biases in autophagous loops, showing that biased sampling can boost synthetic data quality but at the expense of diversity.\n4. The authors provide a detailed analysis of the fully synthetic loop, showing that unbiased sampling degrades synthetic data quality and diversity, while biased sampling can increase quality but decrease diversity.\n\nWeaknesses:\n\n1. The paper assumes that the generative models used are perfect, which may not be the case in real-world scenarios.\n2. The authors do not provide a comprehensive analysis of the potential benefits of autophagous loops, such as the ability to generate high-quality synthetic data.\n3. The paper focuses primarily on image generation models and does not explore the implications of autophagous loops on other data types, such as text or audio.\n4. The authors do not provide a clear solution to mitigate the negative effects of autophagous loops, such as developing autophagy-aware watermarking techniques.\n\nSuggestions for Improvement:\n\n1. The authors should consider exploring the potential benefits of autophagous loops and provide a more comprehensive analysis of their implications.\n2. The paper should be extended to include a more detailed analysis of the potential effects of autophagous loops on other data types, such as text or audio.\n3. The authors should provide a clear solution to mitigate the negative effects of autophagous loops, such as developing autophagy-aware watermarking techniques.\n4. The paper should be expanded to include a more detailed analysis of the limitations of the proposed models and the potential for future research in this area.\n\nRecommendation:\n\nThe paper provides a comprehensive analysis of autophagous loops and their negative impact on the quality and diversity of synthetic data. While there are some limitations to the paper, the authors' findings are significant and provide a valuable contribution to the field of generative AI. Therefore, I recommend that the paper be accepted for publication in its current form, with the suggested improvements for future research.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper explores the concept of autophagous generative models, where a sequence of generative models are trained on synthetic data produced by previous generations. The authors investigate the potential consequences of repeating this process, leading to an autophagous (\"self-consuming\") loop. They conduct a thorough analytical and empirical analysis using state-of-the-art generative image models to study the behavior of three families of autophagous loops that differ in how fixed or fresh real training data is available through the generations of training and whether the samples from previous-generation models have been biased to trade off data quality versus diversity. The primary conclusion across all scenarios is that without enough fresh real data in each generation of an autophagous loop, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease. The authors term this condition Model Autophagy Disorder (MAD), by analogy to mad cow disease.\n\nStrengths:\n1. The paper provides a comprehensive analysis of the autophagous generative model, which is a critical concept in the field of generative models.\n2. The authors use state-of-the-art generative image models to study the behavior of autophagous loops, making their findings more reliable and generalizable.\n3. The paper highlights the importance of fresh real data in each generation of an autophagous loop, which is a critical aspect that has been overlooked in previous studies.\n4. The authors provide a clear and concise definition of Model Autophagy Disorder (MAD) and its consequences, making it easier for readers to understand the implications of their findings.\n\nWeaknesses:\n1. The paper assumes that the generative models are trained from scratch in each generation, which may not be the case in practice.\n2. The authors do not consider the impact of different sampling biases on the performance of autophagous loops, which could be an important aspect to investigate.\n3. The paper focuses primarily on generative image models, and it would be beneficial to explore the concept of autophagous loops in other types of generative models, such as language models.\n4. The authors do not provide a clear explanation of how to prevent or mitigate the effects of Model Autophagy Disorder (MAD), which is an important aspect to address in future research.\n\nSuggestions for Improvement:\n1. Investigate the impact of different sampling biases on the performance of autophagous loops.\n2. Explore the concept of autophagous loops in other types of generative models, such as language models.\n3. Provide a clear explanation of how to prevent or mitigate the effects of Model Autophagy Disorder (MAD).\n4. Consider the implications of autophagous loops in practice, such as in applications where generative models are used to generate synthetic data.\n\nRecommendation:\nThe paper provides a comprehensive analysis of the autophagous generative model and highlights the importance of fresh real data in each generation of an autophagous loop. The findings are significant, and the paper has the potential to contribute to the advancement of the field. However, the paper assumes that the generative models are trained from scratch in each generation, and it would be beneficial to explore the concept of autophagous loops in other types of generative models. With some modifications, the paper has the potential to become a high-impact contribution to the field.\n\nRating: 8/10\n\nRecommendation for Publication: Accept with revisions.",
        "gr_score_with": 0.8132678132678133,
        "gr_score_without": 0.7205128205128205,
        "output_without": {
            "num_tokens_scored": 390,
            "num_green_tokens": 281,
            "green_fraction": 0.7205128205128205,
            "z_score": -3.924011202354161,
            "p_value": 0.9999564566584475
        },
        "output_with": {
            "num_tokens_scored": 407,
            "num_green_tokens": 331,
            "green_fraction": 0.8132678132678133,
            "z_score": 0.6694769197634892,
            "p_value": 0.25159564940444906
        }
    },
    {
        "title": "IDEAL: INFLUENCE-DRIVEN SELECTIVE ANNOTA- TIONS EMPOWER IN-CONTEXT LEARNERS IN LARGE LANGUAGE MODELS",
        "abstract": "In-context learning is a promising paradigm that utilizes in-context examples as prompts for the predictions of large language models. These prompts are crucial for achieving strong performance. However, since the prompts need to be sampled from a large volume of annotated examples, finding the right prompt may result in high annotation costs. To address this challenge, this paper introduces an influence-driven selective annotation method that aims to minimize annotation costs while improving the quality of in-context examples. The essence of our method is to select a pivotal subset from a large-scale unlabeled data pool to annotate for the subsequent sampling of prompts. Specifically, a directed graph is first constructed to represent unlabeled data. Afterward, the influence of candidate unlabeled subsets is quantified with a diffusion process. A simple yet effective greedy algorithm for unlabeled data selection is lastly introduced. It iteratively selects the data if it provides a maximum marginal gain with respect to quantified influence. Compared with previous efforts on selective annotations, our influencedriven method works in an end-to-end manner, avoids an intractable explicit balance between data diversity and representativeness, and enjoys theoretical support. Experiments confirm the superiority of the proposed method on various benchmarks, achieving better performance under lower time consumption during subset selection. The project page is available at https://skzhang1.github.io/IDEAL/.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes an influence-driven selective annotation method, called IDEAL, to minimize annotation costs while improving the quality of in-context examples for large language models (LLMs). The method constructs a directed graph to represent unlabeled data and uses a diffusion process to quantify the influence of each candidate subset. A greedy algorithm is introduced to select the subset with maximum influence, which approximates the vast unlabeled data. The paper provides theoretical analysis and empirical evaluations on various benchmarks, demonstrating the superiority of IDEAL over previous state-of-the-art selective annotation baselines.\n\nStrengths:\n\n1. The paper presents a novel approach to selective annotations, addressing the challenges of balancing diversity and representativeness.\n2. The method is end-to-end, unsupervised, and efficient, reducing unnecessary costs at inference time.\n3. The paper provides theoretical analysis, establishing an upper limit for the global optimal solution and demonstrating the reliability of the greedy search algorithm.\n4. Empirical evaluations demonstrate the superiority of IDEAL across various benchmarks, including classification, multi-choice, dialogue, and generation tasks.\n\nWeaknesses:\n\n1. The paper assumes access to a large-scale unlabeled data pool, which may not be feasible in practice.\n2. The method relies on a similarity-based prompt retrieval method, which may not be optimal in all cases.\n3. The paper does not provide a comprehensive comparison with other graph-based methods, such as K-Means and MFL.\n4. The case study on automatic annotation is limited to a small set of datasets and may not be representative of all scenarios.\n\nSuggestions for Improvement:\n\n1. Investigate alternative methods for constructing the directed graph, such as using different similarity metrics or graph neural networks.\n2. Evaluate the performance of IDEAL on other datasets and tasks, including those with varying levels of complexity and domain adaptation.\n3. Compare IDEAL with other graph-based methods, such as K-Means and MFL, to demonstrate its superiority.\n4. Provide a more detailed analysis of the time complexity and scalability of IDEAL, especially for large datasets.\n\nRecommendation:\n\nThe paper presents a novel and efficient approach to selective annotations, addressing the challenges of balancing diversity and representativeness. The theoretical analysis and empirical evaluations demonstrate the superiority of IDEAL over previous state-of-the-art selective annotation baselines. While there are some limitations and areas for improvement, the paper provides a valuable contribution to the field of in-context learning and large language models. I recommend acceptance of the paper, with the suggested improvements and additional evaluations to further strengthen the results.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes an influence-driven selective annotation method, called IDEAL, to minimize annotation costs while improving the quality of in-context examples for large language models (LLMs). The method works in an end-to-end manner, avoiding the intractable explicit balance between data diversity and representativeness. IDEAL constructs a directed graph to represent unlabeled data, quantifies the influence of candidate unlabeled subsets through a diffusion process, and uses a simple greedy algorithm to select the subset with maximum influence. Theoretical analysis provides an upper bound for the global optimal solution and demonstrates that the greedy search algorithm selects a subset with influence at least as substantial as a specific proportion of the optimal solution's influence. Empirical evaluations on various benchmarks show that IDEAL achieves better performance than previous state-of-the-art selective annotation baselines while reducing the time required for subset selection.\n\nStrengths:\n1. The paper proposes an innovative approach to selective annotations, addressing the issues of existing work.\n2. Theoretical analysis provides an upper bound for the global optimal solution, demonstrating the effectiveness of the greedy search algorithm.\n3. Empirical evaluations show that IDEAL achieves better performance than previous state-of-the-art selective annotation baselines.\n4. The paper provides a comprehensive evaluation of IDEAL on various benchmarks, covering different tasks and datasets.\n5. The authors provide a clear and concise description of the method, making it easy to understand and implement.\n\nWeaknesses:\n1. The paper assumes that the influence function satisfies the submodular condition, which may not always be the case in practice.\n2. The empirical evaluations are conducted on a limited set of datasets, and it would be beneficial to evaluate IDEAL on more diverse datasets.\n3. The paper does not provide a thorough analysis of the computational complexity of the greedy search algorithm.\n4. The authors do not provide a detailed discussion of the limitations of IDEAL, such as the potential impact of noise in the data or the influence function.\n5. The paper could benefit from a more comprehensive comparison with other selective annotation methods, such as Vote-k and Fast Vote-k.\n\nSuggestions for Improvement:\n1. Investigate the robustness of IDEAL to non-submodular influence functions.\n2. Evaluate IDEAL on a larger and more diverse set of datasets.\n3. Analyze the computational complexity of the greedy search algorithm.\n4. Discuss the limitations of IDEAL in more detail, such as the potential impact of noise in the data or the influence function.\n5. Compare IDEAL with other selective annotation methods, such as Vote-k and Fast Vote-k, in a more comprehensive manner.\n\nRecommendation:\nThe paper proposes an innovative approach to selective annotations, addressing the issues of existing work. The theoretical analysis provides an upper bound for the global optimal solution, demonstrating the effectiveness of the greedy search algorithm. Empirical evaluations show that IDEAL achieves better performance than previous state-of-the-art selective annotation baselines. While there are some limitations to the paper, it provides a comprehensive evaluation of IDEAL on various benchmarks, making it a valuable contribution to the field of selective annotations. Therefore, I recommend accepting the paper for publication after addressing the suggested improvements.\n$$$",
        "gr_score_with": 0.7931034482758621,
        "gr_score_without": 0.7258485639686684,
        "output_without": {
            "num_tokens_scored": 383,
            "num_green_tokens": 278,
            "green_fraction": 0.7258485639686684,
            "z_score": -3.6275829230633345,
            "p_value": 0.9998569565462577
        },
        "output_with": {
            "num_tokens_scored": 406,
            "num_green_tokens": 322,
            "green_fraction": 0.7931034482758621,
            "z_score": -0.3470859010484301,
            "p_value": 0.6357366073903868
        }
    },
    {
        "title": "TOWARDS TRANSPARENT TIME SERIES FORECASTING",
        "abstract": "Transparent machine learning (ML) models are essential for ensuring interpretability and trustworthiness in decision-making systems, particularly in highstakes domains such as healthcare, finance, and criminal justice. While transparent machine learning models have been proposed for classification and regression, time series forecasting presents some unique challenges for ensuring transparency. In particular, currently used bottom-up approaches that focus on the values of the time series at specific time points (usually regularly spaced) do not provide a holistic understanding of the entire time series. This limits the applicability of ML in many critical areas. To open up these domains for ML, we propose a top-down framework of bi-level transparency, which involves understanding the higher-level trends and the lower-level properties of the predicted time series. Applying this framework, we develop TIMEVIEW, a transparent ML model for time series forecasting based on static features, complemented with an interactive visualization tool. Through a series of experiments, we demonstrate the efficacy and interpretability of our approach, paving the way for more transparent and reliable applications of ML in various domains.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel top-down approach to transparent time series forecasting, which is essential for decision-making systems in high-stakes domains. The authors introduce bi-level transparency, a framework that involves understanding the higher-level trends and the lower-level properties of the predicted time series. They develop TIMEVIEW, a transparent ML model for time series forecasting based on static features, complemented with an interactive visualization tool. The paper demonstrates the efficacy and interpretability of their approach through a series of experiments.\n\nStrengths:\n\n1. The paper provides a comprehensive overview of the challenges of transparent time series forecasting and the limitations of current bottom-up approaches.\n2. The authors introduce a novel top-down approach to trajectory comprehension, which is more consistent with the natural way people understand trajectories.\n3. The bi-level transparency framework is well-defined and provides a clear understanding of the impact of the input on the trends and properties of the predicted trajectory.\n4. The authors develop TIMEVIEW, a transparent ML model for time series forecasting based on static features, which is accompanied by an interactive visualization tool.\n5. The paper demonstrates the efficacy and interpretability of their approach through a series of experiments on real-world and synthetic datasets.\n\nWeaknesses:\n\n1. The paper assumes that the input space is static features, which may not be the case in many real-world applications where the input may contain the previous part of the trajectory or other exogenous time series.\n2. The authors do not provide a thorough comparison of their approach with existing methods for time series forecasting, which may make it difficult to evaluate the novelty and impact of their contribution.\n3. The paper does not provide a detailed analysis of the computational complexity of the bi-level transparency framework and the TIMEVIEW model.\n4. The authors do not discuss the potential limitations and risks of their approach, such as the potential for overfitting or the difficulty of interpreting complex models.\n\nSuggestions for Improvement:\n\n1. The authors should extend their approach to handle dynamic inputs, such as previous parts of the trajectory or other exogenous time series.\n2. The paper should provide a more thorough comparison of their approach with existing methods for time series forecasting, including both transparent and black-box models.\n3. The authors should provide a detailed analysis of the computational complexity of the bi-level transparency framework and the TIMEVIEW model.\n4. The paper should discuss the potential limitations and risks of their approach, such as the potential for overfitting or the difficulty of interpreting complex models.\n\nRecommendation:\n\nThis paper makes a significant contribution to the field of transparent time series forecasting by introducing a novel top-down approach and developing a transparent ML model for time series forecasting based on static features. The paper demonstrates the efficacy and interpretability of their approach through a series of experiments on real-world and synthetic datasets. However, the paper has some limitations, such as assuming static inputs and not providing a thorough comparison with existing methods. Therefore, I recommend that the authors address these limitations and provide a more comprehensive evaluation of their approach to make it more impactful and widely applicable.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel framework for transparent time series forecasting, called TIMEVIEW, which enables the interpretation of the predicted trajectory in terms of trends and properties. The authors propose a top-down approach, called bi-level transparency, which involves understanding the higher-level trends and the lower-level properties of the predicted time series. They formalize this framework using the concepts of motifs and compositions, and demonstrate its application in a time series forecasting model based on static features. The authors also provide a visualization tool to facilitate the understanding of the predicted trajectory.\n\nStrengths:\n\n* The paper presents a novel and well-motivated approach to transparent time series forecasting.\n* The authors provide a clear and concise formalization of the framework, making it easy to understand and replicate.\n* The paper includes a thorough comparison with existing methods, demonstrating the superiority of TIMEVIEW in terms of performance and interpretability.\n* The authors provide a comprehensive discussion of the limitations and challenges of the proposed approach, as well as potential future directions.\n\nWeaknesses:\n\n* The paper focuses primarily on the theoretical framework and the implementation of TIMEVIEW, with limited experimental evaluation of the proposed approach.\n* The authors do not provide a detailed comparison of the performance of TIMEVIEW with other state-of-the-art methods, beyond the comparison with transparent models and closed-form expressions.\n* The paper does not address the issue of handling multiple inputs, which is a common challenge in time series forecasting.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed experimental evaluation of TIMEVIEW, including a comparison with other state-of-the-art methods and a discussion of the results.\n* The paper should address the issue of handling multiple inputs, either by developing new methods or by providing a discussion of the limitations and challenges of the proposed approach.\n* The authors should provide a more detailed discussion of the visualization tool, including its limitations and challenges, as well as potential future directions.\n\nRecommendation:\n\nThe paper presents a novel and well-motivated approach to transparent time series forecasting, with a clear and concise formalization of the framework. However, the paper could benefit from a more detailed experimental evaluation and a discussion of the limitations and challenges of the proposed approach. Overall, the paper is well-written and well-organized, and the authors have made a significant contribution to the field of time series forecasting.\n\nRecommendation: Accept",
        "gr_score_with": 0.9124579124579124,
        "gr_score_without": 0.8275862068965517,
        "output_without": {
            "num_tokens_scored": 348,
            "num_green_tokens": 288,
            "green_fraction": 0.8275862068965517,
            "z_score": 1.286810891131923,
            "p_value": 0.09908010495364655
        },
        "output_with": {
            "num_tokens_scored": 297,
            "num_green_tokens": 271,
            "green_fraction": 0.9124579124579124,
            "z_score": 4.84537349553312,
            "p_value": 6.318685696865526e-07
        }
    },
    {
        "title": "DOLA: DECODING BY CONTRASTING LAYERS IMPROVES FACTUALITY IN LARGE LANGUAGE MODELS",
        "abstract": "Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional finetuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.1",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel decoding strategy, Decoding by Contrasting Layers (DoLa), to reduce hallucinations in large language models (LLMs). The approach exploits the hierarchical encoding of factual knowledge within transformer LLMs by contrasting the logits of higher and lower layers to improve factuality in the decoding process. The method dynamically selects the premature layer based on a distributional distance measure, and the contrasted output is used as the next-word prediction. The authors conduct experiments on multiple tasks, including multiple-choice, open-ended generation, and chain-of-thought reasoning, and demonstrate that DoLa significantly improves truthfulness without external information retrieval or model fine-tuning.\n\nStrengths:\n\n1. The paper provides a clear and concise introduction to the problem of hallucinations in LLMs and the need for a novel decoding strategy to address this issue.\n2. The proposed method, DoLa, is well-motivated and exploits the hierarchical encoding of factual knowledge within transformer LLMs.\n3. The authors provide a thorough experimental evaluation of DoLa on multiple tasks, including multiple-choice, open-ended generation, and chain-of-thought reasoning.\n4. The results demonstrate that DoLa significantly improves truthfulness without external information retrieval or model fine-tuning.\n\nWeaknesses:\n\n1. The paper assumes that the hierarchical encoding of factual knowledge within transformer LLMs is a reliable and accurate representation of the model's knowledge, which may not always be the case.\n2. The dynamic layer selection strategy used in DoLa may not be suitable for all tasks or datasets, and may require additional hyperparameter tuning.\n3. The paper does not provide a detailed analysis of the computational cost of DoLa, which may be a concern for large-scale applications.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational cost of DoLa and explore ways to optimize it for large-scale applications.\n2. The paper should provide a more thorough evaluation of the robustness of DoLa to different tasks, datasets, and hyperparameter settings.\n3. The authors should explore ways to combine DoLa with other methods, such as reinforcement learning from human feedback or external knowledge retrieval, to address the limitations of the current approach.\n\nRecommendation:\n\nThe paper provides a novel and well-motivated approach to reducing hallucinations in LLMs, and the experimental results demonstrate its effectiveness on multiple tasks. However, the paper assumes a reliable and accurate representation of the model's knowledge, and the dynamic layer selection strategy may require additional hyperparameter tuning. Therefore, I recommend that the authors address these limitations and provide a more detailed analysis of the computational cost of DoLa to make it a more robust and scalable approach.\n\nOverall, the paper provides a significant contribution to the field of NLP and demonstrates the potential of DoLa as a novel decoding strategy for reducing hallucinations in LLMs.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel decoding strategy, Decoding by Contrasting Layers (DoLa), to reduce hallucinations in large language models (LLMs). The approach exploits the hierarchical encoding of factual knowledge within transformer LLMs by contrasting the logits of higher and lower layers to improve factuality in the decoding process. The method dynamically selects appropriate layers based on token complexity and requires only a forward pass with early exiting from the same model itself. Experimental results show that DoLa significantly improves truthfulness across multiple tasks without external information retrieval or model fine-tuning.\n\nStrengths:\n1. The paper presents a novel approach to reducing hallucinations in LLMs, which is a significant problem in natural language processing.\n2. The method exploits the hierarchical encoding of factual knowledge within transformer LLMs, which is a key insight in understanding how LLMs work.\n3. The approach requires only a forward pass with early exiting from the same model itself, making it efficient and practical to implement.\n4. The experimental results show that DoLa significantly improves truthfulness across multiple tasks, demonstrating its effectiveness.\n\nWeaknesses:\n1. The paper focuses on factuality and does not explore other dimensions such as reinforcement learning from human feedback or using external knowledge bases for fine-tuning.\n2. The method relies on the model's internal knowledge without using external retrieval modules, which may limit its ability to correct misinformation acquired during training.\n3. The paper assumes that the LLM has been pre-trained on a large dataset, which may not be the case in all scenarios.\n\nSuggestions for Improvement:\n1. The authors could explore extending DoLa to other dimensions such as reinforcement learning from human feedback or using external knowledge bases for fine-tuning.\n2. The method could be combined with other techniques, such as inference-time self-consistency checks or multi-agent debating, to improve its effectiveness.\n3. The authors could investigate how DoLa performs on different types of tasks, such as text classification or sentiment analysis, to further evaluate its effectiveness.\n\nRecommendation:\nThe paper presents a novel and effective approach to reducing hallucinations in LLMs. The method is efficient, practical to implement, and demonstrates significant improvements in truthfulness across multiple tasks. While there are some limitations to the approach, such as its focus on factuality and reliance on internal knowledge, the authors provide a solid foundation for future work in this area. I recommend accepting the paper for publication, with the hope that the authors will continue to explore and improve their approach in future work.\n\n$$$",
        "gr_score_with": 0.8328767123287671,
        "gr_score_without": 0.7935656836461126,
        "output_without": {
            "num_tokens_scored": 373,
            "num_green_tokens": 296,
            "green_fraction": 0.7935656836461126,
            "z_score": -0.3103634275536617,
            "p_value": 0.6218576987690932
        },
        "output_with": {
            "num_tokens_scored": 365,
            "num_green_tokens": 304,
            "green_fraction": 0.8328767123287671,
            "z_score": 1.5705513192741691,
            "p_value": 0.058143452537057154
        }
    },
    {
        "title": "DOUBLY ROBUST PROXIMAL CAUSAL LEARNING FOR CONTINUOUS TREATMENTS",
        "abstract": "Proximal causal learning is a powerful framework for identifying the causal effect under the existence of unmeasured confounders. Within this framework, the doubly robust (DR) estimator was derived and has shown its effectiveness in estimation, especially when the model assumption is violated. However, the current form of the DR estimator is restricted to binary treatments, while the treatments can be continuous in many real-world applications. The primary obstacle to continuous treatments resides in the delta function present in the original DR estimator, making it infeasible in causal effect estimation and introducing a heavy computational burden in nuisance function estimation. To address these challenges, we propose a kernel-based DR estimator that can well handle continuous treatments for proximal causal learning. Equipped with its smoothness, we show that its oracle form is a consistent approximation of the influence function. Further, we propose a new approach to efficiently solve the nuisance functions. We then provide a comprehensive convergence analysis in terms of the mean square error. We demonstrate the utility of our estimator on synthetic datasets and real-world applications1.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a kernel-based doubly robust estimator for continuous treatments within the proximal causal framework. The authors address the challenge of estimating causal effects with continuous treatments by integrating a kernel function into the proximal inverse probability weighted estimator. They derive the kernel-based doubly robust estimator and show that it can approximate the influence function of the causal effect. The authors also propose a new approach to efficiently solve the nuisance functions, which is achieved by estimating the policy function and incorporating it into a min-max optimization problem. The convergence analysis of the proposed estimator is provided, and the authors demonstrate the utility of their method using synthetic data and the legalized abortion and crime dataset.\n\nStrengths:\n\n1. The paper addresses a significant challenge in causal inference, which is the estimation of causal effects with continuous treatments.\n2. The proposed kernel-based doubly robust estimator is a significant contribution to the field of causal inference, and it can handle continuous treatments.\n3. The authors provide a comprehensive convergence analysis of the proposed estimator, which is essential for understanding its performance.\n4. The paper demonstrates the utility of the proposed method using synthetic data and real-world data, which provides evidence of its effectiveness.\n\nWeaknesses:\n\n1. The paper assumes that the kernel function is bounded and differentiable, which may not be true for all kernel functions.\n2. The authors do not provide a detailed comparison of their method with existing methods, which makes it difficult to evaluate its performance.\n3. The paper assumes that the policy function can be estimated accurately, which may not be the case in practice.\n4. The authors do not provide a detailed discussion of the limitations of their method and potential future directions.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed comparison of the proposed method with existing methods, including a discussion of their strengths and weaknesses.\n2. Investigate the effect of different kernel functions on the performance of the proposed estimator.\n3. Provide a more detailed discussion of the limitations of the proposed method and potential future directions.\n4. Consider incorporating additional regularization techniques to improve the stability of the proposed estimator.\n\nRecommendation:\n\nThe paper is well-written and provides a significant contribution to the field of causal inference. However, it would benefit from a more detailed comparison with existing methods and a more detailed discussion of the limitations and potential future directions. With these improvements, the paper would be more comprehensive and provide a clearer understanding of the proposed method's performance.\n\nRating: 8/10\n\nRecommendation for Publication: Yes\n\nNote: The paper has the potential to be published in a top-tier journal in the field of causal inference, but it requires further improvements to make it more comprehensive and convincing.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a kernel-based doubly robust (DR) estimator for estimating causal effects under continuous treatments within the proximal causal learning (PCL) framework. The proposed estimator, called Proximal Kernel DR (PKDR), overcomes the challenges of estimating causal effects with continuous treatments by replacing the conventional indicator function with a kernel function. The PKDR estimator is shown to be consistent and has a convergence rate of O(n\u22124/5) in mean squared error (MSE). The paper also proposes a more efficient approach to estimating the nuisance function q0 by estimating the policy function and incorporating it into a min-max optimization. The utility of the PKDR estimator is demonstrated through synthetic and real-world data experiments.\n\nStrengths:\n\n* The paper addresses a critical challenge in causal inference, namely estimating causal effects with continuous treatments, and proposes a novel solution.\n* The proposed PKDR estimator is shown to be consistent and has a convergence rate of O(n\u22124/5) in MSE, which is faster than existing methods.\n* The paper proposes a more efficient approach to estimating the nuisance function q0, which is a key component of the PKDR estimator.\n* The utility of the PKDR estimator is demonstrated through synthetic and real-world data experiments.\n\nWeaknesses:\n\n* The paper assumes that the kernel function is bounded differentiable, which may not be a realistic assumption in practice.\n* The paper does not provide a comprehensive comparison with existing methods, such as regression-based models and generalized propensity score-based models.\n* The paper assumes that the policy function is known, which may not be a realistic assumption in practice.\n* The paper does not provide a thorough discussion of the limitations and potential biases of the PKDR estimator.\n\nSuggestions for Improvement:\n\n* The paper should provide a more comprehensive comparison with existing methods, such as regression-based models and generalized propensity score-based models.\n* The paper should relax the assumption that the kernel function is bounded differentiable and provide a more realistic assumption.\n* The paper should provide a more thorough discussion of the limitations and potential biases of the PKDR estimator.\n* The paper should provide a more detailed analysis of the variance reduction method, which is mentioned as a potential solution for large variance.\n\nRecommendation:\n\nThe paper proposes a novel and effective solution for estimating causal effects with continuous treatments within the PCL framework. However, there are some limitations and potential biases that need to be addressed. With some modifications and additional analysis, the paper has the potential to make a significant contribution to the field of causal inference.\n\nRating: 7/10\n\n$$$\nNote: The review is based on the provided paper and does not include any external knowledge or references. The review is intended to provide a critical evaluation of the paper's strengths and weaknesses, and suggestions for improvement.",
        "gr_score_with": 0.8134110787172012,
        "gr_score_without": 0.7584269662921348,
        "output_without": {
            "num_tokens_scored": 356,
            "num_green_tokens": 270,
            "green_fraction": 0.7584269662921348,
            "z_score": -1.9606789240763383,
            "p_value": 0.9750417554256988
        },
        "output_with": {
            "num_tokens_scored": 343,
            "num_green_tokens": 279,
            "green_fraction": 0.8134110787172012,
            "z_score": 0.6212231703660385,
            "p_value": 0.26722639814217797
        }
    },
    {
        "title": "GEOLLM: EXTRACTING GEOSPATIAL KNOWLEDGE FROM LARGE LANGUAGE MODELS",
        "abstract": "The application of machine learning (ML) in a range of geospatial tasks is increasingly common but often relies on globally available covariates such as satellite imagery that can either be expensive or lack predictive power. Here we explore the question of whether the vast amounts of knowledge found in Internet language corpora, now compressed within large language models (LLMs), can be leveraged for geospatial prediction tasks. We first demonstrate that LLMs embed remarkable spatial information about locations, but naively querying LLMs using geographic coordinates alone is ineffective in predicting key indicators like population density. We then present GeoLLM, a novel method that can effectively extract geospatial knowledge from LLMs with auxiliary map data from OpenStreetMap. We demonstrate the utility of our approach across multiple tasks of central interest to the international community, including the measurement of population density and economic livelihoods. Across these tasks, our method demonstrates a 70% improvement in performance (measured using Pearson\u2019s r) relative to baselines that use nearest neighbors or use information directly from the prompt, and performance equal to or exceeding satellite-based benchmarks in the literature. With GeoLLM, we observe that GPT-3.5 outperforms Llama 2 and RoBERTa by 19% and 51% respectively, suggesting that the performance of our method scales well with the size of the model and its pretraining dataset. Our experiments reveal that LLMs are remarkably sample-efficient, rich in geospatial information, and robust across the globe. Crucially, GeoLLM shows promise in mitigating the limitations of existing geospatial covariates and complementing them well. Code is available on the project website: https://rohinmanvi.github.io/GeoLLM",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces GeoLLM, a novel method that extracts geospatial knowledge from large language models (LLMs) using auxiliary map data from OpenStreetMap. The authors demonstrate that LLMs possess substantial geospatial knowledge and that their method can effectively unlock this knowledge across various models and tasks. They fine-tune LLMs using prompts constructed with map data and evaluate their performance on a range of geospatial prediction tasks. The results show that GeoLLM outperforms traditional baselines, including k-NN and XGBoost, and even exceeds the performance of satellite-based methods. The authors also observe that the performance of their method scales well with the size of the model and its pretraining dataset.\n\nStrengths:\n\n1. The paper presents a novel method for extracting geospatial knowledge from LLMs, which is a significant contribution to the field.\n2. The authors provide a comprehensive evaluation of their method on a range of geospatial prediction tasks, demonstrating its effectiveness and scalability.\n3. The paper highlights the potential of LLMs for geospatial tasks, which is an exciting area of research with many potential applications.\n4. The authors provide a clear and concise explanation of their method and its evaluation, making it easy to follow and understand.\n\nWeaknesses:\n\n1. The paper assumes that the reader is familiar with LLMs and their capabilities, which may not be the case for all readers.\n2. The evaluation of the method is limited to a few geospatial prediction tasks, and it would be beneficial to see a more comprehensive evaluation on a wider range of tasks.\n3. The paper does not provide a detailed explanation of the map data used in the prompts, which is a crucial component of the method.\n4. The authors do not discuss the potential biases of LLMs in geospatial tasks, which is an important consideration for applications in real-world settings.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the map data used in the prompts and its role in the method.\n2. Evaluate the method on a wider range of geospatial prediction tasks to demonstrate its scalability and effectiveness.\n3. Discuss the potential biases of LLMs in geospatial tasks and how they can be mitigated.\n4. Provide more information on the computational cost of the method and how it can be optimized for real-world applications.\n\nRecommendation:\n\nThe paper presents a novel and effective method for extracting geospatial knowledge from LLMs, which has significant potential for applications in geospatial tasks. While there are some limitations and areas for improvement, the paper provides a clear and concise explanation of the method and its evaluation, making it an excellent contribution to the field. I recommend the paper for publication in a top-tier conference or journal in the field of geospatial AI.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper introduces GeoLLM, a novel method that efficiently extracts geospatial knowledge from large language models (LLMs) by fine-tuning them on prompts constructed with auxiliary map data from OpenStreetMap. The authors demonstrate that LLMs possess substantial geospatial knowledge and that this knowledge can be unlocked using their method. They show that GeoLLM outperforms traditional baselines and satellite-based methods on various geospatial prediction tasks, including population density, asset wealth, and economic livelihoods. The authors also find that the performance of GeoLLM scales well with the size of the model and its pretraining dataset.\n\nStrengths:\n\n1. The paper presents a novel and effective method for extracting geospatial knowledge from LLMs.\n2. The authors demonstrate the utility of their method across various tasks and datasets, including population density, asset wealth, and economic livelihoods.\n3. The paper shows that GeoLLM outperforms traditional baselines and satellite-based methods on geospatial prediction tasks.\n4. The authors provide a comprehensive evaluation of their method, including an ablation study and comparison with state-of-the-art methods.\n5. The paper highlights the potential of GeoLLM to mitigate the limitations of traditional geospatial covariates.\n\nWeaknesses:\n\n1. The paper relies heavily on the assumption that LLMs possess geospatial knowledge, which may not be universally true.\n2. The authors do not provide a detailed analysis of the limitations of their method, including potential biases and errors.\n3. The paper does not discuss the potential ethical implications of using LLMs for geospatial prediction tasks.\n4. The authors do not provide a clear explanation of how they handled issues of data quality and availability.\n5. The paper does not discuss the potential for future work, including the extension of GeoLLM to other domains and the integration of other data sources.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the limitations of the method, including potential biases and errors.\n2. Discuss the potential ethical implications of using LLMs for geospatial prediction tasks.\n3. Provide a clear explanation of how the authors handled issues of data quality and availability.\n4. Discuss the potential for future work, including the extension of GeoLLM to other domains and the integration of other data sources.\n5. Provide more detailed information on the pre-processing and feature engineering steps involved in preparing the data for the experiments.\n\nRecommendation:\n\nThis paper presents a novel and effective method for extracting geospatial knowledge from LLMs. The authors demonstrate the utility of their method across various tasks and datasets, and show that GeoLLM outperforms traditional baselines and satellite-based methods on geospatial prediction tasks. While the paper has some limitations, the authors provide a comprehensive evaluation of their method and highlight the potential of GeoLLM to mitigate the limitations of traditional geospatial covariates. I recommend this paper for publication in the conference proceedings.\n\nRating: 8/10\n\nReviewer: [Your Name]",
        "gr_score_with": 0.9050445103857567,
        "gr_score_without": 0.8105263157894737,
        "output_without": {
            "num_tokens_scored": 380,
            "num_green_tokens": 308,
            "green_fraction": 0.8105263157894737,
            "z_score": 0.5132871524269715,
            "p_value": 0.3038752348141878
        },
        "output_with": {
            "num_tokens_scored": 337,
            "num_green_tokens": 305,
            "green_fraction": 0.9050445103857567,
            "z_score": 4.82113207010676,
            "p_value": 7.137290370410598e-07
        }
    },
    {
        "title": "PLUG-AND-PLAY: AN EFFICIENT POST-TRAINING PRUNING METHOD FOR LARGE LANGUAGE MODELS",
        "abstract": "With the rapid growth of large language models (LLMs), there is increasing demand for memory and computation for LLMs. Recent efforts on post-training pruning of LLMs aim to reduce the model size and computation, yet the performance is still sub-optimal. In this paper, we present a plug-and-play solution for post-training pruning of LLMs. The proposed solution has two innovative components: 1) Relative Importance and Activations (RIA), a new pruning metric that jointly considers the weight and activations efficiently on LLMs; and 2) Channel Permutation, a new approach to maximally preserve important weights under N:M sparsity. The proposed two components can be readily combined to further enhance the N:M structured pruned LLMs. Our empirical experiments show that RIA alone can already surpass all existing post-training pruning methods on prevalent LLMs, e.g., LLaMA ranging from 7B to 65B. Furthermore, N:M structured pruning with channel permutation can even outperform the original LLaMA2 70B on zero-shot tasks, together with practical speed-up on specific hardware.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a plug-and-play post-training pruning method for large language models (LLMs) that combines two novel components: Relative Importance and Activations (RIA) and Channel Permutation. RIA is a new pruning metric that jointly considers the weight and activations of LLMs, while Channel Permutation is a new approach to maximize the preservation of important weights under N:M sparsity. The authors demonstrate that RIA alone can outperform existing post-training pruning methods on prevalent LLMs, and that Channel Permutation can even outperform the original LLaMA2 70B on zero-shot tasks.\n\nStrengths:\n\n1. The paper proposes two novel and innovative components, RIA and Channel Permutation, which can be combined to further enhance the N:M structured pruned LLMs.\n2. The authors demonstrate the effectiveness of RIA and Channel Permutation through extensive experiments on prominent LLMs like LLaMA, LLaMA2, and OPT across varying model sizes.\n3. The paper provides a thorough analysis of the performance of RIA and Channel Permutation under different sparsity levels, calibration data, and zero-shot performance.\n4. The authors demonstrate the hardware-friendliness of their method, ensuring enhanced inference acceleration.\n\nWeaknesses:\n\n1. The paper assumes that the input and output channels of weight have unique information, which may not always be the case in LLMs.\n2. The authors do not provide a thorough analysis of the computational complexity of their method, which may be a concern for large-scale LLMs.\n3. The paper assumes that the Hungarian algorithm can efficiently solve the linear sum assignment problem, which may not always be the case in practice.\n4. The authors do not provide a clear comparison with other pruning methods that do not require calibration data.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more thorough analysis of the computational complexity of their method and its implications for large-scale LLMs.\n2. The paper should include a more detailed comparison with other pruning methods that do not require calibration data.\n3. The authors should provide more insights into the assumption that the input and output channels of weight have unique information in LLMs.\n4. The paper should include more experiments to evaluate the robustness of RIA and Channel Permutation under different scenarios.\n\nRecommendation:\n\nThe paper proposes a novel and effective plug-and-play post-training pruning method for LLMs that combines RIA and Channel Permutation. The authors demonstrate the effectiveness of their method through extensive experiments and provide a thorough analysis of its performance under different sparsity levels, calibration data, and zero-shot performance. However, the paper assumes that the input and output channels of weight have unique information, and does not provide a thorough analysis of the computational complexity of their method. Therefore, the paper is recommended for publication after addressing these concerns.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a plug-and-play post-training pruning method for large language models (LLMs). The proposed solution has two key components: Relative Importance and Activations (RIA), a new pruning metric that jointly considers the weight and activations efficiently on LLMs; and Channel Permutation, a new approach to maximally preserve important weights under N:M sparsity. The authors demonstrate that RIA alone can surpass all existing post-training pruning methods on prevalent LLMs and when combined with Channel Permutation, can even outperform the original LLaMA2 70B on zero-shot tasks.\n\nStrengths:\n\n1. The paper proposes a novel and efficient post-training pruning method for LLMs.\n2. The authors demonstrate state-of-the-art performance on various benchmarks and datasets.\n3. The proposed method is hardware-friendly and enables inference acceleration.\n4. The paper provides a comprehensive analysis of the strengths and weaknesses of existing methods and highlights the advantages of the proposed approach.\n\nWeaknesses:\n\n1. The paper assumes a pre-trained model as input, which may not be feasible in all scenarios.\n2. The authors do not provide a detailed analysis of the computational complexity of the proposed method.\n3. The paper relies heavily on experiments and does not provide a theoretical analysis of the proposed approach.\n4. The authors do not discuss potential applications of the proposed method beyond language modeling tasks.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the computational complexity of the proposed method.\n2. Discuss potential applications of the proposed method beyond language modeling tasks.\n3. Consider providing a theoretical analysis of the proposed approach.\n4. Evaluate the proposed method on more diverse and challenging datasets.\n\nRecommendation:\n\nThe paper proposes a novel and efficient post-training pruning method for LLMs, which demonstrates state-of-the-art performance on various benchmarks and datasets. However, the paper assumes a pre-trained model as input and does not provide a detailed analysis of the computational complexity of the proposed method. Despite these limitations, the paper is well-written and provides a comprehensive analysis of the strengths and weaknesses of existing methods and highlights the advantages of the proposed approach. I recommend this paper for publication in a top-tier conference in the field of natural language processing.\n\nRating: 8/10\n$$",
        "gr_score_with": 0.782312925170068,
        "gr_score_without": 0.6923076923076923,
        "output_without": {
            "num_tokens_scored": 338,
            "num_green_tokens": 234,
            "green_fraction": 0.6923076923076923,
            "z_score": -4.949402895448642,
            "p_value": 0.999999627792468
        },
        "output_with": {
            "num_tokens_scored": 294,
            "num_green_tokens": 230,
            "green_fraction": 0.782312925170068,
            "z_score": -0.7578991546325657,
            "p_value": 0.7757443217951121
        }
    },
    {
        "title": "TEST: TEXT PROTOTYPE ALIGNED EMBEDDING TO ACTIVATE LLM\u2019S ABILITY FOR TIME SERIES",
        "abstract": "This work summarizes two ways to accomplish Time-Series (TS) tasks in today\u2019s Large Language Model (LLM) context: LLM-for-TS (model-centric) designs and trains a fundamental large model, or fine-tunes a pre-trained LLM for TS data; TS-for-LLM (data-centric) converts TS into a model-friendly representation to enable the pre-trained LLM to handle TS data. Given the lack of data, limited resources, semantic context requirements, and so on, this work focuses on TSfor-LLM, where we aim to activate LLM\u2019s ability for TS data by designing a TS embedding method suitable for LLM. The proposed method is named TEST. It first tokenizes TS, builds an encoder to embed TS via instance-wise, feature-wise, and text-prototype-aligned contrast, where the TS embedding space is aligned to LLM\u2019s embedding layer space, then creates soft prompts to make LLM more open to that embeddings, and finally implements TS tasks using the frozen LLM. We also demonstrate the feasibility of TS-for-LLM through theory and experiments. Experiments are carried out on TS classification, forecasting, and representation tasks using eight frozen LLMs with various structures and sizes. The results show that the pre-trained LLM with TEST strategy can achieve better or comparable performance than today\u2019s SOTA TS models and offer benefits for few-shot and generalization. By treating LLM as the pattern machine, TEST can endow LLM\u2019s ability to process TS data without compromising language ability. We hope that this study will serve as a foundation for future work to support TS+LLM progress.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method called TEST (Text-Prototype-Aligned Embedding to Activate LLM's Ability for Time Series) to enable Large Language Models (LLMs) to handle Time-Series (TS) data. The method focuses on the TS-for-LLM paradigm, where TS is converted into a model-friendly representation to enable LLMs to understand TS data. The proposed TEST method tokenizes TS, builds an encoder to embed TS via instance-wise, feature-wise, and text-prototype-aligned contrast, and creates soft prompts to make LLM more open to the embeddings. The authors demonstrate the feasibility of TS-for-LLM through theory and experiments on TS classification, forecasting, and representation tasks using eight frozen LLMs with various structures and sizes.\n\nStrengths:\n\n1. The paper proposes a novel method, TEST, to enable LLMs to handle TS data, which is a significant contribution to the field.\n2. The authors provide a thorough analysis of the TS-for-LLM paradigm and its potential methods, which is a valuable addition to the existing literature.\n3. The experiments demonstrate the effectiveness of TEST on various TS tasks, including classification, forecasting, and representation tasks.\n4. The authors provide a detailed comparison with existing methods, including LLM-QA methods, SFT LLM-for-TS methods, classical TS models, and SOTA TS models.\n\nWeaknesses:\n\n1. The paper assumes that the LLMs used are pre-trained and frozen, which may not be the case in real-world applications where LLMs are fine-tuned or trained from scratch.\n2. The authors do not provide a thorough analysis of the impact of the LLM size and type on the results, which is an important aspect to consider in real-world applications.\n3. The paper assumes that the TS data is available in a format that can be easily tokenized and embedded, which may not be the case in real-world applications where TS data may be noisy or missing.\n4. The authors do not provide a detailed comparison with other TS-for-LLM methods, which would be a valuable addition to the existing literature.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the impact of the LLM size and type on the results, including the effect of different architectures and training procedures.\n2. The authors should provide a more thorough comparison with other TS-for-LLM methods, including a discussion of the strengths and weaknesses of each method.\n3. The authors should provide a more detailed analysis of the TS data used in the experiments, including a discussion of the noise and missing values present in the data.\n4. The authors should provide a more detailed discussion of the potential applications of TEST in real-world scenarios, including a discussion of the potential benefits and limitations of the method.\n\nRecommendation:\n\nThe paper proposes a novel method, TEST, to enable LLMs to handle TS data, which is a significant contribution to the field. The experiments demonstrate the effectiveness of TEST on various TS tasks, including classification, forecasting, and representation tasks. However, the paper assumes that the LLMs used are pre-trained and frozen, which may not be the case in real-world applications. Therefore, the authors should provide a more detailed analysis of the impact of the LLM size and type on the results, including the effect of different architectures and training procedures. Additionally, the authors should provide a more thorough comparison with other TS-for-LLM methods, including a discussion of the strengths and weaknesses of each method.\n\nIn conclusion, the paper proposes a novel method, TEST, to enable LLMs to handle TS data, which is a significant contribution to the field. However, the paper has some limitations that need to be addressed in future work.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes an approach called TEST (Text Embedding Space Transformation) to enable large language models (LLMs) to perform time-series (TS) tasks without significant modifications. TEST focuses on the TS-for-LLM paradigm, where the existing pre-trained LLM is adapted to handle TS data by designing a TS embedding method suitable for LLM. The proposed method includes three steps: (1) tokenizing TS, (2) building an encoder to embed TS via instance-wise, feature-wise, and text-prototype-aligned contrast, and (3) creating soft prompts to make LLM more open to TS embeddings.\n\nStrengths:\n\n1. The paper presents a comprehensive analysis of the TS+LLM paradigm, highlighting the benefits of the TS-for-LLM approach, which includes using relatively small datasets, generalizing to various TS tasks, and maintaining the original language ability of the pre-trained LLM.\n2. The proposed TEST method is novel and demonstrates the feasibility of TS-for-LLM through theoretical analysis and experiments on various TS tasks, including classification, forecasting, and representation learning.\n3. The experiments demonstrate the effectiveness of TEST in achieving comparable performance to state-of-the-art (SOTA) TS models, even when using smaller LLMs.\n\nWeaknesses:\n\n1. The paper relies heavily on the pre-trained LLM, which may limit the applicability of TEST to specific tasks or domains, particularly when the pre-trained LLM is not suitable for the task at hand.\n2. The proposed method assumes that the pre-trained LLM has an embedding layer that can be skipped, which may not be the case for all pre-trained LLMs.\n3. The paper does not provide a clear explanation of how the text prototypes are selected, which may be crucial for the success of TEST.\n\nSuggestions for Improvement:\n\n1. Provide more details on the selection of text prototypes, including how they are chosen, how many are used, and how their performance is evaluated.\n2. Investigate the robustness of TEST to different pre-trained LLMs, including those with different embedding layers or architectures.\n3. Consider exploring the applicability of TEST to other tasks or domains, such as image or speech processing, to demonstrate its versatility.\n\nRecommendation:\n\nThe paper presents an interesting approach to enabling LLMs to perform TS tasks, and the experiments demonstrate its effectiveness. However, to fully understand the strengths and weaknesses of TEST, more research is needed to address the limitations mentioned above. With further improvements, TEST has the potential to become a valuable tool for adapting pre-trained LLMs to various tasks and domains.\n\nRating: 7/10\n\n$$$",
        "gr_score_with": 0.8568019093078759,
        "gr_score_without": 0.7912371134020618,
        "output_without": {
            "num_tokens_scored": 388,
            "num_green_tokens": 307,
            "green_fraction": 0.7912371134020618,
            "z_score": -0.4312099146274895,
            "p_value": 0.6668421272482181
        },
        "output_with": {
            "num_tokens_scored": 419,
            "num_green_tokens": 359,
            "green_fraction": 0.8568019093078759,
            "z_score": 2.9070504129706443,
            "p_value": 0.0018242719628901248
        }
    },
    {
        "title": "SHARING RATIO DECOMPOSITION",
        "abstract": "The truthfulness of existing explanation methods in authentically elucidating the underlying model\u2019s decision-making process has been questioned. Existing methods have deviated from faithfully representing the model, thus susceptible to adversarial attacks. To address this, we propose a novel eXplainable AI (XAI) method called SRD (Sharing Ratio Decomposition), which sincerely reflects the model\u2019s inference process, resulting in significantly enhanced robustness in our explanations. Different from the conventional emphasis on the neuronal level, we adopt a vector perspective to consider the intricate nonlinear interactions between filters. We also introduce an interesting observation termed Activation-PatternOnly Prediction (APOP), letting us emphasize the importance of inactive neurons and redefine relevance encapsulating all relevant information including both active and inactive neurons. Our method, SRD, allows for the recursive decomposition of a Pointwise Feature Vector (PFV), providing a high-resolution Effective Receptive Field (ERF) at any layer.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel eXplainable AI (XAI) method called Sharing Ratio Decomposition (SRD), which aims to provide faithful and robust explanations for deep learning models. SRD analyzes the model using Pointwise Feature Vectors (PFVs) and decomposes relevance with sharing ratios, accounting for nonlinear interactions between filters. The method is based on the observation of Activation-Pattern-Only Prediction (APOP), which highlights the importance of inactive neurons in model behavior. SRD is shown to outperform other saliency map methods across various metrics, including effectiveness, sophistication, and resilience, particularly in terms of robustness against random noise perturbation and targeted adversarial attacks.\n\nStrengths:\n\n* The paper presents a novel and comprehensive XAI method, SRD, which addresses the limitations of existing explanation methods.\n* The method is based on a thorough reflection of the model's behavior, which is reflected in its robustness against random noise perturbation and targeted adversarial attacks.\n* The paper provides a clear and well-structured explanation of the method, including its mathematical formulation and experimental results.\n* The authors have conducted a comprehensive analysis, including qualitative and quantitative results, which demonstrate the superiority of SRD over other saliency map methods.\n\nWeaknesses:\n\n* The paper assumes that the sharing ratio can be derived at any point, even during inference, which may not be feasible in practice.\n* The method requires prior knowledge of the sharing ratio, which may not be available in all cases.\n* The paper does not provide a clear explanation of how the sharing ratio is calculated, which may be a limitation of the method.\n* The experimental results are limited to a specific dataset and model architecture, which may not be representative of all scenarios.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of how the sharing ratio is calculated and how it is used in the method.\n* Investigate the feasibility of deriving the sharing ratio at any point during inference and provide a clear explanation of the challenges and limitations.\n* Conduct experiments on a wider range of datasets and model architectures to demonstrate the robustness and generalizability of SRD.\n* Provide a more detailed analysis of the APOP phenomenon and its implications for model behavior.\n\nRecommendation:\n\nThe paper presents a novel and comprehensive XAI method, SRD, which addresses the limitations of existing explanation methods. While there are some limitations and areas for improvement, the paper provides a clear and well-structured explanation of the method and demonstrates its superiority over other saliency map methods. Therefore, I recommend this paper for publication in a reputable conference or journal.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel eXplainable AI (XAI) method called Sharing Ratio Decomposition (SRD) to provide faithful and robust explanations for deep neural networks. SRD adopts a vector perspective to analyze the intricate nonlinear interactions between filters and introduces an interesting observation called Activation-PatternOnly Prediction (APOP) to highlight the importance of inactive neurons. The method decomposes the decision-making process of the model into a series of sharing ratios, allowing for the recursive decomposition of Pointwise Feature Vectors (PFVs) and the derivation of high-resolution Effective Receptive Fields (ERFs) at any layer.\n\nStrengths:\n\n1. SRD provides a comprehensive and faithful explanation of the model's decision-making process, which is essential for understanding and trustworthiness.\n2. The method adopts a vector perspective, which accounts for nonlinear interactions between filters and provides a more accurate representation of the model's behavior.\n3. APOP highlights the importance of inactive neurons, which has been overlooked by conventional approaches.\n4. SRD achieves superior performance in various metrics, including effectiveness, sophistication, and resilience, compared to other saliency map methods.\n5. The method provides a high-resolution Effective Receptive Field (ERF) at any layer, enabling comprehensive exploration from local to global explanations.\n\nWeaknesses:\n\n1. The paper assumes that the sharing ratio between layers can be derived at any point, which may not be feasible in practice.\n2. The method relies on the availability of model-generated information, which may not be possible in some scenarios.\n3. The paper does not provide a thorough analysis of the computational complexity of SRD, which is essential for large-scale applications.\n4. The method requires prior knowledge of the sharing ratio, which can be challenging to obtain in some cases.\n5. The paper does not provide a clear comparison with other state-of-the-art XAI methods, which makes it difficult to assess the method's effectiveness.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the computational complexity of SRD and its scalability to large-scale applications.\n2. Investigate the possibility of deriving the sharing ratio between layers without prior knowledge.\n3. Compare SRD with other state-of-the-art XAI methods to assess its effectiveness.\n4. Provide more examples and case studies to demonstrate the method's applicability and effectiveness.\n5. Investigate the possibility of extending SRD to other types of neural networks, such as recurrent neural networks (RNNs) and transformers.\n\nRecommendation:\n\nThe paper proposes an innovative XAI method that provides faithful and robust explanations for deep neural networks. While there are some limitations and areas for improvement, the method has the potential to significantly contribute to the field of XAI. I recommend the paper for publication in a top-tier conference, with the suggestion that the authors address the limitations and areas for improvement mentioned above.",
        "gr_score_with": 0.8925233644859814,
        "gr_score_without": 0.8092783505154639,
        "output_without": {
            "num_tokens_scored": 388,
            "num_green_tokens": 314,
            "green_fraction": 0.8092783505154639,
            "z_score": 0.457207589671297,
            "p_value": 0.32376092182531135
        },
        "output_with": {
            "num_tokens_scored": 428,
            "num_green_tokens": 382,
            "green_fraction": 0.8925233644859814,
            "z_score": 4.785612259529333,
            "p_value": 8.523343630526342e-07
        }
    },
    {
        "title": "CROSSLOCO: HUMAN MOTION DRIVEN CONTROL OF LEGGED ROBOTS VIA GUIDED UNSUPERVISED REIN-",
        "abstract": "Human motion driven control (HMDC) is an effective approach for generating natural and compelling robot motions while preserving high-level semantics. However, establishing the correspondence between humans and robots with different body structures is not straightforward due to the mismatches in kinematics and dynamics properties, which causes intrinsic ambiguity to the problem. Many previous algorithms approach this motion retargeting problem with unsupervised learning, which requires the prerequisite skill sets. However, it will be extremely costly to learn all the skills without understanding the given human motions, particularly for high-dimensional robots. In this work, we introduce CrossLoco, a guided unsupervised reinforcement learning framework that simultaneously learns robot skills and their correspondence to human motions. Our key innovation is to introduce a cycle-consistency-based reward term designed to maximize the mutual information between human motions and robot states. We demonstrate that the proposed framework can generate compelling robot motions by translating diverse human motions, such as running, hopping, and dancing. We quantitatively compare our CrossLoco against the manually engineered and unsupervised baseline algorithms along with the ablated versions of our framework and demonstrate that our method translates human motions with better accuracy, diversity, and user preference. We also showcase its utility in other applications, such as synthesizing robot movements from language input and enabling interactive robot control.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents CrossLoco, a guided unsupervised reinforcement learning framework that enables simultaneous learning of human-robot motion correspondence and robot motion control. The framework introduces a cycle-consistency-based correspondence reward term that maximizes the mutual information between human motions and synthesized robot movements. The authors demonstrate that CrossLoco can translate a large set of human motions for robots, including walking, running, and dancing, and achieve better quantitative results in terms of accuracy, diversity, and user preference compared to baseline methods.\n\nStrengths:\n\n* The paper presents a novel framework for learning human-robot motion correspondence and robot motion control in an unsupervised manner.\n* The cycle-consistency-based correspondence reward term is a key innovation that enables the learning of complex robot skills and human-robot motion correspondence.\n* The authors provide a thorough evaluation of their method, including quantitative and qualitative results, as well as a comparison with baseline methods.\n* The paper explores the potential applications of CrossLoco, including language2text motion synthesis and interactive robot control.\n\nWeaknesses:\n\n* The paper assumes that the human motion dataset is available, which may not be the case in many real-world scenarios.\n* The authors do not provide a clear explanation of how the cycle-consistency-based correspondence reward term is computed, which may make it difficult for readers to reproduce the results.\n* The paper does not discuss the scalability of CrossLoco to more complex scenarios, such as long-horizon human demonstrations or tool manipulation on a legged-manipulation robot.\n* The authors do not provide a clear comparison with other unsupervised reinforcement learning methods that do not use cycle-consistency.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of how the cycle-consistency-based correspondence reward term is computed, including the specific mathematical formulation and the parameters used.\n* Discuss the scalability of CrossLoco to more complex scenarios, such as long-horizon human demonstrations or tool manipulation on a legged-manipulation robot.\n* Compare CrossLoco with other unsupervised reinforcement learning methods that do not use cycle-consistency to provide a more comprehensive evaluation of the method.\n* Provide more details on the implementation of the framework, including the specific neural network architectures used and the hyperparameters employed.\n\nRecommendation:\n\nThe paper presents a novel and innovative framework for learning human-robot motion correspondence and robot motion control in an unsupervised manner. While there are some limitations and areas for improvement, the paper provides a thorough evaluation of the method and explores its potential applications. I recommend this paper for publication in a top-tier conference in the field of robotics and machine learning.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper presents CrossLoco, a guided unsupervised reinforcement learning framework for learning human-robot motion correspondence and robot locomotion control. The framework introduces a cycle-consistency-based correspondence reward term that maximizes the mutual information between human motions and synthesized robot movements. The authors demonstrate that CrossLoco can translate diverse human motions to robots with different morphologies, including walking, running, and dancing. They also showcase the framework's applications in language-to-text motion synthesis and interactive robot control.\n\nStrengths:\n1. The paper presents a novel framework for learning human-robot motion correspondence and robot locomotion control, which is a challenging problem in robotics.\n2. The authors introduce a cycle-consistency-based correspondence reward term, which is a key innovation in the paper.\n3. The paper provides a thorough evaluation of the framework, including quantitative comparisons with baseline methods and user studies.\n4. The authors showcase the framework's applications in language-to-text motion synthesis and interactive robot control, which is a significant contribution to the field.\n\nWeaknesses:\n1. The paper assumes a high-level understanding of reinforcement learning and human-robot motion correspondence, which may make it challenging for readers without a strong background in these areas.\n2. The authors do not provide a detailed analysis of the computational complexity of the framework, which may be a concern for large-scale applications.\n3. The paper relies on a specific dataset (LaFAN1) for training and evaluation, which may limit the generalizability of the results.\n4. The authors do not provide a clear explanation of how the framework can be extended to more complex scenarios, such as long-horizon human demonstrations.\n\nSuggestions for Improvement:\n1. Provide a more detailed explanation of the framework's assumptions and limitations, particularly in terms of computational complexity.\n2. Evaluate the framework on a broader range of datasets and applications to improve generalizability.\n3. Provide a clearer explanation of how the framework can be extended to more complex scenarios, such as long-horizon human demonstrations.\n4. Consider including additional visualizations and results to provide a more comprehensive understanding of the framework's performance.\n\nRecommendation:\nOverall, the paper presents a novel and promising framework for learning human-robot motion correspondence and robot locomotion control. While there are some limitations and areas for improvement, the paper provides a thorough evaluation and showcases significant contributions to the field. I recommend accepting the paper for publication, subject to the authors addressing the suggestions for improvement.\n\nRating: 8.5/10\n$$$",
        "gr_score_with": 0.8662790697674418,
        "gr_score_without": 0.768361581920904,
        "output_without": {
            "num_tokens_scored": 354,
            "num_green_tokens": 272,
            "green_fraction": 0.768361581920904,
            "z_score": -1.4878724129188727,
            "p_value": 0.9316077278871177
        },
        "output_with": {
            "num_tokens_scored": 344,
            "num_green_tokens": 298,
            "green_fraction": 0.8662790697674418,
            "z_score": 3.073487313477209,
            "p_value": 0.0010578634167512531
        }
    },
    {
        "title": "A PRECISE CHARACTERIZATION OF SGD STABILITY USING LOSS SURFACE GEOMETRY",
        "abstract": "Stochastic Gradient Descent (SGD) stands as a cornerstone optimization algorithm with proven real-world empirical successes but relatively limited theoretical understanding. Recent research has illuminated a key factor contributing to its practical efficacy: the implicit regularization it instigates. Several studies have investigated the linear stability property of SGD in the vicinity of a stationary point as a predictive proxy for sharpness and generalization error in overparameterized neural networks (Wu et al., 2022; Jastrzebski et al., 2019; Cohen et al., 2021). In this paper, we delve deeper into the relationship between linear stability and sharpness. More specifically, we meticulously delineate the necessary and sufficient conditions for linear stability, contingent on hyperparameters of SGD and the sharpness at the optimum. Towards this end, we introduce a novel coherence measure of the loss Hessian that encapsulates pertinent geometric properties of the loss function that are relevant to the linear stability of SGD. It enables us to provide a simplified sufficient condition for identifying linear instability at an optimum. Notably, compared to previous works, our analysis relies on significantly milder assumptions and is applicable for a broader class of loss functions than known before, encompassing not only mean-squared error but also cross-entropy loss.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a precise characterization of the stability of Stochastic Gradient Descent (SGD) using loss surface geometry. The authors focus on the linearized dynamics of SGD close to the optimum, and introduce a novel coherence measure \u03c3 to capture the relevant geometric characteristics of the loss surface. They provide a sufficient condition for the linear dynamics to diverge, which relies solely on the values of \u03bb1(H), \u03b7, B, n, and \u03c3. The authors also demonstrate that their sufficient condition is nearly optimal across a natural range of SGD hyperparameters.\n\nStrengths:\n\n* The paper presents a novel coherence measure \u03c3 that summarizes relevant information in the loss surface geometry.\n* The authors provide a precise characterization of the stability of SGD using linearized dynamics.\n* The paper demonstrates that the sufficient condition for divergence is nearly optimal across a natural range of SGD hyperparameters.\n* The authors provide a series of experiments to validate their theoretical results.\n\nWeaknesses:\n\n* The paper assumes that the loss function is additively decomposable, which may not be the case in all scenarios.\n* The authors do not provide a detailed comparison of their results with other existing works in the field.\n* The paper assumes that the learning rate and batch size are fixed, which may not be the case in all scenarios.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed comparison of their results with other existing works in the field.\n* The paper should be extended to consider more general loss functions and learning rates.\n* The authors should provide a more detailed analysis of the coherence measure \u03c3 and its relation to the stability of SGD.\n\nRecommendation:\n\nThe paper presents a novel and precise characterization of the stability of SGD using loss surface geometry. The authors provide a sufficient condition for divergence that is nearly optimal across a natural range of SGD hyperparameters. The paper also provides a series of experiments to validate their theoretical results. However, the paper assumes that the loss function is additively decomposable and does not provide a detailed comparison of their results with other existing works in the field. Overall, the paper is well-written and provides a significant contribution to the field of optimization.\n\nRating: 8/10\n\nRecommendation for Publication: Yes",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a precise characterization of stochastic gradient descent (SGD) stability using loss surface geometry. The authors focus on the linear stability property of SGD in the vicinity of a stationary point as a predictive proxy for sharpness and generalization error in overparameterized neural networks. They introduce a novel coherence measure of the loss Hessian that encapsulates pertinent geometric properties of the loss function relevant to the linear stability of SGD. The authors provide a simplified sufficient condition for identifying linear instability at an optimum and demonstrate that this condition is nearly optimal across a natural range of SGD hyperparameters.\n\nStrengths:\n\n* The paper provides a novel and interpretable sufficient condition for identifying linear instability at an optimum using a coherence measure of the loss Hessian.\n* The authors demonstrate that this condition is nearly optimal across a natural range of SGD hyperparameters.\n* The paper presents a rigorous theoretical framework for understanding the linear stability of SGD in the vicinity of a stationary point.\n* The authors provide a detailed comparison of their results with previous work in the field.\n\nWeaknesses:\n\n* The paper assumes a fixed point w\u2217 as a local minimum of \u2113i(\u00b7) for all i \u2208 [n], which may not hold in all cases.\n* The authors rely on a quadratic approximation to the loss around a fixed point w\u2217, which may not be accurate in all cases.\n* The paper assumes a specific form of the loss function, which may not be generalizable to all types of loss functions.\n\nSuggestions for Improvement:\n\n* The authors should provide more details on how to compute the coherence measure \u03c3 in practice, especially for large neural networks.\n* The paper should provide more experimental results to validate the theoretical findings, especially for different types of loss functions and neural network architectures.\n* The authors should provide more insights on how to use the coherence measure \u03c3 to improve the stability of SGD in practice.\n\nRecommendation:\n\nThe paper provides a significant contribution to the field of optimization and machine learning by providing a novel and interpretable sufficient condition for identifying linear instability at an optimum using a coherence measure of the loss Hessian. The authors demonstrate that this condition is nearly optimal across a natural range of SGD hyperparameters, which provides a rigorous theoretical framework for understanding the linear stability of SGD in the vicinity of a stationary point. However, the paper assumes a fixed point w\u2217 as a local minimum of \u2113i(\u00b7) for all i \u2208 [n], which may not hold in all cases, and relies on a quadratic approximation to the loss around a fixed point w\u2217, which may not be accurate in all cases. To further improve the paper, the authors should provide more details on how to compute the coherence measure \u03c3 in practice, especially for large neural networks, and provide more experimental results to validate the theoretical findings, especially for different types of loss functions and neural network architectures.\n\nOverall, the paper is well-written, and the authors provide a clear and concise presentation of their results. The paper has the potential to make a significant impact in the field of optimization and machine learning by providing a novel and interpretable sufficient condition for identifying linear instability at an optimum using a coherence measure of the loss Hessian.\n\nRecommendation: Accept",
        "gr_score_with": 0.832807570977918,
        "gr_score_without": 0.7846153846153846,
        "output_without": {
            "num_tokens_scored": 260,
            "num_green_tokens": 204,
            "green_fraction": 0.7846153846153846,
            "z_score": -0.6199149802660949,
            "p_value": 0.732343118660655
        },
        "output_with": {
            "num_tokens_scored": 317,
            "num_green_tokens": 264,
            "green_fraction": 0.832807570977918,
            "z_score": 1.4605660447533022,
            "p_value": 0.07206728519141255
        }
    },
    {
        "title": "DON\u2019T TRUST: VERIFY \u2013 GROUNDING LLM QUANTI-",
        "abstract": "Large language models (LLM), such as Google\u2019s Minerva and OpenAI\u2019s GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code \u2014 which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting \u2014 the previously best method to identify correct answers, by more than 12% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes an approach called \"Don't Trust: Verify\" (DTV) to identify the correct answer among multiple informal solution samples generated by large language models (LLMs) for quantitative reasoning problems. DTV leverages the autoformalization capability of LLMs through few-shot prompting to translate informal statements into formal statements, and then uses a formal theorem proving environment to verify the correctness of the formal statements. The authors demonstrate the effectiveness of DTV by evaluating it on three quantitative reasoning datasets: GSM8K, MATH, and MultiArith. The results show that DTV consistently outperforms vanilla majority voting, the best previous approach, and leads to improvement across different model sizes from 8B, 62B, and 540B.\n\nStrengths:\n\n* The paper proposes a novel approach to identify the correct answer among multiple informal solution samples generated by LLMs.\n* The authors demonstrate the effectiveness of DTV on three quantitative reasoning datasets: GSM8K, MATH, and MultiArith.\n* The paper provides a clear and concise description of the DTV approach and its implementation details.\n* The authors discuss the limitations of the current approach and provide suggestions for future work.\n\nWeaknesses:\n\n* The paper assumes that the LLMs used for autoformalization have sufficient training data and capabilities to translate informal statements into formal statements accurately.\n* The authors do not provide a thorough analysis of the errors made by the LLMs in translating informal statements into formal statements.\n* The paper relies on the consistency of the formal environment and its automated theorem prover to verify the correctness of the formal statements, which may not always be the case.\n* The authors do not provide a detailed discussion of the computational cost and time complexity of the DTV approach.\n\nSuggestions for Improvement:\n\n* The authors should provide a more thorough analysis of the errors made by the LLMs in translating informal statements into formal statements and discuss ways to mitigate these errors.\n* The paper should include a more detailed discussion of the computational cost and time complexity of the DTV approach and provide suggestions for improving its efficiency.\n* The authors should provide more case studies and examples to demonstrate the effectiveness of DTV on a wider range of quantitative reasoning problems.\n* The paper should include a more detailed discussion of the limitations of the current approach and provide suggestions for future work to overcome these limitations.\n\nRecommendation:\n\nThe paper is well-written and provides a clear and concise description of the DTV approach and its implementation details. The authors demonstrate the effectiveness of DTV on three quantitative reasoning datasets: GSM8K, MATH, and MultiArith. However, the paper assumes that the LLMs used for autoformalization have sufficient training data and capabilities to translate informal statements into formal statements accurately, which may not always be the case. Therefore, I recommend that the authors provide a more thorough analysis of the errors made by the LLMs in translating informal statements into formal statements and discuss ways to mitigate these errors. I also recommend that the authors provide more case studies and examples to demonstrate the effectiveness of DTV on a wider range of quantitative reasoning problems.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes an approach called \"Don't Trust: Verify\" (DTV) to identify the correct answer among many informal solution samples generated by large language models (LLMs) for quantitative reasoning problems. The method leverages the autoformalization capability of LLMs to translate informal mathematical statements into formal Isabelle code, which can be verified automatically for internal consistency. The proposed approach consists of three main components: statement formalization, solution formalization and verification, and filtering unfaithful statement formalizations. The authors evaluate their approach on three quantitative reasoning datasets: GSM8K, MATH, and MultiArith, and demonstrate that it consistently outperforms vanilla majority voting, the previously best method to identify correct answers.\n\nStrengths:\n\n1. The paper presents a novel approach to improving the accuracy of LLMs in solving quantitative reasoning problems.\n2. The method is lightweight and does not require any training or finetuning of the LLMs.\n3. The evaluation results show that the proposed approach consistently outperforms vanilla majority voting on all three datasets.\n4. The paper provides a detailed analysis of the limitations of the approach and potential future directions.\n\nWeaknesses:\n\n1. The approach relies on the autoformalization capability of LLMs, which may not always be accurate.\n2. The method may not work well for problems that require advanced mathematical reasoning or formalizations that are beyond the capabilities of the theorem proving environment.\n3. The evaluation results are based on a limited number of datasets and may not generalize to other domains.\n4. The approach may require significant computational resources and time to formalize and verify the solutions.\n\nSuggestions for Improvement:\n\n1. Investigate the use of more effective filters to detect unfaithful formalizations.\n2. Explore the use of reinforcement learning from human feedback to further finetune language models on aligned mathematical formalizations directly.\n3. Evaluate the approach on a wider range of datasets and domains to ensure its generalizability.\n4. Investigate the use of other theorem proving environments to see if they can provide better support for formalizing geometric and probabilistic reasoning.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to improving the accuracy of LLMs in solving quantitative reasoning problems. While there are some limitations and potential areas for improvement, the approach has the potential to be a significant contribution to the field. I recommend accepting the paper for publication, pending revisions to address the suggestions for improvement.\n\n$$$",
        "gr_score_with": 0.8608247422680413,
        "gr_score_without": 0.8068535825545171,
        "output_without": {
            "num_tokens_scored": 321,
            "num_green_tokens": 259,
            "green_fraction": 0.8068535825545171,
            "z_score": 0.3072558574093159,
            "p_value": 0.37932431612763906
        },
        "output_with": {
            "num_tokens_scored": 388,
            "num_green_tokens": 334,
            "green_fraction": 0.8608247422680413,
            "z_score": 2.9955433162392584,
            "p_value": 0.001369781941120121
        }
    },
    {
        "title": "PERTURBATION-BASED GNN EXPLAINERS THROUGH",
        "abstract": "Numerous explainability methods have been proposed to shed light on the inner workings of GNNs. Despite the inclusion of empirical evaluations in all the proposed algorithms, the interrogative aspects of these evaluations lack diversity. As a result, various facets of explainability pertaining to GNNs, such as a comparative analysis of counterfactual reasoners, their stability to variational factors such as different GNN architectures, noise, stochasticity in non-convex loss surfaces, feasibility amidst domain constraints, and so forth, have yet to be formally investigated. Motivated by this need, we present a benchmarking study on perturbation-based explainability methods for GNNs, aiming to systematically evaluate and compare a wide range of explainability techniques. Among the key findings of our study, we identify the Pareto-optimal methods that exhibit superior efficacy and stability in the presence of noise. Nonetheless, our study reveals that all algorithms are affected by stability issues when faced with noisy data. Furthermore, we have established that the current generation of counterfactual explainers often fails to provide feasible recourses due to violations of topological constraints encoded by domain-specific considerations. Overall, this benchmarking study empowers stakeholders in the field of GNNs with a comprehensive understanding of the state-of-the-art explainability methods, potential research problems for further enhancement, and the implications of their application in real-world scenarios.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a comprehensive benchmarking study on perturbation-based explainability methods for Graph Neural Networks (GNNs). The authors evaluate and compare a wide range of explainability techniques, including factual and counterfactual reasoners, and identify their strengths and limitations. The study surfaces new insights that have not been studied in existing benchmarking literature, including the stability to noise and variational factors and the feasibility of counterfactual recourses. The authors also provide a meticulously curated, publicly accessible code base for the benchmarking study.\n\nStrengths:\n\n* The paper provides a comprehensive evaluation of perturbation-based explainability methods for GNNs, including both factual and counterfactual reasoners.\n* The study surfaces new insights that have not been studied in existing benchmarking literature, including the stability to noise and variational factors and the feasibility of counterfactual recourses.\n* The authors provide a meticulously curated, publicly accessible code base for the benchmarking study.\n\nWeaknesses:\n\n* The paper focuses primarily on perturbation-based explainability methods, and it would be beneficial to include other types of explainability methods in the study.\n* The evaluation metrics used in the study are limited to sufficiency, stability, and feasibility, and it would be beneficial to include other metrics, such as interpretability and reliability.\n* The paper does not provide a clear explanation of how the explainability methods were implemented and evaluated, which makes it difficult to replicate the results.\n\nSuggestions for Improvement:\n\n* Include other types of explainability methods in the study, such as model-based and hybrid methods.\n* Use a more comprehensive set of evaluation metrics, including interpretability and reliability.\n* Provide a clear explanation of how the explainability methods were implemented and evaluated, including the code and datasets used.\n* Consider using more advanced evaluation metrics, such as the accuracy of the explanations and the robustness of the explanations to noise and variability.\n\nRecommendation:\n\nThe paper provides a comprehensive evaluation of perturbation-based explainability methods for GNNs, and it surfaces new insights that have not been studied in existing benchmarking literature. However, the study has some limitations, including the focus on perturbation-based methods and the limited evaluation metrics used. Therefore, I recommend that the authors consider including other types of explainability methods in the study and using a more comprehensive set of evaluation metrics. Additionally, the authors should provide a clear explanation of how the explainability methods were implemented and evaluated, including the code and datasets used. With these improvements, the paper will provide a more comprehensive and reliable evaluation of explainability methods for GNNs.\n\nRating: 7/10\n\nThis paper provides a comprehensive evaluation of perturbation-based explainability methods for GNNs, and it surfaces new insights that have not been studied in existing benchmarking literature. However, the study has some limitations, including the focus on perturbation-based methods and the limited evaluation metrics used. With some improvements, the paper will provide a more comprehensive and reliable evaluation of explainability methods for GNNs.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a comprehensive benchmarking study on perturbation-based explainability methods for Graph Neural Networks (GNNs). The authors evaluate and compare a wide range of explainability techniques, including factual and counterfactual reasoners. The study identifies the Pareto-optimal methods that exhibit superior efficacy and stability in the presence of noise. However, it also reveals that all algorithms are affected by stability issues when faced with noisy data. The authors also investigate the feasibility of counterfactual explanations and find deviations in topological distribution from the original graphs. The study provides new insights into the limitations of current GNN explainers and opens up new avenues for advancing GNN explainers.\n\nStrengths:\n\n* The paper presents a comprehensive benchmarking study on perturbation-based explainability methods for GNNs.\n* The authors evaluate and compare a wide range of explainability techniques, including factual and counterfactual reasoners.\n* The study identifies the Pareto-optimal methods that exhibit superior efficacy and stability in the presence of noise.\n* The authors investigate the feasibility of counterfactual explanations and find deviations in topological distribution from the original graphs.\n\nWeaknesses:\n\n* The paper assumes that the input graph is undirected, which may not be the case in many real-world applications.\n* The authors use a fixed number of edges for the explanation, which may not be the optimal choice for all datasets.\n* The study focuses on GNN explainers and does not consider other types of explainers, such as attention-based explainers.\n* The authors do not provide a clear comparison between the performance of different explainers on different datasets.\n\nSuggestions for Improvement:\n\n* The authors should consider using different numbers of edges for the explanation to see if it affects the performance of the explainers.\n* The study should be extended to include other types of explainers, such as attention-based explainers.\n* The authors should provide a clear comparison between the performance of different explainers on different datasets.\n* The paper should be extended to include more real-world applications of GNN explainers.\n\nRecommendation:\n\nThe paper is well-written and provides new insights into the limitations of current GNN explainers. However, it has some limitations that need to be addressed. Overall, the paper is worth publishing in a top-tier conference or journal.",
        "gr_score_with": 0.7846715328467153,
        "gr_score_without": 0.6928327645051194,
        "output_without": {
            "num_tokens_scored": 293,
            "num_green_tokens": 203,
            "green_fraction": 0.6928327645051194,
            "z_score": -4.5856984134288785,
            "p_value": 0.9999977376432853
        },
        "output_with": {
            "num_tokens_scored": 274,
            "num_green_tokens": 215,
            "green_fraction": 0.7846715328467153,
            "z_score": -0.6340626590023782,
            "p_value": 0.7369800346102388
        }
    },
    {
        "title": "SWE-BENCH: CAN LANGUAGE MODELS RESOLVE REAL-WORLD GITHUB ISSUES?",
        "abstract": "Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-ofthe-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere 1.96% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces SWE-bench, a novel benchmark for evaluating the capabilities of language models (LMs) in software engineering tasks. SWE-bench consists of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. The benchmark tasks require LMs to edit codebases to address specific issues, often involving complex reasoning, context processing, and code editing. The paper evaluates several state-of-the-art LMs, including proprietary models and fine-tuned models, and finds that they struggle to solve even the simplest issues, with the best-performing model, Claude 2, able to resolve only 1.96% of the issues.\n\nStrengths:\n\n1. Novel benchmark: SWE-bench is a unique and challenging benchmark that pushes the limits of LMs in software engineering tasks.\n2. Real-world relevance: The benchmark is based on real-world GitHub issues and pull requests, making it highly relevant to software engineering practice.\n3. Comprehensive evaluation: The paper evaluates multiple LMs, including proprietary and fine-tuned models, providing a comprehensive assessment of their capabilities.\n4. Detailed analysis: The paper provides a thorough analysis of the results, including a discussion of the difficulties faced by LMs in software engineering tasks.\n\nWeaknesses:\n\n1. Limited scope: The paper focuses on Python repositories, which may limit the generalizability of the results to other programming languages.\n2. Evaluation metrics: The paper uses a single metric, percentage of issues resolved, which may not capture the full range of LM capabilities.\n3. Lack of human evaluation: The paper does not include human evaluation of the LM-generated patches, which could provide additional insights into their quality and effectiveness.\n4. Limited discussion of future work: The paper does not discuss potential future directions for research, such as improving LM performance or exploring new software engineering tasks.\n\nSuggestions for Improvement:\n\n1. Expand the scope of the benchmark: Consider including repositories from other programming languages to increase the generalizability of the results.\n2. Develop additional evaluation metrics: Consider using metrics such as patch quality, code maintainability, or developer productivity to provide a more comprehensive assessment of LM capabilities.\n3. Include human evaluation: Conduct human evaluation of the LM-generated patches to provide additional insights into their quality and effectiveness.\n4. Discuss future work: Provide a discussion of potential future directions for research, such as improving LM performance or exploring new software engineering tasks.\n\nRecommendation:\n\nThe paper presents a novel and challenging benchmark for evaluating the capabilities of LMs in software engineering tasks. While there are some limitations to the paper, the comprehensive evaluation and detailed analysis make it a valuable contribution to the field. I recommend the paper for publication in a top-tier conference or journal, with the suggestion to address the limitations mentioned above.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper introduces SWE-bench, a new benchmark for evaluating language models (LMs) on software engineering tasks. SWE-bench consists of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. The tasks require models to generate patches that resolve issues, which involves understanding and coordinating changes across multiple functions, classes, and even files simultaneously. The paper evaluates multiple state-of-the-art LMs on SWE-bench and finds that they fail to solve all except the simplest issues. The best-performing model, Claude 2, is able to solve only 1.96% of the issues. The paper also introduces SWE-Llama, a fine-tuned model that can resolve some issues, and discusses its performance and limitations. The authors conclude that SWE-bench represents a step towards developing more practical, intelligent, and autonomous LMs for software engineering tasks.\n\nStrengths:\n\n* The paper introduces a new and challenging benchmark for evaluating LMs on software engineering tasks.\n* The benchmark is constructed using real-world GitHub issues and corresponding pull requests, making it more realistic and relevant to software engineering tasks.\n* The paper provides a detailed analysis of the performance of various LMs on SWE-bench, highlighting their strengths and weaknesses.\n* The authors also discuss the limitations of their work and provide suggestions for future research directions.\n* The paper is well-structured and easy to follow, making it accessible to a wide range of readers.\n\nWeaknesses:\n\n* The paper focuses primarily on Python repositories, which may limit its generalizability to other programming languages.\n* The benchmark may be biased towards specific types of software engineering tasks, such as bug fixing and code refactoring.\n* The paper does not provide a comprehensive evaluation of the performance of SWE-Llama on different types of software engineering tasks.\n* The authors do not discuss the potential risks and challenges associated with using LMs for software engineering tasks.\n* The paper could benefit from more detailed information on the construction of SWE-bench, including the filtering criteria used to select task instances.\n\nSuggestions for Improvement:\n\n* The authors could consider expanding SWE-bench to include software engineering tasks from other programming languages.\n* They could also explore using more diverse and representative software engineering tasks to make the benchmark more comprehensive.\n* The paper could benefit from a more detailed evaluation of SWE-Llama's performance on different types of software engineering tasks.\n* The authors could discuss the potential risks and challenges associated with using LMs for software engineering tasks, such as the potential for introducing new bugs or vulnerabilities.\n* The paper could be improved by providing more detailed information on the construction of SWE-bench, including the filtering criteria used to select task instances.\n\nRecommendation:\n\nThe paper is a significant contribution to the field of natural language processing and software engineering, and it has the potential to inspire new research directions. The authors' efforts to develop a challenging benchmark for evaluating LMs on software engineering tasks are commendable, and their analysis of the performance of various LMs on SWE-bench is informative. However, the paper could benefit from more detailed information on the construction of SWE-bench and a more comprehensive evaluation of SWE-Llama's performance. With these improvements, the paper could be even more influential and impactful in the field.",
        "gr_score_with": 0.8329411764705882,
        "gr_score_without": 0.7805486284289277,
        "output_without": {
            "num_tokens_scored": 401,
            "num_green_tokens": 313,
            "green_fraction": 0.7805486284289277,
            "z_score": -0.9734598797783244,
            "p_value": 0.8348376045812738
        },
        "output_with": {
            "num_tokens_scored": 425,
            "num_green_tokens": 354,
            "green_fraction": 0.8329411764705882,
            "z_score": 1.6980509909381418,
            "p_value": 0.04474906915143892
        }
    },
    {
        "title": "HYPO: HYPERSPHERICAL OUT-OF-DISTRIBUTION GENERALIZATION",
        "abstract": "Out-of-distribution (OOD) generalization is critical for machine learning models deployed in the real world. However, achieving this can be fundamentally challenging, as it requires the ability to learn invariant features across different domains or environments. In this paper, we propose a novel framework HYPO (HYPerspherical OOD generalization) that provably learns domain-invariant representations in a hyperspherical space. In particular, our hyperspherical learning algorithm is guided by intra-class variation and inter-class separation principles\u2014ensuring that features from the same class (across different training domains) are closely aligned with their class prototypes, while different class prototypes are maximally separated. We further provide theoretical justifications on how our prototypical learning objective improves the OOD generalization bound. Through extensive experiments on challenging OOD benchmarks, we demonstrate that our approach outperforms competitive baselines and achieves superior performance. Code is available at https://github.com/deeplearning-wisc/hypo.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework, HYPO (HYPerspherical OOD generalization), for out-of-distribution (OOD) generalization. The framework is designed to learn domain-invariant representations in a hyperspherical space, where samples from the same class across different training domains are closely aligned with their class prototypes, while different class prototypes are maximally separated. The authors provide theoretical justifications on how their prototypical learning objective improves the OOD generalization bound and demonstrate the effectiveness of HYPO on challenging OOD benchmarks.\n\nStrengths:\n\n1. The paper provides a comprehensive and well-structured approach to OOD generalization, including theoretical justifications and empirical evaluations.\n2. The authors propose a novel framework, HYPO, which is designed to learn domain-invariant representations in a hyperspherical space.\n3. The paper provides a thorough analysis of the OOD generalization problem, including the definition of intra-class variation and inter-class separation.\n4. The authors provide a theoretical understanding of how their loss function reduces the OOD generalization error.\n\nWeaknesses:\n\n1. The paper assumes that the class prototypes are known, which may not be the case in practice.\n2. The authors do not provide a detailed analysis of the computational complexity of their algorithm.\n3. The paper assumes that the training data is i.i.d., which may not be the case in practice.\n4. The authors do not provide a detailed comparison with other state-of-the-art methods for OOD generalization.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational complexity of their algorithm.\n2. The paper should include a more detailed comparison with other state-of-the-art methods for OOD generalization.\n3. The authors should provide a more detailed explanation of how they handle the case where the class prototypes are not known.\n4. The paper should include more detailed results on the robustness of HYPO to different types of corruption and domain shift.\n\nRecommendation:\n\nThe paper presents a novel and comprehensive approach to OOD generalization, and the authors provide a thorough analysis of the OOD generalization problem. The paper is well-structured and easy to follow, and the authors provide a clear explanation of their method. However, the paper assumes that the class prototypes are known, which may not be the case in practice, and the authors do not provide a detailed analysis of the computational complexity of their algorithm. Therefore, I recommend that the paper be accepted for publication with minor revisions.\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper proposes a novel framework, HYPO (HYPerspherical OOD generalization), that learns domain-invariant representations in a hyperspherical space. The framework is designed to promote low variation and high separation, which are essential properties for out-of-distribution (OOD) generalization. The authors provide theoretical justification for how their framework reduces the OOD generalization error and demonstrate its effectiveness on several benchmarks.\n\nStrengths:\n\n1. The paper presents a well-structured and clear exposition of the problem and the proposed solution.\n2. The authors provide a theoretical justification for their framework, which is a significant contribution to the field.\n3. The experiments demonstrate the effectiveness of the proposed framework on several benchmarks, including CIFAR-10, PACS, Office-Home, and VLCS.\n4. The authors provide a thorough analysis of the results and discuss the implications of their findings.\n\nWeaknesses:\n\n1. The paper assumes that the data is available from a finite subset of training environments, which may not be realistic in many real-world scenarios.\n2. The authors do not provide a clear explanation of how their framework can be adapted to handle multiple tasks or multiple environments.\n3. The experiments are limited to a few benchmarks, and it would be beneficial to see more extensive experimentation on a wider range of tasks and environments.\n4. The authors do not provide a clear explanation of how their framework can be used in practice, and more details on the implementation and deployment of the framework would be beneficial.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of how their framework can be adapted to handle multiple tasks or multiple environments.\n2. The experiments should be extended to include more benchmarks and more extensive experimentation on a wider range of tasks and environments.\n3. The authors should provide more details on the implementation and deployment of the framework, including how it can be used in practice.\n4. The authors should provide a clear explanation of how their framework can be used in real-world scenarios, including how it can be adapted to handle missing data or other challenges that may arise in practice.\n\nRecommendation:\n\nThis paper presents a significant contribution to the field of out-of-distribution generalization, and the proposed framework shows promise in improving the performance of OOD generalization tasks. However, the authors should address the limitations and weaknesses mentioned above in order to make the framework more practical and widely applicable. With further development and refinement, this framework has the potential to make a significant impact in the field.\n\n$$$",
        "gr_score_with": 0.7874251497005988,
        "gr_score_without": 0.7482993197278912,
        "output_without": {
            "num_tokens_scored": 294,
            "num_green_tokens": 220,
            "green_fraction": 0.7482993197278912,
            "z_score": -2.215911711728366,
            "p_value": 0.9866512253573474
        },
        "output_with": {
            "num_tokens_scored": 334,
            "num_green_tokens": 263,
            "green_fraction": 0.7874251497005988,
            "z_score": -0.5742427347364089,
            "p_value": 0.7170982227652358
        }
    },
    {
        "title": "DEEP TEMPORAL GRAPH CLUSTERING",
        "abstract": "Deep graph clustering has recently received significant attention due to its ability to enhance the representation learning capabilities of models in unsupervised scenarios. Nevertheless, deep clustering for temporal graphs, which could capture crucial dynamic interaction information, has not been fully explored. It means that in many clustering-oriented real-world scenarios, temporal graphs can only be processed as static graphs. This not only causes the loss of dynamic information but also triggers huge computational consumption. To solve the problem, we propose a general framework for deep Temporal Graph Clustering called TGC, which introduces deep clustering techniques to suit the interaction sequence-based batch-processing pattern of temporal graphs. In addition, we discuss differences between temporal graph clustering and static graph clustering from several levels. To verify the superiority of the proposed framework TGC, we conduct extensive experiments. The experimental results show that temporal graph clustering enables more flexibility in finding a balance between time and space requirements, and our framework can effectively improve the performance of existing temporal graph learning methods. The code is released: https://github.com/MGitHubL/ Deep-Temporal-Graph-Clustering.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a general framework for deep Temporal Graph Clustering (TGC), which introduces two deep clustering modules to suit the interaction sequence-based batch-processing pattern of temporal graphs. The authors discuss the differences between temporal graph clustering and static graph clustering from several levels, including intuition, complexity, data, and experiment. To verify the superiority of the proposed framework TGC, the authors conduct extensive experiments and show that temporal graph clustering enables more flexibility in finding a balance between time and space requirements.\n\nStrengths:\n1. The paper provides a comprehensive introduction to temporal graph clustering and discusses its differences with static graph clustering.\n2. The proposed framework TGC is general and can be easily applied to different temporal graph methods.\n3. The authors conduct extensive experiments to demonstrate the effectiveness of TGC on existing temporal graph learning methods.\n4. The paper highlights the flexibility of temporal graph clustering in finding a balance between time and space requirements.\n\nWeaknesses:\n1. The paper lacks a thorough analysis of the complexity of the proposed framework TGC.\n2. The authors do not provide a detailed comparison of the performance of TGC with other state-of-the-art methods.\n3. The paper assumes that the interaction sequence-based batch-processing pattern is the most suitable approach for temporal graph clustering, without considering other possible approaches.\n4. The authors do not discuss the limitations of the proposed framework TGC and potential areas for future research.\n\nSuggestions for Improvement:\n1. Provide a more detailed analysis of the complexity of the proposed framework TGC and compare it with other state-of-the-art methods.\n2. Conduct a more thorough comparison of the performance of TGC with other state-of-the-art methods.\n3. Consider other possible approaches for temporal graph clustering and compare them with the proposed framework TGC.\n4. Discuss the limitations of the proposed framework TGC and potential areas for future research.\n\nRecommendation:\nThe paper proposes a general framework for deep Temporal Graph Clustering (TGC) and conducts extensive experiments to demonstrate its effectiveness. However, the paper lacks a thorough analysis of the complexity of the proposed framework and a detailed comparison of its performance with other state-of-the-art methods. Therefore, I recommend that the authors revise the paper to address these issues and provide a more comprehensive evaluation of the proposed framework. With these revisions, the paper has the potential to make a significant contribution to the field of temporal graph clustering.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a general framework for deep Temporal Graph Clustering (TGC), which introduces deep clustering techniques to suit the interaction sequence-based batch-processing pattern of temporal graphs. The authors discuss the differences between temporal graph clustering and static graph clustering from several levels, including intuition, complexity, data, and experiment. They propose a simple general framework TGC, which includes two deep clustering modules: a temporal module for time information mining and a clustering module for node clustering. The authors conduct extensive experiments to validate the clustering performance, flexibility, and transferability of TGC, and demonstrate its superiority over existing methods.\n\nStrengths:\n\n1. The paper provides a comprehensive introduction to temporal graph clustering, discussing its advantages and challenges.\n2. The authors propose a simple yet effective framework for deep Temporal Graph Clustering (TGC), which can adapt to the interaction sequence-based batch-processing pattern of temporal graphs.\n3. The paper conducts extensive experiments to validate the clustering performance, flexibility, and transferability of TGC, and demonstrates its superiority over existing methods.\n4. The authors discuss the limitations of the current approach and suggest potential future directions for research.\n\nWeaknesses:\n\n1. The paper assumes that the interaction sequence is available, which may not always be the case in real-world scenarios.\n2. The authors do not provide a thorough comparison with other state-of-the-art methods for static graph clustering.\n3. The paper focuses on the performance of TGC on a limited set of datasets, and it would be beneficial to conduct experiments on a larger and more diverse set of datasets.\n4. The authors do not discuss the computational complexity of TGC, which is an important consideration for large-scale graph clustering tasks.\n\nSuggestions for Improvement:\n\n1. The authors should investigate the robustness of TGC to different types of interaction sequences and missing values.\n2. The paper should include a more thorough comparison with other state-of-the-art methods for static graph clustering, including both clustering and node classification tasks.\n3. The authors should conduct experiments on a larger and more diverse set of datasets to demonstrate the scalability and generalizability of TGC.\n4. The paper should discuss the computational complexity of TGC and its implications for large-scale graph clustering tasks.\n\nRecommendation:\n\nThe paper presents a novel and effective framework for deep Temporal Graph Clustering (TGC), which has the potential to improve the clustering performance of existing temporal graph learning methods. The authors demonstrate the superiority of TGC over existing methods through extensive experiments, and discuss its limitations and potential future directions for research. However, the paper could benefit from a more thorough comparison with other state-of-the-art methods and a more comprehensive evaluation of its scalability and generalizability.",
        "gr_score_with": 0.8605341246290801,
        "gr_score_without": 0.8204225352112676,
        "output_without": {
            "num_tokens_scored": 284,
            "num_green_tokens": 233,
            "green_fraction": 0.8204225352112676,
            "z_score": 0.8606694281130765,
            "p_value": 0.19471006776325106
        },
        "output_with": {
            "num_tokens_scored": 337,
            "num_green_tokens": 290,
            "green_fraction": 0.8605341246290801,
            "z_score": 2.7784007950592122,
            "p_value": 0.002731359562477537
        }
    },
    {
        "title": "RECOMBINER: ROBUST AND ENHANCED COMPRESSION WITH BAYESIAN IMPLICIT NEURAL REPRESENTATIONS",
        "abstract": "COMpression with Bayesian Implicit NEural Representations (COMBINER) is a recent data compression method that addresses a key inefficiency of previous Implicit Neural Representation (INR)-based approaches: it avoids quantization and enables direct optimization of the rate-distortion performance. However, COMBINER still has significant limitations: 1) it uses factorized priors and posterior approximations that lack flexibility; 2) it cannot effectively adapt to local deviations from global patterns in the data; and 3) its performance can be susceptible to modeling choices and the variational parameters\u2019 initializations. Our proposed method, Robust and Enhanced COMBINER (RECOMBINER), addresses these issues by 1) enriching the variational approximation while retaining a low computational cost via a linear reparameterization of the INR weights, 2) augmenting our INRs with learnable positional encodings that enable them to adapt to local details and 3) splitting high-resolution data into patches to increase robustness and utilizing expressive hierarchical priors to capture dependency across patches. We conduct extensive experiments across several data modalities, showcasing that RECOMBINER achieves competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on lowresolution images at low bitrates. Our PyTorch implementation is available at https://github.com/cambridge-mlg/RECOMBINER/.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel data compression method called RECOMBINER, which builds upon the previous work of COMBINER (Guo et al., 2023). RECOMBINER addresses several limitations of COMBINER, including the use of factorized priors and posterior approximations that lack flexibility, the inability to adapt to local deviations from global patterns in the data, and the sensitivity to modeling choices and the variational parameters' initializations. The authors propose several non-trivial extensions to COMBINER, including a linear reparameterization for neural network weights, learnable positional encodings, and an expressive hierarchical Bayesian model for high-resolution signals. The proposed method, RECOMBINER, is evaluated on various data modalities, including image, audio, video, and 3D protein structure data, and demonstrates competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on low-resolution images at low bitrates.\n\nStrengths:\n\n1. The paper proposes several novel and non-trivial extensions to the previous work of COMBINER, which significantly improve the rate-distortion performance and robustness to modeling choices.\n2. The authors provide a comprehensive evaluation of the proposed method on various data modalities, including image, audio, video, and 3D protein structure data.\n3. The paper demonstrates competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on low-resolution images at low bitrates.\n4. The authors provide a clear and concise explanation of the proposed method and its extensions, making it easy to follow and understand.\n\nWeaknesses:\n\n1. The encoding time complexity of RECOMBINER is high, which may limit its practical application.\n2. The paper does not provide a detailed comparison with other state-of-the-art compression methods, such as VC-INR (Schwarz et al., 2023).\n3. The authors do not provide a clear explanation of how the proposed method can be extended to other data modalities.\n4. The paper does not discuss the potential applications of RECOMBINER in real-world scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed comparison with other state-of-the-art compression methods, such as VC-INR (Schwarz et al., 2023).\n2. The paper should provide a clear explanation of how the proposed method can be extended to other data modalities.\n3. The authors should discuss the potential applications of RECOMBINER in real-world scenarios.\n4. The paper should provide a more detailed analysis of the encoding time complexity of RECOMBINER and propose potential solutions to reduce it.\n\nRecommendation:\n\nThe paper proposes a novel and competitive data compression method called RECOMBINER, which addresses several limitations of the previous work of COMBINER. The authors provide a comprehensive evaluation of the proposed method on various data modalities and demonstrate competitive results with the best INR-based methods. However, the encoding time complexity of RECOMBINER is high, which may limit its practical application. Therefore, I recommend that the authors provide a more detailed comparison with other state-of-the-art compression methods, discuss the potential applications of RECOMBINER in real-world scenarios, and propose potential solutions to reduce the encoding time complexity.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel data compression method called RECOMBINER, which is an extension of the recent data compression method COMBINER. COMBINER addresses the key inefficiency of previous Implicit Neural Representation (INR)-based approaches by avoiding quantization and enabling direct optimization of the rate-distortion performance. However, COMBINER still has significant limitations, including the use of factorized priors and posterior approximations that lack flexibility, and its performance can be susceptible to modeling choices and the variational parameters\u2019 initializations. The proposed RECOMBINER method addresses these issues by introducing a linear reparameterization of the INR weights, augmenting the INRs with learnable positional encodings, and splitting high-resolution data into patches to increase robustness. The method also utilizes expressive hierarchical priors to capture dependency across patches. The authors conduct extensive experiments across several data modalities, including image, audio, video, and protein structure data, and demonstrate that RECOMBINER achieves competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on low-resolution images at low bitrates.\n\nStrengths:\n\n* The paper proposes a novel and effective method for data compression, which addresses the limitations of previous INR-based approaches.\n* The method is extensively evaluated on several data modalities, including image, audio, video, and protein structure data, and demonstrates competitive results.\n* The authors provide a detailed description of the method, including the mathematical formulation and the experimental setup.\n* The paper includes a comprehensive comparison with state-of-the-art methods, including INR-based and autoencoder-based codecs.\n\nWeaknesses:\n\n* The encoding time complexity of the method is a major limitation, and the authors acknowledge that this is an area of future work.\n* The method relies on the use of patches to increase robustness, which can lead to block artifacts.\n* The approximate samples given by A\u02da coding can impact the method's performance, and the authors suggest that an exact REC algorithm may be adapted to solve this issue.\n\nSuggestions for Improvement:\n\n* To address the encoding time complexity, the authors suggest reducing the number of parameters to optimize over and switching from inference over weights to modulations using, e.g. FiLM layers.\n* To reduce the block artifacts caused by patching, the authors suggest exploring alternative methods, such as patch-based compression without block artifacts.\n* To address the issue of approximate samples given by A\u02da coding, the authors suggest adapting an exact REC algorithm, such as the recently developed greedy Poisson rejection sampler.\n\nRecommendation:\n\nThe paper proposes a novel and effective method for data compression, which addresses the limitations of previous INR-based approaches. The method is extensively evaluated on several data modalities, and demonstrates competitive results. However, the encoding time complexity and the block artifacts caused by patching are major limitations that need to be addressed in future work. Overall, the paper is well-written and well-organized, and the authors provide a comprehensive comparison with state-of-the-art methods. Therefore, I recommend the paper for publication in a reputable journal or conference.",
        "gr_score_with": 0.8963133640552995,
        "gr_score_without": 0.8666666666666667,
        "output_without": {
            "num_tokens_scored": 345,
            "num_green_tokens": 299,
            "green_fraction": 0.8666666666666667,
            "z_score": 3.095949371392033,
            "p_value": 0.0009809192772419178
        },
        "output_with": {
            "num_tokens_scored": 434,
            "num_green_tokens": 389,
            "green_fraction": 0.8963133640552995,
            "z_score": 5.016426712440365,
            "p_value": 2.632067167382797e-07
        }
    },
    {
        "title": "A UNIFIED SAMPLING FRAMEWORK FOR SOLVER SEARCHING OF DIFFUSION PROBABILISTIC MODELS",
        "abstract": "Recent years have witnessed the rapid progress and broad application of diffusion probabilistic models (DPMs). Sampling from DPMs can be viewed as solving an ordinary differential equation (ODE). Despite the promising performance, the generation of DPMs usually consumes much time due to the large number of function evaluations (NFE). Though recent works have accelerated the sampling to around 20 steps with high-order solvers, the sample quality with less than 10 NFE can still be improved. In this paper, we propose a unified sampling framework (USF) to study the optional strategies for solver. Under this framework, we further reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, and a carefully designed solver schedule has the potential to improve the sample quality by a large margin. Therefore, we propose a new sampling framework based on the exponential integral formulation that allows free choices of solver strategy at each step and design specific decisions for the framework. Moreover, we propose S, a predictor-based search method that automatically optimizes the solver schedule to get a better time-quality trade-off of sampling. We demonstrate that S can find outstanding solver schedules which outperform the state-of-the-art sampling methods on CIFAR-10, CelebA, ImageNet, and LSUN-Bedroom datasets. Specifically, we achieve 2.69 FID with 10 NFE and 6.86 FID with 5 NFE on CIFAR-10 dataset, outperforming the SOTA method significantly. We further apply S to Stable-Diffusion model and get an acceleration ratio of 2\u00d7, showing the feasibility of sampling in very few steps without retraining the neural network.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a unified sampling framework (USF) for diffusion probabilistic models (DPMs) to systematically study the optional strategies for solver searching. The authors reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, and a carefully designed solver schedule has the potential to improve the sample quality by a large margin. They propose a new sampling framework based on the exponential integral formulation that allows free choices of solver strategy at each step and design specific decisions for the framework. Moreover, they propose S3, a predictor-based search method that automatically optimizes the solver schedule to get a better time-quality trade-off of sampling.\n\nStrengths:\n\n* The paper proposes a unified sampling framework (USF) that systematically studies the optional strategies for solver searching, which is a novel contribution to the field of diffusion probabilistic models.\n* The authors reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, which is a significant insight.\n* The proposed framework allows free choices of solver strategy at each step, which provides a high degree of flexibility.\n* The authors propose a predictor-based search method (S3) that automatically optimizes the solver schedule, which is a useful tool for practitioners.\n\nWeaknesses:\n\n* The paper assumes that the reader has a good understanding of diffusion probabilistic models and their applications, which may not be the case for all readers.\n* The authors do not provide a detailed comparison with other state-of-the-art methods, which makes it difficult to evaluate the effectiveness of their proposed framework.\n* The paper does not provide a clear explanation of how the predictor-based search method (S3) works, which may make it difficult for readers to understand and replicate the results.\n* The authors do not provide a detailed analysis of the computational cost of their proposed framework, which may be a concern for practitioners who need to implement the framework in practice.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed explanation of the diffusion probabilistic models and their applications to make the paper more accessible to a wider audience.\n* The authors should provide a more detailed comparison with other state-of-the-art methods to evaluate the effectiveness of their proposed framework.\n* The authors should provide a clear explanation of how the predictor-based search method (S3) works, including the details of the predictor and the search process.\n* The authors should provide a detailed analysis of the computational cost of their proposed framework, including the time and memory requirements.\n\nRecommendation:\n\nThe paper proposes a novel and useful framework for diffusion probabilistic models, and the authors provide a clear explanation of their proposed framework. However, the paper assumes a good understanding of the topic, and the authors do not provide a detailed comparison with other state-of-the-art methods. Therefore, I recommend that the authors provide a more detailed explanation of the diffusion probabilistic models and their applications, and provide a more detailed comparison with other state-of-the-art methods. With these improvements, the paper would be more accessible to a wider audience and would provide a more comprehensive evaluation of the effectiveness of their proposed framework.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a unified sampling framework (USF) for diffusion probabilistic models (DPMs) to systematically study the optional strategies for solver searching. The authors reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, and a carefully designed solver schedule has the potential to improve the sample quality by a large margin. They propose a new sampling framework based on the exponential integral formulation that allows free choices of solver strategy at each step and design specific decisions for the framework. Additionally, they propose S3, a predictor-based search method that automatically optimizes the solver schedule to get a better time-quality trade-off of sampling.\n\nStrengths:\n\n* The paper proposes a unified sampling framework (USF) that systematically studies the optional strategies for solver searching, which is a novel contribution to the field of diffusion probabilistic models.\n* The authors reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, which provides new insights into the sampling process of DPMs.\n* The paper proposes S3, a predictor-based search method that automatically optimizes the solver schedule to get a better time-quality trade-off of sampling, which is a significant improvement over existing methods.\n* The experimental results show that the proposed method can boost the sample quality under a very tight budget by a large margin, making it feasible to generate samples with very few NFE.\n\nWeaknesses:\n\n* The paper assumes that the user has prior knowledge of the optimal solver schedule, which may not be the case in practice. The authors should provide more guidance on how to choose the optimal solver schedule.\n* The search space of the solver schedule is extremely large, which makes it difficult to find the optimal solution using random search. The authors should provide more details on how to reduce the search space.\n* The paper uses FID score as the evaluation metric, which may not be the most suitable metric for all applications. The authors should provide more details on why they chose FID score as the evaluation metric.\n* The paper does not provide a comprehensive analysis of the computational cost of the proposed method, which may be a significant limitation in practice.\n\nSuggestions for Improvement:\n\n* Provide more guidance on how to choose the optimal solver schedule, including the choice of hyperparameters.\n* Provide more details on how to reduce the search space of the solver schedule, including the use of heuristics or approximation algorithms.\n* Provide more details on the choice of evaluation metric, including the reasons why FID score was chosen.\n* Provide a comprehensive analysis of the computational cost of the proposed method, including the time and memory requirements.\n\nRecommendation:\n\nThe paper proposes a novel and effective method for sampling from diffusion probabilistic models, which can boost the sample quality under a very tight budget by a large margin. While there are some limitations to the paper, including the assumption of prior knowledge of the optimal solver schedule and the use of FID score as the evaluation metric, the method is still worth considering for its potential to improve the efficiency of sampling from DPMs. With some modifications to address the limitations, the paper can be considered for publication in a top-tier conference or journal.\n\nRating: 8/10\n$$$",
        "gr_score_with": 0.8029925187032418,
        "gr_score_without": 0.7703488372093024,
        "output_without": {
            "num_tokens_scored": 344,
            "num_green_tokens": 265,
            "green_fraction": 0.7703488372093024,
            "z_score": -1.374562560468253,
            "p_value": 0.9153664487612172
        },
        "output_with": {
            "num_tokens_scored": 401,
            "num_green_tokens": 322,
            "green_fraction": 0.8029925187032418,
            "z_score": 0.1501233609630173,
            "p_value": 0.4403336447299637
        }
    },
    {
        "title": "MOTION GUIDANCE: DIFFUSION-BASED IMAGE EDIT-",
        "abstract": "Diffusion models are capable of generating impressive images conditioned on text descriptions, and extensions of these models allow users to edit images at a relatively coarse scale. However, the ability to precisely edit the layout, position, pose, and shape of objects in images with diffusion models is still difficult. To this end, we propose motion guidance, a zero-shot technique that allows a user to specify dense, complex motion fields that indicate where each pixel in an image should move. Motion guidance works by steering the diffusion sampling process with the gradients through an off-the-shelf optical flow network. Specifically, we design a guidance loss that encourages the sample to have the desired motion, as estimated by a flow network, while also being visually similar to the source image. By simultaneously sampling from a diffusion model and guiding the sample to have low guidance loss, we can obtain a motion-edited image. We demonstrate that our technique works on complex motions and produces high quality edits of real and generated images.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel technique called motion guidance, which enables users to specify dense and complex motion fields to edit images. The method works by steering the diffusion sampling process using an off-the-shelf optical flow network, while also encouraging the generated image to be visually similar to the source image. The authors demonstrate that their technique can handle a wide range of complex motion fields, including translations, rotations, homographies, stretching, deformations, and even flow fields extracted from a video.\n\nStrengths:\n\n1. The paper proposes a novel and effective technique for motion-based image editing, which can handle complex motion fields and produce high-quality edits.\n2. The method is simple, zero-shot, and does not require any training or explicit constraints on the diffusion network architecture.\n3. The authors provide a comprehensive evaluation of their method, including qualitative and quantitative results, and comparisons with state-of-the-art baselines.\n4. The paper explores the possibility of repurposing other low-level computer vision models for image generation tasks through diffusion guidance, which is a promising area of research.\n\nWeaknesses:\n\n1. The paper relies on the performance of the off-the-shelf optical flow network, which may not be optimal for all cases.\n2. The method may be susceptible to various weaknesses, such as slow sampling speed and limitations of the optical flow method.\n3. The paper does not provide a thorough analysis of the trade-offs between the flow loss and the CLIP similarity metrics.\n4. The evaluation of the method is limited to a specific set of datasets and target flows, which may not be representative of all possible scenarios.\n\nSuggestions for Improvement:\n\n1. Investigate the use of more advanced optical flow networks or other motion estimation models to improve the performance of the method.\n2. Explore the possibility of using other guidance signals, such as classifier guidance or text-based guidance, to improve the flexibility and robustness of the method.\n3. Provide a more thorough analysis of the trade-offs between the flow loss and the CLIP similarity metrics, including a deeper understanding of the underlying factors that influence these metrics.\n4. Evaluate the method on a more diverse set of datasets and target flows to better understand its limitations and potential applications.\n\nRecommendation:\n\nThe paper presents a novel and effective technique for motion-based image editing, which has the potential to improve the flexibility and robustness of image manipulation models. While the method has some limitations, it is a promising area of research that deserves further exploration. I recommend the paper for publication in a top-tier conference, with the suggestion that the authors address the weaknesses and limitations mentioned above.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel technique called motion guidance, which allows users to edit images by specifying a dense, complex motion field that indicates where each pixel should move. This technique uses an off-the-shelf optical flow network to guide the diffusion sampling process, enabling the creation of high-quality motion-edited images. The authors demonstrate the effectiveness of their method on both real and generated images, showing that it can handle a wide range of complex motions, including translations, rotations, homographies, stretching, deformations, and even flow fields extracted from a video.\n\nStrengths:\n\n*   The paper presents a novel and effective technique for motion-guided image editing.\n*   The method is zero-shot, meaning it does not require any training or explicit constraints on the diffusion network architecture.\n*   The authors demonstrate the effectiveness of their method on both real and generated images, showing that it can handle a wide range of complex motions.\n*   The paper provides a comprehensive discussion of the method's limitations and potential areas for future research.\n\nWeaknesses:\n\n*   The paper does not provide a detailed comparison with state-of-the-art methods in the field of image editing.\n*   The authors rely heavily on the performance of the off-the-shelf optical flow network, which may not always produce high-quality results.\n*   The paper does not discuss the potential applications of the method in fields such as computer vision, robotics, or video editing.\n\nSuggestions for Improvement:\n\n*   Conduct a more comprehensive comparison with state-of-the-art methods in the field of image editing.\n*   Investigate the use of more advanced optical flow networks or techniques to improve the accuracy and robustness of the method.\n*   Explore the potential applications of the method in fields such as computer vision, robotics, or video editing.\n\nRecommendation:\n\nThe paper presents a novel and effective technique for motion-guided image editing, and the authors demonstrate its effectiveness on both real and generated images. While the paper has some weaknesses, such as the reliance on the performance of the off-the-shelf optical flow network, it provides a comprehensive discussion of the method's limitations and potential areas for future research. I recommend this paper for publication in the conference, with the understanding that the authors will address the weaknesses and suggestions for improvement mentioned above.\n\n$$$",
        "gr_score_with": 0.8415841584158416,
        "gr_score_without": 0.803030303030303,
        "output_without": {
            "num_tokens_scored": 396,
            "num_green_tokens": 318,
            "green_fraction": 0.803030303030303,
            "z_score": 0.15106421851546753,
            "p_value": 0.4399625290071818
        },
        "output_with": {
            "num_tokens_scored": 303,
            "num_green_tokens": 255,
            "green_fraction": 0.8415841584158416,
            "z_score": 1.8098779903355213,
            "p_value": 0.03515735500088076
        }
    },
    {
        "title": "LEARNING OPTIMAL CONTRACTS: HOW TO EXPLOIT SMALL ACTION SPACES",
        "abstract": "We study principal-agent problems in which a principal commits to an outcomedependent payment scheme\u2014called contract\u2014in order to induce an agent to take a costly, unobservable action leading to favorable outcomes. We consider a generalization of the classical (single-round) version of the problem in which the principal interacts with the agent by committing to contracts over multiple rounds. The principal has no information about the agent, and they have to learn an optimal contract by only observing the outcome realized at each round. We focus on settings in which the size of the agent\u2019s action space is small. We design an algorithm that learns an approximately-optimal contract with high probability in a number of rounds polynomial in the size of the outcome space, when the number of actions is constant. Our algorithm solves an open problem by Zhu et al. (2023). Moreover, it can also be employed to provide a \u00d5(T ) regret bound in the related online learning setting in which the principal aims at maximizing their cumulative utility over rounds, considerably improving previously-known regret bounds.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper studies the problem of learning optimal contracts in hidden-action principal-agent problems. The principal and the agent interact over multiple rounds, and the principal has no knowledge about the agent's actions. The goal is to design algorithms that prescribe the principal a contract to commit to at each round in order to learn an \"approximately-optimal\" contract with high probability by using the minimum possible number of rounds.\n\nStrengths:\n\n* The paper provides a novel algorithm, called Discover-and-Cover, that learns an optimal bounded contract with high probability in a number of rounds polynomial in the size of the problem instance when the number of agent's actions is constant.\n* The algorithm solves an open problem by Zhu et al. (2023) and can also be employed to provide a \u00d5(T) regret bound in the related online learning setting.\n* The paper provides a comprehensive analysis of the algorithm, including the number of rounds required and the probability of success.\n\nWeaknesses:\n\n* The paper assumes that the number of agent's actions is constant, which may not be the case in all real-world scenarios.\n* The algorithm may not perform well in scenarios with a large number of outcomes.\n* The paper does not provide a comparison with other existing algorithms for learning optimal contracts.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the algorithm's performance in scenarios with a large number of outcomes.\n* The authors should compare the performance of the Discover-and-Cover algorithm with other existing algorithms for learning optimal contracts.\n* The authors should provide more details about the computational complexity of the algorithm and the time required to execute it.\n\nRecommendation:\n\nThe paper provides a novel and comprehensive solution to the problem of learning optimal contracts in hidden-action principal-agent problems. The Discover-and-Cover algorithm is efficient and can be employed in a variety of scenarios. However, the paper assumes a constant number of agent's actions, which may not be the case in all real-world scenarios. The authors should provide a more detailed analysis of the algorithm's performance in scenarios with a large number of outcomes and compare its performance with other existing algorithms.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper presents an algorithm called Discover-and-Cover (D&C) for learning optimal contracts in hidden-action principal-agent problems. In this setting, a principal interacts with an agent over multiple rounds, committing to a contract at each round. The agent's action is not observed by the principal, but only an outcome is observed. The goal is to learn an optimal contract that maximizes the principal's expected utility. The D&C algorithm builds a set of meta-actions, which group together agent's actions associated with similar distributions over outcomes. It then uses these meta-actions to find an approximately-optimal contract by iteratively covering a set of contracts using approximate best-response regions. The algorithm has a polynomial-time guarantee in terms of the size of the problem instance when the number of agent's actions is constant.\n\nStrengths:\n\n1. The paper presents a novel algorithm for learning optimal contracts in hidden-action principal-agent problems.\n2. The algorithm has a polynomial-time guarantee in terms of the size of the problem instance when the number of agent's actions is constant.\n3. The paper provides a detailed analysis of the algorithm's performance, including a proof of its polynomial-time guarantee.\n4. The algorithm is shown to be effective in learning approximately-optimal contracts in a variety of settings.\n\nWeaknesses:\n\n1. The paper assumes that the agent's action space is small, which may not be realistic in many applications.\n2. The algorithm's performance guarantee relies on the assumption that the number of agent's actions is constant.\n3. The paper does not provide a comparison with other existing algorithms for learning optimal contracts in hidden-action principal-agent problems.\n4. The algorithm's empirical performance is not evaluated in the paper.\n\nSuggestions for Improvement:\n\n1. Consider relaxing the assumption that the agent's action space is small.\n2. Provide a comparison with other existing algorithms for learning optimal contracts in hidden-action principal-agent problems.\n3. Evaluate the algorithm's empirical performance in a variety of settings.\n4. Consider providing a more detailed analysis of the algorithm's performance in terms of its regret.\n\nRecommendation:\n\nThis paper presents a novel algorithm for learning optimal contracts in hidden-action principal-agent problems. While it has a polynomial-time guarantee in terms of the size of the problem instance when the number of agent's actions is constant, it assumes that the agent's action space is small, which may not be realistic in many applications. Therefore, it would be beneficial to consider relaxing this assumption and providing a comparison with other existing algorithms.",
        "gr_score_with": 0.8257839721254355,
        "gr_score_without": 0.7153284671532847,
        "output_without": {
            "num_tokens_scored": 274,
            "num_green_tokens": 196,
            "green_fraction": 0.7153284671532847,
            "z_score": -3.503609042489923,
            "p_value": 0.9997705006546129
        },
        "output_with": {
            "num_tokens_scored": 287,
            "num_green_tokens": 237,
            "green_fraction": 0.8257839721254355,
            "z_score": 1.0922718734170629,
            "p_value": 0.13735680964698765
        }
    },
    {
        "title": "LIPSCHITZ SINGULARITIES IN DIFFUSION MODELS",
        "abstract": "Diffusion models, which employ stochastic differential equations to sample images through integrals, have emerged as a dominant class of generative models. However, the rationality of the diffusion process itself receives limited attention, leaving the question of whether the problem is well-posed and well-conditioned. In this paper, we explore a perplexing tendency of diffusion models: they often display the infinite Lipschitz property of the network with respect to time variable near the zero point. We provide theoretical proofs to illustrate the presence of infinite Lipschitz constants and empirical results to confirm it. The Lipschitz singularities pose a threat to the stability and accuracy during both the training and inference processes of diffusion models. Therefore, the mitigation of Lipschitz singularities holds great potential for enhancing the performance of diffusion models. To address this challenge, we propose a novel approach, dubbed E-TSDM, which alleviates the Lipschitz singularities of the diffusion model near the zero point of timesteps. Remarkably, our technique yields a substantial improvement in performance. Moreover, as a byproduct of our method, we achieve a dramatic reduction in the Fr\u00e9chet Inception Distance of acceleration methods relying on network Lipschitz, including DDIM and DPM-Solver, by over 33%. Extensive experiments on diverse datasets validate our theory and method. Our work may advance the understanding of the general diffusion process, and also provide insights for the design of diffusion models.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper explores the issue of Lipschitz singularities in diffusion models, which have been shown to be a dominant class of generative models. The authors provide theoretical proofs and empirical results to confirm that the noise prediction and v-prediction diffusion models often exhibit infinite Lipschitz constants near the zero point. This issue poses a threat to the stability and accuracy of the diffusion process, particularly during training and inference processes. To address this challenge, the authors propose a novel approach called Early Timestep-shared Diffusion Model (E-TSDM), which alleviates the Lipschitz singularities by sharing the timestep conditions in the interval with large Lipschitz constants. Experimental results demonstrate that E-TSDM outperforms the baseline on various datasets, including unconditional generation, conditional generation, and fast sampling.\n\nStrengths:\n\n* The paper provides a thorough theoretical analysis of the Lipschitz singularities issue in diffusion models, which is a significant contribution to the field.\n* The authors propose a novel approach called E-TSDM, which is easy to implement and effective in alleviating the Lipschitz singularities.\n* Experimental results demonstrate the superiority of E-TSDM over the baseline on various datasets.\n* The paper explores the generalizability of E-TSDM on different noise schedules and continuous-time diffusion models, which is a significant extension of the approach.\n\nWeaknesses:\n\n* The paper assumes that the noise schedule satisfies d\u03b1tdt |t=0 \u0338= 0, which may not be the case for all noise schedules.\n* The authors do not provide a detailed analysis of the impact of E-TSDM on large-scale text-to-image generative models, which is a significant limitation of the approach.\n* The paper does not provide a thorough comparison of E-TSDM with other potential methods to address the challenge of large Lipschitz constants, which is a significant limitation of the approach.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the impact of E-TSDM on large-scale text-to-image generative models.\n* The authors should provide a thorough comparison of E-TSDM with other potential methods to address the challenge of large Lipschitz constants.\n* The authors should explore the possibility of applying E-TSDM to other types of generative models, such as variational autoencoders (VAEs) and generative adversarial networks (GANs).\n\nRecommendation:\n\nThe paper provides a significant contribution to the field of generative models by exploring the issue of Lipschitz singularities and proposing a novel approach to address this challenge. The experimental results demonstrate the superiority of E-TSDM over the baseline on various datasets. However, the paper has some limitations, such as assuming that the noise schedule satisfies d\u03b1tdt |t=0 \u0338= 0 and not providing a thorough comparison with other potential methods. Therefore, I recommend that the authors address these limitations in future work.\n\nOverall, I believe that the paper is well-written, and the authors provide a thorough analysis of the issue and a novel approach to address it. However, the paper has some limitations that need to be addressed in future work.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper explores the issue of Lipschitz singularities in diffusion models, which have been shown to be a dominant class of generative models. The authors provide theoretical proofs and empirical results to confirm that diffusion models often display infinite Lipschitz properties with respect to time variable near the zero point. This issue poses a threat to the stability and accuracy of diffusion models during both training and inference processes. To address this challenge, the authors propose a novel approach called Early Timestep-shared Diffusion Model (E-TSDM), which alleviates the Lipschitz singularities by sharing timestep conditions in the interval with large Lipschitz constants. The proposed method is easy to implement and has been shown to outperform existing approaches on various datasets.\n\nStrengths:\n1. The paper provides a clear and concise summary of the issue of Lipschitz singularities in diffusion models and its impact on stability and accuracy.\n2. The authors provide theoretical proofs and empirical results to confirm the existence of infinite Lipschitz properties in diffusion models.\n3. The proposed method, E-TSDM, is easy to implement and has been shown to outperform existing approaches on various datasets.\n4. The paper explores the potential of E-TSDM to be combined with popular fast sampling techniques, such as DDIM and DPM-Solver.\n\nWeaknesses:\n1. The paper assumes that the noise schedule conforms to a specific behavior, which may not be true in all cases.\n2. The authors use a fixed value of t\u0303 (100) in their experiments, which may not be optimal for all datasets.\n3. The paper does not provide a detailed analysis of the potential impact of E-TSDM on large-scale text-to-image generative models.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the potential impact of E-TSDM on large-scale text-to-image generative models.\n2. The paper should include a more thorough exploration of the potential limitations of E-TSDM, such as its potential impact on the effectiveness of large-scale generative models.\n3. The authors should provide a more detailed discussion of the potential risks and challenges associated with reducing Lipschitz constants to zero within each sub-interval in E-TSDM.\n\nRecommendation:\nThe paper presents a novel approach to address the issue of Lipschitz singularities in diffusion models. While there are some limitations and potential risks associated with the proposed method, the results presented in the paper suggest that E-TSDM is a promising approach to improving the stability and accuracy of diffusion models. Therefore, I recommend that the paper be accepted for publication in its current form, with some minor revisions to address the suggestions for improvement.\n\nRating: 8/10\n\nReviewer's expertise: The reviewer has expertise in machine learning, deep learning, and generative models. The reviewer has a good understanding of the concepts and techniques presented in the paper, including diffusion models, Lipschitz singularities, and early timestep-shared diffusion models.",
        "gr_score_with": 0.8951406649616368,
        "gr_score_without": 0.8670212765957447,
        "output_without": {
            "num_tokens_scored": 376,
            "num_green_tokens": 326,
            "green_fraction": 0.8670212765957447,
            "z_score": 3.249241300528665,
            "p_value": 0.0005785663794442855
        },
        "output_with": {
            "num_tokens_scored": 391,
            "num_green_tokens": 350,
            "green_fraction": 0.8951406649616368,
            "z_score": 4.703465497898074,
            "p_value": 1.278911683622111e-06
        }
    },
    {
        "title": "TEXTFIELD3D: TOWARDS ENHANCING OPEN- VOCABULARY 3D GENERATION WITH NOISY TEXT FIELDS",
        "abstract": "Generative models have shown remarkable progress in 3D aspect. Recent works learn 3D representation explicitly under text-3D guidance. However, limited text3D data restricts the vocabulary scale and text control of generations. Generators may easily fall into a stereotype concept for certain text prompts, thus losing openvocabulary generation ability. To tackle this issue, we introduce a conditional 3D generative model, namely TextField3D.Specifically, rather than using the text prompts as input directly, we suggest to inject dynamic noise into the latent space of given text prompts, i.e., Noisy Text Fields (NTFs). In this way, limited 3D data can be mapped to the appropriate range of textual latent space that is expanded by NTFs. To this end, an NTFGen module is proposed to model general text latent code in noisy fields. Meanwhile, an NTFBind module is proposed to align viewinvariant image latent code to noisy fields, further supporting image-conditional 3D generation. To guide the conditional generation in both geometry and texture, multi-modal discrimination is constructed with a text-3D discriminator and a text2.5D discriminator. Compared to previous methods, TextField3D includes three merits: 1) large vocabulary, 2) text consistency, and 3) low latency. Extensive experiments demonstrate that our method achieves a potential open-vocabulary 3D generation capability.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel 3D generative model called TextField3D, which aims to enhance the open-vocabulary capability of 3D generative models. The model introduces Noisy Text Fields (NTFs) to boost the latent mapping between V-L concepts and 3D representations. The authors also propose multi-modal discrimination to enhance the supervision of 3D generation. The overall framework is designed to allow for efficient generation of various 3D contents with complicated text prompts.\n\nStrengths:\n\n1. The paper presents a novel approach to enhancing the open-vocabulary capability of 3D generative models.\n2. The authors propose a new module, NTFGen, which generates noisy text latent code to enhance the mapping of V-L pre-trained knowledge and 3D training data.\n3. The paper introduces a multi-modal discrimination framework that includes both text-3D and text-2.5D discriminators to supervise the generation quality and text consistency.\n4. The authors provide extensive experimental results to demonstrate the effectiveness of their proposed method.\n\nWeaknesses:\n\n1. The paper relies heavily on pre-trained models, which may not be available for all users.\n2. The authors assume that the scale of 3D data is steadily increasing, which may not be the case in reality.\n3. The paper does not provide a detailed analysis of the limitations of their proposed method, such as its dependence on the vocabulary of training data.\n4. The authors do not provide a clear explanation of how their proposed method can be used in real-world applications.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the limitations of their proposed method and how they plan to address them.\n2. The paper should include a more detailed discussion of the potential applications of TextField3D and how it can be used in real-world scenarios.\n3. The authors should provide a more comprehensive evaluation of their proposed method, including a comparison with other state-of-the-art methods.\n4. The paper should include a more detailed explanation of the NTFBind module and how it is used in the proposed method.\n\nRecommendation:\n\nThe paper presents a novel approach to enhancing the open-vocabulary capability of 3D generative models. While it has some limitations, the proposed method shows promising results and has the potential to be used in real-world applications. I recommend that the authors address the weaknesses and suggestions for improvement mentioned above and provide a more comprehensive evaluation of their proposed method. With further refinement, TextField3D has the potential to become a state-of-the-art method for 3D generative modeling.\n\nScore: 7.5/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents TextField3D, a conditional 3D generative model that enhances open-vocabulary generation capability by introducing Noisy Text Fields (NTFs) to boost the latent mapping between V-L concepts and 3D representations. TextField3D consists of an NTFGen module that generates noisy text latent code, an NTFBind module that binds view-invariant image latent code to NTFs, and multi-modal discrimination that includes both text-3D and text-2.5D discriminators.\n\nStrengths:\n\n*   The paper introduces a novel approach to enhance open-vocabulary generation capability by injecting noise into the latent space of text prompts.\n*   The NTFGen module and NTFBind module are well-designed and effectively improve generation quality and text control consistency.\n*   The multi-modal discrimination framework is comprehensive and guides the generation process effectively.\n*   The paper presents extensive experiments on various categories and complicated text prompts, demonstrating the effectiveness of TextField3D.\n\nWeaknesses:\n\n*   The paper assumes that the noise injection is sufficient to map limited 3D data to comprehensive V-L pre-trained concepts, which may not be the case in all scenarios.\n*   The training process of TextField3D is computationally expensive, which may limit its practical applications.\n*   The paper does not provide a thorough analysis of the limitations of TextField3D, such as its dependence on the vocabulary of training data.\n\nSuggestions for Improvement:\n\n*   Investigate alternative noise injection methods to further enhance the mapping of limited 3D data to comprehensive V-L pre-trained concepts.\n*   Explore methods to reduce the computational cost of training TextField3D, such as using transfer learning or parallel processing.\n*   Provide a more detailed analysis of the limitations of TextField3D, including its dependence on the vocabulary of training data.\n\nRecommendation:\n\nThe paper presents a novel approach to enhance open-vocabulary generation capability, and the experiments demonstrate its effectiveness. However, there are some limitations that need to be addressed. With further investigation and improvement, TextField3D has the potential to become a powerful tool for 3D generation.\n\n$$",
        "gr_score_with": 0.7993527508090615,
        "gr_score_without": 0.7420289855072464,
        "output_without": {
            "num_tokens_scored": 345,
            "num_green_tokens": 256,
            "green_fraction": 0.7420289855072464,
            "z_score": -2.6915883894992474,
            "p_value": 0.9964443674225062
        },
        "output_with": {
            "num_tokens_scored": 309,
            "num_green_tokens": 247,
            "green_fraction": 0.7993527508090615,
            "z_score": -0.02816956254517797,
            "p_value": 0.5112365434217448
        }
    }
]