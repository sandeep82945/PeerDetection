[
    {
        "title": "CONCEPT BOTTLENECK GENERATIVE MODELS",
        "abstract": "We introduce a generative model with an intrinsically interpretable layer\u2014a concept bottleneck layer\u2014that constrains the model to encode human-understandable concepts. The concept bottleneck layer partitions the generative model into three parts: the pre-concept bottleneck portion, the CB layer, and the post-concept bottleneck portion. To train CB generative models, we complement the traditional task-based loss function for training generative models with a concept loss and an orthogonality loss. The CB layer and these loss terms are model agnostic, which we demonstrate by applying the CB layer to three different families of generative models: generative adversarial networks, variational autoencoders, and diffusion models. On multiple datasets across different types of generative models, steering a generative model, with the CB layer, outperforms all baselines\u2014in some cases, it is 10 times more effective. In addition, we show how the CB layer can be used to interpret the output of the generative model and debug the model during or post training.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces concept bottleneck generative models (CBGMs), a novel approach to generative modeling that incorporates an intrinsically interpretable layer, the concept bottleneck (CB) layer. This layer is designed to constrain the model to encode human-understandable concepts, enabling the model to be steered, interpreted, and debugged. The CB layer is model-agnostic, meaning it can be applied to various generative model families, including variational autoencoders (VAEs), generative adversarial networks (GANs), and diffusion models. The authors demonstrate the effectiveness of CBGMs in steering generative models, interpreting their output, and debugging them during and post-training.\n\nStrengths:\n\n1. The paper presents a novel and innovative approach to generative modeling, addressing a significant limitation of current generative models, which are largely inscrutable and difficult to steer.\n2. The concept bottleneck layer is model-agnostic, allowing it to be applied to various generative model families, increasing its versatility and potential impact.\n3. The authors provide a comprehensive evaluation of CBGMs, including steering, interpretation, and debugging experiments, which demonstrate their effectiveness and robustness.\n4. The paper discusses the limitations of current approaches to interpreting and steering generative models, highlighting the need for more interpretable and controllable models.\n\nWeaknesses:\n\n1. The paper requires a significant amount of background knowledge in generative modeling, which may make it challenging for readers without a strong foundation in the field.\n2. The authors rely heavily on existing work, which may make it difficult to distinguish the novelty and contributions of the paper.\n3. The paper does not provide a clear explanation of how the concept bottleneck layer is trained, which is a critical aspect of the method.\n4. The authors do not discuss the potential scalability issues of the method, particularly when dealing with large and complex datasets.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the concept bottleneck layer, including its training procedure and how it is integrated into the generative model.\n2. Include a more comprehensive discussion of the limitations of the method, including potential scalability issues and the need for large amounts of annotated data.\n3. Consider including more qualitative and quantitative evaluations of the method, such as user studies or expert evaluations, to provide a more comprehensive understanding of its effectiveness.\n4. Provide a clear explanation of how the method can be applied to other domains, such as text or audio, and discuss the potential challenges and opportunities of doing so.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to generative modeling, addressing a significant limitation of current generative models. The concept bottleneck layer is model-agnostic, allowing it to be applied to various generative model families, increasing its versatility and potential impact. However, the paper requires a significant amount of background knowledge in generative modeling, and the authors rely heavily on existing work. Despite these limitations, the paper is well-written, and the authors provide a comprehensive evaluation of the method. I recommend this paper for readers interested in generative modeling and interpretable AI.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes a novel generative model architecture, called concept bottleneck generative models (CBGMs), which integrates a concept bottleneck (CB) layer to enable interpretability, steerability, and debugging of generative models. The CB layer is a model-agnostic component that can be applied to different generative model families, including VAEs, GANs, and diffusion models. The authors show that the CB layer can effectively steer the output of generative models, improve their interpretability, and facilitate debugging during and post-training. The proposed framework is evaluated on multiple datasets and generative models, demonstrating its effectiveness and versatility.\n\nStrengths:\n\n1. The paper proposes a novel and innovative approach to generative models, addressing a significant challenge in the field.\n2. The authors provide a comprehensive framework for concept bottleneck generative models, including a clear description of the architecture, loss functions, and training procedure.\n3. The paper presents extensive experimental results, demonstrating the effectiveness of the proposed method across different datasets and generative models.\n4. The authors provide a thorough analysis of the limitations and challenges of their proposed method, acknowledging potential issues and discussing potential future directions.\n\nWeaknesses:\n\n1. The paper assumes that pre-defined human-understandable features are available, which may not always be the case in practice.\n2. The authors do not provide a clear explanation of how to determine the number of concepts and their specific definitions.\n3. The paper does not explore the potential limitations of the CB layer in terms of its ability to capture complex relationships between concepts.\n4. The authors do not provide a detailed analysis of the computational costs associated with training and deploying CBGMs.\n\nSuggestions for Improvement:\n\n1. Provide more detailed guidance on how to determine the number of concepts and their specific definitions.\n2. Explore alternative approaches for dealing with complex relationships between concepts.\n3. Conduct a more thorough analysis of the computational costs associated with training and deploying CBGMs.\n4. Discuss potential applications and extensions of the proposed method, such as its use in text-to-image models or other generative models.\n\nRecommendation:\n\nThis paper presents a novel and innovative approach to generative models, addressing a significant challenge in the field. The proposed framework is well-motivated, well-designed, and effectively evaluated. The authors provide a thorough analysis of the limitations and challenges of their proposed method, acknowledging potential issues and discussing potential future directions. Overall, this paper is a significant contribution to the field of generative models and has the potential to inspire new research directions and applications.\n\nRating: 8.5/10\n\nRecommendation for Publication: Yes",
        "gr_score_with": 0.8942857142857142,
        "gr_score_without": 0.817258883248731,
        "output_without": {
            "num_tokens_scored": 394,
            "num_green_tokens": 322,
            "green_fraction": 0.817258883248731,
            "z_score": 0.856747135696705,
            "p_value": 0.19579232362621968
        },
        "output_with": {
            "num_tokens_scored": 350,
            "num_green_tokens": 313,
            "green_fraction": 0.8942857142857142,
            "z_score": 4.410050648782306,
            "p_value": 5.167322227182673e-06
        }
    },
    {
        "title": "SWAP: SPARSE ENTROPIC WASSERSTEIN REGRESSION FOR ROBUST NETWORK PRUNING",
        "abstract": "This study addresses the challenge of inaccurate gradients in computing the empirical Fisher Information Matrix during neural network pruning. We introduce SWAP, a formulation of Entropic Wasserstein regression (EWR) for pruning, capitalizing on the geometric properties of the optimal transport problem. The \u201cswap\u201d of the commonly used linear regression with the EWR in optimization is analytically demonstrated to offer noise mitigation effects by incorporating neighborhood interpolation across data points with only marginal additional computational cost. The unique strength of SWAP is its intrinsic ability to balance noise reduction and covariance information preservation effectively. Extensive experiments performed on various networks and datasets show comparable performance of SWAP with state-of-the-art (SoTA) network pruning algorithms. Our proposed method outperforms the SoTA when the network size or the target sparsity is large, the gain is even larger with the existence of noisy gradients, possibly from noisy data, analog memory, or adversarial attacks. Notably, our proposed method achieves a gain of 6% improvement in accuracy and 8% improvement in testing loss for MobileNetV1 with less than one-fourth of the network parameters remaining.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method for neural network pruning called Sparse Entropic Wasserstein Regression Pruning (SWAP), which leverages the Entropic Wasserstein regression (EWR) to mitigate the effect of noisy gradients in computing the empirical Fisher Information Matrix (FIM). The proposed method, SWAP, is shown to outperform state-of-the-art (SoTA) network pruning algorithms, especially in the presence of noisy gradients.\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of the network pruning problem and its relation to the optimization problem, which is a significant contribution to the field.\n2. The proposed method, SWAP, is shown to be robust to noisy gradients, which is a critical aspect of neural network pruning.\n3. The paper provides a thorough comparison of SWAP with other SoTA methods, including magnitude pruning, WoodFisher, and Combinatorial Brain Surgeon.\n4. The authors provide a clear and concise algorithmic framework for SWAP, which is easy to implement and understand.\n\nWeaknesses:\n\n1. The paper assumes that the neural network is pre-trained, which may not be the case in practice.\n2. The proposed method requires the computation of the Wasserstein distance, which can be computationally expensive for large datasets.\n3. The paper does not provide a thorough analysis of the computational complexity of SWAP.\n4. The authors do not provide a clear explanation of how to choose the regularization parameter \u03bb and the entropic regularization term \u03b5.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the computational complexity of SWAP and its scalability to large datasets.\n2. Investigate the effect of different regularization parameters \u03bb and entropic regularization terms \u03b5 on the performance of SWAP.\n3. Provide a more comprehensive comparison of SWAP with other SoTA methods, including a more detailed analysis of their strengths and weaknesses.\n4. Investigate the applicability of SWAP to other machine learning tasks, such as clustering and dimensionality reduction.\n\nRecommendation:\n\nThe paper provides a novel and robust method for neural network pruning, which has the potential to improve the performance of neural networks in the presence of noisy gradients. However, the paper requires further investigation and improvement in terms of computational complexity, scalability, and applicability to other machine learning tasks. Therefore, I recommend that the authors provide a more detailed analysis of the computational complexity of SWAP and its scalability to large datasets, and investigate the effect of different regularization parameters \u03bb and entropic regularization terms \u03b5 on the performance of SWAP.\n\nOverall, I believe that the paper provides a significant contribution to the field of neural network pruning and has the potential to improve the performance of neural networks in the presence of noisy gradients.\n\nRating: 7.5/10\n\nRecommendation: Accept with minor revisions.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method called SWAP (Sparse Entropic Wasserstein Regression) for neural network pruning. The method aims to address the challenge of inaccurate gradients in computing the empirical Fisher Information Matrix during neural network pruning. The authors introduce a formulation of Entropic Wasserstein regression (EWR) for pruning, which capitalizes on the geometric properties of the optimal transport problem. The proposed method, SWAP, is demonstrated to be more robust to noisy gradients and achieves comparable performance with state-of-the-art (SoTA) network pruning algorithms. The authors also provide a theoretical analysis of the method, showing that it strikes a balance between gradient noise reduction and covariance capturing.\n\nStrengths:\n\n* The proposed method, SWAP, is novel and addresses a significant challenge in neural network pruning.\n* The authors provide a thorough theoretical analysis of the method, including a discussion on its robustness to noisy gradients.\n* The experimental results demonstrate that SWAP outperforms or matches state-of-the-art methods in various scenarios.\n* The authors provide a clear and concise presentation of the method and its results.\n\nWeaknesses:\n\n* The paper assumes that the training data is i.i.d., which may not be the case in practice.\n* The authors do not provide a detailed analysis of the computational complexity of the proposed method.\n* The experimental results are based on a limited set of datasets and networks, and it is not clear whether the method generalizes to other scenarios.\n* The authors do not provide a clear comparison with existing methods in terms of computational efficiency.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the computational complexity of the proposed method.\n* The experimental results should be extended to a wider range of datasets and networks to demonstrate the generalizability of the method.\n* The authors should provide a clear comparison with existing methods in terms of computational efficiency.\n* The authors should investigate the robustness of the proposed method to non-i.i.d. training data.\n\nRecommendation:\n\nThe proposed method, SWAP, is a novel and promising approach to neural network pruning. The authors provide a thorough theoretical analysis and experimental results that demonstrate its effectiveness. However, the method has some limitations, and further research is needed to address these issues. Overall, I recommend this paper for publication in a top-tier conference, but with the expectation that the authors will address the weaknesses and suggestions for improvement mentioned above.",
        "gr_score_with": 0.8392857142857143,
        "gr_score_without": 0.8117977528089888,
        "output_without": {
            "num_tokens_scored": 356,
            "num_green_tokens": 289,
            "green_fraction": 0.8117977528089888,
            "z_score": 0.5567865987532468,
            "p_value": 0.2888366205684008
        },
        "output_with": {
            "num_tokens_scored": 336,
            "num_green_tokens": 282,
            "green_fraction": 0.8392857142857143,
            "z_score": 1.8005623758674256,
            "p_value": 0.03588594191629517
        }
    },
    {
        "title": "BEYOND IMITATION: LEVERAGING FINE-GRAINED QUALITY SIGNALS FOR ALIGNMENT",
        "abstract": "Alignment with human preference is a desired property of large language models (LLMs). Currently, the main alignment approach is based on reinforcement learning from human feedback (RLHF). Despite the effectiveness of RLHF, it is intricate to implement and train, thus recent studies explore how to develop alternative alignment approaches based on supervised fine-tuning (SFT). A major limitation of SFT is that it essentially does imitation learning, which cannot fully understand what are the expected behaviors. To address this issue, we propose an improved alignment approach named FIGA. Different from prior methods, we incorporate fine-grained (i.e., token or phrase level) quality signals that are derived by contrasting good and bad responses. Our approach has made two major contributions. Firstly, we curate a refined alignment dataset that pairs initial responses and the corresponding revised ones. Secondly, we devise a new loss function can leverage fine-grained quality signals to instruct the learning of LLMs for alignment. Extensive experiments have demonstrated the effectiveness of our approaches by comparing a number of competitive baselines. We release all the above-mentioned resources at https://github.com/RUCAIBox/FIGA.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach called FIGA (Fine-grained Quality-Aware Alignment) for aligning language models with human preferences. The approach leverages fine-grained quality signals derived by contrasting good and bad responses to instruct the learning of language models for alignment. The authors curate a refined alignment dataset called SPA (SubPar Alignment) that pairs initial responses with revised responses on queries where a language model performs less well. They also design a new loss function that incorporates fine-grained quality signals to encourage the model to learn good behaviors and penalize bad behaviors. The approach is evaluated on several benchmarks, showing superior performance compared to existing alignment methods.\n\nStrengths:\n\n1. The paper presents a novel approach for aligning language models with human preferences, which is a critical problem in natural language processing.\n2. The approach leverages fine-grained quality signals, which provides more detailed information about what makes a response good or bad.\n3. The authors curate a refined alignment dataset called SPA, which is a significant contribution to the field.\n4. The approach is evaluated on several benchmarks, showing superior performance compared to existing alignment methods.\n\nWeaknesses:\n\n1. The paper assumes the availability of a reward model, which may not always be the case in real-world scenarios.\n2. The approach relies on the quality of the revised responses, which may not always be available or reliable.\n3. The paper does not provide a detailed analysis of the Levenshtein distance metric used to quantify the similarity between initial and revised responses.\n4. The approach may not generalize well to out-of-distribution scenarios, where the reward model may not perform well.\n\nSuggestions for Improvement:\n\n1. Investigate alternative methods for deriving fine-grained quality signals, such as using other metrics or incorporating additional information.\n2. Evaluate the approach on more diverse and challenging datasets to assess its generalizability.\n3. Provide more detailed analysis of the Levenshtein distance metric and its limitations.\n4. Investigate the use of other reward models or alternative methods for aligning language models with human preferences.\n\nRecommendation:\n\nThe paper presents a novel and promising approach for aligning language models with human preferences. While there are some limitations and areas for improvement, the approach shows superior performance compared to existing alignment methods. I recommend this paper for publication in a top-tier conference, with the suggestion to address the weaknesses and improve the approach as mentioned above.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$ \nPaper Summary:\n\nThe paper proposes an alignment approach named FIGA to improve the alignment of large language models (LLMs) with human preferences. FIGA incorporates fine-grained quality signals derived by contrasting good and bad responses to instruct the learning of LLMs for alignment. The authors curate a refined alignment dataset called SPA that pairs initial responses and the corresponding revised ones. They also devise a new loss function that can leverage fine-grained quality signals to capture the underlying effect to generate high-quality responses.\n\nStrengths:\n\n* The paper proposes an innovative approach to alignment by leveraging fine-grained quality signals.\n* The authors curate a refined alignment dataset that pairs initial responses and the corresponding revised ones.\n* The proposed loss function can capture the underlying effect to generate high-quality responses.\n* The authors conduct extensive experiments to demonstrate the effectiveness of their approach.\n\nWeaknesses:\n\n* The paper assumes that a reward model for assessing the alignment level is available, which may not always be the case.\n* The authors do not provide a detailed analysis of the distribution shift between the LLM to be aligned and the ground-truth demonstrations.\n* The proposed loss function may not be robust to out-of-distribution scenarios.\n\nSuggestions for Improvement:\n\n* Provide more details about the reward model and its training process.\n* Conduct a more thorough analysis of the distribution shift between the LLM to be aligned and the ground-truth demonstrations.\n* Investigate the robustness of the proposed loss function to out-of-distribution scenarios.\n\nRecommendation:\n\nThe paper proposes an innovative approach to alignment by leveraging fine-grained quality signals. The authors conduct extensive experiments to demonstrate the effectiveness of their approach. However, the paper assumes that a reward model for assessing the alignment level is available, and the proposed loss function may not be robust to out-of-distribution scenarios. With some modifications to address these limitations, the paper has the potential to make a significant contribution to the field of alignment.",
        "gr_score_with": 0.8530612244897959,
        "gr_score_without": 0.8096590909090909,
        "output_without": {
            "num_tokens_scored": 352,
            "num_green_tokens": 285,
            "green_fraction": 0.8096590909090909,
            "z_score": 0.45333878623509694,
            "p_value": 0.3251524038781556
        },
        "output_with": {
            "num_tokens_scored": 245,
            "num_green_tokens": 209,
            "green_fraction": 0.8530612244897959,
            "z_score": 2.0765686321995616,
            "p_value": 0.018920695263948403
        }
    },
    {
        "title": "GUAGE MODEL POWERED DIALOGUE AGENTS",
        "abstract": "Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play simulation. In this manner, the LLM-powered dialogue agent can not only be generalized to different cases after the training, but also be applicable to different applications by just substituting the learned plug-in. In addition, we propose to evaluate the policy planning capability of dialogue systems under the interactive setting. Experimental results demonstrate that PPDPP consistently and substantially outperforms existing approaches on three different proactive dialogue applications, including negotiation, emotional support, and tutoring dialogues.1",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel paradigm for strategizing large language models (LLMs) powered dialogue agents, called Plug-and-Play Dialogue Policy Planner (PPDPP). PPDPP is designed to tackle the challenges of proactive dialogue problems, where the dialogue agent needs to strategically take the initiative to steer the conversation towards an anticipated goal. The proposed framework consists of a tunable language model plug-in that can be fine-tuned with available human-annotated data and reinforced learning from goal-oriented AI feedback. The authors demonstrate the effectiveness of PPDPP on three different proactive dialogue problems, including negotiation, emotional support, and tutoring dialogues.\n\nStrengths:\n1. The paper proposes a novel and effective approach to tackle the challenges of proactive dialogue problems.\n2. The framework is designed to be modular and flexible, allowing for easy adaptation to different applications and domains.\n3. The authors provide a thorough evaluation of PPDPP on three different datasets, demonstrating its superiority over existing approaches.\n4. The paper highlights the potential of tunable plug-ins to address specific shortcomings in LLMs and integrate with multiple plug-ins to tackle more complex dialogue challenges.\n\nWeaknesses:\n1. The paper assumes the availability of human-annotated data, which may not be feasible in all scenarios.\n2. The framework relies on the quality of the goal-oriented AI feedback, which may not always be available or reliable.\n3. The paper does not provide a detailed analysis of the computational resources required for training and deploying PPDPP.\n4. The authors do not discuss the potential limitations and biases of using LLMs in dialogue systems.\n\nSuggestions for Improvement:\n1. Provide more detailed information on the data requirements and preprocessing steps for training PPDPP.\n2. Discuss the potential limitations and biases of using LLMs in dialogue systems and propose potential solutions.\n3. Provide a more comprehensive analysis of the computational resources required for training and deploying PPDPP.\n4. Consider exploring other types of goal-oriented AI feedback, such as multi-modal feedback, to improve the robustness of PPDPP.\n\nRecommendation:\nThe paper proposes a novel and effective approach to tackle the challenges of proactive dialogue problems. While there are some limitations and areas for improvement, the framework shows promise for a wide range of applications. I recommend the paper for publication, with the understanding that the authors will address the suggested improvements in future work.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel dialogue policy planning paradigm, Plug-and-Play Dialogue Policy Planner (PPDPP), to strategize large language models (LLMs) for proactive dialogue problems. PPDPP leverages a tunable language model plug-in to predict the dialogue strategy at the next turn, which can be fine-tuned with available human-annotated corpora and reinforcement learning from goal-oriented AI feedback. The framework enables the LLM-powered dialogue agent to generalize to different cases and exhibit versatility across diverse applications.\n\nStrengths:\n1. The paper introduces a novel dialogue policy planning paradigm, PPDPP, which addresses the limitations of existing approaches in handling proactive dialogue problems.\n2. The framework leverages reinforcement learning from goal-oriented AI feedback, which enables the LLM-powered dialogue agent to learn from dynamic interactions and improve its policy planning capability.\n3. The paper proposes an LLM-based interactive evaluation approach, which assesses the policy planning capability of dialogue systems under the interactive setting.\n\nWeaknesses:\n1. The paper relies heavily on the quality of the human-annotated corpora, which may not be available for all dialogue problems.\n2. The framework requires significant computational resources and expertise in reinforcement learning, which may be a barrier for adoption in practical applications.\n3. The paper does not provide a thorough comparison of the PPDPP framework with existing LLM-based dialogue systems, which makes it difficult to evaluate its effectiveness.\n\nSuggestions for Improvement:\n1. The authors should provide more detailed information about the human-annotated corpora used for training and evaluation, including their size, quality, and annotation process.\n2. The paper should include a more comprehensive comparison of the PPDPP framework with existing LLM-based dialogue systems, including their strengths and limitations.\n3. The authors should provide more information about the computational resources and expertise required to implement and train the PPDPP framework.\n\nRecommendation:\nThe paper proposes an innovative dialogue policy planning paradigm, PPDPP, which has the potential to improve the performance of LLM-powered dialogue agents in proactive dialogue problems. While the paper has some limitations, it provides valuable insights into the importance of reinforcement learning from goal-oriented AI feedback and the need for more interactive evaluation protocols. With further development and refinement, the PPDPP framework could become a valuable tool for improving the effectiveness and efficiency of dialogue systems.\n\nRating: 7/10\n$$$",
        "gr_score_with": 0.8176470588235294,
        "gr_score_without": 0.766016713091922,
        "output_without": {
            "num_tokens_scored": 359,
            "num_green_tokens": 275,
            "green_fraction": 0.766016713091922,
            "z_score": -1.6094141496670091,
            "p_value": 0.9462370933314552
        },
        "output_with": {
            "num_tokens_scored": 340,
            "num_green_tokens": 278,
            "green_fraction": 0.8176470588235294,
            "z_score": 0.8137672353672707,
            "p_value": 0.20788915421013876
        }
    },
    {
        "title": "THE EFFECTIVENESS OF RANDOM FORGETTING FOR ROBUST GENERALIZATION",
        "abstract": "Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy. Adversarial Training (AT) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of AT is robust overfitting, where the network\u2019s robust performance on test data deteriorates with further training, thus hindering generalization. Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called \u201cForget to Mitigate Overfitting (FOMO)\". FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model\u2019s information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. Our experiments on benchmark datasets and adversarial attacks show that FOMO alleviates robust overfitting by significantly reducing the gap between the best and last robust test accuracy while improving the state-of-the-art robustness. Furthermore, FOMO provides a better trade-off between standard and robust accuracy, outperforming baseline adversarial methods. Finally, our framework is robust to AutoAttacks and increases generalization in many real-world scenarios.1",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel adversarial training paradigm called Forget to Mitigate Overfitting (FOMO) to address the problem of robust overfitting in deep neural networks (DNNs). FOMO alternates between the forgetting phase, where a subset of weights is randomly forgotten through reinitialization, and the relearning phase, where the network emphasizes learning generalizable features. The authors demonstrate the efficacy of FOMO on benchmark datasets and adversarial attacks, showing that it alleviates robust overfitting, improves robustness, and provides a better trade-off between standard and robust accuracy.\n\nStrengths:\n\n1. The paper provides a comprehensive overview of the problem of robust overfitting in adversarial training and its significance in real-world applications.\n2. The authors propose a novel and innovative solution, FOMO, which is grounded in the concept of active forgetting and relearning, inspired by the human brain's learning mechanisms.\n3. The experimental results demonstrate the effectiveness of FOMO in alleviating robust overfitting, improving robustness, and providing a better trade-off between standard and robust accuracy.\n4. The authors conduct extensive experiments on multiple datasets, architectures, and perturbation types, showcasing the scalability and robustness of FOMO.\n\nWeaknesses:\n\n1. The paper assumes that the forgetting phase is beneficial for generalization, but it is unclear whether this assumption holds in all cases.\n2. The authors do not provide a thorough analysis of the convergence properties of FOMO, which is crucial for understanding its robustness and generalization capabilities.\n3. The paper relies heavily on empirical results, and more theoretical analysis is needed to understand the underlying mechanisms of FOMO.\n4. The authors do not discuss the computational costs and complexity of implementing FOMO, which may be a limitation in practice.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed analysis of the forgetting phase and its impact on generalization.\n2. Investigate the convergence properties of FOMO and its robustness to different perturbation types.\n3. Conduct more theoretical analysis to understand the underlying mechanisms of FOMO and its relationship to the effectiveness of adversarial training.\n4. Discuss the computational costs and complexity of implementing FOMO and provide strategies for reducing these costs.\n\nRecommendation:\n\nThe paper makes a significant contribution to the field of adversarial training by proposing a novel and innovative solution, FOMO. The experimental results demonstrate the effectiveness of FOMO, and the authors provide a comprehensive overview of the problem and its significance. However, more theoretical analysis and discussion of the computational costs are needed to fully understand the potential of FOMO. With further development and refinement, FOMO has the potential to become a widely adopted technique for improving the robustness and generalization of deep neural networks.\n\nOverall, I recommend this paper for publication in a top-tier conference or journal, with the suggestion that the authors address the limitations and weaknesses mentioned above.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel adversarial training paradigm called Forget to Mitigate Overfitting (FOMO) to improve the performance and generalization of Deep Neural Networks (DNNs) in adversarial learning. FOMO alternates between a forgetting phase, where a random subset of weights is reinitialized, and a relearning phase, where the network emphasizes learning generalizable features. The authors demonstrate the effectiveness of FOMO in alleviating robust overfitting and achieving significant results across multiple architectures and datasets.\n\nStrengths:\n\n* The paper provides a thorough analysis of the problem of robust overfitting in adversarial training and proposes a novel solution that is based on the concept of active forgetting.\n* The authors provide a clear and concise explanation of the FOMO method and its components, including the forgetting phase, relearning phase, and consolidation phase.\n* The paper includes an extensive set of experiments that demonstrate the effectiveness of FOMO in improving both standard and robust performance and generalization across a wide range of architectures, datasets, and perturbation types.\n* The authors provide a comprehensive ablation study that evaluates the importance of each component of the FOMO method.\n\nWeaknesses:\n\n* The paper assumes that the forgetting phase is necessary to alleviate robust overfitting, but it does not provide a clear explanation of why this is the case. It would be helpful to provide more theoretical justification for this assumption.\n* The paper does not provide a clear comparison with existing methods that use early stopping to prevent robust overfitting. It would be helpful to provide a more detailed analysis of the trade-offs between FOMO and these existing methods.\n* The paper uses a relatively small set of datasets and architectures in the experiments. It would be helpful to extend the experiments to a larger set of datasets and architectures to demonstrate the generalizability of the FOMO method.\n\nSuggestions for Improvement:\n\n* Provide a more detailed theoretical analysis of the FOMO method, including a clear explanation of why the forgetting phase is necessary to alleviate robust overfitting.\n* Conduct a more comprehensive comparison with existing methods that use early stopping to prevent robust overfitting.\n* Extend the experiments to a larger set of datasets and architectures to demonstrate the generalizability of the FOMO method.\n\nRecommendation:\n\nThis paper presents a novel and effective solution to the problem of robust overfitting in adversarial training. The FOMO method is well-designed and easy to implement, and the authors provide a comprehensive set of experiments that demonstrate its effectiveness. However, the paper would benefit from a more detailed theoretical analysis and a more comprehensive comparison with existing methods. Overall, I would recommend this paper for publication in a top conference in the field of adversarial learning.\n\nRating: 8/10\n\n$$$",
        "gr_score_with": 0.9108635097493036,
        "gr_score_without": 0.8614609571788413,
        "output_without": {
            "num_tokens_scored": 397,
            "num_green_tokens": 342,
            "green_fraction": 0.8614609571788413,
            "z_score": 3.0617771372022524,
            "p_value": 0.0011001360240621098
        },
        "output_with": {
            "num_tokens_scored": 359,
            "num_green_tokens": 327,
            "green_fraction": 0.9108635097493036,
            "z_score": 5.251643190706128,
            "p_value": 7.537413669191202e-08
        }
    },
    {
        "title": "SYMPHONY: SYMMETRY-EQUIVARIANT POINT- CENTERED SPHERICAL HARMONICS FOR MOLECULE GENERATION",
        "abstract": "We present Symphony, an E(3)-equivariant autoregressive generative model for 3D molecular geometries that iteratively builds a molecule from molecular fragments. Existing autoregressive models such as G-SchNet (Gebauer et al., 2019) and G-SphereNet (Luo & Ji, 2022) for molecules utilize rotationally invariant features to respect the 3D symmetries of molecules. In contrast, Symphony uses message-passing with higher-degree E(3)-equivariant features. This allows a novel representation of probability distributions via spherical harmonic signals to efficiently model the 3D geometry of molecules. We show that Symphony is able to accurately generate small molecules from the QM9 dataset, outperforming existing autoregressive models and approaching the performance of diffusion models.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents Symphony, an E(3)-equivariant autoregressive generative model for 3D molecular geometries that iteratively builds a molecule from molecular fragments. Unlike existing autoregressive models that use rotationally invariant features, Symphony uses higher-degree E(3)-equivariant features and spherical harmonic projections to build molecules while respecting the E(3) symmetries of molecular fragments. The authors demonstrate that Symphony outperforms existing autoregressive models and approaches the performance of diffusion models on various metrics.\n\nStrengths:\n\n1. The paper presents a novel approach to autoregressive molecular generation using higher-degree E(3)-equivariant features and spherical harmonic projections.\n2. The authors demonstrate that Symphony outperforms existing autoregressive models and approaches the performance of diffusion models on various metrics.\n3. The paper proposes a comprehensive set of tests to evaluate the quality of generated 3D structures, including validity, capturing training set statistics, and generalization capabilities.\n4. The authors provide a detailed comparison between their approach and existing methods, highlighting the advantages of their approach.\n\nWeaknesses:\n\n1. The paper assumes that the training set molecules are representative of the underlying distribution, which may not always be the case.\n2. The authors use a lookup table to assign bond orders, which may not be accurate for all molecules.\n3. The paper does not provide a clear explanation of how the spherical harmonic projections are used to model the radial distribution.\n4. The authors do not provide a detailed comparison between their approach and existing diffusion models, such as EDM.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of how the spherical harmonic projections are used to model the radial distribution.\n2. Use a more accurate method to assign bond orders, such as using a neural network to predict bond orders.\n3. Provide a more detailed comparison between the approach and existing diffusion models, such as EDM.\n4. Investigate the use of normalizing flows to smoothly model the radial distribution without any discretization.\n\nRecommendation:\n\nThe paper presents a novel and promising approach to autoregressive molecular generation using higher-degree E(3)-equivariant features and spherical harmonic projections. While there are some limitations and areas for improvement, the paper provides a comprehensive evaluation of the approach and demonstrates its effectiveness on various metrics. I recommend accepting the paper for publication, subject to the suggested improvements.\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\nThe paper presents Symphony, an E(3)-equivariant autoregressive generative model for 3D molecular geometries that builds molecules from molecular fragments. Symphony uses higher-degree E(3)-equivariant features and spherical harmonic projections to predict the focus node, target atomic number, and target position of new atoms, while respecting the E(3) symmetries of molecular fragments. The authors show that Symphony outperforms existing autoregressive models and approaches the performance of diffusion models on the QM9 dataset.\n\nStrengths:\n1. The paper presents a novel approach to molecular generation using E(3)-equivariant features and spherical harmonic projections, which allows for a more accurate representation of 3D molecular geometries.\n2. The authors provide a comprehensive evaluation of Symphony's performance on various metrics, including validity, uniqueness, bispectrum, and generalization capabilities.\n3. The paper highlights the advantages of autoregressive models over diffusion models, including faster inference speeds and improved scalability.\n4. The authors provide a clear explanation of the methods used in the paper, including the EMBEDDER, MLP, and linear layers.\n\nWeaknesses:\n1. The paper assumes access to a large dataset of molecular structures, which may not be available for all researchers.\n2. The authors do not provide a thorough comparison of Symphony's performance on different types of molecules, such as small molecules, proteins, and complexes.\n3. The paper does not discuss the computational cost of training Symphony, which may be a significant limitation for large-scale molecular generation tasks.\n4. The authors do not provide a clear explanation of how Symphony can be used for molecular design and optimization tasks.\n\nSuggestions for Improvement:\n1. Provide a more thorough comparison of Symphony's performance on different types of molecules.\n2. Discuss the computational cost of training Symphony and explore methods to reduce it.\n3. Provide a clear explanation of how Symphony can be used for molecular design and optimization tasks.\n4. Consider exploring normalizing flows to smoothly model the radial distribution without any discretization.\n\nRecommendation:\nThe paper presents a novel approach to molecular generation using E(3)-equivariant features and spherical harmonic projections, which shows promising results on molecular generation and completion. However, the paper assumes access to a large dataset of molecular structures, and the computational cost of training Symphony may be a significant limitation. Further exploration of normalizing flows and placing entire local environment motifs at once could improve the performance and scalability of Symphony.",
        "gr_score_with": 0.8899082568807339,
        "gr_score_without": 0.814935064935065,
        "output_without": {
            "num_tokens_scored": 308,
            "num_green_tokens": 251,
            "green_fraction": 0.814935064935065,
            "z_score": 0.6555393182961124,
            "p_value": 0.25606029014945914
        },
        "output_with": {
            "num_tokens_scored": 327,
            "num_green_tokens": 291,
            "green_fraction": 0.8899082568807339,
            "z_score": 4.064793734190551,
            "p_value": 2.4037479553718794e-05
        }
    },
    {
        "title": "ONE-HOT GENERALIZED LINEAR MODEL FOR SWITCHING BRAIN STATE DISCOVERY",
        "abstract": "Exposing meaningful and interpretable neural interactions is critical to understanding neural circuits. Inferred neural interactions from neural signals primarily reflect functional connectivity. In a long experiment, subject animals may experience different stages defined by the experiment, stimuli, or behavioral states, and hence functional connectivity can change over time. To model dynamically changing functional connectivity, prior work employs state-switching generalized linear models with hidden Markov models (i.e., HMM-GLMs). However, we argue they lack biological plausibility, as functional connectivities are shaped and confined by the underlying anatomical connectome. Here, we propose two novel priorinformed state-switching GLMs, called Gaussian HMM-GLM (Gaussian prior) and one-hot HMM-GLM (Gumbel-Softmax one-hot prior). We show that the learned prior should capture the state-invariant interaction, shedding light on the underlying anatomical connectome and revealing more likely physical neuron interactions. The state-dependent interaction modeled by each GLM offers traceability to capture functional variations across multiple brain states. Our methods effectively recover true interaction structures in simulated data, achieve the highest predictive likelihood, and enhance the interpretability of interaction patterns and hidden states when applied to real neural data. The code is available at https://github.com/JerrySoybean/onehot-hmmglm.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach for estimating time-varying functional connectivity in multi-state neural systems, called One-Hot Generalized Linear Model for Switching Brain State Discovery. The model, referred to as One-Hot HMM-GLM (OHG), decomposes the traditional weight matrix in GLMs into a discrete adjacency matrix representing the connection type and a positive-valued strength matrix. The authors argue that the regulated adjacency matrices with their shared prior should inform us about the underlying anatomical connectome, while the less restricted strength matrices can provide flexibility to capture functional variations across multiple brain states. The model is compared to several baseline methods on simulated and real neural datasets, showing superior performance in terms of log-likelihood, state accuracy, weight error, adjacency accuracy, and adjacency prior accuracy.\n\nStrengths:\n\n1. The paper proposes a novel and biologically plausible approach for estimating time-varying functional connectivity in multi-state neural systems.\n2. The model is compared to several baseline methods on simulated and real neural datasets, showing superior performance.\n3. The authors provide a clear and concise explanation of the model and its components.\n4. The paper includes a detailed experimental evaluation, including quantitative results and visualizations of the learned models.\n\nWeaknesses:\n\n1. The paper assumes that the underlying anatomical connectome is static, which may not be the case in reality.\n2. The model requires a large number of hyperparameters to be pre-defined, which can be challenging in practice.\n3. The paper does not provide a clear explanation of how the Gumbel-Softmax prior is used to enforce the adjacency matrices to learn shared information.\n4. The experimental evaluation is limited to two real neural datasets, and it would be beneficial to include more datasets to demonstrate the robustness of the model.\n\nSuggestions for Improvement:\n\n1. Consider incorporating a more dynamic model of the underlying anatomical connectome to better capture the complexity of neural systems.\n2. Provide a more detailed explanation of the Gumbel-Softmax prior and its role in enforcing shared information among adjacency matrices.\n3. Include more real neural datasets in the experimental evaluation to demonstrate the robustness of the model.\n4. Consider using more advanced techniques, such as Bayesian inference or deep learning, to improve the accuracy and robustness of the model.\n\nRecommendation:\n\nThe paper presents a novel and biologically plausible approach for estimating time-varying functional connectivity in multi-state neural systems. While there are some limitations and areas for improvement, the results demonstrate the potential of the model to improve our understanding of neural systems. I recommend this paper for publication in a reputable journal, with the hope that the authors will address the limitations and suggestions for improvement.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel one-hot generalized linear model (OHG) for switching brain state discovery. The authors aim to improve upon previous state-switching generalized linear models (GLMs) with hidden Markov models (HMMs) by incorporating a learnable prior that constrains the search space for interaction weights. The proposed OHG model decomposes the weight matrix into a discrete adjacency matrix and a positive-valued strength matrix, with a common Gumbel-Softmax prior imposed on the adjacency matrices. The authors demonstrate that the regulated adjacency matrices inform us about the underlying anatomical connectome, while the less restricted strength matrices capture functional variations across multiple brain states.\n\nStrengths:\n\n1. The authors provide a clear and concise explanation of the proposed OHG model, making it easy to follow for readers with a background in neuroscience and machine learning.\n2. The paper presents a well-structured and organized structure, with each section clearly outlining the contributions and limitations of the proposed model.\n3. The authors provide a comprehensive comparison of the proposed OHG model with state-of-the-art baseline methods, including GLM, HMM Corr, HMM Bern, HG, and GHG.\n4. The experimental evaluation is extensive, with the authors applying the proposed model to both simulated data and real neural datasets, including the prefrontal cortex (PFC) and barrel cortex datasets.\n5. The paper presents a clear and interpretable representation of the inferred hidden states and neural connectivities, making it easy to visualize and understand the results.\n\nWeaknesses:\n\n1. The paper assumes a specific form for the Gumbel-Softmax prior, which may not be biologically plausible in all cases.\n2. The authors do not provide a comprehensive analysis of the sensitivity of the proposed model to hyperparameters, which may affect the performance of the model.\n3. The paper assumes that the number of hidden states is known, which may not be the case in real-world scenarios.\n4. The authors do not provide a clear explanation of how the proposed model can be scaled to larger datasets.\n5. The paper assumes that the proposed model can be applied to any neural dataset, which may not be the case in all scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more comprehensive analysis of the sensitivity of the proposed model to hyperparameters, including the Gumbel-Softmax temperature and the strength of the shared prior.\n2. The paper should include a more detailed discussion of the assumptions made in the proposed model, including the specific form of the Gumbel-Softmax prior and the assumption that the number of hidden states is known.\n3. The authors should provide a clear explanation of how the proposed model can be scaled to larger datasets, including strategies for reducing the computational complexity of the model.\n4. The paper should include a more comprehensive evaluation of the proposed model, including a comparison with other state-of-the-art methods and a discussion of the limitations of the proposed model.\n5. The authors should provide a clear and concise summary of the contributions and limitations of the proposed model, making it easy for readers to understand the significance of the work.\n\nRecommendation:\n\nThe proposed OHG model is a significant contribution to the field of neural decoding and brain state discovery. The authors provide a clear and concise explanation of the proposed model, and the experimental evaluation is extensive and well-organized. However, the paper assumes a specific form for the Gumbel-Softmax prior, which may not be biologically plausible in all cases. Additionally, the authors do not provide a comprehensive analysis of the sensitivity of the proposed model to hyperparameters, which may affect the performance of the model. Therefore, I recommend that the authors address these limitations in future work, including a more comprehensive analysis of the sensitivity of the proposed model to hyperparameters and a more detailed discussion of the assumptions made in the proposed model.\n\nOverall, the proposed OHG model is a significant contribution to the field of neural decoding and brain state discovery, and the authors should be commended for their innovative work.\n\n$$$",
        "gr_score_with": 0.9246119733924612,
        "gr_score_without": 0.8972431077694235,
        "output_without": {
            "num_tokens_scored": 399,
            "num_green_tokens": 358,
            "green_fraction": 0.8972431077694235,
            "z_score": 4.856328581494316,
            "p_value": 5.979110402624432e-07
        },
        "output_with": {
            "num_tokens_scored": 451,
            "num_green_tokens": 417,
            "green_fraction": 0.9246119733924612,
            "z_score": 6.6161403965174115,
            "p_value": 1.8434911254334276e-11
        }
    },
    {
        "title": "DRM: MASTERING VISUAL REINFORCEMENT LEARN-",
        "abstract": "Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents\u2019 inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio (Sokar et al., 2023) as a metric to measure inactivity in the RL agent\u2019s network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent\u2019s activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM , a method that uses three core mechanisms to guide agents\u2019 exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations. 1 0% 20% 40% 60% 80% 100% Training Progress 0 100 200 300 400 500 600 Ep iso de R ew ar d Deepmind Control Suite (8 Hard Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e MetaWorld (8 Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e Adroit (3 Tasks) DrM DrQ-v2 ALIX TACO Figure 1: Success rate and episode reward as a function of training progress for each of the three domains that we consider (Deepmind Control Suite, MetaWorld, Adroit). All results are averaged over 4 random seeds, and the shaded region stands for standard deviation across different random seeds.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method, DrM, for visual reinforcement learning (RL) that aims to improve sample efficiency and asymptotic performance. The authors identify a major shortcoming in existing visual RL methods, where agents often exhibit sustained inactivity during early training, limiting their ability to explore effectively. They introduce three simple mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio, a metric that measures inactivity in the RL agent's network. The proposed method, DrM, is evaluated across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit, and demonstrates significant improvements in sample efficiency and asymptotic performance compared to existing state-of-the-art model-free algorithms.\n\nStrengths:\n\n1. The paper identifies a crucial shortcoming in existing visual RL methods and proposes a novel solution to address this issue.\n2. The authors introduce a new metric, dormant ratio, which provides valuable insights into agent behavior and can be used to balance exploration and exploitation.\n3. The proposed method, DrM, is evaluated extensively across three benchmark environments and demonstrates significant improvements in sample efficiency and asymptotic performance.\n4. The authors provide a comprehensive analysis of the dormant ratio and its relationship with agent behavior, which is a valuable contribution to the field of visual RL.\n\nWeaknesses:\n\n1. The paper assumes a fixed network capacity, which may not be realistic in many practical scenarios.\n2. The authors do not provide a detailed analysis of the computational efficiency of the proposed method.\n3. The paper focuses primarily on continuous control tasks and does not explore the applicability of the proposed method to discrete action tasks.\n4. The authors do not provide a clear explanation of why the dormant ratio is a better metric for balancing exploration and exploitation compared to other metrics.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational efficiency of the proposed method and explore ways to optimize its performance.\n2. The paper should be extended to include a more comprehensive evaluation of the proposed method on a wider range of tasks and environments.\n3. The authors should provide a clear explanation of why the dormant ratio is a better metric for balancing exploration and exploitation compared to other metrics.\n4. The paper should be revised to include a more detailed discussion of the potential limitations and challenges of the proposed method.\n\nRecommendation:\n\nThe paper proposes a novel method for visual RL that addresses a significant shortcoming in existing methods. The proposed method, DrM, demonstrates significant improvements in sample efficiency and asymptotic performance compared to existing state-of-the-art model-free algorithms. However, the paper assumes a fixed network capacity, does not provide a detailed analysis of computational efficiency, and focuses primarily on continuous control tasks. Despite these limitations, the paper makes a valuable contribution to the field of visual RL and provides a promising direction for future research.\n\nRating: 7/10\n\nRecommendation for Publication: Yes\n\nRecommendation for Future Research: Yes\n\nRecommendation for Further Development: Yes\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper introduces a novel online reinforcement learning algorithm, DrM (Dormant Ratio Minimization), which addresses the challenges of visual continuous control tasks. The authors identify a critical issue in existing visual RL methods: agents often exhibit sustained inactivity during early training, limiting their ability to explore effectively. They propose three simple yet effective mechanisms in DrM to balance exploration and exploitation while lowering the dormant ratio: a periodical neural network weight perturbation mechanism, a dormant-ratio-based exploration scheduler, and a dormant-ratio-based exploitation mechanism. The authors empirically verify that DrM achieves significant improvements in sample efficiency and asymptotic performance across three continuous control benchmark environments: DeepMind Control Suite, MetaWorld, and Adroit. Most notably, DrM is the first documented model-free algorithm that reliably solves complex tasks such as Dog and Manipulator in the DeepMind Control Suite and Adroit dexterous hand manipulation tasks without demonstrations from pixels.\n\nStrengths:\n\n1. The paper provides a clear and concise introduction to the problem of visual continuous control tasks and the challenges faced by existing visual RL methods.\n2. The authors propose a novel and effective approach to addressing the issue of sustained inactivity during early training, which is a critical problem in visual RL.\n3. The three mechanisms introduced in DrM are simple yet effective and can be easily implemented and computed efficiently.\n4. The authors provide extensive empirical evidence that DrM achieves significant improvements in sample efficiency and asymptotic performance across three benchmark environments.\n5. The paper provides a detailed analysis and comparison of the dormant ratio changes during the training process of DrM and three baseline algorithms, which demonstrates the effectiveness of DrM in minimizing the dormant ratio.\n\nWeaknesses:\n\n1. The paper assumes that the reader is familiar with the basics of reinforcement learning and visual RL, which may limit its accessibility to a broader audience.\n2. The authors do not provide a detailed explanation of the mathematical derivations behind the three mechanisms introduced in DrM, which may make it difficult for readers to fully understand the approach.\n3. The paper relies heavily on empirical evidence to demonstrate the effectiveness of DrM, which may not be sufficient to establish its theoretical foundations.\n4. The authors do not provide a clear explanation of how the dormant ratio is calculated and how it is used to guide the exploration and exploitation trade-off in DrM.\n5. The paper does not discuss potential limitations and challenges of the approach, such as the potential for the agent to get stuck in a sub-optimal policy if the dormant ratio is not properly managed.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of the mathematical derivations behind the three mechanisms introduced in DrM to make it easier for readers to understand the approach.\n2. Include a more detailed discussion of the theoretical foundations of DrM and its relationship to existing work in visual RL.\n3. Provide a clearer explanation of how the dormant ratio is calculated and how it is used to guide the exploration and exploitation trade-off in DrM.\n4. Discuss potential limitations and challenges of the approach, such as the potential for the agent to get stuck in a sub-optimal policy if the dormant ratio is not properly managed.\n5. Consider including more experiments and comparisons with other state-of-the-art visual RL algorithms to further establish the effectiveness of DrM.\n\nRecommendation:\n\nThe paper provides a novel and effective approach to addressing the challenges of visual continuous control tasks. While there are some limitations and areas for improvement, the paper is well-written and provides extensive empirical evidence to support its claims. I recommend accepting the paper for publication in a top-tier conference in the field of reinforcement learning and artificial intelligence.",
        "gr_score_with": 0.8135593220338984,
        "gr_score_without": 0.7158176943699732,
        "output_without": {
            "num_tokens_scored": 373,
            "num_green_tokens": 267,
            "green_fraction": 0.7158176943699732,
            "z_score": -4.064227230300033,
            "p_value": 0.9999759040700673
        },
        "output_with": {
            "num_tokens_scored": 472,
            "num_green_tokens": 384,
            "green_fraction": 0.8135593220338984,
            "z_score": 0.7367898617581852,
            "p_value": 0.23062507472399507
        }
    },
    {
        "title": "ING VIA TASK-DRIVEN FEATURE SELECTION",
        "abstract": "This paper presents a data-driven, task-specific paradigm for experimental design, to shorten acquisition time, reduce costs, and accelerate the deployment of imaging devices. Current approaches in experimental design focus on model-parameter estimation and require specification of a particular model, whereas in imaging, other tasks may drive the design. Furthermore, such approaches often lead to intractable optimization problems in real-world imaging applications. Here we present a new paradigm for experimental design that simultaneously optimizes the design (set of image channels) and trains a machine-learning model to execute a userspecified image-analysis task. The approach obtains data densely-sampled over the measurement space (many image channels) for a small number of acquisitions, then identifies a subset of channels of prespecified size that best supports the task. We propose a method: TADRED for TAsk-DRiven Experimental Design in imaging, to identify the most informative channel-subset whilst simultaneously training a network to execute the task given the subset. Experiments demonstrate the potential of TADRED in diverse imaging applications: several clinicallyrelevant tasks in magnetic resonance imaging; and remote sensing and physiological applications of hyperspectral imaging. Results show substantial improvement over classical experimental design, two recent application-specific methods within the new paradigm, and state-of-the-art approaches in supervised feature selection. We anticipate further applications of our approach. Code is available: Code Link.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to experimental design in imaging applications, specifically for multi-channel imaging. The proposed method, TADRED, is a task-driven experimental design paradigm that simultaneously optimizes the design and trains a machine-learning model to execute a user-specified image-analysis task. TADRED uses a dual-network architecture, consisting of a scoring network and a task network, to identify the most informative channel-subset and train a high-performing network for the task. The paper demonstrates the effectiveness of TADRED in various imaging applications, including clinically relevant tasks in magnetic resonance imaging and remote sensing and physiological applications of hyperspectral imaging.\n\nStrengths:\n\n1. The paper presents a novel and innovative approach to experimental design in imaging applications, which is a significant contribution to the field.\n2. The proposed method, TADRED, is well-motivated and has a clear and logical structure.\n3. The paper provides a thorough evaluation of TADRED using various benchmark datasets and tasks, demonstrating its effectiveness in different imaging applications.\n4. The authors provide a comprehensive analysis of the strengths and limitations of TADRED, as well as comparisons with other state-of-the-art methods.\n\nWeaknesses:\n\n1. The paper assumes a priori knowledge of the task, which may not always be the case in real-world applications.\n2. The proposed method requires a large amount of training data, which may not be feasible in all scenarios.\n3. The paper does not provide a detailed explanation of the hyperparameter tuning process, which is crucial for the success of the method.\n4. The comparison with other methods is limited to a few benchmark datasets, and it would be beneficial to evaluate TADRED on a broader range of datasets and tasks.\n\nSuggestions for Improvement:\n\n1. Investigate the possibility of learning the task from the data, rather than relying on a priori knowledge.\n2. Explore alternative methods for hyperparameter tuning, such as Bayesian optimization or evolutionary algorithms.\n3. Provide a more detailed explanation of the hyperparameter tuning process and its impact on the performance of TADRED.\n4. Evaluate TADRED on a broader range of datasets and tasks to demonstrate its robustness and generalizability.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to experimental design in imaging applications, which has the potential to significantly impact the field. However, there are some limitations and areas for improvement that need to be addressed. With further refinement and evaluation, TADRED has the potential to become a widely used and effective method for experimental design in imaging applications.\n\n$$$\nNote: The review is based on the provided paper and does not include any external knowledge or information.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach for experimental design in imaging applications, dubbed TADRED, which stands for Task-Driven Experimental Design. The proposed method leverages a new paradigm that simultaneously optimizes the design of a set of image channels and trains a machine learning model to execute a user-specified image analysis task. TADRED is a two-stage approach that first trains a scoring network to score individual elements of the densely-sampled design, and then trains a task network to perform the task driving the design. The method is evaluated on six datasets and tasks in both clinically-relevant MRI and remote sensing and physiological applications in hyperspectral imaging, demonstrating substantial improvements over classical experimental design, recent application-specific methods, and state-of-the-art approaches in supervised feature selection.\n\nStrengths:\n\n* The paper presents a novel and innovative approach for experimental design in imaging applications, which is a significant contribution to the field.\n* The proposed method is able to outperform classical experimental design, recent application-specific methods, and state-of-the-art approaches in supervised feature selection on a range of datasets and tasks.\n* The paper provides a thorough evaluation of the proposed method, including comparisons with existing approaches and an ablation study to investigate the importance of different components of the method.\n* The code for the proposed method is made available, allowing for reproducibility and further development of the approach.\n\nWeaknesses:\n\n* The paper assumes a priori model specification, which may not be feasible in all imaging applications.\n* The proposed method requires a large amount of training data, which may not be available in all cases.\n* The paper does not provide a detailed analysis of the computational cost of the proposed method, which may be a limitation for large-scale applications.\n* The proposed method is evaluated on a limited range of datasets and tasks, and it is unclear whether the approach will generalize to other imaging applications.\n\nSuggestions for Improvement:\n\n* Consider developing a method that does not require a priori model specification, such as using a more general machine learning approach.\n* Investigate methods for reducing the amount of training data required, such as using transfer learning or semi-supervised learning.\n* Provide a more detailed analysis of the computational cost of the proposed method, including comparisons with existing approaches.\n* Evaluate the proposed method on a broader range of datasets and tasks, including those that are not clinically-relevant MRI or remote sensing and physiological applications in hyperspectral imaging.\n\nRecommendation:\n\nThe proposed method, TADRED, is a novel and innovative approach for experimental design in imaging applications that outperforms classical experimental design, recent application-specific methods, and state-of-the-art approaches in supervised feature selection. However, there are some limitations and areas for improvement that need to be addressed before the method can be widely adopted. Overall, the paper presents a significant contribution to the field of imaging and machine learning, and it is worth considering for publication in a top-tier conference or journal.",
        "gr_score_with": 0.8641025641025641,
        "gr_score_without": 0.8274111675126904,
        "output_without": {
            "num_tokens_scored": 394,
            "num_green_tokens": 326,
            "green_fraction": 0.8274111675126904,
            "z_score": 1.360533965684311,
            "p_value": 0.08683050601201397
        },
        "output_with": {
            "num_tokens_scored": 390,
            "num_green_tokens": 337,
            "green_fraction": 0.8641025641025641,
            "z_score": 3.1650814590925793,
            "p_value": 0.0007751976950576293
        }
    },
    {
        "title": "DYST: TOWARDS DYNAMIC NEURAL SCENE REPRESENTATIONS ON REAL-WORLD VIDEOS",
        "abstract": "Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, called DyST, which admits separate control over the camera and the content of the scene. The model is trained using a synthetic dataset, DySO, and real-world videos, and it learns to disentangle camera pose and scene dynamics from monocular real-world videos. The authors demonstrate the effectiveness of DyST in view synthesis and scene control capabilities on real-world videos of dynamic scenes.\n\nStrengths:\n\n1. The paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, which is a challenging task.\n2. The authors demonstrate the effectiveness of DyST in view synthesis and scene control capabilities on real-world videos of dynamic scenes.\n3. The paper presents a detailed analysis of the model and its learned latent representations for scene dynamics and camera pose.\n4. The authors propose a novel synthetic dataset, DySO, which is used for co-training DyST.\n\nWeaknesses:\n\n1. The paper assumes that the scene dynamics are known, which may not be the case in real-world scenarios.\n2. The authors use a simple set of camera and object motions in the synthetic dataset, which may not be sufficient to capture the complexity of real-world scenes.\n3. The paper does not provide a detailed analysis of the model's performance on more complex types of videos, such as videos with several independent moving objects or changing lighting conditions.\n4. The authors use a L2 loss for training the model, which may not be suitable for all types of scenes.\n\nSuggestions for Improvement:\n\n1. The authors could explore more complex types of videos and analyze the model's performance on these scenarios.\n2. The paper could benefit from a more detailed analysis of the model's performance on videos with changing lighting conditions.\n3. The authors could explore the use of more advanced loss functions, such as a adversarial loss, to improve the model's performance.\n4. The paper could benefit from a more detailed analysis of the model's learned latent representations and their relationship to the scene dynamics and camera pose.\n\nRecommendation:\n\nThe paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, which is a challenging task. The authors demonstrate the effectiveness of DyST in view synthesis and scene control capabilities on real-world videos of dynamic scenes. While there are some limitations to the paper, it is a significant contribution to the field and has the potential to be improved upon. Therefore, I recommend accepting the paper for publication.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, called DyST, which learns to separate the scene into global content and per-view camera pose and scene dynamics. The model is trained on a synthetic dataset, DySO, and a real-world dataset, Something-Something v2, through a novel co-training scheme that leverages sim-to-real transfer. The authors demonstrate the capabilities of DyST in view synthesis and scene control on real-world videos of dynamic scenes.\n\nStrengths:\n\n* The paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, which is an important and challenging task in computer vision.\n* The authors demonstrate the capabilities of DyST in view synthesis and scene control on real-world videos of dynamic scenes, which is a significant achievement.\n* The paper provides a thorough evaluation of the model, including quantitative and qualitative results, as well as an ablation study to investigate the importance of the latent control swap.\n* The authors provide a detailed description of the model architecture, training procedure, and evaluation metrics, making it easy to follow and reproduce the results.\n\nWeaknesses:\n\n* The paper assumes that the camera pose and scene dynamics are separable, which may not always be the case in real-world scenarios.\n* The authors do not provide a thorough analysis of the limitations of the model, such as its ability to handle complex scenes or long camera trajectories.\n* The paper does not provide a comparison with other state-of-the-art models for generative modeling of dynamic 3D visual scenes.\n* The authors do not provide a discussion on the potential applications of DyST, such as in virtual reality, robotics, or autonomous driving.\n\nSuggestions for Improvement:\n\n* Provide a more thorough analysis of the limitations of the model and discuss potential future work to address these limitations.\n* Compare the performance of DyST with other state-of-the-art models for generative modeling of dynamic 3D visual scenes.\n* Discuss the potential applications of DyST and provide examples of how it can be used in real-world scenarios.\n* Provide a more detailed description of the synthetic dataset, DySO, and how it was generated.\n\nRecommendation:\n\nThe paper proposes a novel approach to generative modeling of dynamic 3D visual scenes, which is an important and challenging task in computer vision. The authors demonstrate the capabilities of DyST in view synthesis and scene control on real-world videos of dynamic scenes, which is a significant achievement. However, the paper assumes that the camera pose and scene dynamics are separable, which may not always be the case in real-world scenarios. Therefore, I recommend that the authors provide a more thorough analysis of the limitations of the model and discuss potential future work to address these limitations.\n\nOverall, the paper is well-written and provides a thorough evaluation of the model. However, it requires further work to address the limitations and provide a more comprehensive analysis of the model's capabilities.\n\nRating: 7/10\n\nRecommendation: Accept",
        "gr_score_with": 0.8723404255319149,
        "gr_score_without": 0.8586206896551725,
        "output_without": {
            "num_tokens_scored": 290,
            "num_green_tokens": 249,
            "green_fraction": 0.8586206896551725,
            "z_score": 2.4959222957595104,
            "p_value": 0.006281505911113371
        },
        "output_with": {
            "num_tokens_scored": 329,
            "num_green_tokens": 287,
            "green_fraction": 0.8723404255319149,
            "z_score": 3.2805856653321843,
            "p_value": 0.0005179590036557874
        }
    },
    {
        "title": "LEARNING MULTI-FACETED PROTOTYPICAL USER INTERESTS",
        "abstract": "We seek to uncover the latent interest units from behavioral data to better learn user preferences under the VAE framework. Existing practices tend to ignore the multiple facets of item characteristics, which may not capture it at appropriate granularity. Moreover, current studies equate the granularity of item space to that of user interests, which we postulate is not ideal as user interests would likely map to a small subset of item space. In addition, the compositionality of user interests has received inadequate attention, preventing the modeling of interactions between explanatory factors driving a user\u2019s decision. To resolve this, we propose to align user interests with multi-faceted item characteristics. First, we involve prototype-based representation learning to discover item characteristics along multiple facets. Second, we compose user interests from uncovered item characteristics via binding mechanism, separating the granularity of user preferences from that of item space. Third, we design a dedicated bi-directional binding block, aiding the derivation of compositional user interests. On real-world datasets, the experimental results demonstrate the strong performance of our proposed method compared to a series of baselines.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method called FACETVAE for learning multi-faceted prototypical user interests. The method aims to address the shortcomings of existing VAE-based disentangled recommendation models, including inadequate item space discovering, same level of granularity between user interests and item space assumption, and improper user interest complexity handling. FACETVAE is characterized by three main innovations: disentangling item space under a multi-faceted manner, binding compositional user interests from low-level ones discovered from item space, and effectively binding user interests via a bi-directional binding block. The authors demonstrate the effectiveness of FACETVAE on three real-world datasets, achieving state-of-the-art performance compared to several baselines.\n\nStrengths:\n\n1. The paper addresses a significant problem in recommendation systems, which is learning multi-faceted user interests.\n2. The proposed method, FACETVAE, is well-motivated and has a clear theoretical foundation.\n3. The authors provide a thorough evaluation of their method on three real-world datasets, demonstrating its effectiveness compared to several baselines.\n4. The paper is well-organized and easy to follow, with clear explanations of the methodology and experimental results.\n\nWeaknesses:\n\n1. The paper assumes that the number of facets and the number of clusters per facet are known, which may not be the case in practice.\n2. The authors do not provide a clear explanation of how to determine the number of facets and the number of clusters per facet in a data-driven manner.\n3. The paper assumes that the user interests are represented as a set of K vectors, which may not be the case in practice.\n4. The authors do not provide a clear explanation of how to handle the case where the user interests are not well-represented by a set of K vectors.\n\nSuggestions for Improvement:\n\n1. Provide a clear explanation of how to determine the number of facets and the number of clusters per facet in a data-driven manner.\n2. Discuss the limitations of the paper, including the assumption that the number of facets and the number of clusters per facet are known.\n3. Provide a clear explanation of how to handle the case where the user interests are not well-represented by a set of K vectors.\n4. Consider adding more experimental results to demonstrate the effectiveness of FACETVAE in different scenarios.\n\nRecommendation:\n\nThe paper is well-written and provides a thorough evaluation of the proposed method, FACETVAE. However, the authors should address the limitations of the paper and provide a clear explanation of how to handle the case where the user interests are not well-represented by a set of K vectors. With these improvements, the paper has the potential to make a significant contribution to the field of recommendation systems.\n\n$$$\nNote: The review is based on the provided paper and is intended to provide constructive feedback to the authors.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel method called FACETVAE for learning multi-faceted prototypical user interests. The method aims to resolve the shortcomings of existing VAE-based disentangled recommendation models by disentangling item space under a multi-faceted manner, binding compositional user interests from low-level ones discovered from item space, and effectively binding user interests via a bi-directional binding block. The experimental results demonstrate the strong performance of FACETVAE compared to a series of baselines on three real-world datasets.\n\nStrengths:\n\n1. The paper proposes a novel method that addresses the shortcomings of existing VAE-based disentangled recommendation models.\n2. The method disentangles item space under a multi-faceted manner, which is a unique innovation in the field of recommendation systems.\n3. The bi-directional binding block is a novel component that effectively binds user interests from low-level ones discovered from item space.\n4. The experimental results demonstrate the strong performance of FACETVAE compared to a series of baselines.\n\nWeaknesses:\n\n1. The paper assumes that the number of facets and the number of clusters per facet are known in advance, which may not always be the case in real-world scenarios.\n2. The method requires a large number of hyperparameters to be tuned, which can be time-consuming and may not always lead to the optimal results.\n3. The paper does not provide a clear explanation of how the bi-directional binding block works and how it affects the performance of the model.\n4. The experimental results are limited to three real-world datasets, and it would be beneficial to conduct more experiments on other datasets to demonstrate the robustness of the method.\n\nSuggestions for Improvement:\n\n1. Investigate methods for automatically discovering the number of facets and the number of clusters per facet in a data-driven manner.\n2. Provide a clear explanation of how the bi-directional binding block works and how it affects the performance of the model.\n3. Conduct more experiments on other datasets to demonstrate the robustness of the method.\n4. Investigate methods for reducing the number of hyperparameters that need to be tuned.\n\nRecommendation:\n\nThe paper proposes a novel method that addresses the shortcomings of existing VAE-based disentangled recommendation models. The experimental results demonstrate the strong performance of FACETVAE compared to a series of baselines. However, there are some limitations and suggestions for improvement that need to be addressed. Overall, the paper is well-written and contributes to the field of recommendation systems. I recommend it for publication in a reputable conference or journal.\n\n$$$",
        "gr_score_with": 0.8443708609271523,
        "gr_score_without": 0.7896253602305475,
        "output_without": {
            "num_tokens_scored": 347,
            "num_green_tokens": 274,
            "green_fraction": 0.7896253602305475,
            "z_score": -0.4828491868434438,
            "p_value": 0.6853985883532724
        },
        "output_with": {
            "num_tokens_scored": 302,
            "num_green_tokens": 255,
            "green_fraction": 0.8443708609271523,
            "z_score": 1.927956824907589,
            "p_value": 0.026930248340548354
        }
    },
    {
        "title": "WIN-WIN: TRAINING HIGH-RESOLUTION VISION TRANSFORMERS",
        "abstract": "Transformers have become the standard in state-of-the-art vision architectures, achieving impressive performance on both image-level and dense pixelwise tasks. However, training vision transformers for high-resolution pixelwise tasks has a prohibitive cost. Typical solutions boil down to hierarchical architectures, fast and approximate attention, or training on low-resolution crops. This latter solution does not constrain architectural choices, but it leads to a clear performance drop when testing at resolutions significantly higher than that used for training, thus requiring ad-hoc and slow post-processing schemes. In this paper, we propose a novel strategy for efficient training and inference of high-resolution vision transformers: the key principle is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. We show that this strategy is effective when using relative positional embedding such as rotary embeddings. It is 4 times faster to train than a full-resolution network, and it is straightforward to use at test time compared to existing approaches. We apply this strategy to two dense prediction tasks with high resolution data. First, we show on the task of semantic segmentation that a simple setting with 2 windows performs best, hence the name of our method: Win-Win. To demonstrate the generality of our contribution, we further extend it to the binocular task of optical flow, reaching state-of-the-art performance on the Spring benchmark that contains Full-HD images with an inference time an order of magnitude faster than the best competitor. 0 10 20 30 40 50 60 Training time (hours) 45 50 55 60 m ea n Io U Semantic segmentation on BDD100k Win-Win (31GB) Full-res. (58GB) Full-res ViTDet (32GB) 0 25 50 75 100 125 150 175 Training time (hours) 1.6 2.0 2.4 2.8 3.2 En dPo in t E rro r ( EP E) Optical flow estimation on MPI-Sintel-clean Win-Win (14GB) Full-res. (28GB) Full-res ViT-Det (21GB) 80 100 120 140 160 180 Training time (hours) 0.0 0.5 1.0 1.5 In fe re nc e tim e (s ec on ds ) Optical flow estimation on MPI-Sintel-clean Full-res ViT Full-res ViT-Det Win-Win ViT + Tiling Figure 1: Validation performance vs. training time on semantic segmentation (left) and optical flow (middle). We compare our two-window training (Win-Win) to a standard full-resolution training as well as a sparsification of the attention following ViT-Det (Li et al., 2022a). We indicate the memory usage in parenthesis in the legend. Compared to full-resolution training, Win-Win allows to reduce the training time by a factor 3-4 and to half the memory usage while reaching a similar performance. Training and inference times on optical flow, for Win-Win vs. other strategies (right). ViT+Tiling corresponds to a setup similar to CroCo-Flow (Weinzaepfel et al., 2023) where the model is trained on random crops, but requires a tiling strategy at inference. While Win-Win is as fast to train as the latter, it can directly process full-resolution inputs at test time.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel strategy for efficient training and inference of high-resolution vision transformers, called Win-Win. The key principle of Win-Win is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. The authors show that this strategy is effective when using relative positional embedding such as rotary embeddings, and is 4 times faster to train than a full-resolution network.\n\nStrengths:\n\n* The paper proposes a novel and efficient strategy for training high-resolution vision transformers.\n* The Win-Win approach allows the model to learn local and global interactions, which is key for dense prediction tasks.\n* The paper shows that the Win-Win approach can achieve state-of-the-art performance on two dense prediction tasks: semantic segmentation and optical flow estimation.\n* The authors provide a thorough experimental evaluation of the Win-Win approach, including comparisons to other baselines and ablation studies.\n\nWeaknesses:\n\n* The paper assumes that the input images are already preprocessed, which may not be the case in real-world scenarios.\n* The authors do not provide a detailed analysis of the computational cost of the Win-Win approach, which may be a concern for large-scale applications.\n* The paper does not provide a clear explanation of why the Win-Win approach works, which may make it difficult for readers to understand and replicate the results.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the computational cost of the Win-Win approach, including a discussion of the trade-offs between training speed and inference speed.\n* Provide a more thorough evaluation of the Win-Win approach on other tasks and datasets, to demonstrate its generality and robustness.\n* Provide a more detailed explanation of why the Win-Win approach works, including a discussion of the theoretical foundations and the empirical evidence.\n\nRecommendation:\n\nThe paper proposes a novel and efficient strategy for training high-resolution vision transformers, and provides a thorough experimental evaluation of the approach. While there are some limitations and weaknesses, the paper is well-written and well-organized, and the results are impressive. I recommend this paper for publication in a top-tier conference.\n\nGrade: B+\n\nReviewer's Comments:\n\nThe paper is well-written and well-organized, and the results are impressive. However, there are some limitations and weaknesses that need to be addressed. The paper assumes that the input images are already preprocessed, which may not be the case in real-world scenarios. The authors do not provide a detailed analysis of the computational cost of the Win-Win approach, which may be a concern for large-scale applications. The paper does not provide a clear explanation of why the Win-Win approach works, which may make it difficult for readers to understand and replicate the results.\n\nOverall, I believe that the paper has the potential to make a significant contribution to the field, and I recommend it for publication in a top-tier conference. However, the authors need to address the limitations and weaknesses mentioned above to make the paper more robust and convincing.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel strategy, called Win-Win, for efficient training and inference of high-resolution vision transformers. The key principle of Win-Win is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. The authors demonstrate the effectiveness of Win-Win on two dense prediction tasks: semantic segmentation and optical flow estimation.\n\nStrengths:\n1. The paper proposes a novel and effective approach for efficient training and inference of high-resolution vision transformers.\n2. The Win-Win strategy is easy to implement and can be used with any vision transformer architecture.\n3. The paper provides a thorough experimental evaluation of Win-Win on two dense prediction tasks, including semantic segmentation and optical flow estimation.\n4. The results show that Win-Win achieves state-of-the-art performance on the Spring benchmark and outperforms other methods on the MPI-Sintel benchmark.\n\nWeaknesses:\n1. The paper assumes that the input image is divided into non-overlapping windows, which may not be the case in practice.\n2. The Win-Win strategy may not work well for tasks that require large receptive fields or global interactions.\n3. The paper does not provide a thorough analysis of the computational cost of Win-Win compared to other methods.\n4. The paper assumes that the training data is available at full resolution, which may not be the case in practice.\n\nSuggestions for Improvement:\n1. Investigate the effect of overlapping windows on the performance of Win-Win.\n2. Analyze the computational cost of Win-Win compared to other methods and provide a detailed comparison.\n3. Evaluate the performance of Win-Win on tasks that require large receptive fields or global interactions.\n4. Investigate the effect of different window sampling strategies on the performance of Win-Win.\n\nRecommendation:\nThe paper proposes a novel and effective approach for efficient training and inference of high-resolution vision transformers. The Win-Win strategy is easy to implement and can be used with any vision transformer architecture. The paper provides a thorough experimental evaluation of Win-Win on two dense prediction tasks, including semantic segmentation and optical flow estimation. However, the paper assumes that the input image is divided into non-overlapping windows, which may not be the case in practice. Therefore, I recommend that the authors investigate the effect of overlapping windows on the performance of Win-Win and provide a detailed comparison of the computational cost of Win-Win compared to other methods.\n\nOverall, I believe that the paper makes a significant contribution to the field of computer vision and deserves to be published in a top conference.\n\nRecommendation: Accept.",
        "gr_score_with": 0.8789808917197452,
        "gr_score_without": 0.837696335078534,
        "output_without": {
            "num_tokens_scored": 382,
            "num_green_tokens": 320,
            "green_fraction": 0.837696335078534,
            "z_score": 1.8422034697269045,
            "p_value": 0.03272269655539201
        },
        "output_with": {
            "num_tokens_scored": 314,
            "num_green_tokens": 276,
            "green_fraction": 0.8789808917197452,
            "z_score": 3.4990978180131065,
            "p_value": 0.00023341764187875927
        }
    },
    {
        "title": "FAKE IT TILL MAKE IT: FEDERATED LEARNING WITH CONSENSUS-ORIENTED GENERATION",
        "abstract": "In federated learning (FL), data heterogeneity is one key bottleneck that causes model divergence and limits performance. Addressing this, existing methods often regard data heterogeneity as an inherent property and propose to mitigate its adverse effects by correcting models. In this paper, we seek to break this inherent property by generating data to complement the original dataset to fundamentally mitigate heterogeneity level. As a novel attempt from the perspective of data, we propose federated learning with consensus-oriented generation (FedCOG). FedCOG consists of two key components at the client side: complementary data generation, which generates data extracted from the shared global model to complement the original dataset, and knowledge-distillation-based model training, which distills knowledge from global model to local model based on the generated data to mitigate over-fitting the original heterogeneous dataset. FedCOG has two critical advantages: 1) it can be a plug-and-play module to further improve the performance of most existing FL methods, and 2) it is naturally compatible with standard FL protocols such as Secure Aggregation since it makes no modification in communication process. Extensive experiments on classical and real-world FL datasets show that FedCOG consistently outperforms state-of-the-art methods .",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel federated learning algorithm, called FedCOG, to tackle the issue of data heterogeneity in FL. FedCOG consists of two key components: complementary data generation and knowledge-distillation-based model training. The complementary data generation module generates data from the shared global model to complement the original dataset, reducing the heterogeneity level. The knowledge-distillation-based model training module distills knowledge from the global model to the local model based on the generated data, mitigating the effects of heterogeneity. FedCOG is plug-and-play, compatible with standard FL protocols, and does not compromise on communication cost and privacy. The authors conduct extensive experiments on classical and real-world FL datasets, showing that FedCOG consistently outperforms state-of-the-art methods.\n\nStrengths:\n\n1. The paper addresses a critical issue in FL, data heterogeneity, from a novel perspective of modifying local dataset.\n2. FedCOG is a well-designed algorithm that consists of two key components, complementary data generation and knowledge-distillation-based model training, which are both orthogonal to most existing FL methods.\n3. The authors provide extensive experimental results on classical and real-world FL datasets, demonstrating the effectiveness of FedCOG.\n4. FedCOG is plug-and-play, compatible with standard FL protocols, and does not compromise on communication cost and privacy.\n\nWeaknesses:\n\n1. The paper does not provide a thorough analysis of the theoretical guarantees of FedCOG.\n2. The authors do not provide a detailed comparison with other state-of-the-art methods in terms of computational cost and scalability.\n3. The paper assumes a centralized setting, where the global model is shared among all clients, which may not be realistic in many FL scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the theoretical guarantees of FedCOG, including convergence rates and optimality guarantees.\n2. The authors should conduct a more thorough comparison with other state-of-the-art methods in terms of computational cost and scalability.\n3. The authors should consider a decentralized setting, where the global model is not shared among all clients, and evaluate the performance of FedCOG in such a setting.\n\nRecommendation:\n\nThe paper is well-written, and the proposed algorithm, FedCOG, is innovative and effective. However, the paper lacks a thorough theoretical analysis and a detailed comparison with other state-of-the-art methods. With some improvements, the paper can be considered for publication in a top-tier conference.",
        "peer_review_with_watermark": "$$\nPaper Summary:\n\nThe paper proposes a novel federated learning algorithm, called FedCOG, to tackle the issue of data heterogeneity in FL from the perspective of modifying local dataset. FedCOG consists of two key components: complementary data generation to reduce heterogeneity level and knowledge-distillation-based model training to mitigate the effects of heterogeneity. The algorithm generates data to complement the original dataset for each client, which is achieved by inversely optimizing inputs given the global and local model. The generated dataset is then utilized for the process of local model training, with the optimization objective defined as minimizing the task-driven loss on the original dataset and knowledge distillation loss on the generated dataset. The paper conducts extensive experiments on classical and real-world FL datasets, showing that FedCOG consistently outperforms state-of-the-art methods.\n\nStrengths:\n\n* The paper proposes a novel approach to tackle data heterogeneity in FL from the perspective of modifying local dataset.\n* The algorithm FedCOG is plug-and-play in most existing FL methods and is compatible with standard FL protocol such as Secure Aggregation.\n* The paper conducts extensive experiments on classical and real-world FL datasets, showing that FedCOG consistently outperforms state-of-the-art methods.\n* The paper provides a thorough analysis of the algorithm, including the effects of disagreement loss and knowledge distillation loss on the performance of FedCOG.\n\nWeaknesses:\n\n* The paper does not provide a thorough comparison with existing methods that address data heterogeneity in FL, such as SCAFFOLD and FedDyn.\n* The paper does not provide a detailed analysis of the computational cost of FedCOG, which is an important aspect of FL.\n* The paper assumes that the global model is known to all clients, which may not be the case in practice.\n* The paper does not provide a detailed analysis of the privacy implications of FedCOG.\n\nSuggestions for Improvement:\n\n* Provide a thorough comparison with existing methods that address data heterogeneity in FL.\n* Provide a detailed analysis of the computational cost of FedCOG.\n* Consider the scenario where the global model is not known to all clients.\n* Provide a detailed analysis of the privacy implications of FedCOG.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to tackle data heterogeneity in FL from the perspective of modifying local dataset. The algorithm FedCOG is plug-and-play in most existing FL methods and is compatible with standard FL protocol such as Secure Aggregation. The paper conducts extensive experiments on classical and real-world FL datasets, showing that FedCOG consistently outperforms state-of-the-art methods. However, the paper has some limitations, such as not providing a thorough comparison with existing methods and not providing a detailed analysis of the computational cost and privacy implications of FedCOG. Therefore, I recommend this paper for publication after addressing these limitations.\n\n$$",
        "gr_score_with": 0.8366013071895425,
        "gr_score_without": 0.7446153846153846,
        "output_without": {
            "num_tokens_scored": 325,
            "num_green_tokens": 242,
            "green_fraction": 0.7446153846153846,
            "z_score": -2.4958405722849752,
            "p_value": 0.9937170468020037
        },
        "output_with": {
            "num_tokens_scored": 306,
            "num_green_tokens": 256,
            "green_fraction": 0.8366013071895425,
            "z_score": 1.600907518755053,
            "p_value": 0.05469870202286078
        }
    },
    {
        "title": "GIM: LEARNING GENERALIZABLE IMAGE MATCHER",
        "abstract": "Image matching is a fundamental computer vision problem. While learning-based methods achieve state-of-the-art performance on existing benchmarks, they generalize poorly to in-the-wild images. Such methods typically need to train separate models for different scene types (e.g., indoor vs. outdoor) and are impractical when the scene type is unknown in advance. One of the underlying problems is the limited scalability of existing data construction pipelines, which limits the diversity of standard image matching datasets. To address this problem, we propose GIM, a self-training framework for learning a single generalizable model based on any image matching architecture using internet videos, an abundant and diverse data source. Given an architecture, GIM first trains it on standard domain-specific datasets and then combines it with complementary matching methods to create dense labels on nearby frames of novel videos. These labels are filtered by robust fitting, and then enhanced by propagating them to distant frames. The final model is trained on propagated data with strong augmentations. Not relying on complex 3D reconstruction makes GIM much more efficient and less likely to fail than standard SfM-and-MVS based frameworks. We also propose ZEB, the first zero-shot evaluation benchmark for image matching. By mixing data from diverse domains, ZEB can thoroughly assess the cross-domain generalization performance of different methods. Experiments demonstrate the effectiveness and generality of GIM. Applying GIM consistently improves the zero-shot performance of 3 state-of-theart image matching architectures as the number of downloaded videos increases (Fig. 1 (a)); with 50 hours of YouTube videos, the relative zero-shot performance improves by 8.4% \u2212 18.1%. GIM also enables generalization to extreme crossdomain data such as Bird Eye View (BEV) images of projected 3D point clouds (Fig. 1 (c)). More importantly, our single zero-shot model consistently outperforms domain-specific baselines when evaluated on downstream tasks inherent to their respective domains. The code will be released upon acceptance.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel framework, GIM, for learning generalizable image matching models using self-training on diverse internet videos. GIM first trains a model on standard domain-specific datasets and then uses the trained model to generate dense labels on nearby frames of downloaded videos. The final model is trained on propagated data with strong augmentations. The authors also introduce a new zero-shot evaluation benchmark, ZEB, consisting of data from 8 real-world and 4 simulated domains. Experiments demonstrate the effectiveness and generality of GIM for both image matching and various downstream tasks.\n\nStrengths:\n1. The paper proposes a novel and effective approach for learning generalizable image matching models using self-training on diverse internet videos.\n2. The authors introduce a new zero-shot evaluation benchmark, ZEB, which allows for thorough evaluation of an image matching model in in-the-wild environments.\n3. The paper demonstrates the effectiveness of GIM on various state-of-the-art architectures and downstream tasks.\n4. The authors provide a detailed analysis of the GIM framework and its components, including the self-training process, label propagation, and strong data augmentation.\n\nWeaknesses:\n1. The paper assumes that internet videos are a suitable source of diverse and large-scale training data, which may not be the case for all scenarios.\n2. The authors do not provide a thorough analysis of the limitations and potential failures of the GIM framework.\n3. The paper does not discuss the potential issues with data quality and consistency across different internet videos.\n4. The authors do not provide a clear comparison of the computational efficiency of GIM with other existing methods.\n\nSuggestions for Improvement:\n1. The authors should provide a more thorough analysis of the limitations and potential failures of the GIM framework.\n2. The paper should discuss the potential issues with data quality and consistency across different internet videos.\n3. The authors should provide a clear comparison of the computational efficiency of GIM with other existing methods.\n4. The paper should provide more details on the implementation of the GIM framework, including the specific architecture and hyperparameters used.\n\nRecommendation:\nThe paper proposes a novel and effective approach for learning generalizable image matching models using self-training on diverse internet videos. The authors introduce a new zero-shot evaluation benchmark, ZEB, which allows for thorough evaluation of an image matching model in in-the-wild environments. The paper demonstrates the effectiveness of GIM on various state-of-the-art architectures and downstream tasks. However, the paper assumes that internet videos are a suitable source of diverse and large-scale training data, which may not be the case for all scenarios. The authors should provide a more thorough analysis of the limitations and potential failures of the GIM framework. Despite these limitations, the paper is well-written and provides a clear contribution to the field of image matching.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes GIM, a self-training framework that learns generalizable image matching models using abundant and diverse internet videos. GIM first trains a base model on standard domain-specific datasets and then uses it to generate dense labels on nearby frames of downloaded videos through multi-method matching and label propagation. Strong data augmentations are applied to enhance the supervision signals. GIM also proposes ZEB, a novel zero-shot evaluation benchmark for image matching with diverse in-the-wild data, to thoroughly evaluate the generalization performance of different methods.\n\nStrengths:\n1. GIM proposes an innovative approach to learn generalizable image matching models using self-training and multi-method matching, which can efficiently generate reliable supervision signals on diverse internet videos.\n2. The paper presents a thorough evaluation of GIM's performance on both zero-shot and in-domain tasks, demonstrating its effectiveness and generality.\n3. ZEB, the proposed zero-shot evaluation benchmark, is a valuable contribution to the field of image matching, allowing for thorough evaluation of image matching models in in-the-wild environments.\n\nWeaknesses:\n1. The paper relies heavily on self-training, which may not always be feasible or desirable in certain applications.\n2. The proposed ZEB benchmark is limited to 12 real-world and 4 simulated datasets, which may not capture all possible in-the-wild scenarios.\n3. The paper does not provide a clear explanation of why strong data augmentations are necessary for GIM to work effectively.\n\nSuggestions for Improvement:\n1. Provide more details on why self-training is necessary for GIM and how it can be adapted to different applications.\n2. Consider expanding ZEB to include more real-world and simulated datasets to better capture in-the-wild scenarios.\n3. Provide more insight into why strong data augmentations are necessary for GIM to work effectively.\n\nRecommendation:\nThis paper presents an innovative approach to learning generalizable image matching models using self-training and multi-method matching. The proposed ZEB benchmark is a valuable contribution to the field of image matching, allowing for thorough evaluation of image matching models in in-the-wild environments. However, there are some limitations and areas for improvement that need to be addressed. With some modifications and further research, GIM and ZEB have the potential to become powerful tools for image matching and computer vision tasks.\n\nRating: 7.5/10\n$$$",
        "gr_score_with": 0.8096676737160121,
        "gr_score_without": 0.7175324675324676,
        "output_without": {
            "num_tokens_scored": 308,
            "num_green_tokens": 221,
            "green_fraction": 0.7175324675324676,
            "z_score": -3.6179323199659623,
            "p_value": 0.9998515169882269
        },
        "output_with": {
            "num_tokens_scored": 331,
            "num_green_tokens": 268,
            "green_fraction": 0.8096676737160121,
            "z_score": 0.4399983266387732,
            "p_value": 0.3299691596432879
        }
    },
    {
        "title": "VERA: VECTOR-BASED RANDOM MATRIX ADAPTATION",
        "abstract": "Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous peruser or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA)1, which significantly reduces the number of trainable parameters compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, image classification tasks, and show its application in instruction-tuning of 7B and 13B language models.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents Vector-based Random Matrix Adaptation (VeRA), a novel parameter-efficient finetuning method for large language models. VeRA reduces the number of trainable parameters compared to the state-of-the-art LoRA method, while maintaining comparable results. The method employs a single pair of frozen random matrices shared across all layers and learns small scaling vectors instead. VeRA achieves significant memory savings, making it suitable for scenarios that require frequent swapping of numerous finetuned models, such as cloud-based AI services personalized for individual users.\n\nStrengths:\n\n1. VeRA significantly reduces the number of trainable parameters compared to LoRA, making it more memory-efficient.\n2. The method achieves comparable results to LoRA on various benchmarks, including GLUE, E2E, and image classification tasks.\n3. VeRA is well-suited for scenarios that require frequent swapping of numerous finetuned models, such as cloud-based AI services personalized for individual users.\n4. The paper provides a thorough analysis of the method's components, including the impact of different initializations and the magnitude of adaptation.\n\nWeaknesses:\n\n1. The paper assumes that the frozen random matrices can be regenerated from a random number generator (RNG) seed, which may not be feasible in all scenarios.\n2. The method's performance may benefit from additional refinements, such as dynamic parameter budget allocation, or different initialization and regularization techniques.\n3. The paper focuses on language and vision models with Transformer architecture, and its applicability to other architectures and domains remains an area for future research.\n\nSuggestions for Improvement:\n\n1. Investigate the feasibility of regenerating frozen random matrices from an RNG seed in various scenarios.\n2. Explore additional refinements, such as dynamic parameter budget allocation, or different initialization and regularization techniques, to improve the method's performance.\n3. Conduct experiments on other architectures and domains to evaluate VeRA's applicability and performance.\n\nRecommendation:\n\nVeRA is a promising parameter-efficient finetuning method that achieves comparable results to LoRA while reducing the number of trainable parameters. The method's memory savings make it suitable for scenarios that require frequent swapping of numerous finetuned models. However, further research is needed to explore the method's limitations and potential areas for improvement. I recommend accepting this paper for publication in a reputable conference, with the suggestion that the authors address the limitations and areas for improvement mentioned above.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents Vector-based Random Matrix Adaptation (VeRA), a novel parameter-efficient finetuning method for large language models. VeRA reduces the number of trainable parameters compared to the state-of-the-art LoRA method while maintaining comparable results. The authors demonstrate the effectiveness of VeRA on various downstream tasks, including natural language understanding and generation, instruction-tuning, and image classification.\n\nStrengths:\n\n1. VeRA achieves a significant reduction in trainable parameters compared to LoRA, making it more efficient for large language models.\n2. The authors demonstrate the effectiveness of VeRA on various downstream tasks, including GLUE, E2E, instruction-tuning, and image classification.\n3. The method is particularly well-suited for scenarios that require frequent swapping of numerous finetuned models, such as cloud-based AI services personalized for individual users.\n4. The authors provide a thorough analysis of the method's performance and trade-offs.\n\nWeaknesses:\n\n1. The paper focuses primarily on language models and Transformers architecture, and it is not clear how VeRA would perform on other architectures or domains.\n2. The authors do not provide a detailed analysis of the computational costs and memory requirements of VeRA compared to LoRA.\n3. The paper relies heavily on LoRA as a baseline, and it would be beneficial to compare VeRA with other parameter-efficient finetuning methods.\n4. The authors do not provide a clear explanation of why the d scaling vector is more expressive than the b vector.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational costs and memory requirements of VeRA compared to LoRA.\n2. The authors should compare VeRA with other parameter-efficient finetuning methods to demonstrate its competitiveness.\n3. The authors should provide a clearer explanation of why the d scaling vector is more expressive than the b vector.\n4. The authors should investigate the performance of VeRA on other architectures and domains.\n\nRecommendation:\n\nOverall, the paper presents a novel and efficient finetuning method for large language models. The authors demonstrate the effectiveness of VeRA on various downstream tasks and provide a thorough analysis of the method's performance and trade-offs. However, the paper has some limitations and areas for improvement. With further investigation and refinement, VeRA has the potential to become a widely adopted finetuning method for large language models.\n\nRecommendation: Accept.\n\n$$$",
        "gr_score_with": 0.8327759197324415,
        "gr_score_without": 0.7982954545454546,
        "output_without": {
            "num_tokens_scored": 352,
            "num_green_tokens": 281,
            "green_fraction": 0.7982954545454546,
            "z_score": -0.07965677118064404,
            "p_value": 0.5317448791256855
        },
        "output_with": {
            "num_tokens_scored": 299,
            "num_green_tokens": 249,
            "green_fraction": 0.8327759197324415,
            "z_score": 1.4171246520273824,
            "p_value": 0.07822324310189008
        }
    },
    {
        "title": "FANTASTIC GENERALIZATION MEASURES",
        "abstract": "We study the notion of a generalization bound being uniformly tight, meaning that the difference between the bound and the population loss is small for all learning algorithms and all population distributions. Numerous generalization bounds have been proposed in the literature as potential explanations for the ability of neural networks to generalize in the overparameterized setting. However, in their paper \u201cFantastic Generalization Measures and Where to Find Them,\u201d Jiang et al. (2020) examine more than a dozen generalization bounds, and show empirically that none of them are uniformly tight. This raises the question of whether uniformly-tight generalization bounds are at all possible in the overparameterized setting. We consider two types of generalization bounds: (1) bounds that may depend on the training set and the learned hypothesis (e.g., margin bounds). We prove mathematically that no such bound can be uniformly tight in the overparameterized setting; (2) bounds that may in addition also depend on the learning algorithm (e.g., stability bounds). For these bounds, we show a trade-off between the algorithm\u2019s performance and the bound\u2019s tightness. Namely, if the algorithm achieves good accuracy on certain distributions, then no generalization bound can be uniformly tight for it in the overparameterized setting. We explain how these formal results can, in our view, inform research on generalization bounds for neural networks, while stressing that other interpretations of these results are also possible.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper studies the notion of a generalization bound being uniformly tight, meaning that the difference between the bound and the population loss is small for all learning algorithms and all population distributions. The authors examine two types of generalization bounds: (1) bounds that may depend on the training set and the learned hypothesis, and (2) bounds that may also depend on the learning algorithm. They prove mathematically that no generalization bound of the first type can be uniformly tight in the overparameterized setting, and establish a trade-off between learning performance and estimability for the second type of bounds.\n\nStrengths:\n\n* The paper provides a rigorous and formal analysis of the notion of uniformly tight generalization bounds.\n* The authors introduce a framework for understanding the limitations of generalization bounds in the overparameterized setting.\n* The paper presents several theorems and lemmas that provide a deeper understanding of the relationships between learning performance, estimability, and generalization bounds.\n\nWeaknesses:\n\n* The paper assumes a high level of mathematical background, which may make it challenging for readers without a strong background in machine learning and theoretical computer science.\n* The authors do not provide a comprehensive review of existing generalization bounds and their limitations, which may make it difficult for readers to understand the significance of their results.\n* The paper's conclusions and implications may be subject to interpretation, and the authors acknowledge that different scholarly positions exist on this matter.\n\nSuggestions for Improvement:\n\n* The authors could provide a more accessible introduction to the mathematical framework and notation used in the paper.\n* The paper could benefit from a more comprehensive review of existing generalization bounds and their limitations, which would provide context for the authors' results.\n* The authors could provide more concrete examples and illustrations to demonstrate the implications of their results and make the paper more engaging for readers.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the field of machine learning and theoretical computer science by shedding light on the limitations of generalization bounds in the overparameterized setting. The authors' formal analysis and rigorous proofs provide a solid foundation for understanding the relationships between learning performance, estimability, and generalization bounds. However, the paper's assumptions and conclusions may be subject to interpretation, and the authors could benefit from providing a more comprehensive review of existing generalization bounds and their limitations. Overall, the paper is well-written and well-structured, and it is a valuable contribution to the field.\n\nRecommendation for Publication: Yes\n\nRecommendation for Future Research:\n\n* Investigate the implications of the authors' results for existing generalization bounds and their limitations.\n* Explore the relationships between learning performance, estimability, and generalization bounds in more detail.\n* Develop new generalization bounds that take into account the limitations and trade-offs identified by the authors.\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper studies the concept of generalization bounds in the context of neural networks, specifically in the overparameterized setting. The authors investigate whether it is possible to obtain generalization bounds that are uniformly tight, meaning that they are tight for all learning algorithms and population distributions. The paper provides two main results: (1) a negative result showing that algorithm-independent generalization bounds cannot be uniformly tight in the overparameterized setting, and (2) a trade-off between learnability and estimability, which implies that algorithm-dependent generalization bounds are also limited. The authors discuss the implications of these results and suggest two options for obtaining uniformly tight generalization bounds: (1) specifying a set of 'nice' or 'natural' population distributions, or (2) making explicit assumptions about the learning algorithm.\n\nStrengths:\n\n1. The paper provides a clear and concise introduction to the problem of generalization bounds in the context of neural networks.\n2. The authors provide a rigorous mathematical analysis of the problem, using tools from learning theory and statistics.\n3. The paper presents two main results, which are well-motivated and have significant implications for the field of neural networks.\n4. The authors discuss the implications of their results and suggest two options for obtaining uniformly tight generalization bounds.\n\nWeaknesses:\n\n1. The paper assumes a high level of background knowledge in learning theory and statistics, which may make it difficult for non-experts to follow.\n2. The paper focuses primarily on the mathematical analysis of the problem, and may not provide sufficient empirical evidence to support the claims made.\n3. The authors' suggestion for obtaining uniformly tight generalization bounds (specifying a set of 'nice' or 'natural' population distributions or making explicit assumptions about the learning algorithm) may be seen as restrictive or limiting.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed introduction to the problem of generalization bounds, including a clear explanation of the concepts and terminology used.\n2. Consider providing more empirical evidence to support the claims made, such as experimental results or case studies.\n3. Provide more detail on the assumptions underlying the authors' suggestion for obtaining uniformly tight generalization bounds, and discuss potential limitations or drawbacks of these assumptions.\n\nRecommendation:\n\nThe paper provides a significant contribution to the field of neural networks, and the results presented are well-motivated and have significant implications. However, the paper assumes a high level of background knowledge and focuses primarily on the mathematical analysis of the problem. To make the paper more accessible and impactful, the authors should consider providing more empirical evidence and detail on the assumptions underlying their suggestion for obtaining uniformly tight generalization bounds. With these improvements, the paper has the potential to make a significant contribution to the field of neural networks and promote further research in this area.",
        "gr_score_with": 0.8579545454545454,
        "gr_score_without": 0.8093922651933702,
        "output_without": {
            "num_tokens_scored": 362,
            "num_green_tokens": 293,
            "green_fraction": 0.8093922651933702,
            "z_score": 0.44704154583174127,
            "p_value": 0.3274225315267928
        },
        "output_with": {
            "num_tokens_scored": 352,
            "num_green_tokens": 302,
            "green_fraction": 0.8579545454545454,
            "z_score": 2.718569905251996,
            "p_value": 0.0032782399878469952
        }
    },
    {
        "title": "STEVE-EYE: EQUIPPING LLM-BASED EMBOD-",
        "abstract": "Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to \u201ca blindfolded text-based game.\u201d Consequently, LLMbased agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model to address this limitation. Steve-Eye integrates the LLM with a visual encoder to process visual-text inputs and generate multimodal feedback. We adopt a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, enabling our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks, then carry out experiments from a wide range of perspectives to validate our model\u2019s capability to strategically act and plan. Codes and datasets will be released.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Steve-Eye, a large multimodal model that enables large language model (LLM)-based embodied agents to interact with the open world via visual-text interfaces. Steve-Eye is equipped with three essential functions: multimodal perception, foundational knowledge base, and skill prediction and planning. The authors construct an extensive dataset of 850K open-world instruction pairs to train Steve-Eye and develop three open-world evaluation benchmarks to validate the model's capability to strategically act and plan.\n\nStrengths:\n\n1. The paper presents a novel approach to multimodal interaction in open-world environments, addressing the limitations of text-only agents.\n2. The proposed model, Steve-Eye, demonstrates improved performance in multimodal perception, foundational knowledge question answering, and skill prediction and planning.\n3. The authors provide a comprehensive evaluation of the model's performance on three open-world benchmarks, including environmental visual captioning, foundational knowledge question answering, and skill prediction and planning.\n\nWeaknesses:\n\n1. The paper relies heavily on the use of pre-trained LLMs and visual encoders, which may limit the model's ability to generalize to new tasks and environments.\n2. The dataset collection process is labor-intensive and requires significant human annotation effort, which may not be scalable for larger datasets.\n3. The paper does not provide a thorough analysis of the model's limitations and failure cases, which may be essential for understanding its robustness and reliability.\n\nSuggestions for Improvement:\n\n1. Investigate the use of more robust and generalizable pre-trained models that can handle a wide range of tasks and environments.\n2. Explore more efficient and scalable methods for dataset collection, such as active learning or transfer learning.\n3. Conduct a more comprehensive analysis of the model's limitations and failure cases to better understand its robustness and reliability.\n\nRecommendation:\n\nThis paper presents a novel and promising approach to multimodal interaction in open-world environments. While there are some limitations and areas for improvement, the proposed model demonstrates improved performance on three open-world benchmarks. I recommend this paper for publication in a reputable conference, with the understanding that the authors will address the limitations and suggestions for improvement outlined above.\n\nRating: 7.5/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Steve-Eye, a large multimodal model that combines a pre-trained language model with a visual encoder to enable embodied agents to interact with the open world via visual-text interfaces. Steve-Eye is trained on an extensive dataset of 850K instruction pairs and evaluated on three open-world benchmarks: environmental visual captioning, foundational knowledge question answering, and skill prediction and planning.\n\nStrengths:\n\n1. The paper proposes a novel approach to enable embodied agents to interact with the open world by integrating a pre-trained language model with a visual encoder.\n2. The proposed Steve-Eye model is trained on a large dataset of 850K instruction pairs, which is a significant improvement over previous works.\n3. The paper evaluates Steve-Eye on three open-world benchmarks, providing a comprehensive assessment of its capabilities.\n4. The authors provide a detailed description of the training process and the architecture of the proposed model.\n\nWeaknesses:\n\n1. The paper assumes that the pre-trained language model is already available, which may not be the case for all researchers.\n2. The paper does not provide a clear explanation of how the visual encoder is trained and fine-tuned.\n3. The paper evaluates Steve-Eye on a specific dataset and benchmarks, which may not be representative of all open-world scenarios.\n4. The paper does not provide a detailed comparison with other state-of-the-art approaches.\n\nSuggestions for Improvement:\n\n1. Provide a clear explanation of the training process for the visual encoder and how it is fine-tuned.\n2. Evaluate Steve-Eye on a more diverse set of benchmarks and datasets to ensure its generalizability.\n3. Compare Steve-Eye with other state-of-the-art approaches to provide a more comprehensive assessment of its capabilities.\n4. Provide more details on the implementation of the proposed model, including the hyperparameters and training settings.\n\nRecommendation:\n\nThe paper proposes a novel approach to enable embodied agents to interact with the open world by integrating a pre-trained language model with a visual encoder. While there are some limitations and areas for improvement, the paper provides a comprehensive assessment of the proposed model and its capabilities. I recommend this paper for publication, but suggest that the authors address the mentioned weaknesses and provide more details on the implementation of the proposed model.\n\nRating: 7/10\n\n$$$",
        "gr_score_with": 0.8576271186440678,
        "gr_score_without": 0.7987987987987988,
        "output_without": {
            "num_tokens_scored": 333,
            "num_green_tokens": 266,
            "green_fraction": 0.7987987987987988,
            "z_score": -0.05451446450636297,
            "p_value": 0.5217373576416915
        },
        "output_with": {
            "num_tokens_scored": 295,
            "num_green_tokens": 253,
            "green_fraction": 0.8576271186440678,
            "z_score": 2.474684557061376,
            "p_value": 0.006667695030322897
        }
    },
    {
        "title": "PERCEPTUAL GROUP TOKENIZER: BUILDING PERCEPTION WITH ITERATIVE GROUPING",
        "abstract": "Human visual recognition system shows astonishing capability of compressing visual information into a set of tokens containing rich representations without label supervision. One critical driving principle behind it is perceptual grouping (Palmer, 2002; Wagemans et al., 2012; Herzog, 2018). Despite being widely used in computer vision in the early 2010s, it remains a mystery whether perceptual grouping can be leveraged to derive a neural visual recognition backbone that generates as powerful representations. In this paper, we propose the Perceptual Group Tokenizer, a model that entirely relies on grouping operations to extract visual features and perform self-supervised representation learning, where a series of grouping operations are used to iteratively hypothesize the context for pixels or superpixels to refine feature representations. We show that the proposed model can achieve competitive performance compared to state-of-the-art vision architectures, and inherits desirable properties including adaptive computation without re-training, and interpretability. Specifically, Perceptual Group Tokenizer achieves 80.3% on ImageNet-1K self-supervised learning benchmark with linear probe evaluation, establishing a new milestone for this paradigm.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel visual recognition architecture called Perceptual Group Tokenizer (PGT), which entirely relies on perceptual grouping principles to extract visual features and perform self-supervised representation learning. The model consists of a series of grouping operations that iteratively hypothesize the context for pixels or superpixels to refine feature representations. The authors show that PGT achieves competitive performance compared to state-of-the-art vision architectures on self-supervised learning benchmarks and inherits desirable properties such as adaptive computation and high model interpretability.\n\nStrengths:\n1. The paper proposes a novel and innovative approach to visual recognition, which is based on perceptual grouping principles.\n2. The model achieves competitive performance compared to state-of-the-art vision architectures on self-supervised learning benchmarks.\n3. The authors provide a thorough analysis of the model's properties, including adaptive computation and high model interpretability.\n4. The paper provides a clear and concise explanation of the model's architecture and training procedure.\n\nWeaknesses:\n1. The paper assumes that the reader is familiar with the concept of perceptual grouping and its applications in computer vision.\n2. The model's performance on other tasks, such as object detection and segmentation, is not evaluated.\n3. The paper does not provide a thorough comparison with other state-of-the-art models, such as ViT and ResNet.\n4. The model's computational cost is relatively high due to the iterative grouping processes.\n\nSuggestions for Improvement:\n1. Provide a more detailed explanation of the perceptual grouping principles and their applications in computer vision.\n2. Evaluate the model's performance on other tasks, such as object detection and segmentation.\n3. Compare the model's performance with other state-of-the-art models, such as ViT and ResNet.\n4. Explore ways to reduce the model's computational cost, such as using more efficient grouping operations.\n\nRecommendation:\nThe paper proposes a novel and innovative approach to visual recognition, which is based on perceptual grouping principles. The model achieves competitive performance compared to state-of-the-art vision architectures on self-supervised learning benchmarks and inherits desirable properties such as adaptive computation and high model interpretability. However, the paper assumes that the reader is familiar with the concept of perceptual grouping and its applications in computer vision, and the model's performance on other tasks is not evaluated. Therefore, I recommend that the authors provide a more detailed explanation of the perceptual grouping principles and their applications in computer vision, evaluate the model's performance on other tasks, and compare the model's performance with other state-of-the-art models.\n\nScore: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThe paper proposes a novel visual recognition architecture called Perceptual Group Tokenizer (PGT) that relies entirely on perceptual grouping principles for feature extraction and self-supervised representation learning. The model iteratively hypothesizes contexts for pixels or superpixels using a series of grouping operations, which are inspired by the human visual recognition system. PGT achieves competitive performance on the ImageNet-1K benchmark, outperforming state-of-the-art vision architectures, and has desirable properties such as adaptive computation and high model interpretability.\n\nStrengths:\n1. The paper proposes a novel and innovative approach to visual recognition, leveraging perceptual grouping principles to extract features and perform self-supervised representation learning.\n2. The model achieves competitive performance on the ImageNet-1K benchmark, outperforming state-of-the-art vision architectures.\n3. PGT has desirable properties such as adaptive computation and high model interpretability, making it a promising direction for future research.\n4. The paper provides a clear and concise explanation of the model's architecture and training procedure, making it easy for readers to understand and replicate the results.\n\nWeaknesses:\n1. The paper does not provide a thorough comparison with other state-of-the-art vision architectures, and it is unclear how PGT performs on other benchmarks.\n2. The model's computational cost is relatively high due to the iterative grouping processes, which may limit its practical applications.\n3. The paper does not provide a clear explanation of how the model's grouping operation relates to self-attention mechanisms, and how it can be used to improve the performance of other vision architectures.\n4. The model's ability to generalize to other tasks and domains is not thoroughly evaluated, and it is unclear how PGT performs on tasks such as object detection and semantic segmentation.\n\nSuggestions for Improvement:\n1. Provide a more comprehensive comparison with other state-of-the-art vision architectures, and evaluate PGT's performance on other benchmarks.\n2. Investigate alternative grouping operations that can reduce the model's computational cost and improve its practical applications.\n3. Provide a more detailed explanation of how the model's grouping operation relates to self-attention mechanisms, and how it can be used to improve the performance of other vision architectures.\n4. Evaluate PGT's ability to generalize to other tasks and domains, and provide a clear explanation of its strengths and limitations in these areas.\n\nRecommendation:\nThe paper proposes a novel and innovative approach to visual recognition, leveraging perceptual grouping principles to extract features and perform self-supervised representation learning. While the model has desirable properties such as adaptive computation and high model interpretability, its computational cost is relatively high, and its ability to generalize to other tasks and domains is not thoroughly evaluated. Therefore, I recommend that the authors provide a more comprehensive comparison with other state-of-the-art vision architectures, investigate alternative grouping operations, and evaluate PGT's ability to generalize to other tasks and domains before publishing the paper.",
        "gr_score_with": 0.8209876543209876,
        "gr_score_without": 0.7607142857142857,
        "output_without": {
            "num_tokens_scored": 280,
            "num_green_tokens": 213,
            "green_fraction": 0.7607142857142857,
            "z_score": -1.6431591864203916,
            "p_value": 0.9498249991146497
        },
        "output_with": {
            "num_tokens_scored": 324,
            "num_green_tokens": 266,
            "green_fraction": 0.8209876543209876,
            "z_score": 0.9447140844760277,
            "p_value": 0.1724024322465823
        }
    },
    {
        "title": "FOSI: Hybrid First and Second Order Optimization",
        "abstract": "Popular machine learning approaches forgo second-order information due to the difficulty of computing curvature in high dimensions. We present FOSI, a novel meta-algorithm that improves the performance of any base first-order optimizer by efficiently incorporating second-order information during the optimization process. In each iteration, FOSI implicitly splits the function into two quadratic functions defined on orthogonal subspaces, then uses a second-order method to minimize the first, and the base optimizer to minimize the other. We formally analyze FOSI\u2019s convergence and the conditions under which it improves a base optimizer. Our empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of first-order methods such as Heavy-Ball and Adam, and outperforms second-order methods (K-FAC and L-BFGS).",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes FOSI, a hybrid optimization algorithm that combines a first-order base optimizer with Newton's method to improve the convergence of any base first-order optimizer. FOSI iteratively splits the optimization problem into two quadratic problems on orthogonal subspaces, uses Newton's method to minimize one, and the base optimizer to minimize the other. The algorithm is designed to efficiently incorporate second-order information without computing the entire Hessian, making it suitable for large-scale optimization problems. The paper formally analyzes FOSI's convergence and the conditions under which it improves a base optimizer. Empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of first-order methods and outperforms second-order methods on various deep neural network training tasks.\n\nStrengths:\n\n1. FOSI is a hybrid optimization algorithm that combines the benefits of first-order and second-order methods, making it a robust and efficient optimization technique.\n2. The algorithm is designed to efficiently incorporate second-order information without computing the entire Hessian, making it suitable for large-scale optimization problems.\n3. The paper provides a thorough analysis of FOSI's convergence and the conditions under which it improves a base optimizer.\n4. Empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of first-order methods and outperforms second-order methods on various deep neural network training tasks.\n\nWeaknesses:\n\n1. The algorithm requires the number of Lanczos iterations (m) to be set, which can be a hyperparameter that needs to be tuned.\n2. The paper assumes that the Hessian is symmetric and positive definite, which may not be the case in all optimization problems.\n3. The algorithm requires the use of a base first-order optimizer, which may not be the best choice for all optimization problems.\n4. The paper does not provide a thorough analysis of the computational complexity of FOSI.\n\nSuggestions for Improvement:\n\n1. Investigate methods for automatically tuning the number of Lanczos iterations (m) and other hyperparameters of FOSI.\n2. Consider relaxing the assumption that the Hessian is symmetric and positive definite to make FOSI more applicable to a wider range of optimization problems.\n3. Investigate the use of other base first-order optimizers, such as stochastic gradient descent, to see if they can be improved by FOSI.\n4. Provide a thorough analysis of the computational complexity of FOSI and compare it to other optimization algorithms.\n\nRecommendation:\n\nFOSI is a promising optimization algorithm that combines the benefits of first-order and second-order methods. While there are some limitations and areas for improvement, the algorithm shows promising results on various deep neural network training tasks. I recommend further investigation and development of FOSI to make it a more robust and efficient optimization technique.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes FOSI, a hybrid optimization algorithm that improves the performance of any base first-order optimizer by incorporating second-order information. FOSI iteratively splits the optimization problem into pairs of quadratic problems on orthogonal subspaces and uses a second-order method (Newton's method) to minimize one and the base optimizer to minimize the other. The paper provides a detailed description of the FOSI algorithm, a thorough spectral analysis of its preconditioner, and convergence guarantees in the stochastic setting. The empirical evaluation demonstrates that FOSI improves the convergence rate and optimization time of first-order methods and outperforms second-order methods on various deep neural networks training tasks.\n\nStrengths:\n\n1. The paper proposes a novel and innovative approach to optimization, which is a significant contribution to the field.\n2. The paper provides a thorough analysis of the FOSI algorithm, including its spectral properties and convergence guarantees.\n3. The empirical evaluation demonstrates the effectiveness of FOSI on various deep neural networks training tasks.\n4. The paper provides a detailed comparison with state-of-the-art second-order methods, which is helpful for understanding the advantages and limitations of FOSI.\n\nWeaknesses:\n\n1. The paper assumes that the base optimizer is a first-order method, which may limit the applicability of FOSI to other optimization algorithms.\n2. The paper does not provide a detailed analysis of the computational complexity and memory requirements of FOSI, which may be important for large-scale optimization problems.\n3. The paper does not explore the robustness and stability of FOSI in the presence of noisy gradients or non-convex objectives.\n4. The paper does not provide a detailed comparison with other hybrid optimization algorithms, which may be helpful for understanding the relative advantages and limitations of FOSI.\n\nSuggestions for Improvement:\n\n1. Investigate the applicability of FOSI to other optimization algorithms, such as second-order methods or stochastic gradient methods.\n2. Provide a detailed analysis of the computational complexity and memory requirements of FOSI.\n3. Explore the robustness and stability of FOSI in the presence of noisy gradients or non-convex objectives.\n4. Compare FOSI with other hybrid optimization algorithms, such as Kronecker-factored approximations or quasi-Newton methods.\n\nRecommendation:\n\nThe paper proposes a novel and innovative approach to optimization, which is a significant contribution to the field. The empirical evaluation demonstrates the effectiveness of FOSI on various deep neural networks training tasks. However, the paper assumes that the base optimizer is a first-order method, which may limit the applicability of FOSI to other optimization algorithms. Therefore, the paper requires some modifications and improvements before it can be considered a final version.",
        "gr_score_with": 0.8474025974025974,
        "gr_score_without": 0.8126888217522659,
        "output_without": {
            "num_tokens_scored": 331,
            "num_green_tokens": 269,
            "green_fraction": 0.8126888217522659,
            "z_score": 0.5774091470782914,
            "p_value": 0.28183154825638046
        },
        "output_with": {
            "num_tokens_scored": 308,
            "num_green_tokens": 261,
            "green_fraction": 0.8474025974025974,
            "z_score": 2.0800298643834707,
            "p_value": 0.018761396864707684
        }
    },
    {
        "title": "UNLEASHING LARGE-SCALE VIDEO GENERATIVE PRE-TRAINING FOR VISUAL ROBOT MANIPULATION",
        "abstract": "Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations. In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative pre-training. We introduce GR-1, a GPT-style model designed for multi-task language-conditioned visual robot manipulation. GR-1 takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states. It predicts robot actions as well as future images in an end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly finetuned on robot data after pre-trained on a large-scale video dataset. We perform extensive experiments on the challenging CALVIN benchmark and a real robot. On CALVIN benchmark, our method outperforms state-of-the-art baseline methods and improves the success rate from 88.9% to 94.9%. In the setting of zero-shot unseen scene generalization, GR-1 improves the success rate from 53.3% to 85.4%. In real robot experiments, GR-1 also outperforms baseline methods and shows strong potentials in generalization to unseen scenes and objects. We provide inaugural evidence that a unified GPT-style transformer, augmented with large-scale video generative pre-training, exhibits remarkable generalization to multi-task visual robot manipulation. Project page: https://GR1-Manipulation.github.io",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to visual robot manipulation by leveraging large-scale video generative pre-training. The authors introduce GR-1, a GPT-style transformer model that takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states, and predicts robot actions and future images in an end-to-end manner. GR-1 is first pre-trained on language-conditioned video prediction with a large-scale video dataset and then finetuned on robot data. The authors demonstrate the effectiveness of GR-1 on the challenging CALVIN benchmark and a real robot, outperforming state-of-the-art baseline methods in various settings, including multi-task learning, zero-shot unseen scene generalization, small dataset, and zero-shot unseen language generalization.\n\nStrengths:\n\n* The paper presents a novel approach to visual robot manipulation by leveraging large-scale video generative pre-training, which is a promising direction for future research.\n* The authors demonstrate the effectiveness of GR-1 on the challenging CALVIN benchmark and a real robot, outperforming state-of-the-art baseline methods in various settings.\n* The paper provides a comprehensive evaluation of GR-1, including multi-task learning, zero-shot unseen scene generalization, small dataset, and zero-shot unseen language generalization.\n\nWeaknesses:\n\n* The paper relies heavily on pre-training on a large-scale video dataset, which may not be feasible for all applications or domains.\n* The authors do not provide a thorough analysis of the trade-offs between pre-training and finetuning, which may be important for practical applications.\n* The paper assumes access to a large-scale video dataset, which may not be available for all researchers or applications.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the trade-offs between pre-training and finetuning, including the impact on performance, computational resources, and data requirements.\n* Explore alternative approaches to pre-training, such as using smaller datasets or different types of data, to make the method more widely applicable.\n* Provide more information about the scalability of GR-1, including the computational resources required for training and inference.\n\nRecommendation:\n\nThe paper presents a novel and promising approach to visual robot manipulation, and the authors demonstrate its effectiveness on challenging benchmarks. However, the paper relies heavily on pre-training on a large-scale video dataset, and the trade-offs between pre-training and finetuning are not thoroughly analyzed. To make the method more widely applicable and scalable, the authors should explore alternative approaches to pre-training and provide a more detailed analysis of the trade-offs between pre-training and finetuning. With these improvements, the paper has the potential to make a significant impact in the field of visual robot manipulation.\n\nRating: 7/10\n\nRecommendation for Publication: Accept with revisions.",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper proposes a novel approach to visual robot manipulation by leveraging large-scale video generative pre-training. The authors introduce GR-1, a GPT-style transformer model that takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states, and predicts robot actions and future images in an end-to-end manner. GR-1 is first pre-trained on language-conditioned video prediction with a large-scale video dataset and then finetuned on robot data. The authors demonstrate the effectiveness of GR-1 in various experiments, including CALVIN benchmark and real robot experiments, and show that it outperforms state-of-the-art baseline methods in settings of multi-task learning, zero-shot unseen scene generalization, small dataset, and zero-shot unseen language generalization.\n\nStrengths:\n\n1. The paper presents a novel approach to visual robot manipulation by leveraging large-scale video generative pre-training, which is a significant contribution to the field.\n2. The authors demonstrate the effectiveness of GR-1 in various experiments, including CALVIN benchmark and real robot experiments, which shows the potential of GR-1 in practical applications.\n3. The paper provides a detailed explanation of the GR-1 model and its architecture, which makes it easy to understand and replicate the results.\n4. The authors discuss the limitations of the current approach and propose future work, which shows a clear direction for further research.\n\nWeaknesses:\n\n1. The paper assumes that the robot data is scarce, which may not be true in all cases. In some scenarios, robot data may be abundant, and the approach may not be as effective.\n2. The authors do not provide a detailed analysis of the video generative pre-training phase, which may affect the performance of GR-1.\n3. The paper does not provide a clear explanation of how GR-1 generalizes to unseen scenes and objects, which is a critical aspect of visual robot manipulation.\n4. The authors do not provide a detailed comparison with other state-of-the-art methods, which may affect the credibility of the results.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the video generative pre-training phase and its impact on the performance of GR-1.\n2. The paper should provide a clear explanation of how GR-1 generalizes to unseen scenes and objects, which is a critical aspect of visual robot manipulation.\n3. The authors should provide a more detailed comparison with other state-of-the-art methods, which may affect the credibility of the results.\n4. The paper should discuss potential limitations and challenges of the approach, such as dealing with noisy or incomplete data.\n\nRecommendation:\n\nThe paper presents a novel approach to visual robot manipulation by leveraging large-scale video generative pre-training, which has the potential to improve the performance of visual robot manipulation tasks. However, there are some limitations and challenges that need to be addressed, such as providing a more detailed analysis of the video generative pre-training phase and explaining how GR-1 generalizes to unseen scenes and objects. Overall, the paper is well-written and presents interesting results, and with some improvements, it has the potential to make a significant contribution to the field.\n\n$$$",
        "gr_score_with": 0.8469945355191257,
        "gr_score_without": 0.7699115044247787,
        "output_without": {
            "num_tokens_scored": 339,
            "num_green_tokens": 261,
            "green_fraction": 0.7699115044247787,
            "z_score": -1.3846665823685607,
            "p_value": 0.9169227790624354
        },
        "output_with": {
            "num_tokens_scored": 366,
            "num_green_tokens": 310,
            "green_fraction": 0.8469945355191257,
            "z_score": 2.247918043418647,
            "p_value": 0.012290708200129261
        }
    },
    {
        "title": "DOUBLY ROBUST INSTANCE-REWEIGHTED ADVERSARIAL TRAINING",
        "abstract": "Assigning importance weights to adversarial data has achieved great success in training adversarially robust networks under limited model capacity. However, existing instance-reweighted adversarial training (AT) methods heavily depend on heuristics and/or geometric interpretations to determine those importance weights, making these algorithms lack rigorous theoretical justification/guarantee. Moreover, recent research has shown that adversarial training suffers from a severe non-uniform robust performance across the training distribution, e.g., data points belonging to some classes can be much more vulnerable to adversarial attacks than others. To address both issues, in this paper, we propose a novel doubly-robust instance reweighted AT framework, which allows to obtain the importance weights via exploring distributionally robust optimization (DRO) techniques, and at the same time boosts the robustness on the most vulnerable examples. In particular, our importance weights are obtained by optimizing the KL-divergence regularized loss function, which allows us to devise new algorithms with a theoretical convergence guarantee. Experiments on standard classification datasets demonstrate that our proposed approach outperforms related state-of-the-art baseline methods in terms of average robust performance, and at the same time improves the robustness against attacks on the weakest data points. Codes can be found in the Supplement.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel doubly robust instance reweighted adversarial training framework, which combines distributionally robust optimization (DRO) and bilevel optimization to determine instance weights for adversarial training in a theoretically grounded way. The framework addresses the non-uniform issues of traditional adversarial training by boosting the robust performance of the most vulnerable examples. The authors develop a novel algorithm based on implicit function theorem and tracking a running average of the outer level function values to solve the doubly robust optimization problem. The proposed framework leads to a new finite-sum compositional bilevel optimization problem, which can be solved by the developed algorithm with theoretical guarantee.\n\nStrengths:\n\n* The paper proposes a novel and theoretically grounded approach to instance reweighted adversarial training, which addresses the non-uniform issues of traditional adversarial training.\n* The authors develop a novel algorithm based on implicit function theorem and tracking a running average of the outer level function values to solve the doubly robust optimization problem.\n* The proposed framework leads to a new finite-sum compositional bilevel optimization problem, which can be solved by the developed algorithm with theoretical guarantee.\n* The experiments on standard benchmarks demonstrate the effectiveness of the proposed approach in improving average robust performance and robustness against attacks on the weakest data points.\n\nWeaknesses:\n\n* The paper assumes that the attack loss function \u2113\u2032(\u00b7; \u03b8) is twice differentiable, which may not be the case in practice.\n* The authors use a log-barrier penalty method to penalize the linear constraint into the attack objective, which may not be efficient for large-scale problems.\n* The proposed algorithm requires the computation of the implicit gradient \u2202\u03b4\u0302\u2217i(\u03b8)\u2202\u03b8, which can be computationally expensive.\n\nSuggestions for Improvement:\n\n* The authors should investigate the use of alternative penalty methods, such as the entropic penalty method, to penalize the linear constraint into the attack objective.\n* The authors should provide more details on the computational efficiency of the proposed algorithm and the implicit gradient computation.\n* The authors should investigate the use of more efficient algorithms, such as the stochastic compositional gradient descent (SCGD) algorithm, to solve the doubly robust optimization problem.\n\nRecommendation:\n\nThe paper proposes a novel and theoretically grounded approach to instance reweighted adversarial training, which addresses the non-uniform issues of traditional adversarial training. The authors develop a novel algorithm based on implicit function theorem and tracking a running average of the outer level function values to solve the doubly robust optimization problem. The proposed framework leads to a new finite-sum compositional bilevel optimization problem, which can be solved by the developed algorithm with theoretical guarantee. The experiments on standard benchmarks demonstrate the effectiveness of the proposed approach in improving average robust performance and robustness against attacks on the weakest data points. Therefore, I recommend this paper for publication in a top-tier conference.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel doubly robust instance reweighted adversarial training framework, which addresses the limitations of traditional adversarial training methods. The proposed framework uses distributionally robust optimization (DRO) and bilevel optimization to determine the instance weights in a theoretically grounded way, and also boosts the robustness on the most vulnerable examples. The authors provide a new finite-sum compositional bilevel optimization problem and develop a novel algorithm based on implicit function theorem and tracking a running average of the outer level function values. The experiments on standard benchmarks demonstrate that the proposed framework outperforms related state-of-the-art baseline approaches in average robust performance and also improves the robustness against attacks on the weakest data points.\n\nStrengths:\n\n* The paper proposes a novel doubly robust instance reweighted adversarial training framework, which addresses the limitations of traditional adversarial training methods.\n* The proposed framework uses distributionally robust optimization (DRO) and bilevel optimization to determine the instance weights in a theoretically grounded way.\n* The authors provide a new finite-sum compositional bilevel optimization problem and develop a novel algorithm based on implicit function theorem and tracking a running average of the outer level function values.\n* The experiments on standard benchmarks demonstrate that the proposed framework outperforms related state-of-the-art baseline approaches in average robust performance and also improves the robustness against attacks on the weakest data points.\n\nWeaknesses:\n\n* The paper assumes that the adversarial noise is bounded, which may not be realistic in some scenarios.\n* The proposed framework requires the use of DRO, which may not be applicable in some cases where the distribution of the data is unknown.\n* The authors do not provide a detailed analysis of the computational complexity of the proposed algorithm.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the computational complexity of the proposed algorithm.\n* The authors should provide more experimental results on different types of data and scenarios.\n* The authors should provide more details on how the proposed framework can be extended to other types of attacks and defense mechanisms.\n\nRecommendation:\n\nThe paper proposes a novel and interesting approach to adversarial training, and the experimental results demonstrate its effectiveness. However, the paper has some limitations, and the authors should provide more details and experimental results to make the paper more comprehensive. With some minor revisions, the paper can be considered for publication in a top-tier conference.\n\nRating: 8/10\n\n$$$",
        "gr_score_with": 0.8804347826086957,
        "gr_score_without": 0.8295819935691319,
        "output_without": {
            "num_tokens_scored": 311,
            "num_green_tokens": 258,
            "green_fraction": 0.8295819935691319,
            "z_score": 1.3044700912626621,
            "p_value": 0.0960366753295307
        },
        "output_with": {
            "num_tokens_scored": 276,
            "num_green_tokens": 243,
            "green_fraction": 0.8804347826086957,
            "z_score": 3.340927414286313,
            "p_value": 0.0004174952480443263
        }
    },
    {
        "title": "GENSIM: GENERATING ROBOTIC SIMULATION TASKS VIA LARGE LANGUAGE MODELS",
        "abstract": "Collecting large amounts of real-world interaction data to train general robotic policies is often prohibitively expensive, thus motivating the use of simulation data. However, existing methods for data generation have generally focused on scenelevel diversity (e.g., object instances and poses) rather than task-level diversity, due to the human effort required to come up with and verify novel tasks. This has made it challenging for policies trained on simulation data to demonstrate significant task-level generalization. In this paper, we propose to automatically generate rich simulation environments and expert demonstrations by exploiting a large language models\u2019 (LLM) grounding and coding ability. Our approach, dubbed GENSIM, has two modes: goal-directed generation, wherein a target task is given to the LLM and the LLM proposes a task curriculum to solve the target task, and exploratory generation, wherein the LLM bootstraps from previous tasks and iteratively proposes novel tasks that would be helpful in solving more complex tasks. We use GPT4 to expand the existing benchmark by ten times to over 100 tasks, on which we conduct supervised finetuning and evaluate several LLMs including finetuned GPTs and Code Llama on code generation for robotic simulation tasks. Furthermore, we observe that LLMs-generated simulation programs can enhance task-level generalization significantly when used for multitask policy training. We further find that with minimal sim-to-real adaptation, the multitask policies pretrained on GPT4-generated simulation tasks exhibit stronger transfer to unseen long-horizon tasks in the real world and outperform baselines by 25%. 1",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework, GenSim, that utilizes large language models (LLMs) to automatically generate rich simulation environments and expert demonstrations for robotic policy training. The framework operates in two distinct modes: goal-directed generation, where a target task is given to the LLM, and exploratory generation, where the LLM bootstraps from previous tasks to propose novel tasks. The authors demonstrate that LLMs can generate high-quality, achievable, and diverse tasks, which can enhance task-level generalization of robotic policies. They evaluate several LLMs, including finetuned GPTs and Code Llama, on code generation for robotic simulation tasks and show that pretraining on LLM-generated simulation tasks can improve real-world generalization capabilities by 25%.\n\nStrengths:\n\n* The paper presents a novel and scalable framework for generating simulation tasks using LLMs, which can augment diverse simulation tasks for robotic policy training.\n* The authors demonstrate the effectiveness of their framework in generating high-quality, achievable, and diverse tasks, which can enhance task-level generalization of robotic policies.\n* The paper provides a thorough evaluation of several LLMs on code generation for robotic simulation tasks and shows that pretraining on LLM-generated simulation tasks can improve real-world generalization capabilities.\n\nWeaknesses:\n\n* The paper relies heavily on a single LLM model, GPT4, and does not explore the robustness of their framework to different LLM models.\n* The evaluation of the framework is limited to a specific task domain (table-top pick-and-place) and does not consider more complex robotic tasks.\n* The paper does not provide a thorough analysis of the limitations of their framework, such as the potential for hallucinations and lack of grounding in physical and geometric details.\n\nSuggestions for Improvement:\n\n* To improve the robustness of the framework, the authors should explore the use of multiple LLM models and evaluate their framework on different task domains.\n* The authors should provide a more thorough analysis of the limitations of their framework, including the potential for hallucinations and lack of grounding in physical and geometric details.\n* The paper should include a more detailed discussion of the potential applications of their framework, such as generating dexterous and complex robotic tasks.\n\nRecommendation:\n\nThe paper presents a novel and scalable framework for generating simulation tasks using LLMs, which can enhance task-level generalization of robotic policies. While the paper has some limitations, it provides a thorough evaluation of their framework and demonstrates its effectiveness in generating high-quality, achievable, and diverse tasks. Therefore, I recommend this paper for publication in a top-tier conference.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework, GenSim, that utilizes large language models (LLMs) to generate diverse simulation tasks for robotic policy learning. The framework consists of three components: a task creator, a task library, and a language-conditioned multi-task policy training procedure. The task creator generates novel task descriptions and corresponding code implementations, while the task library stores previously generated high-quality tasks to aid in the creation of new tasks. The language-conditioned multi-task policy training procedure leverages the generated tasks to enhance task-level generalization. The authors demonstrate the effectiveness of GenSim by scaling up a human-curated benchmark from 10 to over 100 tasks and evaluating several LLM models, including GPT4 and Code Llama, on simulated manipulation task creations.\n\nStrengths:\n\n* The paper presents a novel and scalable framework for generating diverse simulation tasks, which has the potential to improve task-level generalization in robotic policy learning.\n* The authors demonstrate the effectiveness of GenSim by scaling up a human-curated benchmark and evaluating several LLM models on simulated manipulation task creations.\n* The paper highlights the potential of LLMs to generate high-quality, achievable, and diverse tasks, which can be used to train multitask policies.\n\nWeaknesses:\n\n* The paper relies heavily on the performance of LLMs, which may not be generalizable to all robotic tasks and environments.\n* The evaluation metrics used in the paper, such as syntax-correct, runtime-verified, and task completed, may not capture all aspects of task generation and policy learning.\n* The paper does not provide a thorough analysis of the limitations and challenges of using LLMs for task generation and policy learning.\n\nSuggestions for Improvement:\n\n* The authors should provide a more thorough analysis of the limitations and challenges of using LLMs for task generation and policy learning, such as hallucinations, syntax errors, and lack of grounding in physical and geometric details.\n* The paper should include a more detailed evaluation of the performance of GenSim on a variety of robotic tasks and environments, including complex and dexterous tasks.\n* The authors should explore the use of other evaluation metrics, such as task success rates and policy generalization, to provide a more comprehensive assessment of GenSim.\n\nRecommendation:\n\nThe paper presents a novel and scalable framework for generating diverse simulation tasks, which has the potential to improve task-level generalization in robotic policy learning. However, the paper relies heavily on the performance of LLMs, and the evaluation metrics used may not capture all aspects of task generation and policy learning. Therefore, I recommend that the authors address these limitations and challenges in future work and provide a more comprehensive evaluation of GenSim.\n\n$$$",
        "gr_score_with": 0.8126801152737753,
        "gr_score_without": 0.783625730994152,
        "output_without": {
            "num_tokens_scored": 342,
            "num_green_tokens": 268,
            "green_fraction": 0.783625730994152,
            "z_score": -0.7567360683076866,
            "p_value": 0.7753959992737018
        },
        "output_with": {
            "num_tokens_scored": 347,
            "num_green_tokens": 282,
            "green_fraction": 0.8126801152737753,
            "z_score": 0.590794510901771,
            "p_value": 0.2773290566530554
        }
    },
    {
        "title": "COT3DREF: CHAIN-OF-THOUGHTS DATA-EFFICIENT 3D VISUAL GROUNDING",
        "abstract": "3D visual grounding is the ability to localize objects in 3D scenes conditioned by utterances. Most existing methods devote the referring head to localize the referred object directly, causing failure in complex scenarios. In addition, it does not illustrate how and why the network reaches the final decision. In this paper, we address this question \u201cCan we design an interpretable 3D visual grounding framework that has the potential to mimic the human perception system?\u201d. To this end, we formulate the 3D visual grounding problem as a sequence-to-sequence (Seq2Seq) task by first predicting a chain of anchors and then the final target. Interpretability not only improves the overall performance but also helps us identify failure cases. Following the chain of thoughts approach enables us to decompose the referring task into interpretable intermediate steps, boosting the performance and making our framework extremely data-efficient. Moreover, our proposed framework can be easily integrated into any existing architecture. We validate our approach through comprehensive experiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent performance gains compared to existing methods without requiring manually annotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is significantly data-efficient, whereas on the Sr3D dataset, when trained only on 10% of the data, we match the SOTA performance that trained on the entire data. The code is available at https://cot3dref.github.io/.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel and interpretable framework, CoT3DRef, for 3D visual grounding. The framework formulates the problem of 3D visual grounding from a natural language instruction as a sequence-to-sequence task, predicting a chain of anchor objects that are subsequently utilized to localize the final target object. This sequential approach enhances interpretability and improves overall performance and data efficiency. The framework is data-efficient and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data. The authors also demonstrate the effectiveness of their approach by integrating it into four different baselines and achieving state-of-the-art results on three well-known 3D visual grounding benchmarks.\n\nStrengths:\n\n1. The paper proposes a novel and interpretable framework for 3D visual grounding, which is a significant contribution to the field.\n2. The framework is data-efficient and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data.\n3. The authors demonstrate the effectiveness of their approach by integrating it into four different baselines and achieving state-of-the-art results on three well-known 3D visual grounding benchmarks.\n4. The paper provides a comprehensive analysis of the framework's performance and limitations, including the use of pseudo-labels and the Pathway module.\n\nWeaknesses:\n\n1. The paper relies heavily on pseudo-labels, which may not be accurate and can lead to performance loss on certain datasets.\n2. The Pathway module has limitations, including not handling multi-path scenarios, which may lead to suboptimal performance in certain cases.\n3. The paper does not provide a thorough comparison with other state-of-the-art methods, which makes it difficult to evaluate the framework's performance in a broader context.\n4. The paper assumes that the input utterance is well-formed and does not handle cases where the utterance is ambiguous or contains negations.\n\nSuggestions for Improvement:\n\n1. The authors should investigate alternative methods for collecting pseudo-labels, such as using more advanced natural language processing techniques or incorporating human annotations.\n2. The Pathway module should be modified to handle multi-path scenarios, such as using a graph-based approach to reason about different possibilities.\n3. The paper should provide a more thorough comparison with other state-of-the-art methods, including a detailed analysis of the framework's performance and limitations.\n4. The paper should address the assumption that the input utterance is well-formed and provide a more robust solution for handling ambiguous or negated utterances.\n\nRecommendation:\n\nThe paper proposes a novel and interpretable framework for 3D visual grounding, which is a significant contribution to the field. However, the paper relies heavily on pseudo-labels, which may not be accurate, and the Pathway module has limitations. To improve the framework, the authors should investigate alternative methods for collecting pseudo-labels and modify the Pathway module to handle multi-path scenarios. Additionally, the paper should provide a more thorough comparison with other state-of-the-art methods and address the assumption that the input utterance is well-formed. Overall, the paper is well-written and provides a comprehensive analysis of the framework's performance and limitations.\n\nRecommendation: Accept after revisions.\n\n$$$",
        "peer_review_with_watermark": "$$\nPaper Summary:\n\nThe paper proposes a novel and interpretable framework for 3D visual grounding, termed CoT3DRef. The framework formulates the 3D visual grounding problem as a sequence-to-sequence task, predicting a chain of anchor objects that are subsequently used to localize the final target object. This sequential approach enhances interpretability and improves overall performance and data efficiency. The framework is data-efficient and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data.\n\nStrengths:\n\n*   The proposed framework is interpretable and provides a clear understanding of how the model arrives at its decisions.\n*   The framework is data-efficient and outperforms existing methods on the Nr3D and Sr3D datasets when trained on a limited amount of data.\n*   The use of pseudo-labels eliminates the need for manual annotations, making the framework more efficient and scalable.\n\nWeaknesses:\n\n*   The accuracy of the pseudo-labels plays a vital role in the overall performance, and a significant accuracy gap is measured between automatically collected pseudo-labels and ground-truth data.\n*   The anchor localization block in the pseudo module is tailored on the ScanNet dataset and will need adaptations to be usable on other 3D scene datasets.\n*   The Pathway module does not handle the multi-path scenario, where multiple paths are valid.\n\nSuggestions for Improvement:\n\n*   To improve the performance on the Nr3D dataset, the pseudo-label module should be enhanced to reduce the accuracy gap between automatically collected pseudo-labels and ground-truth data.\n*   The anchor localization block in the pseudo module should be adapted to be usable on other 3D scene datasets.\n*   The Pathway module should be modified to handle the multi-path scenario.\n\nRecommendation:\n\nThe proposed framework, CoT3DRef, is a significant contribution to the field of 3D visual grounding. The framework's interpretability, data efficiency, and ability to eliminate the need for manual annotations make it a valuable tool for researchers and practitioners. However, to fully realize the potential of the framework, the accuracy gap between pseudo-labels and ground-truth data should be addressed, and the Pathway module should be modified to handle the multi-path scenario. With these improvements, the framework has the potential to become a state-of-the-art method for 3D visual grounding.\n\n$$",
        "gr_score_with": 0.8129032258064516,
        "gr_score_without": 0.7907608695652174,
        "output_without": {
            "num_tokens_scored": 368,
            "num_green_tokens": 291,
            "green_fraction": 0.7907608695652174,
            "z_score": -0.4427888101933491,
            "p_value": 0.6710407511084228
        },
        "output_with": {
            "num_tokens_scored": 310,
            "num_green_tokens": 252,
            "green_fraction": 0.8129032258064516,
            "z_score": 0.56822974610264,
            "p_value": 0.28493948785457573
        }
    },
    {
        "title": "REWARD DESIGN FOR JUSTIFIABLE SEQUENTIAL DECISION-MAKING",
        "abstract": "Equipping agents with the capacity to justify made decisions using supporting evidence represents a cornerstone of accountable decision-making. Furthermore, ensuring that justifications are in line with human expectations and societal norms is vital, especially in high-stakes situations such as healthcare. In this work, we propose the use of a debate-based reward model for reinforcement learning agents, where the outcome of a zero-sum debate game quantifies the justifiability of a decision in a particular state. This reward model is then used to train a justifiable policy, whose decisions can be more easily corroborated with supporting evidence. In the debate game, two argumentative agents take turns providing supporting evidence for two competing decisions. Given the proposed evidence, a proxy of a human judge evaluates which decision is better justified. We demonstrate the potential of our approach in learning policies for prescribing and justifying treatment decisions of septic patients. We show that augmenting the reward with the feedback signal generated by the debate-based reward model yields policies highly favored by the judge when compared to the policy obtained solely from the environment rewards, while hardly sacrificing any performance. Moreover, in terms of the overall performance and justifiability of trained policies, the debate-based feedback is comparable to the feedback obtained from an ideal judge proxy that evaluates decisions using the full information encoded in the state. This suggests that the debate game outputs key information contained in states that is most relevant for evaluating decisions, which in turn substantiates the practicality of combining our approach with human-in-the-loop evaluations. Lastly, we showcase that agents trained via multi-agent debate learn to propose evidence that is resilient to refutations and closely aligns with human preferences.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach to designing rewards for reinforcement learning (RL) agents that can justify their decisions using supporting evidence. The authors introduce a debate-based reward model, where two argumentative agents take turns providing supporting evidence for two competing decisions. A proxy of a human judge evaluates which decision is better justified based on the proposed evidence. The authors demonstrate the effectiveness of their approach in learning policies for treating septic patients, showing that the debate-based feedback yields policies highly favored by the judge while hardly sacrificing any performance.\n\nStrengths:\n\n1. The paper proposes a novel and innovative approach to designing rewards for RL agents that can justify their decisions using supporting evidence.\n2. The authors provide a thorough and detailed explanation of their approach, including the formal setup, learning framework, and experiments.\n3. The paper presents a comprehensive evaluation of the effectiveness of the debate-based reward model, including quantitative and qualitative results.\n4. The authors provide a clear and concise description of the experimental setup, including the patient cohort, environment, and evaluation metrics.\n\nWeaknesses:\n\n1. The paper assumes a well-defined argument space, which may not be the case in all domains. The authors acknowledge this limitation and suggest potential alternative approaches.\n2. The paper relies on a proxy of a human judge, which may not accurately capture human preferences and biases.\n3. The authors do not provide a thorough discussion of the potential biases and limitations of the debate-based reward model.\n4. The paper assumes a fixed number of turns in the debate game, which may not be realistic in all scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed discussion of the potential biases and limitations of the debate-based reward model, including the impact of human biases and preferences.\n2. The paper should include a more thorough evaluation of the effectiveness of the debate-based reward model in different domains and scenarios.\n3. The authors should provide a more detailed explanation of the learning framework and the role of the proxy judge in the debate-based reward model.\n4. The paper should include a more comprehensive discussion of the potential applications and limitations of the debate-based reward model.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to designing rewards for RL agents that can justify their decisions using supporting evidence. While there are some limitations and weaknesses to the approach, the paper provides a thorough and detailed evaluation of the effectiveness of the debate-based reward model. I recommend acceptance of the paper for publication in a top-tier conference, with the suggestion that the authors address the limitations and weaknesses mentioned above.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$ \nPaper Summary:\n\nThe paper proposes a framework for designing rewards that incentivize reinforcement learning agents to make justifiable decisions. The framework uses a debate-based reward model, where the outcome of a zero-sum debate game quantifies the justifiability of a decision in a particular state. The debate game involves two argumentative agents that take turns providing supporting evidence for two competing decisions, and a proxy of a human judge evaluates which decision is better justified. The paper demonstrates the effectiveness of the framework in learning policies for prescribing and justifying treatment decisions for septic patients, and shows that the policies trained with the debate-based reward model are highly favored by the human judge.\n\nStrengths:\n\n1. The paper presents a novel and innovative approach to designing rewards for reinforcement learning agents, which is a critical problem in the field.\n2. The framework is well-motivated and grounded in the concept of justifiable decision-making, which is an important aspect of accountable decision-making.\n3. The paper provides a thorough analysis of the framework, including its technical components and the experiments conducted to evaluate its effectiveness.\n4. The paper demonstrates the applicability of the framework to a real-world problem, namely prescribing and justifying treatment decisions for septic patients.\n\nWeaknesses:\n\n1. The paper assumes that the human judge's preferences can be accurately captured and modeled, which may not always be the case.\n2. The paper does not provide a thorough analysis of the computational complexity of the framework, which may be a limitation in certain applications.\n3. The paper assumes that the debate-based reward model is effective in all cases, which may not be the case in certain scenarios.\n4. The paper does not provide a detailed comparison with other reward design methods, which would be useful for understanding the relative strengths and weaknesses of the framework.\n\nSuggestions for Improvement:\n\n1. The paper could provide a more thorough analysis of the computational complexity of the framework, including a discussion of the time and space complexity of the algorithm.\n2. The paper could provide a more detailed comparison with other reward design methods, including a discussion of the strengths and weaknesses of each method.\n3. The paper could provide a more thorough analysis of the limitations of the framework, including a discussion of scenarios in which the framework may not be effective.\n4. The paper could provide a more detailed discussion of the implications of the framework for real-world applications, including a discussion of the potential benefits and challenges of implementing the framework in practice.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to designing rewards for reinforcement learning agents, which has the potential to make a significant impact in the field. The framework is well-motivated and grounded in the concept of justifiable decision-making, and the paper provides a thorough analysis of the framework and its technical components. However, the paper assumes that the human judge's preferences can be accurately captured and modeled, which may not always be the case, and the paper does not provide a thorough analysis of the computational complexity of the framework. Therefore, I recommend that the paper be accepted for publication, but with the suggestion that the authors address these limitations in future work.",
        "gr_score_with": 0.8854748603351955,
        "gr_score_without": 0.8181818181818182,
        "output_without": {
            "num_tokens_scored": 319,
            "num_green_tokens": 261,
            "green_fraction": 0.8181818181818182,
            "z_score": 0.8121131574797527,
            "p_value": 0.20836335199937417
        },
        "output_with": {
            "num_tokens_scored": 358,
            "num_green_tokens": 317,
            "green_fraction": 0.8854748603351955,
            "z_score": 4.04339839329746,
            "p_value": 2.634099605296072e-05
        }
    },
    {
        "title": "ERAL GEOMETRY FOR KNOWLEDGE DISTILLATION",
        "abstract": "Knowledge distillation aims to train a compact student network using soft supervision from a larger teacher network and hard supervision from ground truths. However, determining an optimal knowledge fusion ratio that balances these supervisory signals remains challenging. Prior methods generally resort to a constant or heuristic-based fusion ratio, which often falls short of a proper balance. In this study, we introduce a novel adaptive method for learning a sample-wise knowledge fusion ratio, exploiting both the correctness of teacher and student, as well as how well the student mimics the teacher on each sample. Our method naturally leads to the intra-sample trilateral geometric relations among the student prediction (S), teacher prediction (T ), and ground truth (G). To counterbalance the impact of outliers, we further extend to the inter-sample relations, incorporating the teacher\u2019s global average prediction (T\u0304 ) for samples within the same class. A simple neural network then learns the implicit mapping from the intraand inter-sample relations to an adaptive, sample-wise knowledge fusion ratio in a bilevel-optimization manner. Our approach provides a simple, practical, and adaptable solution for knowledge distillation that can be employed across various architectures and model sizes. Extensive experiments demonstrate consistent improvements over other loss re-weighting methods on image classification, attack detection, and click-through rate prediction.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach named TGeo-KD for learning sample-wise knowledge fusion ratios during knowledge distillation (KD). The authors argue that existing methods for determining the knowledge fusion ratio often rely on heuristic rules or constant values, which may not capture the nuanced dynamics of the knowledge transfer process. To address this limitation, the authors introduce a novel adaptive method that learns a sample-wise knowledge fusion ratio by exploiting both the correctness of the teacher and student, as well as the discrepancy between their predictions. The proposed method, TGeo-KD, leverages the trilateral geometry among the signals from the student, teacher, and ground truth by modeling both intra- and inter-sample geometric relations.\n\nStrengths:\n\n1. The paper presents a well-structured and clear introduction to the problem of knowledge distillation and the limitations of existing methods.\n2. The proposed approach, TGeo-KD, is innovative and well-motivated, leveraging the trilateral geometry among the signals from the student, teacher, and ground truth.\n3. The authors provide extensive experiments across diverse domains, including image classification, attack detection, and click-through rate prediction, demonstrating the consistent superiority of TGeo-KD over other re-weighting methods.\n4. The paper provides a clear and concise explanation of the proposed method, including the bilevel optimization problem and the neural network architecture.\n\nWeaknesses:\n\n1. The paper assumes that the teacher network is pre-trained, which may not be the case in many real-world scenarios.\n2. The proposed method relies on the availability of ground truth labels, which may not be available in some applications.\n3. The paper does not provide a thorough comparison with other methods that do not use trilateral geometry, making it difficult to evaluate the effectiveness of the proposed approach.\n4. The authors do not provide a clear explanation of how the proposed method can be extended to more complex scenarios, such as multi-task learning or transfer learning.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed comparison with other methods that do not use trilateral geometry, including a discussion of the advantages and disadvantages of each approach.\n2. Explore the use of TGeo-KD in scenarios where the teacher network is not pre-trained, such as in self-distillation or multi-task learning.\n3. Investigate the use of TGeo-KD in applications where ground truth labels are not available, such as in unsupervised learning or semi-supervised learning.\n4. Provide a more detailed explanation of how the proposed method can be extended to more complex scenarios, including a discussion of the challenges and potential solutions.\n\nRecommendation:\n\nThe paper presents a novel and innovative approach to knowledge distillation that leverages the trilateral geometry among the signals from the student, teacher, and ground truth. The extensive experiments across diverse domains demonstrate the consistent superiority of TGeo-KD over other re-weighting methods. However, the paper assumes that the teacher network is pre-trained, and the proposed method relies on the availability of ground truth labels. Therefore, I recommend that the authors address these limitations in future work and provide a more detailed comparison with other methods that do not use trilateral geometry.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes an innovative method named TGeo-KD for learning sample-wise knowledge fusion ratios in knowledge distillation (KD). The authors argue that existing methods often use a uniform or heuristic-based fusion ratio, which may not balance the supervisory signals well. The proposed TGeo-KD method learns a sample-wise knowledge fusion ratio by exploiting the trilateral geometry among the signals from the student, teacher, and ground truth. The method captures both intra-sample and inter-sample geometric relations and adapts the fusion ratio using a neural network. The authors conduct extensive experiments on various tasks, including image classification, attack detection, and click-through rate prediction, and demonstrate the superiority of TGeo-KD over other re-weighting methods.\n\nStrengths:\n\n1. The paper proposes an innovative method for learning sample-wise knowledge fusion ratios in KD, which addresses the limitation of existing methods.\n2. The method captures both intra-sample and inter-sample geometric relations, providing a more comprehensive understanding of the knowledge fusion process.\n3. The authors conduct extensive experiments on various tasks, demonstrating the effectiveness of TGeo-KD in different domains.\n4. The paper provides a clear and concise explanation of the method and its underlying principles.\n\nWeaknesses:\n\n1. The paper relies heavily on the use of neural networks to learn the fusion ratio, which may not be suitable for all applications or domains.\n2. The method may not be effective in cases where the teacher model is not well-trained or has poor performance.\n3. The paper does not provide a clear explanation of how the method can be extended to other domains or tasks beyond image classification.\n\nSuggestions for Improvement:\n\n1. The authors should provide more detailed explanations of how the method can be extended to other domains or tasks beyond image classification.\n2. The paper should include more discussions on the limitations of the method and potential areas for future research.\n3. The authors should provide more quantitative evaluations of the method's performance in different scenarios.\n\nRecommendation:\n\nThe paper proposes an innovative method for learning sample-wise knowledge fusion ratios in KD, which addresses the limitation of existing methods. The method captures both intra-sample and inter-sample geometric relations, providing a more comprehensive understanding of the knowledge fusion process. The authors conduct extensive experiments on various tasks, demonstrating the effectiveness of TGeo-KD in different domains. While the paper has some limitations, it provides a clear and concise explanation of the method and its underlying principles. Therefore, I recommend this paper for publication in the conference.",
        "gr_score_with": 0.8278145695364238,
        "gr_score_without": 0.7989821882951654,
        "output_without": {
            "num_tokens_scored": 393,
            "num_green_tokens": 314,
            "green_fraction": 0.7989821882951654,
            "z_score": -0.05013360562382587,
            "p_value": 0.5199920400081065
        },
        "output_with": {
            "num_tokens_scored": 302,
            "num_green_tokens": 250,
            "green_fraction": 0.8278145695364238,
            "z_score": 1.2086710650157877,
            "p_value": 0.11339462114749693
        }
    },
    {
        "title": "TAPMO: SHAPE-AWARE MOTION GENERATION OF SKELETON-FREE CHARACTERS",
        "abstract": "Previous motion generation methods are limited to the pre-rigged 3D human model, hindering their applications in the animation of various non-rigged characters. In this work, we present TapMo, a Text-driven Animation Pipeline for synthesizing Motion in a broad spectrum of skeleton-free 3D characters. The pivotal innovation in TapMo is its use of shape deformation-aware features as a condition to guide the diffusion model, thereby enabling the generation of meshspecific motions for various characters. Specifically, TapMo comprises two main components Mesh Handle Predictor and Shape-aware Diffusion Module. Mesh Handle Predictor predicts the skinning weights and clusters mesh vertices into adaptive handles for deformation control, which eliminates the need for traditional skeletal rigging. Shape-aware Motion Diffusion synthesizes motion with mesh-specific adaptations. This module employs text-guided motions and mesh features extracted during the first stage, preserving the geometric integrity of the animations by accounting for the character\u2019s shape and deformation. Trained in a weakly-supervised manner, TapMo can accommodate a multitude of nonhuman meshes, both with and without associated text motions. We demonstrate the effectiveness and generalizability of TapMo through rigorous qualitative and quantitative experiments. Our results reveal that TapMo consistently outperforms existing auto-animation methods, delivering superior-quality animations for both seen or unseen heterogeneous 3D characters. The project page: https://semanticdh.github.io/TapMo.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel text-driven animation pipeline called TapMo, which enables the generation of realistic and anthropomorphic motion for a wide range of skeleton-free 3D characters. The pipeline consists of two key components: the Mesh Handle Predictor and the Shape-aware Motion Diffusion. The Mesh Handle Predictor predicts the skinning weights and clusters mesh vertices into adaptive handles for semantic control, while the Shape-aware Motion Diffusion generates text-guided motions considering the specific deformation properties of the mesh. The authors propose a weakly-supervised training strategy for adaptive mesh handle learning and mesh-specific motion learning to train TapMo with limited ground-truth data. The paper demonstrates the effectiveness and generalizability of TapMo through extensive experiments and shows that it achieves state-of-the-art performance compared to baseline methods.\n\nStrengths:\n\n1. The paper presents a novel and comprehensive solution for text-driven animation of skeleton-free 3D characters.\n2. The authors propose a weakly-supervised training strategy for adaptive mesh handle learning and mesh-specific motion learning, which is innovative and effective.\n3. The paper demonstrates the effectiveness and generalizability of TapMo through extensive experiments and shows that it achieves state-of-the-art performance compared to baseline methods.\n4. The authors provide a clear and concise explanation of the proposed method and its components.\n\nWeaknesses:\n\n1. The paper assumes that the input mesh is provided, which may not be the case in practice. The authors should provide more information on how to handle this issue.\n2. The paper does not provide a detailed comparison with other methods that can handle skeleton-free 3D characters.\n3. The authors should provide more information on the computational cost of the proposed method and how it compares to other methods.\n4. The paper assumes that the text description is provided, which may not be the case in practice. The authors should provide more information on how to handle this issue.\n\nSuggestions for Improvement:\n\n1. Provide more information on how to handle the case where the input mesh is not provided.\n2. Provide a detailed comparison with other methods that can handle skeleton-free 3D characters.\n3. Provide more information on the computational cost of the proposed method and how it compares to other methods.\n4. Provide more information on how to handle the case where the text description is not provided.\n5. Consider providing more information on the robustness of the proposed method to different types of meshes and text descriptions.\n\nRecommendation:\n\nThe paper presents a novel and comprehensive solution for text-driven animation of skeleton-free 3D characters. The authors propose a weakly-supervised training strategy for adaptive mesh handle learning and mesh-specific motion learning, which is innovative and effective. The paper demonstrates the effectiveness and generalizability of TapMo through extensive experiments and shows that it achieves state-of-the-art performance compared to baseline methods. However, the paper assumes that the input mesh is provided, which may not be the case in practice. The authors should provide more information on how to handle this issue. Overall, the paper is well-written and provides a clear and concise explanation of the proposed method and its components. I recommend this paper for publication in the conference.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel text-driven animation pipeline called TapMo, which enables the generation of realistic and anthropomorphic motion for a wide range of skeleton-free 3D characters. TapMo consists of two key components: the Mesh Handle Predictor and the Shape-aware Motion Diffusion. The Mesh Handle Predictor predicts the skinning weights and clusters mesh vertices into adaptive handles for semantic control, while the Shape-aware Motion Diffusion generates text-guided motions considering the specific deformation properties of the mesh. To train TapMo with limited ground-truth data, the authors propose a weakly-supervised training strategy for adaptive mesh handle learning and mesh-specific motion learning. The authors demonstrate the effectiveness of TapMo through extensive experiments and show that it achieves state-of-the-art performance compared to baseline methods.\n\nStrengths:\n\n1. Novelty: TapMo is a novel text-driven animation pipeline that enables the generation of realistic and anthropomorphic motion for a wide range of skeleton-free 3D characters.\n2. Effectiveness: The authors demonstrate the effectiveness of TapMo through extensive experiments and show that it achieves state-of-the-art performance compared to baseline methods.\n3. Weakly-supervised training strategy: The authors propose a weakly-supervised training strategy for adaptive mesh handle learning and mesh-specific motion learning, which is suitable for training TapMo with limited ground-truth data.\n\nWeaknesses:\n\n1. Complexity: TapMo is a complex system consisting of two key components, which may be difficult to implement and train.\n2. Limited training data: TapMo requires a large amount of training data, which may be difficult to obtain, especially for skeleton-free 3D characters.\n3. Evaluation metrics: The authors use a combination of metrics to evaluate the performance of TapMo, which may not be comprehensive and may not capture all aspects of the animation quality.\n\nSuggestions for Improvement:\n\n1. Simplify the implementation and training of TapMo by introducing more efficient algorithms and techniques.\n2. Collect more training data, especially for skeleton-free 3D characters, to improve the robustness and generalizability of TapMo.\n3. Evaluate the performance of TapMo using more comprehensive metrics, such as user studies and expert evaluations, to better understand its strengths and limitations.\n\nRecommendation:\n\nI recommend accepting this paper for publication in the conference. TapMo is a novel and effective text-driven animation pipeline that has the potential to revolutionize the field of computer animation. While there are some limitations and areas for improvement, the authors have made significant contributions to the field and have demonstrated the effectiveness of TapMo through extensive experiments. With some further refinement and improvement, TapMo has the potential to become a widely-used tool for animators and researchers.\n\n$$$",
        "gr_score_with": 0.8876712328767123,
        "gr_score_without": 0.8874172185430463,
        "output_without": {
            "num_tokens_scored": 302,
            "num_green_tokens": 268,
            "green_fraction": 0.8874172185430463,
            "z_score": 3.798099800626272,
            "p_value": 7.290480866874135e-05
        },
        "output_with": {
            "num_tokens_scored": 365,
            "num_green_tokens": 324,
            "green_fraction": 0.8876712328767123,
            "z_score": 4.187640324926219,
            "p_value": 1.409348727207542e-05
        }
    },
    {
        "title": "DiLu : A Knowledge-Driven Approach to Autonomous Driving with Large Language Models",
        "abstract": "Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models (LLMs) with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu\u2019s capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems. To the best of our knowledge, we are the first to leverage knowledge-driven capability in decision-making for autonomous vehicles. Through the proposed DiLu framework, LLM is strengthened to apply knowledge and to reason causally in the autonomous driving domain. Project page: https://pjlab-adg.github.io/DiLu/",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework called DiLu, which integrates an interactive environment, a driver agent, and a memory component to address the challenges of autonomous driving. DiLu leverages large language models (LLMs) to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. The framework consists of a Reasoning and a Reflection module, which enable the system to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. The paper presents extensive experiments that prove DiLu's capability to accumulate experience and generalize to different environments.\n\nStrengths:\n\n* The paper proposes a novel framework that addresses the challenges of autonomous driving by leveraging LLMs and a memory component.\n* The framework is designed to enable the system to perform decision-making based on common-sense knowledge and evolve continuously.\n* The paper presents extensive experiments that demonstrate the effectiveness of DiLu in accumulating experience and generalizing to different environments.\n* The paper highlights the potential of DiLu to be deployed on practical autonomous driving systems.\n\nWeaknesses:\n\n* The paper assumes that the LLMs used in the framework are pre-trained and available, which may not be the case in real-world scenarios.\n* The paper does not provide a thorough analysis of the decision-making latency of the DiLu framework, which is a critical aspect of autonomous driving systems.\n* The paper does not provide a comparison with other knowledge-driven approaches to autonomous driving.\n* The paper assumes that the memory component is updated continuously, which may not be feasible in real-world scenarios.\n\nSuggestions for Improvement:\n\n* Provide a more thorough analysis of the decision-making latency of the DiLu framework and propose methods to reduce it.\n* Compare DiLu with other knowledge-driven approaches to autonomous driving and highlight its advantages.\n* Provide a more detailed explanation of the memory component and its update process.\n* Discuss the potential limitations of using LLMs in the framework and propose methods to address them.\n\nRecommendation:\n\nThe paper proposes a novel and effective framework for autonomous driving that leverages LLMs and a memory component. While there are some weaknesses and limitations, the paper presents extensive experiments that demonstrate the effectiveness of DiLu. Therefore, I recommend that the paper be accepted for publication, with the suggestion that the authors address the weaknesses and limitations mentioned above.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes the DiLu framework, a novel knowledge-driven approach to autonomous driving using large language models (LLMs). The authors aim to instill human-level knowledge into autonomous driving systems, addressing the limitations of data-driven methods, such as dataset bias, overfitting, and uninterpretability. The DiLu framework integrates an interactive environment, a driver agent, and a memory component, leveraging LLMs for decision-making based on common-sense knowledge and continuous evolution. The authors conduct extensive experiments, demonstrating the framework's capability to accumulate experience, exhibit strong generalization ability, and acquire experiences from real-world datasets.\n\nStrengths:\n\n1. The paper presents a unique perspective on autonomous driving, highlighting the importance of human-level knowledge in decision-making.\n2. The authors propose a novel framework, DiLu, which integrates LLMs with a memory module, allowing for continuous experience accumulation and decision-making.\n3. The experimental results demonstrate the framework's strong generalization ability compared to state-of-the-art reinforcement learning-based methods.\n4. The authors provide a clear explanation of the framework's components, including the Reasoning, Reflection, and Memory modules.\n\nWeaknesses:\n\n1. The paper assumes that LLMs can be directly applied to autonomous driving tasks without extensive fine-tuning or adaptation, which might not be the case in real-world scenarios.\n2. The authors do not provide a comprehensive analysis of the potential risks associated with relying on LLMs in autonomous driving systems, such as hallucinations or biased decision-making.\n3. The framework's decision-making latency of 5-10 seconds might be a significant limitation in real-time autonomous driving applications.\n4. The authors do not provide a clear explanation of how the framework handles edge cases, such as unusual or unexpected scenarios.\n\nSuggestions for Improvement:\n\n1. Conduct more comprehensive experiments to evaluate the framework's performance in real-world scenarios, including edge cases and unusual situations.\n2. Provide a more detailed analysis of the potential risks associated with relying on LLMs in autonomous driving systems.\n3. Investigate methods to reduce the framework's decision-making latency, such as using more efficient LLMs or optimizing the framework's architecture.\n4. Consider incorporating additional components, such as sensor data or expert knowledge, to improve the framework's decision-making capabilities.\n\nRecommendation:\n\nThe DiLu framework presents an innovative approach to autonomous driving, leveraging LLMs for decision-making based on human-level knowledge. While the framework has demonstrated strong generalization ability, there are potential risks associated with relying on LLMs in autonomous driving systems. Further research is needed to address these limitations and ensure the framework's safe and reliable deployment in real-world scenarios.\n\nOverall, the paper presents a valuable contribution to the field of autonomous driving, highlighting the importance of human-level knowledge in decision-making. The authors provide a clear explanation of the framework's components and demonstrate its strong generalization ability. However, further research is needed to address the potential risks associated with relying on LLMs in autonomous driving systems.\n\n$$$",
        "gr_score_with": 0.8253164556962025,
        "gr_score_without": 0.7564935064935064,
        "output_without": {
            "num_tokens_scored": 308,
            "num_green_tokens": 233,
            "green_fraction": 0.7564935064935064,
            "z_score": -1.9085436646611325,
            "p_value": 0.9718395073897262
        },
        "output_with": {
            "num_tokens_scored": 395,
            "num_green_tokens": 326,
            "green_fraction": 0.8253164556962025,
            "z_score": 1.2581817198095835,
            "p_value": 0.10416302223004542
        }
    },
    {
        "title": "RTFS-NET: RECURRENT TIME-FREQUENCY MOD-",
        "abstract": "Audio-visual speech separation methods aim to integrate different modalities to generate high-quality separated speech, thereby enhancing the performance of downstream tasks such as speech recognition. Most existing state-of-the-art (SOTA) models operate in the time domain. However, their overly simplistic approach to modeling acoustic features often necessitates larger and more computationally intensive models in order to achieve SOTA performance. In this paper, we present a novel time-frequency domain audio-visual speech separation method: Recurrent Time-Frequency Separation Network (RTFS-Net), which applies its algorithms on the complex time-frequency bins yielded by the Short-Time Fourier Transform. We model and capture the time and frequency dimensions of the audio independently using a multi-layered RNN along each dimension. Furthermore, we introduce a unique attention-based fusion technique for the efficient integration of audio and visual information, and a new mask separation approach that takes advantage of the intrinsic spectral nature of the acoustic features for a clearer separation. RTFS-Net outperforms the prior SOTA method in both inference speed and separation quality while reducing the number of parameters by 90% and MACs by 83%. This is the first time-frequency domain audio-visual speech separation method to outperform all contemporary time-domain counterparts.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel audio-visual speech separation (AVSS) method called RTFS-Net, which operates in the time-frequency domain. The proposed method utilizes a multi-layered recurrent neural network (RNN) to model and capture the time and frequency dimensions of the audio independently. Additionally, it introduces a unique attention-based fusion technique for the efficient integration of audio and visual information, and a new mask separation approach that takes advantage of the intrinsic spectral nature of the acoustic features for a clearer separation. The experimental evaluations conducted on three widely used datasets demonstrate the superiority of RTFS-Net over state-of-the-art (SOTA) methods, achieving better separation quality and reducing computational complexity by 83% and parameter count by 90%.\n\nStrengths:\n\n1. The proposed method, RTFS-Net, is the first time-frequency domain AVSS method to outperform all contemporary T-domain counterparts.\n2. The method achieves remarkable performance improvements while maintaining a significantly reduced computational complexity and parameter count.\n3. The use of a multi-layered RNN to model and capture the time and frequency dimensions of the audio independently is a novel and effective approach.\n4. The attention-based fusion technique and the new mask separation approach are unique and efficient ways to integrate audio and visual information.\n5. The experimental evaluations conducted on multiple datasets demonstrate the superiority of RTFS-Net over SOTA methods.\n\nWeaknesses:\n\n1. The paper assumes that the audio and video inputs are aligned in time, which may not always be the case in real-world scenarios.\n2. The method relies on a pre-trained video encoder, which may not be available for all applications.\n3. The experimental evaluations were conducted on a limited number of datasets, and it would be beneficial to evaluate the method on a larger variety of datasets.\n4. The paper does not provide a detailed analysis of the computational complexity of the method, which may be a concern for large-scale applications.\n5. The code for RTFS-Net is not provided, which makes it difficult for readers to reproduce the results.\n\nSuggestions for Improvement:\n\n1. Investigate methods to align the audio and video inputs in time, even if they are not perfectly aligned.\n2. Provide a more detailed analysis of the computational complexity of the method and explore ways to reduce it further.\n3. Evaluate the method on a larger variety of datasets to demonstrate its robustness and generalizability.\n4. Provide a more detailed explanation of the attention-based fusion technique and the new mask separation approach.\n5. Provide a more detailed comparison with SOTA methods and highlight the advantages of RTFS-Net over these methods.\n\nRecommendation:\n\nThe paper presents a novel and effective AVSS method that outperforms SOTA methods. However, it has some limitations that need to be addressed. With some improvements, RTFS-Net has the potential to become a leading method in the field of AVSS. I recommend this paper for publication in a reputable conference, and I hope that the authors will address the limitations and suggestions for improvement.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel audio-visual speech separation (AVSS) method, Recurrent Time-Frequency Separation Network (RTFS-Net), which operates in the time-frequency domain. RTFS-Net employs a multi-layered RNN to model the time and frequency dimensions of audio independently, a unique attention-based fusion technique for integrating audio and visual information, and a new mask separation approach that takes advantage of the intrinsic spectral nature of acoustic features. The authors conduct comprehensive experiments on three widely used datasets, LRS2, LRS3, and VoxCeleb2, and demonstrate the superiority of RTFS-Net over contemporary T-domain methods in terms of separation quality, inference speed, and computational complexity.\n\nStrengths:\n\n1. RTFS-Net is the first TF-domain AVSS method to outperform all contemporary T-domain methods, achieving this while reducing computational complexity by 83% and parameter count by 90%.\n2. The paper presents a thorough analysis of the challenges faced by TF-domain AVSS methods, including the need to model both time and frequency dimensions independently, the importance of visual information, and the loss of amplitude and phase information during feature extraction.\n3. The authors propose a novel architecture, RTFS-Net, which addresses these challenges through a combination of RNNs, attention-based fusion, and a new mask separation approach.\n4. The paper includes a comprehensive evaluation of RTFS-Net on three widely used datasets, demonstrating its superiority over contemporary T-domain methods.\n\nWeaknesses:\n\n1. The paper assumes a relatively simple audio encoder, which may not be optimal for more complex audio scenarios.\n2. The authors do not provide a detailed analysis of the computational complexity of RTFS-Net, which may be important for large-scale deployments.\n3. While the paper presents a thorough evaluation of RTFS-Net, it would be beneficial to include additional experiments, such as comparisons with other TF-domain AVSS methods or investigations of the robustness of RTFS-Net to different types of noise or distortion.\n\nSuggestions for Improvement:\n\n1. Consider incorporating more complex audio encoders, such as those based on convolutional neural networks (CNNs) or recurrent neural networks (RNNs), to improve the performance of RTFS-Net in more complex audio scenarios.\n2. Provide a more detailed analysis of the computational complexity of RTFS-Net, including a comparison with contemporary T-domain methods, to demonstrate its efficiency in large-scale deployments.\n3. Include additional experiments to evaluate the robustness of RTFS-Net to different types of noise or distortion, such as background noise, music, or other extraneous audio sources.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to audio-visual speech separation, RTFS-Net, which outperforms contemporary T-domain methods in terms of separation quality, inference speed, and computational complexity. While there are some weaknesses in the paper, including the assumption of a simple audio encoder and the lack of a detailed analysis of computational complexity, the authors present a thorough evaluation of RTFS-Net and demonstrate its superiority over contemporary methods. Therefore, I recommend this paper for publication in the conference.\n\n$$$",
        "gr_score_with": 0.8238095238095238,
        "gr_score_without": 0.7643020594965675,
        "output_without": {
            "num_tokens_scored": 437,
            "num_green_tokens": 334,
            "green_fraction": 0.7643020594965675,
            "z_score": -1.8652752093549814,
            "p_value": 0.9689285809063096
        },
        "output_with": {
            "num_tokens_scored": 420,
            "num_green_tokens": 346,
            "green_fraction": 0.8238095238095238,
            "z_score": 1.2201803990246367,
            "p_value": 0.11119824780965593
        }
    },
    {
        "title": "SOHES: SELF-SUPERVISED OPEN-WORLD HIERARCHICAL ENTITY SEGMENTATION",
        "abstract": "Open-world entity segmentation, as an emerging computer vision task, aims at segmenting entities in images without being restricted by pre-defined classes, offering impressive generalization capabilities on unseen images and concepts. Despite its promise, existing entity segmentation methods like Segment Anything Model (SAM) rely heavily on costly expert annotators. This work presents Self-supervised Open-world Hierarchical Entity Segmentation (SOHES), a novel approach that sidesteps the need for human annotations. SOHES operates in three phases: self-exploration, self-instruction, and self-correction. Given a pretrained self-supervised representation, we produce abundant high-quality pseudolabels through visual feature clustering. Then, we train a segmentation model on the pseudo-labels, and rectify the noises in pseudo-labels via a teacher-student mutual-learning procedure. Beyond segmenting entities, SOHES also captures their constituent parts, providing a hierarchical understanding of visual entities. Using raw images as the sole training data, our method achieves unprecedented performance in self-supervised open-world segmentation, marking a significant milestone towards high-quality open-world entity segmentation in the absence of human-annotated masks.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel self-supervised approach, called Self-supervised Open-world Hierarchical Entity Segmentation (SOHES), for open-world entity segmentation. SOHES operates in three phases: self-exploration, self-instruction, and self-correction. In the self-exploration phase, the method generates initial pseudo-labels by clustering visual features from a pre-trained self-supervised representation. In the self-instruction phase, a segmentation model is trained on the initial pseudo-labels to refine the segmentation. Finally, in the self-correction phase, a teacher-student mutual-learning framework is employed to further improve the segmentation model. SOHES achieves state-of-the-art performance on various datasets, including MS-COCO, LVIS, ADE20K, EntitySeg, and SA-1B, and significantly closes the gap between self-supervised and supervised methods.\n\nStrengths:\n\n1. SOHES is a novel and effective approach for open-world entity segmentation, which has the potential to revolutionize the field of computer vision.\n2. The method is self-supervised, which means that it can learn from unlabeled data, making it more efficient and cost-effective than traditional supervised learning approaches.\n3. SOHES can learn hierarchical structures of visual entities, which is a unique feature of this approach.\n4. The method achieves state-of-the-art performance on various datasets, including those with complex scenes and diverse classes.\n5. The paper provides a thorough evaluation of the method, including ablation studies and comparisons with state-of-the-art methods.\n\nWeaknesses:\n\n1. The method requires a large amount of unlabeled data, which may not be readily available in all scenarios.\n2. The self-exploration phase can be computationally expensive, which may limit the scalability of the method.\n3. The teacher-student mutual-learning framework used in the self-correction phase may not be suitable for all types of data and may require additional hyperparameter tuning.\n4. The paper does not provide a detailed analysis of the robustness and generalizability of the method to different types of data and scenarios.\n5. The method may not be suitable for real-time applications due to its computational complexity.\n\nSuggestions for Improvement:\n\n1. Investigate the use of more efficient clustering algorithms in the self-exploration phase to reduce computational complexity.\n2. Explore the use of different teacher-student mutual-learning frameworks to improve the performance of the method.\n3. Provide a more detailed analysis of the robustness and generalizability of the method to different types of data and scenarios.\n4. Investigate the use of transfer learning to adapt the method to new domains and scenarios.\n5. Provide a more detailed evaluation of the method's performance on real-world datasets and scenarios.\n\nRecommendation:\n\nThe paper presents a novel and effective approach for open-world entity segmentation, which has the potential to revolutionize the field of computer vision. The method is self-supervised, which makes it more efficient and cost-effective than traditional supervised learning approaches. However, the method requires a large amount of unlabeled data and may not be suitable for real-time applications due to its computational complexity. Overall, the paper is well-written and provides a thorough evaluation of the method, but it requires additional work to address the weaknesses and limitations of the approach.\n\nRating: 8/10\n\nRecommendation for Publication: Yes\n\nRecommendation for Future Work: Yes\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel self-supervised approach to open-world entity segmentation, titled Self-Supervised Open-World Hierarchical Entity Segmentation (SOHES). The method consists of three phases: self-exploration, self-instruction, and self-correction. In the self-exploration phase, the model generates initial pseudo-labels by clustering visual features from a pre-trained self-supervised representation. The self-instruction phase trains a segmentation model on the initial pseudo-labels to refine the segmentation. In the self-correction phase, a teacher-student mutual-learning framework is employed to further improve the model's performance. The proposed method achieves state-of-the-art performance in self-supervised open-world segmentation and demonstrates its ability to segment entities and their constituent parts in an open world.\n\nStrengths:\n\n1. The paper proposes a novel self-supervised approach to open-world entity segmentation, which is a challenging task.\n2. The method consists of three phases, each of which is well-motivated and effectively addresses a different aspect of the task.\n3. The paper provides a thorough evaluation of the proposed method on various datasets and demonstrates its superiority over existing methods.\n4. The method is able to segment entities and their constituent parts in an open world, which is a significant advancement in the field of computer vision.\n5. The paper provides a clear and concise explanation of the method and its components, making it easy to follow and understand.\n\nWeaknesses:\n\n1. The paper assumes that the pre-trained self-supervised representation is available, which may not always be the case in practice.\n2. The method requires a large number of images for training, which may not be feasible in some scenarios.\n3. The paper does not provide a clear explanation of how the teacher-student mutual-learning framework is implemented and why it is effective.\n4. The paper does not provide a thorough analysis of the computational complexity of the method and its potential bottlenecks.\n5. The paper assumes that the ground-truth annotations are available for evaluation, which may not always be the case in practice.\n\nSuggestions for Improvement:\n\n1. Provide a clear explanation of how to obtain a pre-trained self-supervised representation and how to adapt it to the task of open-world entity segmentation.\n2. Provide a thorough analysis of the computational complexity of the method and its potential bottlenecks.\n3. Provide a clear explanation of how the teacher-student mutual-learning framework is implemented and why it is effective.\n4. Provide a thorough evaluation of the method on a wider range of datasets and scenarios.\n5. Provide a clear explanation of how to handle cases where the ground-truth annotations are not available.\n\nRecommendation:\n\nThe paper proposes a novel and effective self-supervised approach to open-world entity segmentation. The method consists of three phases and demonstrates its ability to segment entities and their constituent parts in an open world. The paper provides a thorough evaluation of the proposed method on various datasets and demonstrates its superiority over existing methods. However, there are some weaknesses and suggestions for improvement that need to be addressed. With some modifications and improvements, the proposed method has the potential to become a state-of-the-art approach to open-world entity segmentation.\n\nRecommendation: Accept with minor revisions.",
        "gr_score_with": 0.8140161725067385,
        "gr_score_without": 0.7605633802816901,
        "output_without": {
            "num_tokens_scored": 426,
            "num_green_tokens": 324,
            "green_fraction": 0.7605633802816901,
            "z_score": -2.0345610018720186,
            "p_value": 0.9790524676379269
        },
        "output_with": {
            "num_tokens_scored": 371,
            "num_green_tokens": 302,
            "green_fraction": 0.8140161725067385,
            "z_score": 0.6752188323863081,
            "p_value": 0.24976837173815514
        }
    },
    {
        "title": "PRE-TRAINING WITH SYNTHETIC DATA HELPS OFFLINE REINFORCEMENT LEARNING",
        "abstract": "Recently, it has been shown that for offline deep reinforcement learning (DRL), pre-training Decision Transformer with a large language corpus can improve downstream performance (Reid et al., 2022). A natural question to ask is whether this performance gain can only be achieved with language pre-training, or can be achieved with simpler pre-training schemes which do not involve language. In this paper, we first show that language is not essential for improved performance, and indeed pre-training with synthetic IID data for a small number of updates can match the performance gains from pre-training with a large language corpus; moreover, pre-training with data generated by a one-step Markov chain can further improve the performance. Inspired by these experimental results, we then consider pre-training Conservative Q-Learning (CQL), a popular offline DRL algorithm, which is Q-learning-based and typically employs a Multi-Layer Perceptron (MLP) backbone. Surprisingly, pre-training with simple synthetic data for a small number of updates can also improve CQL, providing consistent performance improvement on D4RL Gym locomotion datasets. The results of this paper not only illustrate the importance of pre-training for offline DRL but also show that the pre-training data can be synthetic and generated with remarkably simple mechanisms.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper explores the effectiveness of pre-training offline deep reinforcement learning (DRL) models using synthetic data, rather than language data. The authors show that pre-training Decision Transformer (DT) with synthetic data generated from a one-step Markov Chain can match or even surpass the performance of pre-training with a large language corpus. They also demonstrate that pre-training Conservative Q-Learning (CQL) with synthetic Markov Decision Process (MDP) data can provide a significant performance boost. The results suggest that synthetic data can be a surprisingly effective approach to improve offline DRL performance for both transformer-based and Q-learning-based algorithms.\n\nStrengths:\n\n* The paper provides a clear and concise overview of the related work and the contributions of the paper.\n* The authors conduct extensive ablations to demonstrate the robustness of synthetic pre-training over different settings of the synthetic data.\n* The results are presented in a clear and organized manner, with tables and figures that facilitate easy comparison of the different methods.\n* The authors provide theoretical insights into why IID data can still achieve a good performance, which adds depth to the understanding of the results.\n\nWeaknesses:\n\n* The paper assumes that the reader is familiar with the basics of DRL and reinforcement learning, which may make it difficult for non-experts to understand the paper.\n* The authors do not provide a clear explanation of why pre-training with synthetic data is effective, which is a limitation of the paper.\n* The results are based on a limited set of experiments, which may not be representative of all possible scenarios.\n* The paper does not provide a detailed comparison with other pre-training methods, such as pre-training with generic image data or using offline DRL data itself to learn representations.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of why pre-training with synthetic data is effective, including a discussion of the potential benefits of using synthetic data for pre-training.\n* Conduct more extensive experiments to demonstrate the robustness of synthetic pre-training over different settings of the synthetic data.\n* Provide a more detailed comparison with other pre-training methods, including a discussion of the potential advantages and disadvantages of each method.\n* Consider using more diverse and challenging datasets to evaluate the performance of the pre-training methods.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the field of offline DRL by demonstrating the effectiveness of pre-training with synthetic data. However, the paper has some limitations, such as the lack of a clear explanation of why pre-training with synthetic data is effective and the limited scope of the experiments. Overall, I would recommend this paper for publication, but with the suggestion that the authors address the limitations mentioned above.\n\nGrade: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper explores the effectiveness of pre-training offline Deep Reinforcement Learning (DRL) models using synthetic data, specifically Markov Chain (MC) and Markov Decision Process (MDP) data. The authors demonstrate that pre-training Decision Transformer (DT) and Conservative Q-Learning (CQL) with synthetic data can significantly improve their performance compared to standard training without pre-training, and even outperform pre-training with a large language corpus like Wikipedia. The authors also show the robustness of synthetic pre-training over different settings of the synthetic data, including the degree of past dependence, MC state-space size, and the degree of randomness in the transitions.\n\nStrengths:\n\n1. The paper presents a simple yet effective synthetic pre-training scheme for DT and CQL, which challenges the previous view that language pre-training can provide unique benefits for DRL.\n2. The authors provide extensive ablation studies to show the robustness of synthetic pre-training over different settings of the synthetic data.\n3. The paper presents theoretical insights into why IID data can still achieve a good performance, showing that the forward dynamics objective is equivalent to finding the state centroids underlying the synthetic dataset.\n4. The authors demonstrate the efficiency and ease of use of synthetic pre-training, requiring significantly fewer pre-train updates and computation time compared to standard pre-training methods.\n\nWeaknesses:\n\n1. The paper assumes a fixed transition matrix for the MC and MDP data, which may not be realistic for many real-world scenarios where the transition dynamics are uncertain or changing over time.\n2. The authors do not investigate the impact of synthetic pre-training on the robustness and stability of the trained models, which is an important aspect of offline DRL.\n3. The paper focuses primarily on the performance of DT and CQL, and it is unclear how the synthetic pre-training scheme would generalize to other offline DRL algorithms.\n\nSuggestions for Improvement:\n\n1. Investigate the impact of synthetic pre-training on the robustness and stability of the trained models, and explore methods to improve their robustness and stability.\n2. Consider using more realistic transition matrices for the MC and MDP data, and explore the impact of uncertainty and changing transition dynamics on the performance of synthetic pre-training.\n3. Investigate the generalizability of synthetic pre-training to other offline DRL algorithms, and explore the impact of synthetic pre-training on their performance.\n\nRecommendation:\n\nThe paper presents a significant contribution to the field of offline DRL, demonstrating the effectiveness of synthetic pre-training for improving the performance of DT and CQL. The authors provide extensive ablation studies and theoretical insights, and demonstrate the efficiency and ease of use of synthetic pre-training. While there are some limitations and weaknesses in the paper, it is a well-written and well-structured paper that presents a compelling argument for the use of synthetic pre-training in offline DRL.\n\nOverall, I recommend accepting this paper for publication in a top-tier conference in the field of machine learning or reinforcement learning.\n\nScore: 8/10",
        "gr_score_with": 0.8617283950617284,
        "gr_score_without": 0.8381201044386423,
        "output_without": {
            "num_tokens_scored": 383,
            "num_green_tokens": 321,
            "green_fraction": 0.8381201044386423,
            "z_score": 1.8653462370067906,
            "p_value": 0.031066444027457674
        },
        "output_with": {
            "num_tokens_scored": 405,
            "num_green_tokens": 349,
            "green_fraction": 0.8617283950617284,
            "z_score": 3.1059274638226473,
            "p_value": 0.0009484161962238129
        }
    },
    {
        "title": "CROSSQ: BATCH NORMALIZATION",
        "abstract": "Sample efficiency is a crucial problem in deep reinforcement learning. Recent algorithms, such as REDQ and DroQ, found a way to improve the sample efficiency by increasing the update-to-data (UTD) ratio to 20 gradient update steps on the critic per environment sample. However, this comes at the expense of a greatly increased computational cost. To reduce this computational burden, we introduce CrossQ: A lightweight algorithm for continuous control tasks that makes careful use of Batch Normalization and removes target networks to surpass the current state-of-the-art in sample efficiency while maintaining a low UTD ratio of 1. Notably, CrossQ does not rely on advanced bias-reduction schemes used in current methods. CrossQ\u2019s contributions are threefold: (1) it matches or surpasses current state-of-the-art methods in terms of sample efficiency, (2) it substantially reduces the computational cost compared to REDQ and DroQ, (3) it is easy to implement, requiring just a few lines of code on top of SAC.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces CrossQ, a lightweight algorithm for continuous control tasks that achieves superior performance by removing much of the algorithmic design complexity that was added over the years. CrossQ uses Batch Normalization and removes target networks to surpass the current state-of-the-art in sample efficiency while maintaining a low update-to-data (UTD) ratio of 1. The authors present three design choices that constitute CrossQ: removing target networks, using Batch Normalization, and wider critic networks. They provide empirical investigations and hypotheses for CrossQ's success and show that CrossQ matches or surpasses the state-of-the-art algorithms in sample efficiency while being up to 4\u00d7 faster in terms of wallclock time.\n\nStrengths:\n\n* The paper presents a new algorithm, CrossQ, that achieves superior performance in continuous control tasks.\n* The authors provide a thorough analysis of the design choices that constitute CrossQ and their impact on the algorithm's performance.\n* The paper includes a comprehensive set of experiments that demonstrate the effectiveness of CrossQ in various environments.\n* The authors provide a clear and concise description of the algorithm and its components.\n\nWeaknesses:\n\n* The paper assumes a good understanding of deep reinforcement learning and its components, which may make it challenging for readers without a background in the field.\n* The authors do not provide a detailed theoretical analysis of the algorithm's performance, which may limit its applicability to other domains.\n* The paper focuses on continuous control tasks, and it is unclear whether CrossQ can be extended to other domains, such as discrete control tasks or multi-agent systems.\n\nSuggestions for Improvement:\n\n* Provide a more detailed explanation of the design choices that constitute CrossQ and their impact on the algorithm's performance.\n* Include a more comprehensive set of experiments that demonstrate the effectiveness of CrossQ in various environments.\n* Provide a detailed theoretical analysis of the algorithm's performance, including a discussion of its convergence properties and stability.\n* Consider extending CrossQ to other domains, such as discrete control tasks or multi-agent systems.\n\nRecommendation:\n\nThe paper presents a new algorithm, CrossQ, that achieves superior performance in continuous control tasks. The authors provide a thorough analysis of the design choices that constitute CrossQ and their impact on the algorithm's performance. While the paper has some limitations, it is a valuable contribution to the field of deep reinforcement learning. I recommend accepting this paper for publication in a top-tier conference or journal.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper introduces CrossQ, a lightweight algorithm for continuous control tasks that makes careful use of Batch Normalization and removes target networks to surpass the current state-of-the-art in sample efficiency while maintaining a low UTD ratio of 1. The authors claim that CrossQ is computationally more efficient than REDQ and DroQ, and matches or surpasses their performance in terms of sample efficiency. CrossQ's contributions are threefold: it matches or surpasses current state-of-the-art methods in terms of sample efficiency, it substantially reduces the computational cost compared to REDQ and DroQ, and it is easy to implement, requiring just a few lines of code on top of SAC.\n\nStrengths:\n\n1. The authors provide a clear and concise overview of the existing state-of-the-art methods, REDQ and DroQ, and highlight the limitations of these methods.\n2. The authors introduce a new algorithm, CrossQ, which is computationally more efficient and matches or surpasses the performance of REDQ and DroQ in terms of sample efficiency.\n3. The authors provide a thorough analysis of the design choices in CrossQ, including the removal of target networks, the use of Batch Normalization, and wider critic networks.\n4. The authors conduct a series of ablation studies to investigate the impact of different design choices on the performance of CrossQ.\n\nWeaknesses:\n\n1. The authors claim that CrossQ is computationally more efficient than REDQ and DroQ, but they do not provide a thorough analysis of the computational cost of CrossQ.\n2. The authors only evaluate CrossQ on a limited set of environments, and it is unclear whether CrossQ will perform well on more complex tasks.\n3. The authors do not provide a clear explanation of why Batch Normalization is effective in CrossQ, and it is unclear whether this is a generalizable result.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more thorough analysis of the computational cost of CrossQ, including a comparison with REDQ and DroQ.\n2. The authors should evaluate CrossQ on a more diverse set of environments to ensure that it generalizes well to different tasks.\n3. The authors should provide a more detailed explanation of why Batch Normalization is effective in CrossQ, and investigate whether this result is generalizable to other algorithms.\n\nRecommendation:\n\nThis paper is a significant contribution to the field of deep reinforcement learning, as it introduces a new algorithm, CrossQ, which is computationally more efficient and matches or surpasses the performance of existing state-of-the-art methods. However, to fully evaluate the impact of CrossQ, the authors should provide a more thorough analysis of the computational cost and evaluate CrossQ on a more diverse set of environments.",
        "gr_score_with": 0.8057324840764332,
        "gr_score_without": 0.7587301587301587,
        "output_without": {
            "num_tokens_scored": 315,
            "num_green_tokens": 239,
            "green_fraction": 0.7587301587301587,
            "z_score": -1.8308693768700388,
            "p_value": 0.9664399789525553
        },
        "output_with": {
            "num_tokens_scored": 314,
            "num_green_tokens": 253,
            "green_fraction": 0.8057324840764332,
            "z_score": 0.2542230414701789,
            "p_value": 0.3996616253548189
        }
    },
    {
        "title": "Never Train from Scratch: FAIR COMPARISON OF LONG- SEQUENCE MODELS REQUIRES DATA-DRIVEN PRIORS",
        "abstract": "Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using only the downstream task data, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 absolute points. Subsequently, we analyze the utility of previously-proposed structured parameterizations for SSMs and show they become mostly redundant in the presence of data-driven initialization obtained through pretraining. Our work shows that, when evaluating different architectures on supervised tasks, incorporation of data-driven priors via pretraining is essential for reliable performance estimation, and can be done efficiently.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper presents a novel approach to evaluate the performance of long-range sequence models by incorporating self-supervised pretraining on the task data itself. The authors argue that training models from scratch using random initialization can lead to gross overestimation of the differences between architectures, and that pretraining with standard denoising objectives using only the downstream task data can lead to dramatic gains across multiple architectures. The paper demonstrates the effectiveness of self-supervised pretraining on the Long Range Arena (LRA) benchmark, a standard benchmark for long sequence modeling, and shows that pretraining leads to large empirical gains over previously-reported performances on LRA across a range of architectures. The authors also analyze the utility of structured parameterizations for State Space Models (SSMs) and show that they become mostly redundant in the presence of data-driven initialization obtained through pretraining.\n\nStrengths:\n\n* The paper presents a novel and efficient approach to evaluate the performance of long-range sequence models.\n* The authors provide a comprehensive array of experiments to demonstrate the effectiveness of self-supervised pretraining on the LRA benchmark.\n* The paper highlights the importance of accounting for the pretraining stage while designing and evaluating novel architectures.\n* The authors provide a thorough analysis of the utility of structured parameterizations for SSMs and show that they become mostly redundant in the presence of data-driven initialization obtained through pretraining.\n\nWeaknesses:\n\n* The paper assumes that the downstream task data is available, which may not be the case in all scenarios.\n* The authors do not provide a detailed analysis of the computational overhead of self-supervised pretraining.\n* The paper does not provide a thorough comparison with other pretraining methods, such as supervised pretraining or transfer learning.\n* The authors do not provide a clear explanation of how to choose the pretraining objective and the number of pretraining iterations.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the computational overhead of self-supervised pretraining and discuss ways to reduce it.\n* Compare self-supervised pretraining with other pretraining methods, such as supervised pretraining or transfer learning, to demonstrate its effectiveness.\n* Provide a clear explanation of how to choose the pretraining objective and the number of pretraining iterations.\n* Discuss the potential limitations of self-supervised pretraining and how to address them.\n\nRecommendation:\n\nThis paper presents a novel and efficient approach to evaluate the performance of long-range sequence models. The authors provide a comprehensive array of experiments to demonstrate the effectiveness of self-supervised pretraining on the LRA benchmark. While there are some weaknesses and limitations to the paper, it makes a significant contribution to the field and has the potential to impact the design and evaluation of novel architectures. I recommend this paper for publication in a top-tier conference or journal.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper explores the importance of pretraining in evaluating the performance of long-sequence models on benchmarks such as Long Range Arena (LRA). The authors argue that training models from scratch, as is common practice, leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives using only the downstream task data leads to dramatic gains across multiple architectures. They demonstrate that vanilla Transformers match the performance of state-of-the-art S4 models on LRA when properly pretrained and that pretraining leads to significant improvements in the performance of State Space models (SSMs). The authors also show that the data-driven priors learned via pretraining become redundant in the presence of hand-crafted modeling biases used in current solutions.\n\nStrengths:\n\n* The paper makes a strong argument for the importance of pretraining in evaluating the performance of long-sequence models.\n* The authors provide a comprehensive array of experiments that demonstrate the effectiveness of self-supervised pretraining on the task data itself.\n* The paper highlights the need to account for the pretraining stage while designing and evaluating novel architectures in the future.\n\nWeaknesses:\n\n* The paper could benefit from a more detailed discussion of the potential limitations and pitfalls of self-supervised pretraining.\n* The authors could provide more insight into the mechanisms by which pretraining leads to improved performance, particularly in the context of long-sequence models.\n* The paper could benefit from a more nuanced discussion of the trade-offs between pretraining and fine-tuning.\n\nSuggestions for Improvement:\n\n* The authors could provide a more detailed discussion of the potential limitations and pitfalls of self-supervised pretraining, such as overfitting or the need for large amounts of data.\n* The authors could provide more insight into the mechanisms by which pretraining leads to improved performance, particularly in the context of long-sequence models.\n* The paper could benefit from a more nuanced discussion of the trade-offs between pretraining and fine-tuning, including the potential for pretraining to overfit the task data.\n\nRecommendation:\n\nOverall, the paper makes a strong case for the importance of pretraining in evaluating the performance of long-sequence models and provides a comprehensive array of experiments that demonstrate the effectiveness of self-supervised pretraining on the task data itself. However, the paper could benefit from a more detailed discussion of the potential limitations and pitfalls of self-supervised pretraining and a more nuanced discussion of the trade-offs between pretraining and fine-tuning.\n\nI recommend accepting the paper for publication, pending revisions to address the suggestions for improvement outlined above.\n\n$$$",
        "gr_score_with": 0.8677966101694915,
        "gr_score_without": 0.7976539589442815,
        "output_without": {
            "num_tokens_scored": 341,
            "num_green_tokens": 272,
            "green_fraction": 0.7976539589442815,
            "z_score": -0.10801685071037054,
            "p_value": 0.543008837258166
        },
        "output_with": {
            "num_tokens_scored": 295,
            "num_green_tokens": 256,
            "green_fraction": 0.8677966101694915,
            "z_score": 2.9113463325326676,
            "p_value": 0.0017993743848342036
        }
    },
    {
        "title": "LEMUR: INTEGRATING LARGE LANGUAGE MODELS",
        "abstract": "The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that demands high-level abstract reasoning about program properties that is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework, LEMUR, that integrates Large Language Models (LLMs) with automated reasoners for automated program verification. The framework combines the strengths of LLMs, which can perform high-level abstract reasoning, and automated reasoners, which can perform precise low-level reasoning. The authors present a formal calculus, LEMUR, that operates over a configuration, which is either a distinguished symbol or a tuple of a program, an assumption, and a list of properties. The calculus performs oracle calls to LLMs to propose new properties and revise them. The authors also present a sound and terminating algorithm based on LEMUR, which can be used to instantiate the calculus as an automated decision procedure.\n\nStrengths:\n\n1. The paper proposes a novel and innovative framework that combines the strengths of LLMs and automated reasoners for automated program verification.\n2. The authors present a formal calculus, LEMUR, that provides a theoretical foundation for the integration of LLMs and automated reasoners.\n3. The paper presents a sound and terminating algorithm based on LEMUR, which can be used to instantiate the calculus as an automated decision procedure.\n4. The authors demonstrate the practicality of LEMUR by instantiating it as a fully automated framework and evaluating its efficiency on standard benchmark sets.\n\nWeaknesses:\n\n1. The paper assumes that the LLMs can generate correct and useful properties, which may not always be the case.\n2. The authors do not provide a thorough evaluation of the performance of LEMUR on a wide range of benchmarks.\n3. The paper assumes that the automated reasoners can perform precise low-level reasoning, which may not always be the case.\n4. The authors do not provide a clear explanation of how the LEMUR calculus can be used to handle complex properties and large programs.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more thorough evaluation of the performance of LEMUR on a wide range of benchmarks.\n2. The paper should provide a clearer explanation of how the LEMUR calculus can be used to handle complex properties and large programs.\n3. The authors should provide a more detailed description of the LLMs used in the paper and their limitations.\n4. The paper should provide a more thorough discussion of the strengths and weaknesses of the LEMUR framework compared to other automated program verification approaches.\n\nRecommendation:\n\nThe paper proposes a novel and innovative framework that combines the strengths of LLMs and automated reasoners for automated program verification. The authors present a formal calculus, LEMUR, that provides a theoretical foundation for the integration of LLMs and automated reasoners. The paper also presents a sound and terminating algorithm based on LEMUR, which can be used to instantiate the calculus as an automated decision procedure. However, the paper assumes that the LLMs can generate correct and useful properties, which may not always be the case. Therefore, the paper should provide a more thorough evaluation of the performance of LEMUR on a wide range of benchmarks and provide a clearer explanation of how the LEMUR calculus can be used to handle complex properties and large programs.\n\nIn summary, the paper has the potential to make a significant contribution to the field of automated program verification, but it requires further evaluation and refinement to be considered a complete and reliable solution.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes a novel framework, LEMUR, that integrates Large Language Models (LLMs) with automated reasoners for automated program verification. The framework, LEMUR, is designed to leverage the high-level abstract reasoning capabilities of LLMs to propose new properties, which are then checked by automated reasoners. The authors present a formal calculus for LEMUR, prove its soundness, and describe an instantiation of the calculus as a sound automated verification procedure. They also present experimental results showing that LEMUR can solve hard benchmarks from the Software-Verification Competition 2023 that were beyond the reach of state-of-the-art conventional program verifiers.\n\nStrengths:\n\n* The paper presents a novel framework that integrates LLMs with automated reasoners for automated program verification, which is a significant contribution to the field.\n* The authors provide a formal calculus for LEMUR, which is an important aspect of the paper.\n* The experimental results show that LEMUR can solve hard benchmarks that were beyond the reach of state-of-the-art conventional program verifiers.\n* The paper highlights the importance of LLMs in augmenting formal reasoning and provides a clear understanding of the challenges and limitations of using LLMs in program verification.\n\nWeaknesses:\n\n* The paper assumes that the LLMs will always generate correct properties, which may not be the case in practice.\n* The paper does not provide a thorough analysis of the scalability of LEMUR to larger programs.\n* The paper relies heavily on the performance of the LLMs, which may not be reliable or consistent.\n* The paper does not provide a clear comparison with other state-of-the-art program verification tools.\n\nSuggestions for Improvement:\n\n* The authors should provide a more thorough analysis of the scalability of LEMUR to larger programs.\n* The authors should investigate the reliability and consistency of the LLMs and provide a more detailed analysis of their performance.\n* The authors should compare LEMUR with other state-of-the-art program verification tools to demonstrate its effectiveness.\n* The authors should provide more experimental results to demonstrate the robustness of LEMUR.\n\nRecommendation:\n\nThis paper presents a novel framework that integrates LLMs with automated reasoners for automated program verification, which is a significant contribution to the field. While the paper has some weaknesses, it provides a clear understanding of the challenges and limitations of using LLMs in program verification. I recommend this paper for publication, but with the suggestion that the authors address the weaknesses mentioned above.\n\nRating: 8/10\n\nReviewers will appreciate the novelty of the framework, the clarity of the presentation, and the experimental results. They may have some concerns about the scalability, reliability, and consistency of the LLMs, as well as the comparison with other state-of-the-art program verification tools. Overall, this is a well-written paper that makes a significant contribution to the field of program verification.",
        "gr_score_with": 0.8424068767908309,
        "gr_score_without": 0.7753164556962026,
        "output_without": {
            "num_tokens_scored": 316,
            "num_green_tokens": 245,
            "green_fraction": 0.7753164556962026,
            "z_score": -1.0966706759814477,
            "p_value": 0.863607310170764
        },
        "output_with": {
            "num_tokens_scored": 349,
            "num_green_tokens": 294,
            "green_fraction": 0.8424068767908309,
            "z_score": 1.9808327422988352,
            "p_value": 0.023805016818113353
        }
    },
    {
        "title": "MOVINGPARTS: MOTION-BASED 3D PART DISCOV-",
        "abstract": "We present MovingParts, a NeRF-based method for dynamic scene reconstruction and part discovery. We consider motion as an important cue for identifying parts, that all particles on the same part share the common motion pattern. From the perspective of fluid simulation, existing deformation-based methods for dynamic NeRF can be seen as parameterizing the scene motion under the Eulerian view, i.e., focusing on specific locations in space through which the fluid flows as time passes. However, it is intractable to extract the motion of constituting objects or parts using the Eulerian view representation. In this work, we introduce the dual Lagrangian view and enforce representations under the Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangian view, we parameterize the scene motion by tracking the trajectory of particles on objects. The Lagrangian view makes it convenient to discover parts by factorizing the scene motion as a composition of part-level rigid motions. Experimentally, our method can achieve fast and high-quality dynamic scene reconstruction from even a single moving camera, and the induced part-based representation allows direct applications of part tracking, animation, 3D scene editing, etc.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\nThe paper presents MovingParts, a novel NeRF-based approach for dynamic scene reconstruction and part discovery. The method considers motion as an important cue for identifying parts and proposes a hybrid approach that learns motion under both the Eulerian and Lagrangian views. The paper introduces a novel motion grouping module that enables unsupervised object part discovery via motion grouping. The method achieves high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods and is the only one that simultaneously achieves part discovery.\n\nStrengths:\n1. The paper presents a novel approach for dynamic scene reconstruction and part discovery, which is a challenging problem in computer vision and graphics.\n2. The method considers motion as an important cue for identifying parts, which is a novel insight in the field.\n3. The paper introduces a hybrid approach that learns motion under both the Eulerian and Lagrangian views, which is a unique contribution.\n4. The method achieves high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods.\n5. The paper provides a thorough evaluation of the method on the D-NeRF 360\u25e6 synthetic dataset and a synthetic dataset with ground-truth motion masks.\n\nWeaknesses:\n1. The paper assumes that the scene is piece-wise rigid, which may not be the case in all scenarios.\n2. The method requires a large number of parameters, which may lead to overfitting.\n3. The paper does not provide a thorough analysis of the time complexity of the method.\n4. The method assumes that the motion is smooth and continuous, which may not be the case in all scenarios.\n5. The paper does not provide a clear explanation of how the method handles occlusions and self-occlusions.\n\nSuggestions for Improvement:\n1. The authors should provide a more thorough analysis of the time complexity of the method.\n2. The authors should provide a more detailed explanation of how the method handles occlusions and self-occlusions.\n3. The authors should provide a more thorough evaluation of the method on real-world datasets.\n4. The authors should provide a more detailed explanation of how the method can be applied to other domains, such as human-object interaction.\n5. The authors should provide a more thorough discussion of the limitations of the method and potential future work.\n\nRecommendation:\nThe paper presents a novel approach for dynamic scene reconstruction and part discovery, which is a challenging problem in computer vision and graphics. The method achieves high-quality dynamic scene reconstruction and realistic rendering results on par with state-of-the-art methods and is the only one that simultaneously achieves part discovery. However, the paper assumes that the scene is piece-wise rigid, which may not be the case in all scenarios, and the method requires a large number of parameters, which may lead to overfitting. Therefore, the paper is recommended for publication after addressing these limitations.\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper presents MovingParts, a novel NeRF-based method for dynamic scene reconstruction and part discovery. The authors introduce a hybrid approach that models scene motion under both the Eulerian and Lagrangian views, enabling high-quality dynamic scene reconstruction and automatic rigid part discovery. The method consists of three modules: a canonical module that models scene geometry and appearance, an Eulerian module that records particle motion, and a Lagrangian module that tracks particle trajectories. The authors also introduce a cycle-consistency loss to enforce reciprocity between the Eulerian and Lagrangian modules. The method is evaluated on the D-NeRF 360\u25e6 synthetic dataset and a synthetic dataset with ground-truth motion masks, demonstrating its ability to achieve high-quality reconstruction and part discovery.\n\nStrengths:\n\n* The paper presents a novel and effective method for dynamic scene reconstruction and part discovery.\n* The hybrid approach that models scene motion under both the Eulerian and Lagrangian views is a key contribution.\n* The use of a cycle-consistency loss to enforce reciprocity between the Eulerian and Lagrangian modules is a clever idea.\n* The method is evaluated on multiple datasets, including a synthetic dataset with ground-truth motion masks.\n* The paper demonstrates the ability of the method to achieve high-quality reconstruction and part discovery.\n\nWeaknesses:\n\n* The paper relies on a complex and computationally expensive model architecture.\n* The method requires a large amount of training data and computational resources.\n* The paper does not provide a clear explanation of how the method can be adapted to real-world scenarios.\n* The method is limited to reconstructing scenes with piece-wise rigid motion.\n* The paper does not provide a clear discussion of the limitations and potential biases of the method.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed explanation of the method's architecture and how it can be adapted to real-world scenarios.\n* The authors should provide more information about the computational resources required to train the model and the time complexity of the method.\n* The authors should provide a clearer discussion of the limitations and potential biases of the method.\n* The authors should provide more information about the datasets used in the evaluation and how they were created.\n* The authors should provide a more detailed explanation of the cycle-consistency loss and how it is used to enforce reciprocity between the Eulerian and Lagrangian modules.\n\nRecommendation:\n\nThe paper presents a novel and effective method for dynamic scene reconstruction and part discovery. However, the method relies on a complex and computationally expensive model architecture, and the paper does not provide a clear explanation of how the method can be adapted to real-world scenarios. Nevertheless, the method demonstrates the ability to achieve high-quality reconstruction and part discovery, and the paper provides a thorough evaluation of the method on multiple datasets. I recommend this paper for acceptance, but with the suggestion that the authors address the limitations and potential biases of the method.\n\n$$$",
        "gr_score_with": 0.8365384615384616,
        "gr_score_without": 0.7551724137931034,
        "output_without": {
            "num_tokens_scored": 290,
            "num_green_tokens": 219,
            "green_fraction": 0.7551724137931034,
            "z_score": -1.9081778439039765,
            "p_value": 0.9718158828834175
        },
        "output_with": {
            "num_tokens_scored": 312,
            "num_green_tokens": 261,
            "green_fraction": 0.8365384615384616,
            "z_score": 1.613751342204663,
            "p_value": 0.05329068628909476
        }
    },
    {
        "title": "EARLY NEURON ALIGNMENT IN TWO-LAYER RELU NETWORKS WITH SMALL INITIALIZATION",
        "abstract": "This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons\u2019 directional dynamics allows us to provide an O( logn \u221a\u03bc ) upper bound on the time it takes for all neurons to achieve good alignment with the input data, where n is the number of data points and \u03bc measures how well the data are separated. After the early alignment phase, the loss converges to zero at a O( 1t ) rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThis paper studies the problem of training a binary classifier via gradient flow on two-layer ReLU networks with small initialization. The authors consider a training dataset with well-separated input vectors and provide a careful analysis of the neurons' directional dynamics. They show that during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. The authors also provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data, which is O(logn\u221a\u00b5). After the early alignment phase, the loss converges to zero at a O(1/t) rate, and the weight matrix on the first layer is approximately low-rank. The authors validate their theoretical findings using a numerical experiment on classifying two digits from the MNIST dataset.\n\nStrengths:\n\n1. The paper provides a complete analysis of the dynamics of gradient flow for training a two-layer ReLU network on well-separated data with small initialization.\n2. The authors provide a formal analysis for the early alignment phase with a finite but small initialization scale \u03f5, which is a significant improvement over previous work.\n3. The paper provides a quantitative bound on the time it takes for good alignment to be achieved, which is O(logn\u221a\u00b5).\n4. The authors validate their theoretical findings using a numerical experiment on classifying two digits from the MNIST dataset.\n\nWeaknesses:\n\n1. The paper assumes that the training dataset has well-separated input vectors, which may not be a realistic assumption in practice.\n2. The authors use a balanced initialization scheme, which may not be a common initialization scheme in practice.\n3. The paper does not provide a detailed comparison with other training schemes, such as gradient descent only on the first-layer weights.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed comparison with other training schemes, such as gradient descent only on the first-layer weights.\n2. The paper should be extended to consider more realistic initialization schemes and training datasets.\n3. The authors should provide more numerical experiments to validate their theoretical findings.\n\nRecommendation:\n\nThis paper provides a significant contribution to the understanding of the dynamics of gradient flow for training two-layer ReLU networks with small initialization. The authors' analysis is thorough and provides a quantitative bound on the time it takes for good alignment to be achieved. However, the paper assumes a balanced initialization scheme and well-separated input vectors, which may not be realistic in practice. Therefore, I recommend that the authors extend their analysis to more realistic initialization schemes and training datasets.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\nThis paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. The authors consider a training dataset with well-separated input vectors and provide a complete analysis of the dynamics of gradient flow for training a two-layer ReLU network on well-separated data with small initialization. They show that during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. The authors also provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data and show that after the early alignment phase, the loss converges to zero at a O(1/t) rate and the weight matrix on the first layer is approximately low-rank.\n\nStrengths:\n1. The paper provides a complete analysis of the dynamics of gradient flow for training a two-layer ReLU network on well-separated data with small initialization.\n2. The authors provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data, which is a significant improvement over previous work.\n3. The paper shows that after the early alignment phase, the loss converges to zero at a O(1/t) rate and the weight matrix on the first layer is approximately low-rank, which is a desirable property for neural networks.\n4. The numerical experiment on classifying two digits from the MNIST dataset correlates with the theoretical findings, which provides strong evidence for the correctness of the analysis.\n\nWeaknesses:\n1. The paper assumes that the data are well-separated, which may not be the case in practice.\n2. The analysis relies on a number of technical assumptions, such as the balancedness condition and the assumption that the data points are not orthogonal to each other.\n3. The paper does not provide a clear explanation for why the alignment phase ends and the fitting phase begins.\n4. The numerical experiment is limited to a simple binary classification task and does not provide a comprehensive evaluation of the method.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed explanation for why the alignment phase ends and the fitting phase begins.\n2. The paper should be extended to include a more comprehensive evaluation of the method, including experiments on more complex datasets and tasks.\n3. The authors should provide a more detailed analysis of the implications of the technical assumptions used in the paper.\n4. The paper should be extended to include a more detailed discussion of the relationship between the alignment phase and the fitting phase.\n\nRecommendation:\nThis paper provides a significant contribution to the field of neural networks by providing a complete analysis of the dynamics of gradient flow for training a two-layer ReLU network on well-separated data with small initialization. The paper shows that the alignment phase ends and the fitting phase begins, which is a desirable property for neural networks. The numerical experiment provides strong evidence for the correctness of the analysis, and the paper provides a clear and concise presentation of the results. Therefore, I recommend this paper for publication in a top-tier conference or journal in the field of neural networks.",
        "gr_score_with": 0.8291316526610645,
        "gr_score_without": 0.7791044776119403,
        "output_without": {
            "num_tokens_scored": 335,
            "num_green_tokens": 261,
            "green_fraction": 0.7791044776119403,
            "z_score": -0.9558305460099322,
            "p_value": 0.8304210734329848
        },
        "output_with": {
            "num_tokens_scored": 357,
            "num_green_tokens": 296,
            "green_fraction": 0.8291316526610645,
            "z_score": 1.3763444622922496,
            "p_value": 0.08435750678500437
        }
    },
    {
        "title": "SIVE NEURAL NETWORK GENERATION",
        "abstract": "Spatio-temporal modeling is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPD, for spatio-temporal few-shot learning with urban knowledge transfer. Unlike conventional approaches that heavily rely on common feature extraction or intricate few-shot learning designs, our solution takes a novel approach by performing generative pre-training on a collection of neural network parameters optimized with data from source cities. We recast spatio-temporal few-shot learning as pre-training a generative diffusion model, which generates tailored neural networks guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPD employs a Transformer-based denoising diffusion model, which is model-agnostic to integrate with powerful spatio-temporal neural networks. By addressing challenges arising from data gaps and the complexity of generalizing knowledge across cities, our framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction. The implementation of our approach is available: https://github.com/tsinghua-fib-lab/GPD.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel generative pre-training framework, called GPD, for spatio-temporal few-shot learning with urban knowledge transfer. The framework leverages a Transformer-based diffusion model to generate tailored neural networks guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. The authors demonstrate that their framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction.\n\nStrengths:\n\n1. The paper presents a novel approach to address the challenges of spatio-temporal few-shot learning, which is a pressing problem in smart city applications.\n2. The authors provide a comprehensive overview of the existing literature on spatio-temporal few-shot learning and highlight the limitations of existing approaches.\n3. The proposed framework is model-agnostic, ensuring compatibility with state-of-the-art spatio-temporal prediction models.\n4. The authors conduct extensive experiments on multiple real-world datasets and demonstrate the effectiveness of their framework.\n\nWeaknesses:\n\n1. The paper assumes that the region prompts are available, which may not always be the case in real-world scenarios.\n2. The authors do not provide a detailed analysis of the hyperparameters and their impact on the performance of the framework.\n3. The paper does not discuss the scalability of the framework to large-scale datasets.\n4. The authors do not provide a detailed comparison of the computational cost of the proposed framework with existing approaches.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the hyperparameters and their impact on the performance of the framework.\n2. The paper should discuss the scalability of the framework to large-scale datasets and provide a detailed comparison of the computational cost with existing approaches.\n3. The authors should provide a more detailed explanation of the region prompts and how they are generated.\n4. The paper should discuss the potential applications of the proposed framework in real-world smart city scenarios.\n\nRecommendation:\n\nThe paper presents a novel and effective approach to address the challenges of spatio-temporal few-shot learning. The proposed framework is model-agnostic, ensuring compatibility with state-of-the-art spatio-temporal prediction models. The authors conduct extensive experiments on multiple real-world datasets and demonstrate the effectiveness of their framework. However, the paper assumes that the region prompts are available, and the authors do not provide a detailed analysis of the hyperparameters and their impact on the performance of the framework. Therefore, I recommend that the authors address these limitations and provide a more detailed comparison of the computational cost with existing approaches.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper proposes a novel generative pre-training framework, called GPD, for spatio-temporal few-shot learning with urban knowledge transfer. The authors aim to address the challenges of data scarcity and heterogeneity in smart city applications. GPD leverages a Transformer-based diffusion model to generate tailored neural networks guided by prompts, enabling adaptability to diverse data distributions and city-specific characteristics. The authors demonstrate the effectiveness of GPD on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction, achieving superior performance compared to state-of-the-art baselines.\n\nStrengths:\n\n1. The paper presents a pioneering approach to address the challenges of data scarcity and heterogeneity in smart city applications.\n2. The authors propose a novel generative pre-training framework, GPD, which leverages a Transformer-based diffusion model to generate tailored neural networks guided by prompts.\n3. The paper provides extensive experiments on multiple real-world datasets, demonstrating the effectiveness of GPD in achieving superior performance compared to state-of-the-art baselines.\n4. The authors demonstrate the model-agnostic nature of GPD, ensuring compatibility with existing urban computing models.\n\nWeaknesses:\n\n1. The paper assumes access to rich data in source cities, which may not be feasible in real-world scenarios.\n2. The authors do not provide a thorough analysis of the prompt design and selection, which is crucial for the success of GPD.\n3. The paper focuses on two specific tasks, traffic speed prediction and crowd flow prediction, and it is unclear how GPD would perform on other spatio-temporal prediction tasks.\n4. The authors do not provide a thorough discussion on the potential limitations and challenges of GPD, such as the need for large-scale computational resources and the potential for overfitting.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more thorough analysis of the prompt design and selection, including the selection of features and the design of the prompt vector.\n2. The paper should provide more details on the experimental setup, including the specific datasets used and the evaluation metrics employed.\n3. The authors should investigate the generalizability of GPD to other spatio-temporal prediction tasks and provide more details on the potential limitations and challenges of the approach.\n4. The paper should provide more details on the computational resources required to train and deploy GPD, and discuss potential strategies for reducing the computational burden.\n\nRecommendation:\n\nI recommend acceptance of this paper for publication in the conference proceedings. The authors have presented a novel and effective approach to addressing the challenges of data scarcity and heterogeneity in smart city applications. The paper provides extensive experiments and demonstrates the effectiveness of GPD on multiple real-world datasets. While there are some limitations and areas for improvement, the paper represents a significant advancement in urban transfer learning and has the potential to revolutionize smart city applications in data-scarce environments.\n\n$$$",
        "gr_score_with": 0.8442622950819673,
        "gr_score_without": 0.7965517241379311,
        "output_without": {
            "num_tokens_scored": 290,
            "num_green_tokens": 231,
            "green_fraction": 0.7965517241379311,
            "z_score": -0.1465377880385817,
            "p_value": 0.5582515693362942
        },
        "output_with": {
            "num_tokens_scored": 366,
            "num_green_tokens": 309,
            "green_fraction": 0.8442622950819673,
            "z_score": 2.1172424783191763,
            "p_value": 0.017119636982646142
        }
    },
    {
        "title": "SPURIOUS FEATURES IN PROMPT DESIGN or: How I learned to start worrying about prompt formatting",
        "abstract": "As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FORMATSPREAD, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights1. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper introduces FORMATSPREAD, an algorithm designed to efficiently search the space of plausible prompt formats under a user-specified computational budget. The authors analyze the sensitivity of widely used, open-source language models (LLMs) to prompt formatting choices, finding that performance spreads across tasks, models, and number of few-shot examples. They propose a grammar that defines the space of plausible prompt formats and semantic equivalence between formats, allowing for the efficient exploration of the space. The authors demonstrate that FORMATSPREAD can be used to estimate the performance spread across prompt formatting choices, and that this spread is large regardless of model choice, even when increasing model size, number of few-shots, or using instruction tuning.\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of the sensitivity of LLMs to prompt formatting choices, which is a crucial aspect of using these models in real-world applications.\n2. The authors propose a novel grammar that defines the space of plausible prompt formats and semantic equivalence between formats, which allows for the efficient exploration of the space.\n3. The paper introduces FORMATSPREAD, an algorithm that can be used to estimate the performance spread across prompt formatting choices, which is a valuable tool for researchers and practitioners.\n4. The authors provide a thorough evaluation of the algorithm on a large set of tasks and models, demonstrating its effectiveness in estimating performance spread.\n\nWeaknesses:\n\n1. The paper assumes that the grammar is manually constructed, which may not be practical for large-scale applications.\n2. The authors do not provide a detailed analysis of the computational cost of FORMATSPREAD, which may be a limitation for large-scale applications.\n3. The paper focuses on classification tasks and does not explore other types of tasks, such as open-ended text generation.\n4. The authors do not provide a detailed analysis of the robustness of FORMATSPREAD to different types of perturbations, such as adversarial attacks.\n\nSuggestions for Improvement:\n\n1. Develop a more automated method for constructing the grammar, which could be based on machine learning algorithms or other techniques.\n2. Provide a more detailed analysis of the computational cost of FORMATSPREAD and explore ways to reduce it.\n3. Explore the use of FORMATSPREAD on other types of tasks, such as open-ended text generation.\n4. Conduct a more thorough analysis of the robustness of FORMATSPREAD to different types of perturbations, such as adversarial attacks.\n\nRecommendation:\n\nThe paper provides a valuable contribution to the field of natural language processing and language modeling, and the authors should be commended for their thorough analysis of the sensitivity of LLMs to prompt formatting choices. However, to make the paper more impactful, the authors should address the limitations mentioned above and provide a more detailed analysis of the algorithm's computational cost and robustness.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper introduces FORMATSPREAD, an algorithm designed to efficiently search the space of plausible prompt formats under a user-specified computational budget. The authors analyze the sensitivity of large language models (LLMs) to prompt formatting choices, finding significant performance spread across tasks, even with increased model size, number of few-shot examples, or instruction tuning. They propose a grammar to define the space of plausible prompt formats, which is used to quantify model sensitivity in terms of performance range in a target task across the space of equivalent prompt formats. The authors also present a suite of analyses to characterize model sensitivity to formatting, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.\n\nStrengths:\n\n1. The paper provides a comprehensive analysis of the sensitivity of LLMs to prompt formatting choices, which is a crucial aspect of effective model usage.\n2. The authors propose a well-defined grammar to describe the space of plausible prompt formats, which allows for a systematic analysis of model sensitivity.\n3. The study demonstrates the significance of considering performance spread across plausible formats, rather than relying on a single format.\n4. The authors present a suite of analyses to characterize model sensitivity to formatting, providing a deeper understanding of the underlying mechanisms.\n\nWeaknesses:\n\n1. The study is limited to a specific set of LLMs and tasks, which may not be representative of all possible scenarios.\n2. The authors rely on a manual construction of the grammar, which may not be feasible for more complex tasks or formats.\n3. The study assumes that the grammar is complete and accurate, which may not be the case in all situations.\n4. The authors do not provide a detailed analysis of the computational cost of FORMATSPREAD, which may be a limitation in certain scenarios.\n\nSuggestions for Improvement:\n\n1. Expand the study to include a broader range of LLMs, tasks, and formats to increase the generalizability of the findings.\n2. Develop a more automated method for constructing the grammar, which could reduce the reliance on manual construction.\n3. Provide a more detailed analysis of the computational cost of FORMATSPREAD, including the time and resources required to run the algorithm.\n4. Consider incorporating additional analysis methods, such as sensitivity analysis or uncertainty quantification, to provide a more comprehensive understanding of model sensitivity.\n\nRecommendation:\n\nThe paper provides a significant contribution to the field of LLMs, highlighting the importance of considering performance spread across plausible formats. While the study has some limitations, the proposed grammar and analysis methods are well-defined and effective. I recommend this paper for publication in a top-tier conference, with the understanding that the authors address the suggested improvements in future work.\n\nOverall, the paper is well-written, and the authors provide a clear and concise presentation of their findings. The study is well-structured, and the authors effectively communicate the significance of their results. However, the limitations of the study should be addressed in future work to increase the generalizability and robustness of the findings.",
        "gr_score_with": 0.8654292343387471,
        "gr_score_without": 0.8248587570621468,
        "output_without": {
            "num_tokens_scored": 354,
            "num_green_tokens": 292,
            "green_fraction": 0.8248587570621468,
            "z_score": 1.1695665251464868,
            "p_value": 0.12108772755073455
        },
        "output_with": {
            "num_tokens_scored": 431,
            "num_green_tokens": 373,
            "green_fraction": 0.8654292343387471,
            "z_score": 3.3961495259705092,
            "p_value": 0.0003417050751323205
        }
    },
    {
        "title": "ERROR-FREE DIFFERENTIABLE SWAP FUNCTIONS",
        "abstract": "Sorting is a fundamental operation of all computer systems, having been a longstanding significant research topic. Beyond the problem formulation of traditional sorting algorithms, we consider sorting problems for more abstract yet expressive inputs, e.g., multi-digit images and image fragments, through a neural sorting network. To learn a mapping from a high-dimensional input to an ordinal variable, the differentiability of sorting networks needs to be guaranteed. In this paper we define a softening error by a differentiable swap function, and develop an error-free swap function that holds a non-decreasing condition and differentiability. Furthermore, a permutation-equivariant Transformer network with multi-head attention is adopted to capture dependency between given inputs and also leverage its model capacity with self-attention. Experiments on diverse sorting benchmarks show that our methods perform better than or comparable to baseline methods.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel sorting network that learns to sort high-dimensional inputs, such as multi-digit images and image fragments, using a neural network-based sorting network with an error-free differentiable swap function (DSF). The proposed method is differentiable, which allows for effective training with a gradient-based learning scheme. The authors also propose an error-free DSF that resolves the error accumulation problem induced by a soft DSF. The paper demonstrates the effectiveness of the proposed method on diverse sorting benchmarks.\n\nStrengths:\n\n* The paper proposes a novel sorting network that learns to sort high-dimensional inputs.\n* The proposed method is differentiable, which allows for effective training with a gradient-based learning scheme.\n* The authors provide a detailed analysis of the error accumulation problem and propose an error-free DSF to resolve it.\n* The paper demonstrates the effectiveness of the proposed method on diverse sorting benchmarks.\n\nWeaknesses:\n\n* The paper assumes that the inputs are high-dimensional and unordered, which may not be the case in all real-world applications.\n* The proposed method may not be effective for sorting low-dimensional inputs or inputs with clear ordering information.\n* The paper does not provide a detailed analysis of the computational complexity of the proposed method.\n* The paper assumes that the supervision is provided in the form of permutation matrices, which may not be available in all cases.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed analysis of the computational complexity of the proposed method.\n* The paper should discuss the limitations of the proposed method and provide examples of real-world applications where it may not be effective.\n* The authors should provide more experimental results to demonstrate the effectiveness of the proposed method on a wider range of benchmarks.\n* The paper should discuss the potential negative societal impact of the proposed method, such as the potential for bias in the sorting process.\n\nRecommendation:\n\nThe paper proposes a novel sorting network that learns to sort high-dimensional inputs using a neural network-based sorting network with an error-free DSF. The proposed method is differentiable, which allows for effective training with a gradient-based learning scheme. The paper demonstrates the effectiveness of the proposed method on diverse sorting benchmarks. However, the paper assumes that the inputs are high-dimensional and unordered, which may not be the case in all real-world applications. Therefore, I recommend that the authors provide a more detailed analysis of the computational complexity of the proposed method and discuss the limitations of the proposed method in the future.\n\nOverall, the paper is well-written and provides a clear and concise introduction to the problem of sorting high-dimensional inputs. The proposed method is novel and effective, and the paper provides a detailed analysis of the error accumulation problem and proposes an error-free DSF to resolve it. However, the paper assumes that the inputs are high-dimensional and unordered, which may not be the case in all real-world applications. Therefore, I recommend that the authors provide a more detailed analysis of the computational complexity of the proposed method and discuss the limitations of the proposed method in the future.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel sorting network that learns to sort high-dimensional inputs, such as multi-digit images and image fragments, using a neural network-based sorting network with an error-free differentiable swap function (DSF). The proposed method is differentiable, which allows for end-to-end training using gradient-based optimization. The authors also propose a permutation-equivariant Transformer network with multi-head attention to capture dependency between high-dimensional inputs. Experiments on diverse sorting benchmarks show that the proposed method performs better than or comparable to baseline methods.\n\nStrengths:\n\n1. The paper proposes a novel sorting network that learns to sort high-dimensional inputs using a neural network-based sorting network with an error-free DSF.\n2. The proposed method is differentiable, which allows for end-to-end training using gradient-based optimization.\n3. The authors also propose a permutation-equivariant Transformer network with multi-head attention to capture dependency between high-dimensional inputs.\n4. Experiments on diverse sorting benchmarks show that the proposed method performs better than or comparable to baseline methods.\n\nWeaknesses:\n\n1. The paper assumes that the input data is high-dimensional, which may not be the case for all sorting tasks.\n2. The proposed method may not be suitable for sorting tasks with large input sizes.\n3. The paper does not provide a detailed analysis of the computational complexity of the proposed method.\n4. The paper assumes that the DSF is differentiable, which may not be the case for all DSFs.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the computational complexity of the proposed method.\n2. The authors should experiment with different DSFs to see how they affect the performance of the proposed method.\n3. The authors should provide more details on how the permutation-equivariant Transformer network with multi-head attention is trained.\n4. The authors should provide a more detailed analysis of the performance of the proposed method on sorting tasks with large input sizes.\n\nRecommendation:\n\nThe paper proposes a novel sorting network that learns to sort high-dimensional inputs using a neural network-based sorting network with an error-free DSF. The proposed method is differentiable, which allows for end-to-end training using gradient-based optimization. Experiments on diverse sorting benchmarks show that the proposed method performs better than or comparable to baseline methods. However, the paper assumes that the input data is high-dimensional, which may not be the case for all sorting tasks. Therefore, the paper requires further experimentation to validate its performance on sorting tasks with different input sizes.\n\nOverall, the paper is well-written and provides a clear explanation of the proposed method. However, it requires further experimentation to validate its performance on sorting tasks with different input sizes.\n\nRecommendation: Accept\n\n$$$",
        "gr_score_with": 0.9033613445378151,
        "gr_score_without": 0.8178438661710037,
        "output_without": {
            "num_tokens_scored": 269,
            "num_green_tokens": 220,
            "green_fraction": 0.8178438661710037,
            "z_score": 0.7319001108630064,
            "p_value": 0.23211476925894653
        },
        "output_with": {
            "num_tokens_scored": 238,
            "num_green_tokens": 215,
            "green_fraction": 0.9033613445378151,
            "z_score": 3.986646844956435,
            "p_value": 3.350682915924868e-05
        }
    },
    {
        "title": "WEAKER MVI CONDITION: EXTRAGRADIENT METH-",
        "abstract": "This paper proposes a new framework of algorithms that is extended from the celebrated extragradient algorithm. The min-max problem has attracted increasing attention because of its applications in machine learning tasks such as generative adversarial networks (GANs) training. While there has been exhaustive research on convex-concave setting, problem of nonconvex-nonconcave setting faces many challenges, such as convergence to limit cycles. Given that general min-max optimization has been found to be intractable, recent research efforts have shifted towards tackling structured problems. One of these follows the weak Minty variational inequality (weak MVI), which is motivated by relaxing Minty variational inequality (MVI) without compromising convergence guarantee of extragradient algorithm. Existing extragradient-type algorithms involve one exploration step and one update step per iteration. We analyze the algorithms with multiple exploration steps and show that current assumption can be further relaxed when more exploration is introduced. Furthermore, we design an adaptive algorithm that explores until the optimal improvement is achieved. This process exploits information from the whole trajectory and effectively tackles cyclic behaviors.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThis paper proposes a new framework for extragradient-type algorithms, which extends the celebrated extragradient algorithm. The authors focus on the weak Minty variational inequality (weak MVI) problem, which is a relaxation of the Minty variational inequality without compromising convergence guarantees. They analyze the algorithms with multiple exploration steps and show that the current assumption can be further relaxed when more exploration is introduced. Furthermore, they design an adaptive algorithm that explores until the optimal improvement is achieved. The paper provides bound analysis on the framework of multi-step extrapolation EG+ algorithms, relaxes the condition \u03c1 > \u22121/2L to \u03c1 > \u2212(1\u22121/e)/L, and captures past algorithms as special cases.\n\nStrengths:\n\n1. The paper provides a new framework for extragradient-type algorithms, which extends the celebrated extragradient algorithm.\n2. The authors analyze the algorithms with multiple exploration steps and show that the current assumption can be further relaxed when more exploration is introduced.\n3. The paper designs an adaptive algorithm that explores until the optimal improvement is achieved.\n4. The paper provides bound analysis on the framework of multi-step extrapolation EG+ algorithms.\n5. The authors relax the condition \u03c1 > \u22121/2L to \u03c1 > \u2212(1\u22121/e)/L, which is a significant improvement.\n\nWeaknesses:\n\n1. The paper assumes that the function f(x,y) is smooth and possibly nonconvex-nonconcave, which may not be the case in practice.\n2. The authors use a tolerance \u03b52 to prevent the algorithm from incorrect convergence, which may not be necessary in some cases.\n3. The paper does not provide a detailed comparison with other algorithms, such as gradient descent ascent and stochastic extragradient.\n4. The authors do not provide a theoretical analysis of the convergence rate of the adaptive algorithm.\n5. The paper assumes that the weak MVI condition holds, which may not be the case in practice.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed comparison with other algorithms, such as gradient descent ascent and stochastic extragradient.\n2. Theoretically analyze the convergence rate of the adaptive algorithm.\n3. Relax the assumption that the function f(x,y) is smooth and possibly nonconvex-nonconcave.\n4. Provide a more detailed explanation of the tolerance \u03b52 and its role in preventing incorrect convergence.\n5. Provide more experimental results to demonstrate the effectiveness of the adaptive algorithm.\n\nRecommendation:\n\nThis paper provides a new framework for extragradient-type algorithms, which extends the celebrated extragradient algorithm. The authors analyze the algorithms with multiple exploration steps and show that the current assumption can be further relaxed when more exploration is introduced. The paper provides bound analysis on the framework of multi-step extrapolation EG+ algorithms, relaxes the condition \u03c1 > \u22121/2L to \u03c1 > \u2212(1\u22121/e)/L, and captures past algorithms as special cases. However, the paper assumes that the function f(x,y) is smooth and possibly nonconvex-nonconcave, and uses a tolerance \u03b52 to prevent the algorithm from incorrect convergence. Therefore, I recommend that the authors provide a more detailed comparison with other algorithms, theoretically analyze the convergence rate of the adaptive algorithm, relax the assumption that the function f(x,y) is smooth and possibly nonconvex-nonconcave, provide a more detailed explanation of the tolerance \u03b52, and provide more experimental results to demonstrate the effectiveness of the adaptive algorithm.\n\nOverall, this paper provides a significant contribution to the field of optimization and machine learning, and I recommend that it be published in a reputable journal.\n\nRating: 8/10\n\nRecommendation: Publish",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel framework for solving weak Minty variational inequality (MVI) type saddle point problems. The framework generalizes the extragradient (EG) algorithm to multi-step cases and provides a bound analysis for the convergence of the algorithm. The authors relax the condition \u03c1 > \u22121/2L to \u03c1 > \u2212(1\u22121/e)/L, which allows for a wider range of problem parameters. The paper also proposes an adaptive algorithm that explores aggressively and can handle problems with limit cycles.\n\nStrengths:\n\n1. The paper provides a novel framework for solving weak MVI type saddle point problems.\n2. The authors relax the condition \u03c1 > \u22121/2L to \ufffdho > \u2212(1\u22121/e)/L, which allows for a wider range of problem parameters.\n3. The paper proposes an adaptive algorithm that explores aggressively and can handle problems with limit cycles.\n4. The authors provide a bound analysis for the convergence of the algorithm.\n5. The paper shows that the proposed algorithm can outperform existing methods in certain cases.\n\nWeaknesses:\n\n1. The paper assumes that the problem parameters are known, which may not be the case in practice.\n2. The authors do not provide a clear explanation of how to select the parameters of the algorithm.\n3. The paper does not provide a detailed analysis of the computational complexity of the algorithm.\n4. The authors do not compare the proposed algorithm with other methods that have been designed for solving weak MVI type saddle point problems.\n5. The paper does not provide a clear explanation of how to handle cases where the problem parameters are not known.\n\nSuggestions for Improvement:\n\n1. Provide a clear explanation of how to select the parameters of the algorithm.\n2. Provide a detailed analysis of the computational complexity of the algorithm.\n3. Compare the proposed algorithm with other methods that have been designed for solving weak MVI type saddle point problems.\n4. Provide a clear explanation of how to handle cases where the problem parameters are not known.\n5. Provide more experimental results to demonstrate the effectiveness of the proposed algorithm.\n\nRecommendation:\n\nThe paper provides a novel framework for solving weak MVI type saddle point problems and proposes an adaptive algorithm that can handle problems with limit cycles. The authors relax the condition \u03c1 > \u22121/2L to \u03c1 > \u2212(1\u22121/e)/L, which allows for a wider range of problem parameters. The paper provides a bound analysis for the convergence of the algorithm. However, the authors do not provide a clear explanation of how to select the parameters of the algorithm, do not provide a detailed analysis of the computational complexity of the algorithm, do not compare the proposed algorithm with other methods that have been designed for solving weak MVI type saddle point problems, and do not provide a clear explanation of how to handle cases where the problem parameters are not known. Therefore, I recommend that the authors revise the paper to address these issues.\n\nOverall, the paper has the potential to be a high-quality contribution to the field of optimization, but it requires revisions to address the issues mentioned above.",
        "gr_score_with": 0.781021897810219,
        "gr_score_without": 0.7380952380952381,
        "output_without": {
            "num_tokens_scored": 336,
            "num_green_tokens": 248,
            "green_fraction": 0.7380952380952381,
            "z_score": -2.836513560259239,
            "p_value": 0.9977195479232526
        },
        "output_with": {
            "num_tokens_scored": 274,
            "num_green_tokens": 214,
            "green_fraction": 0.781021897810219,
            "z_score": -0.7850914160280386,
            "p_value": 0.7838000138806724
        }
    },
    {
        "title": "TAIL: TASK-SPECIFIC ADAPTERS FOR IMITATION LEARNING WITH LARGE PRETRAINED MODELS",
        "abstract": "The full potential of large pretrained models remains largely untapped in control domains like robotics. This is mainly due to data scarcity and computational challenges associated with training or fine-tuning large models for such applications. Prior work mainly emphasizes either effective pretraining of large models for decision-making or single-task adaptation. But real-world problems will require data-efficient, continual adaptation for new control tasks. Recognizing these constraints, we introduce TAIL (Task-specific Adapters for Imitation Learning), a framework for efficient adaptation to a stream of new control tasks. Inspired by recent advancements in parameter-efficient fine-tuning in language domains, we explore efficient fine-tuning techniques\u2014e.g., Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA)\u2014in TAIL to adapt large pretrained models for new tasks with limited demonstration data. Our extensive experiments comparing prevalent parameter-efficient fine-tuning techniques and adaptation baselines suggest that TAIL with LoRA can achieve the best post-adaptation performance with only 1% of the trainable parameters of full fine-tuning while avoiding catastrophic forgetting and preserving adaptation plasticity in continual learning settings.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Task-specific Adapters for Imitation Learning (TAIL), a framework for efficient adaptation of large pretrained models to new control tasks. The authors explore parameter-efficient fine-tuning (PEFT) techniques, specifically Low-Rank Adaptation (LoRA), and demonstrate their potential in enhancing adaptation efficiency, mitigating catastrophic forgetting, and ensuring robust performance across diverse tasks. The proposed framework is evaluated on the LIBERO robotic manipulation continual learning benchmark, showcasing its effectiveness in adapting to new tasks while preserving performance on previous tasks.\n\nStrengths:\n\n1. The paper proposes a novel framework for efficient adaptation of large pretrained models, addressing the challenges of catastrophic forgetting and overfitting in continual learning scenarios.\n2. The authors provide a comprehensive exploration of PEFT techniques, including LoRA, Bottleneck Adapter, and Prefix Tuning, and demonstrate their potential in enhancing adaptation efficiency and mitigating catastrophic forgetting.\n3. The paper presents extensive experimental results on the LIBERO benchmark, showcasing the effectiveness of TAIL in adapting to new tasks while preserving performance on previous tasks.\n4. The authors provide a thorough analysis of the strengths and limitations of each adaptation strategy, providing valuable insights for future research.\n\nWeaknesses:\n\n1. The paper assumes a fixed architecture for the pretrained model, which may not be optimal for all applications.\n2. The authors do not provide a detailed comparison of the computational efficiency of TAIL with other adaptation methods.\n3. The paper assumes a fixed dataset for pretraining, which may not be representative of all real-world scenarios.\n4. The authors do not provide a detailed analysis of the robustness of TAIL to noisy or incomplete data.\n\nSuggestions for Improvement:\n\n1. Investigate the impact of varying the architecture of the pretrained model on the performance of TAIL.\n2. Conduct a more detailed comparison of the computational efficiency of TAIL with other adaptation methods.\n3. Evaluate the robustness of TAIL to noisy or incomplete data.\n4. Explore the application of TAIL to other domains, such as reinforcement learning.\n\nRecommendation:\n\nThe paper presents a novel and effective framework for efficient adaptation of large pretrained models in continual learning scenarios. The authors provide a comprehensive exploration of PEFT techniques and demonstrate their potential in enhancing adaptation efficiency and mitigating catastrophic forgetting. While there are some limitations to the paper, the proposed framework has the potential to significantly impact the field of artificial intelligence and robotics. Therefore, I recommend acceptance of the paper.\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes Task-specific Adapters for Imitation Learning (TAIL), a framework for efficient adaptation of large pretrained decision-making models to a stream of new control tasks. TAIL incorporates lightweight adapter modules into pretrained decision-making models and comprehensively compares efficient adaptation techniques implemented in TAIL in a continual imitation learning setting. The authors explore parameter-efficient adaptation techniques such as Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA) to adapt large pretrained models for new tasks with limited demonstration data. The experiments demonstrate that TAIL with LoRA can achieve the best post-adaptation performance with only 1% of the trainable parameters of full fine-tuning while avoiding catastrophic forgetting and preserving adaptation plasticity.\n\nStrengths:\n\n* The paper proposes a novel and efficient framework for adapting large pretrained decision-making models to new control tasks.\n* The authors provide a comprehensive comparison of different efficient adaptation techniques, including Bottleneck Adapters, P-Tuning, and LoRA.\n* The experiments demonstrate the effectiveness of TAIL with LoRA in achieving state-of-the-art performance on the LIBERO benchmark.\n* The authors provide a thorough analysis of the advantages and limitations of each adaptation strategy.\n\nWeaknesses:\n\n* The paper assumes a specific pretraining architecture and frozen CLIP visual/language encoders, which may limit its generalizability to other architectures.\n* The experiments are conducted on a specific benchmark (LIBERO) and may not be representative of other tasks or environments.\n* The authors do not provide a detailed comparison of TAIL with other state-of-the-art adaptation methods.\n\nSuggestions for Improvement:\n\n* The authors should provide a more detailed comparison of TAIL with other state-of-the-art adaptation methods to demonstrate its superiority.\n* The experiments should be conducted on a more diverse set of tasks and environments to demonstrate the generalizability of TAIL.\n* The authors should provide a more detailed analysis of the advantages and limitations of each adaptation strategy, including a discussion of the trade-offs between efficiency, accuracy, and robustness.\n\nRecommendation:\n\nThe paper proposes a novel and efficient framework for adapting large pretrained decision-making models to new control tasks. The experiments demonstrate the effectiveness of TAIL with LoRA in achieving state-of-the-art performance on the LIBERO benchmark. However, the paper assumes a specific pretraining architecture and frozen CLIP visual/language encoders, which may limit its generalizability to other architectures. The authors should provide a more detailed comparison of TAIL with other state-of-the-art adaptation methods and conduct experiments on a more diverse set of tasks and environments to demonstrate the generalizability of TAIL. With these improvements, the paper has the potential to make a significant contribution to the field of adaptation and learning.\n\n$$$",
        "gr_score_with": 0.8403614457831325,
        "gr_score_without": 0.8165137614678899,
        "output_without": {
            "num_tokens_scored": 327,
            "num_green_tokens": 267,
            "green_fraction": 0.8165137614678899,
            "z_score": 0.7468249567411807,
            "p_value": 0.2275846144467572
        },
        "output_with": {
            "num_tokens_scored": 332,
            "num_green_tokens": 279,
            "green_fraction": 0.8403614457831325,
            "z_score": 1.8388139823702403,
            "p_value": 0.032971275477981114
        }
    },
    {
        "title": "ZERO-MEAN REGULARIZED SPECTRAL CONTRASTIVE LEARNING: IMPLICITLY MITIGATING WRONG CON-",
        "abstract": "Contrastive learning has emerged as a popular paradigm of self-supervised learning that learns representations by encouraging representations of positive pairs to be similar while representations of negative pairs to be far apart. The spectral contrastive loss, in synergy with the notion of positive-pair graphs, offers valuable theoretical insights into the empirical successes of contrastive learning. In this paper, we propose incorporating an additive factor into the term of spectral contrastive loss involving negative pairs. This simple modification can be equivalently viewed as introducing a regularization term that enforces the mean of representations to be zero, which thus is referred to as zero-mean regularization. It intuitively relaxes the orthogonality of representations between negative pairs and implicitly alleviates the adverse effect of wrong connections in the positive-pair graph, leading to better performance and robustness. To clarify this, we thoroughly investigate the role of zero-mean regularized spectral contrastive loss in both unsupervised and supervised scenarios with respect to theoretical analysis and quantitative evaluation. These results highlight the potential of zero-mean regularized spectral contrastive learning to be a promising approach in various tasks.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel regularization technique, zero-mean regularization, for spectral contrastive learning (SpeCL). The authors argue that SpeCL has two limitations: (i) it requires the orthogonality of representations between negative pairs, and (ii) it does not account for wrong connections in the positive-pair graph. To address these limitations, they introduce an additive factor, \u03c4, into the term of SpeCL involving negative pairs. This modification is equivalent to regularizing the mean of representations to be zero, referred to as zero-mean regularization. The authors provide theoretical investigations in both unsupervised and supervised scenarios, demonstrating the effectiveness of zero-mean regularization in mitigating adverse effects of wrong connections.\n\nStrengths:\n\n1. The paper provides a clear and concise introduction to the problem and the proposed solution.\n2. The authors provide a thorough theoretical analysis of the benefits of zero-mean regularization in both unsupervised and supervised scenarios.\n3. The paper presents extensive experimental results to verify the effectiveness of zero-mean regularization on various benchmark datasets.\n4. The authors provide a clear explanation of the implications of zero-mean regularization on the structure of the learned representations.\n\nWeaknesses:\n\n1. The paper assumes a specific form of the positive-pair graph, which may not be realistic in all scenarios.\n2. The authors do not provide a clear explanation of how to choose the value of \u03c4 in practice.\n3. The paper does not discuss the computational complexity of the proposed method.\n4. The authors do not provide a comparison with other regularization techniques.\n\nSuggestions for Improvement:\n\n1. Provide a more detailed explanation of how to choose the value of \u03c4 in practice.\n2. Discuss the computational complexity of the proposed method and provide a comparison with other regularization techniques.\n3. Provide more experimental results on different datasets and scenarios to further verify the effectiveness of zero-mean regularization.\n4. Discuss the implications of zero-mean regularization on the interpretability of the learned representations.\n\nRecommendation:\n\nThe paper presents a novel and effective regularization technique for spectral contrastive learning. The authors provide a thorough theoretical analysis and extensive experimental results to demonstrate the effectiveness of zero-mean regularization. The paper is well-written and easy to follow. However, the authors should address the limitations and suggestions for improvement mentioned above to make the paper more comprehensive and robust. I recommend this paper for publication in a top-tier conference in the field of machine learning.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper proposes zero-mean regularization as an additive factor to the spectral contrastive loss (SpeCL) involving negative pairs. The modification is equivalent to enforcing the mean of representations to be zero, which is referred to as zero-mean regularization. The authors argue that this modification relaxes the orthogonality of representations between negative pairs and alleviates the adverse effect of wrong connections in the positive-pair graph, leading to better performance and robustness. Theoretical investigations in both unsupervised and supervised scenarios demonstrate the effectiveness of zero-mean regularization in mitigating adverse effects of wrong connections. The authors also provide experiments on benchmark datasets to verify the effectiveness of zero-mean regularization in contrastive learning, supervised classification, unsupervised domain adaptation, and learning with noisy labels.\n\nStrengths:\n\n1. The paper provides a clear and concise introduction to the spectral contrastive loss (SpeCL) and its limitations.\n2. The authors propose a simple yet effective modification to SpeCL, which is equivalent to enforcing the mean of representations to be zero.\n3. Theoretical investigations in both unsupervised and supervised scenarios demonstrate the effectiveness of zero-mean regularization in mitigating adverse effects of wrong connections.\n4. The authors provide experiments on benchmark datasets to verify the effectiveness of zero-mean regularization in contrastive learning, supervised classification, unsupervised domain adaptation, and learning with noisy labels.\n5. The paper provides a clear and concise presentation of the main contributions, which makes it easy to follow and understand.\n\nWeaknesses:\n\n1. The paper assumes that the representations learned by SpeCL exhibit the Neural Collapse phenomenon, which may not be true in all cases.\n2. The authors do not provide a clear explanation of why zero-mean regularization is effective in mitigating label noise.\n3. The experiments on benchmark datasets are limited to a few datasets, and more experiments on different datasets would be helpful to verify the effectiveness of zero-mean regularization.\n4. The paper does not provide a clear comparison of zero-mean regularization with other state-of-the-art methods.\n5. The authors do not provide a clear discussion of the limitations of zero-mean regularization and potential future directions.\n\nSuggestions for Improvement:\n\n1. Provide more experiments on different datasets to verify the effectiveness of zero-mean regularization.\n2. Provide a clear comparison of zero-mean regularization with other state-of-the-art methods.\n3. Provide a clear discussion of the limitations of zero-mean regularization and potential future directions.\n4. Provide a clear explanation of why zero-mean regularization is effective in mitigating label noise.\n5. Provide more details on the theoretical investigations, such as the proof of Theorem 3.1 and Theorem 3.3.\n\nRecommendation:\n\nThe paper proposes a simple yet effective modification to SpeCL, which is equivalent to enforcing the mean of representations to be zero. Theoretical investigations in both unsupervised and supervised scenarios demonstrate the effectiveness of zero-mean regularization in mitigating adverse effects of wrong connections. The experiments on benchmark datasets verify the effectiveness of zero-mean regularization in contrastive learning, supervised classification, unsupervised domain adaptation, and learning with noisy labels. Therefore, I recommend accepting this paper for publication in the conference.",
        "gr_score_with": 0.7948717948717948,
        "gr_score_without": 0.7477477477477478,
        "output_without": {
            "num_tokens_scored": 333,
            "num_green_tokens": 249,
            "green_fraction": 0.7477477477477478,
            "z_score": -2.383472880435936,
            "p_value": 0.9914249259683259
        },
        "output_with": {
            "num_tokens_scored": 312,
            "num_green_tokens": 248,
            "green_fraction": 0.7948717948717948,
            "z_score": -0.22617732000308338,
            "p_value": 0.5894682452038101
        }
    },
    {
        "title": "RDESIGN: HIERARCHICAL DATA-EFFICIENT REPRE-",
        "abstract": "While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules\u2019 primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Moreover, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both clusterlevel and sample-level to fully leverage the limited data. By constraining data representations within a limited hyperspherical space, the intrinsic relationships between data points could be explicitly imposed. Moreover, we incorporated extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. Extensive experiments demonstrate the effectiveness of our proposed method, providing a reliable baseline for future RNA design tasks. The source code and benchmark dataset are available at github.com/A4Bio/RDesign.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel data-driven RNA design pipeline called RDesign, which aims to systematically address the challenge of designing RNA sequences that reliably fold into specified tertiary structures. The authors compile a large, well-curated benchmark dataset and design a comprehensive structural modeling approach to represent the complex RNA tertiary structure. They introduce a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster-level and sample-level. Additionally, they incorporate extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. The proposed method, RDesign, demonstrates effectiveness in designing RNA sequences with structures similar to the target structure.\n\nStrengths:\n\n1. The paper addresses a challenging problem in RNA design, which is to design RNA sequences that reliably fold into specified tertiary structures.\n2. The authors propose a novel hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster-level and sample-level.\n3. The incorporation of extracted secondary structures with base pairs as prior knowledge is a novel and effective approach to facilitate the RNA design process.\n4. The authors demonstrate the effectiveness of their proposed method, RDesign, through extensive experiments and visualization examples.\n\nWeaknesses:\n\n1. The paper relies heavily on the availability of a large, well-curated benchmark dataset, which may not be readily available for other researchers to reproduce the results.\n2. The authors do not provide a clear explanation of how the proposed method can be extended to design RNA sequences with complex tertiary structures.\n3. The paper does not provide a detailed analysis of the computational efficiency of the proposed method.\n4. The authors do not provide a clear evaluation of the generalizability of the proposed method to other RNA design tasks.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of how the proposed method can be extended to design RNA sequences with complex tertiary structures.\n2. The authors should provide a more detailed analysis of the computational efficiency of the proposed method.\n3. The authors should provide a clear evaluation of the generalizability of the proposed method to other RNA design tasks.\n4. The authors should provide a clear explanation of how the proposed method can be used to design RNA sequences with specific functional properties.\n\nRecommendation:\n\nThe paper proposes a novel and effective approach to RNA design, which demonstrates the potential to design RNA sequences with structures similar to the target structure. However, the paper has some limitations that need to be addressed, such as the availability of a large, well-curated benchmark dataset and the computational efficiency of the proposed method. Overall, the paper is well-written and provides a clear explanation of the proposed method and its effectiveness. I recommend this paper for publication in a top-tier conference in the field of bioinformatics and computational biology.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes a novel data-driven RNA design pipeline, RDESIGN, which tackles the challenging task of designing RNA sequences that fold into a specified tertiary structure. The authors argue that existing approaches, primarily developed for protein design, are not directly applicable to RNA due to its structural complexity and data scarcity. To address this, the authors design a comprehensive RNA tertiary structure modeling approach and a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster and sample levels. The proposed method incorporates extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. The authors demonstrate the effectiveness of RDESIGN through extensive experiments across standard RNA design benchmarks and generalization ability assessments, providing a reliable pipeline for future research in this field.\n\nStrengths:\n\n1. The paper tackles a challenging problem in RNA design, which is of significant interest due to the importance of RNA molecules in various cellular processes.\n2. The authors provide a comprehensive RNA tertiary structure modeling approach, which is essential for understanding RNA functional mechanisms and discovering RNA-targeted therapies.\n3. The proposed hierarchical data-efficient representation learning framework is novel and effective, leveraging contrastive learning to learn structural representations.\n4. The incorporation of secondary structure information as prior knowledge is a significant improvement over existing approaches.\n5. The authors provide extensive experimental results, demonstrating the effectiveness of RDESIGN across various RNA design benchmarks and generalization ability assessments.\n\nWeaknesses:\n\n1. The paper assumes that the tertiary structure of the RNA molecule is known, which is not always the case.\n2. The proposed method relies heavily on the availability of high-quality RNA tertiary structure data, which is scarce.\n3. The authors do not provide a clear explanation of how the proposed method can be scaled up to design longer RNA sequences.\n4. The paper does not discuss the potential limitations and challenges of applying the proposed method to real-world RNA design problems.\n5. The authors do not provide a thorough analysis of the computational cost and efficiency of the proposed method.\n\nSuggestions for Improvement:\n\n1. The authors should provide more details on how to obtain the tertiary structure of the RNA molecule, which is a critical component of the proposed method.\n2. The authors should discuss the potential limitations and challenges of applying the proposed method to real-world RNA design problems.\n3. The authors should provide a more thorough analysis of the computational cost and efficiency of the proposed method.\n4. The authors should consider incorporating more advanced machine learning techniques, such as transfer learning and ensemble methods, to improve the performance of the proposed method.\n5. The authors should provide more details on how to extend the proposed method to design longer RNA sequences.\n\nRecommendation:\n\nThis paper proposes a novel and effective approach to RNA design, which tackles a challenging problem of significant interest. The proposed method incorporates secondary structure information as prior knowledge and learns structural representations through contrastive learning. While there are some limitations and challenges associated with the proposed method, the authors provide extensive experimental results, demonstrating its effectiveness across various RNA design benchmarks and generalization ability assessments. I highly recommend this paper for publication in a top-tier conference or journal.",
        "gr_score_with": 0.8651399491094147,
        "gr_score_without": 0.8290322580645161,
        "output_without": {
            "num_tokens_scored": 310,
            "num_green_tokens": 257,
            "green_fraction": 0.8290322580645161,
            "z_score": 1.278173735996309,
            "p_value": 0.10059408725614749
        },
        "output_with": {
            "num_tokens_scored": 393,
            "num_green_tokens": 340,
            "green_fraction": 0.8651399491094147,
            "z_score": 3.2286443184977927,
            "p_value": 0.0006218924176961916
        }
    },
    {
        "title": "ONE-TO-MANY POLICY TRANSFER",
        "abstract": "We investigate the problem of transferring an expert policy from a source robot to multiple different robots. To solve this problem, we propose a method named Meta-Evolve that uses continuous robot evolution to efficiently transfer the policy to each target robot through a set of tree-structured evolutionary robot sequences. The robot evolution tree allows the robot evolution paths to be shared, so our approach can significantly outperform naive one-to-one policy transfer. We present a heuristic approach to determine an optimized robot evolution tree. Experiments have shown that our method is able to improve the efficiency of one-to-three transfer of manipulation policy by up to 3.2\u00d7 and one-to-six transfer of agile locomotion policy by 2.4\u00d7 in terms of simulation cost over the baseline of launching multiple independent one-to-one policy transfers. Supplementary videos available at the project website: https://sites.google.com/view/meta-evolve.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a method called Meta-Evolve for transferring an expert policy from a source robot to multiple target robots. The method uses continuous robot evolution to efficiently transfer the policy through a tree-structured evolutionary robot sequence. The paper presents a heuristic approach to determine an optimized robot evolution tree and shows that the method can significantly outperform one-to-one policy transfer baselines in terms of simulation cost and training iterations.\n\nStrengths:\n\n* The paper introduces a new research problem of transferring an expert policy from a source robot to multiple target robots, which is an important problem in robotics.\n* The method proposed, Meta-Evolve, is a novel and efficient approach to solve this problem.\n* The paper presents a heuristic approach to determine the robot evolution tree, which is a challenging problem.\n* The experiments conducted on Hand Manipulation Suite tasks and an agile locomotion task show that the method can significantly outperform one-to-one policy transfer baselines.\n\nWeaknesses:\n\n* The paper assumes that the source and target robots can be matched in their kinematic tree topology, which may not always be the case.\n* The method proposed relies on the assumption that the target robots are mutually similar in morphology and kinematics, which may not always be true.\n* The paper does not provide a formal proof of the convergence of the method, which is an important aspect of the method's theoretical validity.\n* The experiments conducted are limited to two tasks and it is unclear whether the method can generalize to other tasks.\n\nSuggestions for Improvement:\n\n* Provide a formal proof of the convergence of the method to establish its theoretical validity.\n* Investigate the assumption that the source and target robots can be matched in their kinematic tree topology and propose a method to relax this assumption.\n* Explore the possibility of using other machine learning techniques, such as meta-learning, to improve the performance of the method.\n* Conduct more extensive experiments on a variety of tasks to demonstrate the method's generalizability.\n\nRecommendation:\n\nThe paper proposes a novel and efficient method for transferring an expert policy from a source robot to multiple target robots. While the method has some limitations, it shows promising results in the experiments conducted. With further development and refinement, the method has the potential to make significant contributions to the field of robotics. Therefore, I recommend the paper for publication in a reputable conference.\n\nRating: 7/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a method called Meta-Evolve for efficiently transferring a well-trained policy from one source robot to multiple target robots. The method utilizes continuous robot evolution to gradually adapt the policy from the source robot to the target robots through a tree-structured evolutionary robot sequence. The authors present a heuristic approach to determine the optimal robot evolution tree that minimizes the total cost of training and exploration during policy transfer.\n\nStrengths:\n\n1. The paper introduces a new and important research problem of one-to-many policy transfer, which is a significant extension of the traditional one-to-one policy transfer problem.\n2. The proposed Meta-Evolve method is innovative and effective in reducing the cost of policy transfer by sharing evolution paths among multiple target robots.\n3. The authors provide a thorough analysis of the problem and the proposed solution, including a clear explanation of the heuristic approach for determining the robot evolution tree.\n4. The experimental results demonstrate the effectiveness of the Meta-Evolve method in reducing the cost of policy transfer compared to one-to-one policy transfer baselines.\n\nWeaknesses:\n\n1. The paper assumes that the source and target robots have similar kinematic tree topology, which may not always be the case in real-world scenarios.\n2. The proposed heuristic approach for determining the robot evolution tree may not be optimal and may require further refinement or optimization.\n3. The paper does not provide a thorough analysis of the computational complexity of the Meta-Evolve method and its scalability to large-scale problems.\n4. The experimental results are limited to two specific tasks and may not generalize to other tasks or robot morphologies.\n\nSuggestions for Improvement:\n\n1. Investigate alternative approaches for determining the robot evolution tree that may be more optimal or scalable.\n2. Analyze the computational complexity of the Meta-Evolve method and its scalability to large-scale problems.\n3. Provide more experimental results to demonstrate the effectiveness of the Meta-Evolve method on a wider range of tasks and robot morphologies.\n4. Investigate the possibility of learning or optimizing the evolution tree and meta robots to improve the performance of the Meta-Evolve method.\n\nRecommendation:\n\nThe paper is well-written and provides a thorough analysis of the problem and the proposed solution. However, the paper assumes that the source and target robots have similar kinematic tree topology, which may not always be the case in real-world scenarios. Therefore, I recommend that the authors investigate alternative approaches for determining the robot evolution tree that may be more optimal or scalable. Additionally, the authors should provide more experimental results to demonstrate the effectiveness of the Meta-Evolve method on a wider range of tasks and robot morphologies.\n\nRating: 7/10\n\nRecommendation for publication: Yes\n\n$$$",
        "gr_score_with": 0.8228228228228228,
        "gr_score_without": 0.7597597597597597,
        "output_without": {
            "num_tokens_scored": 333,
            "num_green_tokens": 253,
            "green_fraction": 0.7597597597597597,
            "z_score": -1.835482664923095,
            "p_value": 0.9667828973544828
        },
        "output_with": {
            "num_tokens_scored": 333,
            "num_green_tokens": 274,
            "green_fraction": 0.8228228228228228,
            "z_score": 1.0414659665193184,
            "p_value": 0.1488296702836906
        }
    },
    {
        "title": "WHAT\u2019S IN MY BIG DATA?",
        "abstract": "Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose WHAT\u2019S IN MY BIG DATA? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities\u2014count and search\u2014at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. For instance, we find that about 50% of the documents in RedPajama and LAION-2B-en are duplicates. In addition, several datasets used for benchmarking models trained on such corpora are contaminated with respect to important benchmarks, including the Winograd Schema Challenge and parts of GLUE and SuperGLUE. We open-source WIMBD\u2019s code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes WHAT'S IN MY BIG DATA? (WIMBD), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. WIMBD builds on two basic capabilities\u2014count and search\u2014at scale, which allows us to analyze more than 35 terabytes on a standard compute node. The authors apply WIMBD to ten different corpora used to train popular language models, including C4, The Pile, and RedPajama, and uncover several surprising and previously undocumented findings about these corpora.\n\nStrengths:\n\n* The paper proposes a novel platform and set of analyses for understanding the contents of large text corpora.\n* The authors provide a comprehensive analysis of ten different corpora used to train popular language models.\n* The paper highlights the importance of understanding the contents of large text corpora and provides a framework for doing so.\n* The authors provide a clear and concise description of the WIMBD platform and its capabilities.\n* The paper includes a thorough analysis of the results, including both quantitative and qualitative findings.\n\nWeaknesses:\n\n* The paper assumes that the reader has a strong background in natural language processing and machine learning.\n* The authors do not provide a detailed description of the Elasticsearch search engine used in WIMBD.\n* The paper does not provide a clear explanation of the hash function used in the counting and searching algorithms.\n* The authors do not provide a detailed description of the evaluation metrics used to assess the performance of WIMBD.\n* The paper does not provide a clear explanation of the limitations of WIMBD and how they can be addressed.\n\nSuggestions for Improvement:\n\n* Provide a more detailed description of the Elasticsearch search engine used in WIMBD.\n* Provide a clear explanation of the hash function used in the counting and searching algorithms.\n* Provide a detailed description of the evaluation metrics used to assess the performance of WIMBD.\n* Provide a clear explanation of the limitations of WIMBD and how they can be addressed.\n* Consider including a more detailed analysis of the results, including a discussion of the implications of the findings.\n\nRecommendation:\n\nThis paper provides a novel and comprehensive framework for understanding the contents of large text corpora. The authors provide a clear and concise description of the WIMBD platform and its capabilities, and the paper includes a thorough analysis of the results. However, the paper assumes a strong background in natural language processing and machine learning, and some of the technical details are not clearly explained. Overall, this paper is a valuable contribution to the field of natural language processing and machine learning, and it has the potential to be a highly influential paper in the field.\n\nRating: 8/10\n\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a platform called WIMBD (WHAT'S IN MY BIG DATA?) to investigate the content of large text corpora. WIMBD builds on two basic capabilities: count and search, which allow for the analysis of large text corpora. The authors apply WIMBD to ten different corpora used to train language models, including C4, The Pile, and RedPajama. Their analysis reveals several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination. The authors open-source WIMBD's code and artifacts to provide a standard set of evaluations for new text-based corpora and to encourage more analyses and transparency around them.\n\nStrengths:\n\n1. WIMBD is a comprehensive platform for analyzing large text corpora, providing a set of sixteen analyses across four aspects: high-level statistics, data quality, community- and society-relevant measurements, and cross-data analysis.\n2. The authors apply WIMBD to ten different corpora used to train language models, providing a broad understanding of the content of large text corpora.\n3. The analysis reveals several surprising and previously undocumented findings about the corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination.\n4. The authors open-source WIMBD's code and artifacts, providing a standard set of evaluations for new text-based corpora and encouraging more analyses and transparency around them.\n\nWeaknesses:\n\n1. The analysis is limited to ten corpora, and it is unclear whether the findings are generalizable to other corpora.\n2. The authors rely on Elasticsearch for search, which may not be scalable for extremely large corpora.\n3. The analysis does not account for the complexity of natural language, which may lead to inaccurate results.\n4. The authors do not provide a clear evaluation of the impact of the findings on language model performance.\n\nSuggestions for Improvement:\n\n1. Expand the analysis to more corpora to increase the generalizability of the findings.\n2. Investigate alternative search algorithms to improve scalability and accuracy.\n3. Consider the complexity of natural language in the analysis and develop more sophisticated methods to account for it.\n4. Evaluate the impact of the findings on language model performance to provide a more complete understanding of the relationship between corpus quality and language model performance.\n\nRecommendation:\n\nThe paper proposes a comprehensive platform for analyzing large text corpora, providing a set of sixteen analyses across four aspects. While the analysis reveals several surprising and previously undocumented findings about the corpora, the limitations of the analysis, such as the reliance on Elasticsearch and the lack of evaluation of the impact of the findings on language model performance, should be addressed. Therefore, I recommend this paper for publication in a top-tier conference, but with the expectation that the authors will address the limitations and provide a more comprehensive analysis in future work.\n\n$$$",
        "gr_score_with": 0.904891304347826,
        "gr_score_without": 0.8392282958199357,
        "output_without": {
            "num_tokens_scored": 311,
            "num_green_tokens": 261,
            "green_fraction": 0.8392282958199357,
            "z_score": 1.7297511003720634,
            "p_value": 0.04183737725792531
        },
        "output_with": {
            "num_tokens_scored": 368,
            "num_green_tokens": 333,
            "green_fraction": 0.904891304347826,
            "z_score": 5.030650546004888,
            "p_value": 2.4440919349606954e-07
        }
    },
    {
        "title": "COMPLEX PRIORS AND FLEXIBLE INFERENCE IN RECURRENT CIRCUITS WITH DENDRITIC NONLINEARITIES",
        "abstract": "Despite many successful examples in which probabilistic inference can account for perception, we have little understanding of how the brain represents and uses structured priors that capture the complexity of natural input statistics. Here we construct a recurrent circuit model that can implicitly represent priors over latent variables, and combine them with sensory and contextual sources of information to encode task-specific posteriors. Inspired by the recent success of diffusion models as means of learning and using priors over images, our model uses dendritic nonlinearities optimized for denoising, and stochastic somatic integration with the degree of noise modulated by an oscillating global signal. Combining these elements into a recurrent network yields a dynamical system that samples from the prior at a rate prescribed by the period of the global oscillator. Additional inputs reflecting sensory or top-down contextual information alter these dynamics to generate samples from the corresponding posterior, with different input gating patterns selecting different inference tasks. We demonstrate that this architecture can sample from low dimensional nonlinear manifolds and multimodal posteriors. Overall, the model provides a new framework for circuit-level representation of probabilistic information, in a format that facilitates flexible inference.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel circuit-level model for representing complex high-dimensional distributions, inspired by the recent success of diffusion models in machine learning. The model, called the recurrent circuit model, uses dendritic nonlinearities optimized for denoising and stochastic somatic integration with a modulated global signal to sample from the prior at a rate prescribed by the period of the global oscillator. The model can flexibly use sensory and contextual sources of information to encode task-specific posteriors. The authors demonstrate that this architecture can sample from low-dimensional nonlinear manifolds and multimodal posteriors.\n\nStrengths:\n\n1. The paper proposes a novel and biologically-inspired model for representing complex high-dimensional distributions.\n2. The model is flexible and can be used to encode task-specific posteriors using sensory and contextual sources of information.\n3. The authors demonstrate that the model can sample from low-dimensional nonlinear manifolds and multimodal posteriors.\n4. The paper provides a thorough analysis of the model's performance and compares it to traditional diffusion models.\n\nWeaknesses:\n\n1. The paper assumes a fixed architecture for the dendritic tree, which may not be biologically realistic.\n2. The model's performance is evaluated on a limited set of tasks and distributions, and it is unclear how well it generalizes to other scenarios.\n3. The paper does not provide a clear explanation of how the model's parameters are learned, which is a critical component of any machine learning model.\n4. The paper's conclusion is somewhat overstated, as the model's performance is not significantly better than traditional diffusion models in all cases.\n\nSuggestions for Improvement:\n\n1. Consider exploring more biologically realistic architectures for the dendritic tree, such as those with variable branching factors or depths.\n2. Evaluate the model's performance on a wider range of tasks and distributions to assess its generalizability.\n3. Provide a more detailed explanation of how the model's parameters are learned, including any optimization algorithms or techniques used.\n4. Be more cautious in the paper's conclusion, as the model's performance is not uniformly better than traditional diffusion models.\n\nRecommendation:\n\nI recommend accepting this paper for publication, subject to the above suggestions for improvement. The paper proposes a novel and biologically-inspired model for representing complex high-dimensional distributions, and the authors demonstrate its flexibility and performance on a range of tasks. However, the paper's assumptions and limitations should be carefully considered and addressed in future work.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel circuit-level model for representing complex high-dimensional distributions, inspired by the success of diffusion models in machine learning. The authors adapt several ideas from diffusion models to construct a recurrent circuit model that can implicitly represent sensory priors and combine them with other sources of information to encode task-specific posteriors. The proposed circuit uses dendritic nonlinearities, optimized for denoising, and stochastic somatic activity modulated by a global oscillation that determines the effective rate of sampling. The authors demonstrate that this architecture can sample from low-dimensional nonlinear manifolds and multimodal posteriors, and provide a new framework for circuit-level representation of probabilistic information.\n\nStrengths:\n\n1. The paper provides a novel and innovative approach to representing complex high-dimensional distributions, which is inspired by the success of diffusion models in machine learning.\n2. The authors propose a biologically-motivated circuit-level model that can implicitly represent sensory priors and combine them with other sources of information to encode task-specific posteriors.\n3. The proposed circuit uses dendritic nonlinearities, optimized for denoising, and stochastic somatic activity modulated by a global oscillation that determines the effective rate of sampling.\n4. The authors demonstrate that this architecture can sample from low-dimensional nonlinear manifolds and multimodal posteriors.\n5. The paper provides a new framework for circuit-level representation of probabilistic information, which has the potential to be useful for understanding the neural mechanisms of perception and action.\n\nWeaknesses:\n\n1. The paper assumes that the prior distribution is known, which may not be the case in real-world scenarios.\n2. The authors do not provide a detailed explanation of how the proposed circuit would be implemented in biological systems.\n3. The paper relies on simulations to demonstrate the performance of the proposed circuit, which may not be representative of real-world scenarios.\n4. The authors do not provide a clear explanation of how the proposed circuit would be trained and fine-tuned.\n5. The paper assumes that the global oscillation is a fixed parameter, which may not be the case in real-world scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of how the proposed circuit would be implemented in biological systems.\n2. The paper should include more experimental data to demonstrate the performance of the proposed circuit in real-world scenarios.\n3. The authors should provide a clear explanation of how the proposed circuit would be trained and fine-tuned.\n4. The paper should include more discussion on the potential applications and limitations of the proposed circuit.\n5. The authors should provide a more detailed explanation of how the proposed circuit would handle cases where the prior distribution is not known.\n\nRecommendation:\n\nI recommend that the authors revise the paper to address the weaknesses and suggestions for improvement. The proposed circuit has the potential to be a useful tool for understanding the neural mechanisms of perception and action, and with further development and refinement, it could be a valuable contribution to the field.\n\nRating:\n\nI would rate this paper as a 7 out of 10. The proposed circuit is innovative and has the potential to be a useful tool for understanding the neural mechanisms of perception and action. However, the paper assumes that the prior distribution is known, and the authors do not provide a detailed explanation of how the proposed circuit would be implemented in biological systems. With further development and refinement, I believe that this paper could be a valuable contribution to the field.\n\n$$$",
        "gr_score_with": 0.8424068767908309,
        "gr_score_without": 0.7916666666666666,
        "output_without": {
            "num_tokens_scored": 336,
            "num_green_tokens": 266,
            "green_fraction": 0.7916666666666666,
            "z_score": -0.38159100583924016,
            "p_value": 0.6486176216951209
        },
        "output_with": {
            "num_tokens_scored": 349,
            "num_green_tokens": 294,
            "green_fraction": 0.8424068767908309,
            "z_score": 1.9808327422988352,
            "p_value": 0.023805016818113353
        }
    },
    {
        "title": "DEMYSTIFYING LOCAL & GLOBAL FAIRNESS TRADE-OFFS IN FEDERATED LEARNING USING PARTIAL INFORMATION DECOMPOSITION",
        "abstract": "This work presents an information-theoretic perspective to group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender, race, etc. Existing works often focus on either global fairness (overall disparity of the model across all clients) or local fairness (disparity of the model at each client), without always considering their trade-offs. There is a lack of understanding of the interplay between global and local fairness in FL, particularly under data heterogeneity, and if and when one implies the other. To address this gap, we leverage a body of work in information theory called partial information decomposition (PID), which first identifies three sources of unfairness in FL, namely, Unique Disparity, Redundant Disparity, and Masked Disparity. We demonstrate how these three disparities contribute to global and local fairness using canonical examples. This decomposition helps us derive fundamental limits on the trade-off between global and local fairness, highlighting where they agree or disagree. We introduce the Accuracy & Global-Local Fairness Optimality Problem (AGLFOP), a convex optimization that defines the theoretical limits of accuracy and fairness trade-offs, identifying the best possible performance any FL strategy can attain given a dataset and client distribution. We also present experimental results on synthetic datasets and the ADULT dataset to support our theoretical findings.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper \"DEMYSTIFYING LOCAL & GLOBAL FAIRNESS TRADE-OFFS IN FEDERATED LEARNING USING PARTIAL INFORMATION DECOMPOSITION\" presents an information-theoretic perspective to group fairness trade-offs in federated learning (FL) with respect to sensitive attributes, such as gender, race, etc. The authors leverage a body of work in information theory called partial information decomposition (PID) to identify three sources of unfairness in FL: Unique Disparity, Redundant Disparity, and Masked Disparity. They demonstrate how these three disparities contribute to global and local fairness using canonical examples and provide a framework for quantifying accuracy-fairness trade-offs in FL.\n\nStrengths:\n\n1. The paper provides a comprehensive understanding of group fairness trade-offs in FL using an information-theoretic perspective.\n2. The authors leverage PID to identify three sources of unfairness in FL, which is a novel contribution to the field.\n3. The paper provides a framework for quantifying accuracy-fairness trade-offs in FL, which is a significant contribution to the field.\n4. The authors provide experimental results on synthetic and Adult datasets to support their theoretical findings.\n\nWeaknesses:\n\n1. The paper assumes a binary sensitive attribute, which may not be applicable to all real-world scenarios.\n2. The authors do not provide a clear explanation of how to implement the PID decomposition in practice.\n3. The paper assumes a fixed client distribution, which may not be realistic in real-world scenarios.\n4. The authors do not provide a comparison with existing methods for fairness in FL.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed explanation of how to implement the PID decomposition in practice.\n2. The paper should be extended to include a comparison with existing methods for fairness in FL.\n3. The authors should provide a more comprehensive discussion of the limitations of the PID decomposition.\n4. The paper should be extended to include a discussion of how to handle non-binary sensitive attributes.\n\nRecommendation:\n\nThe paper provides a significant contribution to the field of fairness in FL and is well-written and well-organized. However, the authors should address the limitations mentioned above to make the paper more comprehensive and practical. I would recommend this paper for publication in a top-tier conference in the field of machine learning and fairness.",
        "peer_review_with_watermark": "$$$\n\nPaper Summary:\n\nThe paper \"DEMYSTIFYING LOCAL & GLOBAL FAIRNESS TRADE-OFFS IN FEDERATED LEARNING USING PARTIAL INFORMATION DECOMPOSITION\" proposes an information-theoretic approach to understand the interplay between local and global fairness in Federated Learning (FL). The authors leverage partial information decomposition (PID) to identify three sources of unfairness in FL, namely, Unique Disparity, Redundant Disparity, and Masked Disparity. They demonstrate how these three disparities contribute to global and local fairness using canonical examples. The authors introduce the Accuracy & Global-Local Fairness Optimality Problem (AGLFOP), a convex optimization framework that defines the theoretical limits of accuracy and fairness trade-offs in FL. They also present experimental results on synthetic datasets and the Adult dataset to support their theoretical findings.\n\nStrengths:\n\n1. The paper provides a comprehensive understanding of the interplay between local and global fairness in FL using an information-theoretic approach.\n2. The authors introduce the PID framework, which decomposes global and local disparity into three non-negative components, providing insights into the sources of unfairness in FL.\n3. The AGLFOP framework is introduced, which defines the theoretical limits of accuracy and fairness trade-offs in FL, providing a convex optimization approach to exploring the trade-offs between accuracy and fairness.\n4. The paper presents experimental results on synthetic datasets and the Adult dataset, providing empirical evidence to support the theoretical findings.\n\nWeaknesses:\n\n1. The paper assumes that the data is binary, which may not be the case in real-world scenarios.\n2. The PID framework may not capture all the nuances of fairness in FL, as it relies on mutual information, which may not capture all the dependencies between the sensitive attribute, client label, and model predictions.\n3. The AGLFOP framework may not be scalable to large datasets, as it requires computing the mutual information between the sensitive attribute, client label, and model predictions.\n4. The paper does not discuss the practical implications of the AGLFOP framework, such as how to implement it in real-world scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should extend the PID framework to capture more nuanced dependencies between the sensitive attribute, client label, and model predictions.\n2. The authors should discuss the practical implications of the AGLFOP framework, such as how to implement it in real-world scenarios.\n3. The authors should investigate the scalability of the AGLFOP framework to large datasets.\n4. The authors should provide more experimental results on real-world datasets to demonstrate the effectiveness of the AGLFOP framework.\n\nRecommendation:\n\nThe paper provides a comprehensive understanding of the interplay between local and global fairness in FL using an information-theoretic approach. The authors introduce the PID framework, which decomposes global and local disparity into three non-negative components, providing insights into the sources of unfairness in FL. The AGLFOP framework is introduced, which defines the theoretical limits of accuracy and fairness trade-offs in FL, providing a convex optimization approach to exploring the trade-offs between accuracy and fairness. However, the paper assumes that the data is binary, which may not be the case in real-world scenarios, and the PID framework may not capture all the nuances of fairness in FL. The authors should extend the PID framework to capture more nuanced dependencies between the sensitive attribute, client label, and model predictions, discuss the practical implications of the AGLFOP framework, investigate the scalability of the AGLFOP framework to large datasets, and provide more experimental results on real-world datasets to demonstrate the effectiveness of the AGLFOP framework.\n\nIn conclusion, the paper provides a comprehensive understanding of the interplay between local and global fairness in FL using an information-theoretic approach, but it requires further development to capture more nuanced dependencies between the sensitive attribute, client label, and model predictions, discuss the practical implications of the AGLFOP framework, investigate the scalability of the AGLFOP framework to large datasets, and provide more experimental results on real-world datasets to demonstrate the effectiveness of the AGLFOP framework.",
        "gr_score_with": 0.8647887323943662,
        "gr_score_without": 0.802547770700637,
        "output_without": {
            "num_tokens_scored": 314,
            "num_green_tokens": 252,
            "green_fraction": 0.802547770700637,
            "z_score": 0.11314152944657332,
            "p_value": 0.4549591753097201
        },
        "output_with": {
            "num_tokens_scored": 355,
            "num_green_tokens": 307,
            "green_fraction": 0.8647887323943662,
            "z_score": 3.0520412472955996,
            "p_value": 0.0011364543276600927
        }
    },
    {
        "title": "INCREMENTAL RANDOMIZED SMOOTHING CERTIFICATION",
        "abstract": "Randomized smoothing-based certification is an effective approach for obtaining robustness certificates of deep neural networks (DNNs) against adversarial attacks. This method constructs a smoothed DNN model and certifies its robustness through statistical sampling, but it is computationally expensive, especially when certifying with a large number of samples. Furthermore, when the smoothed model is modified (e.g., quantized or pruned), certification guarantees may not hold for the modified DNN, and recertifying from scratch can be prohibitively expensive. We present the first approach for incremental robustness certification for randomized smoothing, IRS. We show how to reuse the certification guarantees for the original smoothed model to certify an approximated model with very few samples. IRS significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees. We experimentally demonstrate the effectiveness of our approach, showing up to 4.1x certification speedup over the certification that applies randomized smoothing of approximate model from scratch.",
        "peer_review_without_watermark": "$$$\nPaper Summary:\n\nThe paper proposes a novel approach for incremental robustness certification of deep neural networks (DNNs) using randomized smoothing (RS). The method, called Incremental Randomized Smoothing (IRS), reuses the certification guarantees of the original smoothed model to certify an approximated model with very few samples. This approach significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees.\n\nStrengths:\n\n1. The paper addresses a critical limitation of existing RS-based certification practices, which requires expensive certification to be re-run from scratch for each approximate network.\n2. The proposed IRS approach is the first incremental RS-based certification framework that efficiently computes the certified radius of the updated smoothed classifier.\n3. The paper presents an extensive evaluation of the performance of IRS, demonstrating speedups of up to 4.1x over the standard non-incremental RS baseline on state-of-the-art classification models.\n4. The authors provide a thorough analysis of the trade-offs involved in choosing the smoothing parameter \u03c3, which affects the efficiency of IRS.\n\nWeaknesses:\n\n1. The paper assumes that the approximate network is similar to the original network, which may not always be the case in real-world scenarios.\n2. The authors do not provide a detailed analysis of the computational cost of storing and retrieving the certification cache, which may become a bottleneck for large-scale applications.\n3. The paper assumes that the user has a larger sample budget, which may not always be the case in real-world scenarios.\n4. The authors do not provide a detailed comparison with other incremental certification methods, such as deterministic certification using formal logic.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed analysis of the assumptions made in the paper, particularly with regards to the similarity between the approximate and original networks.\n2. The paper should include a more detailed analysis of the computational cost of storing and retrieving the certification cache, including the memory requirements and the time complexity of the operations involved.\n3. The authors should provide a more detailed comparison with other incremental certification methods, including deterministic certification using formal logic.\n4. The paper should include a discussion of the potential applications of IRS in real-world scenarios, including approximate tuning and shipping certification caches.\n\nRecommendation:\n\nThe paper presents a novel and efficient approach for incremental robustness certification of DNNs using randomized smoothing. While there are some limitations and weaknesses to the approach, the authors provide a thorough analysis of the trade-offs involved and demonstrate the effectiveness of IRS in practice. Therefore, I recommend accepting the paper for publication in a reputable conference or journal.\n\nRating: 8/10\n\nReview:\n\nThe paper is well-written and easy to follow, with a clear introduction to the problem and a thorough presentation of the proposed approach. The authors provide a detailed analysis of the trade-offs involved in choosing the smoothing parameter \u03c3 and demonstrate the effectiveness of IRS in practice. However, the paper assumes that the approximate network is similar to the original network, which may not always be the case in real-world scenarios. Additionally, the authors do not provide a detailed analysis of the computational cost of storing and retrieving the certification cache, which may become a bottleneck for large-scale applications.\n\nOverall, the paper presents a novel and efficient approach for incremental robustness certification of DNNs using randomized smoothing. While there are some limitations and weaknesses to the approach, the authors provide a thorough analysis of the trade-offs involved and demonstrate the effectiveness of IRS in practice. Therefore, I recommend accepting the paper for publication in a reputable conference or journal.",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper proposes Incremental Randomized Smoothing (IRS) certification, a novel approach to certify the robustness of deep neural networks (DNNs) with reduced computational cost. IRS leverages the certification guarantees obtained from the original smoothed model to certify a smoothed approximated model with very few samples. This approach significantly reduces the computational cost of certification while maintaining strong robustness guarantees. The authors extensively evaluate the performance of IRS on state-of-the-art DNNs and show that it achieves up to 4.1x certification speedup over the standard non-incremental RS baseline.\n\nStrengths:\n\n1. IRS provides a novel approach to incremental certification, which is essential for efficient certification of DNNs in real-world applications.\n2. The authors demonstrate the effectiveness of IRS through extensive experiments on state-of-the-art DNNs and show significant certification speedup over the standard non-incremental RS baseline.\n3. IRS maintains strong robustness guarantees, which is critical for reliable certification of DNNs.\n4. The approach is general and can be applied to various DNN approximations, including quantization and pruning.\n\nWeaknesses:\n\n1. The authors assume that the cache stores the top predicted class index, its lower confidence bound, and the seeds for Gaussian corruptions obtained from the RS execution of the original classifier. However, storing this additional information may require significant memory and may not be feasible for all applications.\n2. The choice of the smoothing parameter \u03c3 affects the efficiency of IRS, and larger values of \u03c3 generally lead to better results. However, this may come at the cost of reduced accuracy in the model.\n3. IRS targets fast certification while maintaining a sufficiently large radius, which may limit its applicability to scenarios where a larger radius is required.\n4. The authors do not provide a detailed analysis of the theoretical guarantees of IRS, which may be essential for understanding its robustness and reliability.\n\nSuggestions for Improvement:\n\n1. Investigate alternative approaches to store the cache, such as using more compact representations or leveraging distributed storage.\n2. Provide a more detailed analysis of the theoretical guarantees of IRS, including its robustness and reliability.\n3. Explore the applicability of IRS to scenarios where a larger radius is required, such as in scenarios with limited computational resources.\n4. Investigate the extension of IRS to other types of DNN approximations, such as weight sharing or knowledge distillation.\n\nRecommendation:\n\nThis paper presents a novel and effective approach to incremental certification of DNNs, which has significant implications for efficient and effective certification of DNNs in real-world applications. While there are some limitations and areas for improvement, the authors provide a thorough evaluation of the approach and demonstrate its effectiveness through extensive experiments. Therefore, I recommend this paper for publication in a reputable conference or journal.",
        "gr_score_with": 0.8481927710843373,
        "gr_score_without": 0.8493827160493828,
        "output_without": {
            "num_tokens_scored": 405,
            "num_green_tokens": 344,
            "green_fraction": 0.8493827160493828,
            "z_score": 2.4848047342081907,
            "p_value": 0.006481125659095672
        },
        "output_with": {
            "num_tokens_scored": 415,
            "num_green_tokens": 352,
            "green_fraction": 0.8481927710843373,
            "z_score": 2.454692430435087,
            "p_value": 0.007050258699899916
        }
    }
]