[
    {
        "title": "CAUSAL MODELLING AGENTS: CAUSAL GRAPH DIS-",
        "abstract": "Centre for Medical Image Computing, UCL, London, United Kingdom Microsoft Research, Cambridge ABSTRACT Scientific discovery hinges on the effective integration of metadata, which refers to a set of conceptual operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the Causal Modelling Agent (CMA), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. We evaluate the CMA\u2019s performance on a number of benchmarks, as well as on the real-world task of modelling the clinical and radiological phenotype of Alzheimer\u2019s Disease (AD). Our experimental results indicate that the CMA can outperform previous purely data-driven or metadata-driven approaches to causal discovery. In our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it.  Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$user\n\nThe peer review format and length should be of standard conference. \\\n            title: CAUSAL MODELLING AGENTS: CAUSAL GRAPH DIS- \\\n            abstract: Centre for Medical Image Computing, UCL, London, United Kingdom Microsoft Research, Cambridge ABSTRACT Scientific discovery hinges on the effective integration of metadata, which refers to a set of conceptual operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the Causal Modelling Agent (CMA), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. We evaluate the CMA\u2019s performance on a number of benchmarks, as well as on the real-world task of modelling the clinical and radiological phenotype of Alzheimer\u2019s Disease (AD). Our experimental results indicate that the CMA can outperform previous purely data-driven or metadata-driven approaches to causal discovery. In our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD. \\\n            paper text: 1 INTRODUCTION  Scientific discovery is the output of successful scientific inquiry, and its objects include events, causes, processes, and hypotheses. Arguably, the main mechanism of scientific reasoning is the generation of novel hypotheses that align with or explain observed datasets, or that allow for the derivation of testable consequences (Schickore, 2014). The \u2018pragmatic logics of discovery\u2019 refer to a set of reasoning strategies which are used in knowledge generation (Letseka & Zireva, 2013), and include both mental operations such as determination of relevant information for a given inquiry, and physical operations such as observation and experimentation (Schickore, 2014; Schiller, 1917). We refer to the former operations as \u2018metadata\u2019 and the latter as \u2018data\u2019. Both metadata and data are foundational to producing, communicating, and validating conjectures, and are important aspects of several theories of the scientific method (Hanson, 1965; Godfrey-Smith, 2009). Whilst we can represent conjectures or hypotheses as Directed Acyclic Graphs (DAGs) in several fields (Spirtes et al., 2000; Sachs et al., 2005; Zhang et al., 2013), we are often faced with the challenging problem of inferring causal structure from its empirical implications, which is known as the causal discovery problem (Peters et al., 2017).\nThere are a number of algorithms which attempt to solve the causal discovery problem by identifying the correct DAG given a dataset (Zheng et al., 2018; Yu et al., 2019; Nauta et al., 2019). However, in attempting to identify the generative process by leveraging asymmetries in the numerical data alone, even state-of-the-art causal discovery algorithms can be ineffective on real-world datasets (Tu et al., 2019; Huang et al., 2021b; Kaiser & Sipos, 2022), and can struggle in the setting where the data consists of as few as five synthetic variables, where each pair of variables can at most have a single confounder (Ashman et al., 2023).\nMore recently, it was demonstrated that Large Language Models (LLMs) establish new state-of-theart performance on multiple causal benchmarks including counterfactual reasoning, actual causality, and causal discovery (K\u0131c\u0131man et al., 2023; Lampinen et al., 2023). It is hypothesized that LLMs are able to capture domain knowledge (as encoded in natural language), which can then be translated into causal graphs or used to identify background causal context. In essence, this describes metadatabased reasoning; a task previously assumed to be restricted to humans (Sahu et al., 2022; Trott et al., 2023).\n\u2217rmapabd@ucl.ac.uk\nCoextensive to the causal discovery problem, causal modelling has seen numerous innovations in creating increasingly flexible models capable of causal reasoning on complex or even multi-modal data. For example, recent advances in probabilistic generative modelling have led to the emergence of Deep Structural Causal Models (DSCMs) (Pawlowski et al., 2020; Khemakhem et al., 2021; Sanchez & Tsaftaris, 2022; Dash et al., 2022), which combine modular Deep Learning (DL) elements with Structural Causal Models (SCMs). DSCMs can perform interventional and counterfactual queries in high-dimensional data settings, including in the imaging space (Pawlowski et al., 2020). However, they are limited in that they require the causal relationships between the variables (i.e., the causal graph) to be known a priori, which is rarely the case for real-world problems. Another limitation is the assumption of no unmeasured confounding, which requires yet more flexible types of graphical models (such as chain graph models) to account for hidden variables.\nIn this work, we investigate the effectiveness of combining LLM-based methods with a generalization of the DSCM framework by proposing the Causal Modelling Agent (CMA). The CMA combines the data-based modelling from DSCMs with the complementary and distinct metadata-based reasoning that LLMs utilise for the task of causal discovery, including for multi-modal datasets. Combining LLM-based methods with data-driven causal modelling approaches is beneficial in two main ways: 1) The LLMs can act as proxies of human knowledge and allow for the efficient exploration of causal graph space, and 2) data-driven causal methods can allow LLMs to formalize, communicate, and ground their reasoning (K\u0131c\u0131man et al., 2023). We assess this framework on a number of synthetic experiments and causal discovery benchmarks, before applying it to the real-world task of modelling the clinical and radiological phenotype of Alzheimer\u2019s disease (AD). Our main contributions are: 1) A unified framework for causal discovery which combines LLMs with deep probabilistic graphical models, and in which the LLMs act as priors, critics, and post-processors over the training of such models; 2) a generalised approach for the automatic construction of a DSCM from an experiment-description file, which allows for flexible experimentation; 3) a novel modelling strategy which parametrises chain graphs with deep learning elements to account for unmeasured confounding; 4) in our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD.2 RELATED WORK  Deep Structural Causal Models A multi-modal DSCM framework is proposed by Pawlowski et al. (2020). Whilst previous work has extended DSCMs and similar causal models to a number of application areas (Reinhold et al., 2021; Li et al., 2023a), previous DSCMs require that the causal graph be known a priori. As a general model class for counterfactual inference in multi-modal data, DSCMs do not directly attempt to solve the causal discovery problem and have not previously been used as part of a causal discovery algorithm. Additionally, there is no method with which to handle unmeasured confounding/associative relationships.\nLLMs and Causality Data-driven causal reasoning methods are well-established (Li et al., 2023b), but LLMs\u2019 application in this field is recent (Willig et al., 2022). Zec\u030cevic\u0301 et al. (2023) found that LLMs may recite causal knowledge instead of reason per se, with improved performance noted under Chain of Thought (CoT) prompting (Wei et al., 2022). Yadlowsky et al. (2023) showed LLMs\u2019 capacity to learn new tasks deteriorates with tasks increasingly divergent from their pretraining data. Contrarily, (Lampinen et al., 2023) observed that transformer-based agents can passively learn and apply generalizable causal strategies if allowed intervention during tests. In light of this nascent research field, a number of encouraging results have been demonstrated for the task of causal discovery. For example, Long et al. (2023b) demonstrated that LLMs can construct correct 3\u20134 variable graphs. Tu et al. (2023) considered causal discovery for a neuropathic pain dataset (Tu et al., 2019) using LLMs alone. Choi et al. (2022) demonstrated that LLMs can produce a prior hypothesis which improved the accuracy of data-based causal discovery algorithms, and Zhiheng et al. (2022) attempted to partially redefine the causal discovery problem such that it included relevant metadata. In these cases, LLMs were used in one stage of the causal modelling process, centring mostly around producing a causal graph alone or creating a causal graph which is used as a prior, for example in Ban et al. (2023). Long et al. (2023a) demonstrated that LLMs can reduce the size of a Markov equivalence class assuming an optimal output from a discovery algorithm; their work viewed LLMs as a post-processing step alone. K\u0131c\u0131man et al. (2023) investigated the graph discovery capabilities of LLMs over a broader set of real-world datasets; however, they did not assess combinations of LLMs with existing causal methods.\nAgents and Reasoning In our setting, an agent is an LLM which has access to one or more \u2018tools\u2019, such as an internet search engine. We briefly summarise recent agent frameworks for reasoning tasks and compare them with our agent, whose tool is a causal modelling framework. The ReAct framework (Yao et al., 2022) aimed to combine reasoning and acting capabilities in language models by using reasoning traces to design and update action plans. Reflexion (Shinn et al., 2023) built on ReAct by adding a \u2018self-reflection\u2019 mechanism to aid in inferring future actions, and AutoGPT (F\u0131rat & Kuleli, 2023) is a framework which decomposed overarching goals into sub-goals, carrying them out in a ReAct-like loop. DERA (Nair et al., 2023) attempted to enhance task completion using dialogue between two LLM agents, and Generative Agents (Park et al., 2023) stored agent experiences as memories which can be retrieved for future planning, however actions here were not executable. An extension of these ideas was Voyager (Wang et al., 2023), an LLM-based embodied agent which combined an automatic curriculum with an iterative prompting mechanism. These frameworks did not consider LLMs for the task of causal modelling, or indeed for inferring causal structure from its empirical implications.\nRelated topics Additional related topics include works by Feder et al. which introduced the statistical challenge of estimating causal effects with text (Feder et al., 2022), and investigated how language representation models can effectively learn a counterfactual representation for a given concept (Feder et al., 2021). Veitch et al. (2021) investigated counterfactual invariance in the context of text classification, and Abraham et al. (2022) introduced a benchmark for assessing concept-based explanation methods in natural language processing (NLP). Mind\u2019s Eye (Liu et al., 2022) attempted to ground language model reasoning through simulation. The MuJoCo physics engine was used to simulate outcomes for given physics questions, and the results were passed to the LLM to improve model reasoning ability. However, this process did not iterate nor consider a causal modelling task. To the best of our knowledge, this is the first work which combines LLM-based methods with a general deep causal modelling approach in which the LLM acts as a prior, critic and hypothesis engine, as well as post-processor. Additionally, in contrast to previous work (Long et al., 2023a; Ban et al., 2023), the CMA does not make any explicit assumptions about the initial graph structure, and is assessed on complex data scenarios beyond synthetic cases.3 CAUSAL MODELLING AGENT (CMA)  We introduce the CMA framework (Figure 1), which unifies metadata- and data-based modelling paradigms to reason over a dataset and its associated data-generating process. Conceptually, the framework is defined as a function that takes as input a dataset D and associated metadata M, which represents the knowledge in the LLM\u2019s training corpus, and outputs a trained causal model and associated causal graph. The framework makes use of four concepts: 1) Hypothesis generation, 2) Model fitting, 3) Post-processing, and 4) Hypothesis amendment.\nHypothesis generation In hypothesis generation, we produce a causal graph by use of an LLM. The only required input is a set of variable names. In our case, during the first iteration (t = 0), the language model function LLM acts as a prior which proposes a representation of a causal graph Gt=0 of the current variables given metadata M and an empty graph G\u2205 (that is, a causal graph with no relationships between variables): Gt=0 = LLM(G\u2205;M). Graph Gt=0 can be encoded into an appropriate structured format Gst=0 = e(Gt=0), where e(\u00b7) is an encoding function and Gst=0 is a structured causal graph (Figure 1a).\nModel fitting At iteration t, the model fitting stage uses a data-driven approach to calculate a metric of fit Ft. As an example, the metric of fit could represent the log-likelihood of a dataset D under the model, given a structured graph Gst : Ft = logPGst (D). Indeed, this will be the metric of fit considered in this work. In our instantiation of a CMA, we use a modelling approach which requires that we account for the order in which modules are constructed to avoid errors and model misspecifications. Therefore, let K be a function which takes as input Gst and transmutes it into a (correctly specified) computational graph CGt, that is, CGt = K(Gst ). Then, the model fitting stage is defined: Ft := logPGst (D) = ME(D,CGt), (1) where ME is a \u2018Modelling Engine\u2019 function which is described in more detail (alongside the motivations and specific choices for the function K) in Appendix A.1.4 (Figure 1c). The model fit Ft is stored at this stage.\nPost-processing The post-processing stage is used to produce a \u2018memory\u2019 \u00b5. This stage requires as input the graph at the current iteration Gst, the graph at the previous iteration Gst\u22121, and their associated metrics of fit, Ft and Ft\u22121, respectively. The function LLM\u00b5 produces a memory at iteration t as \u00b5t = LLM\u00b5(Gst,Gst\u22121, Ft, Ft\u22121). Memories encode information about changes to the causal graph, their implications, and their impact on model fit, and are stored at this stage.\nHypothesis amendment Hypothesis amendment is divided into two phases, which we call the \u2018global\u2019 and \u2018local\u2019 phases. In the global phase, the function LLM acts as a critic which proposes a set of amendments Aglob by taking as input the (structured) graph at t \u2212 1, metadata M, and an optional memory \u00b5t\u22121: Aglob = LLM(Gst\u22121, \u00b5t\u22121;M). This phase is expected to keep track of high-level relationships and account for the overarching structure of the DAG, and can be guided by previous amendments through the memory system (Figure 1a). It should be noted that the hypothesis generation stage can be seen as a special case of the global amendment phase, which takes an empty graph as input (with no memory).\nThe local phase considers pairwise comparisons between vertices u, v \u2208 V, u \u0338= v, where V is the set of vertices in the structured causal graph. This phase makes amendments Aloc as:\nAloc(u, v) = {\nADJUST(u, v;M) if edge exists between u and v ASSESS(u, v;M) otherwise,\n(2)\nwhere ADJUST(u, v;M) is an LLM-based function that outputs one of three actions: {keep, remove, reverse}, and ASSESS(u, v;M) is a similar function which outputs one of the following options: {No direct causality, u \u2192 v, v \u2192 u}. We can augment the metadata of the LLM to M \u222a C, where C is additional domain-specific context. This is known as a Retrieval Augmented Generation (RAG) pattern (Lewis et al., 2020) and can be used to encourage or enforce specific relationships according to domain expertise (Figure 1b). This is expected to introduce edges (under specific domain considerations), and otherwise to critique, validate, and/or prune the edge outputs from the global phase. Hypothesis amendment at iteration t, HAt, can be seen as a compositional set of amendments to the structured graph from iteration t\u2212 1:\nHAt(Gst\u22121, \u00b5t\u22121;M) = {Aglob} \u222a {Aloc(u, v) | \u2200u, v \u2208 V, u \u0338= v}. (3)\nThe hypothesis amendment, model fitting, and post-processing stages (Figure 1a-b, 1c, and 1d) iterate throughout a given experiment. The order of iteration is shown in Algorithm 1. In our implementation of the CMA, the model fitting stage creates a DSCM (Pawlowski et al., 2020), which enables reasoning over multi-modal datasets. We additionally propose a novel modelling approach, whereby we parameterise chain graph models with DL elements to represent associative but not necessarily causal links (for example, due to unmeasured confounding between the endogenous variables). Additional background on DSCMs and chain graphs, as well as further implementation details, can be found in Appendix A.1.\nAlgorithm 1 Iterative procedure of the CMA Framework\nRequire: Empty graph G\u2205, data D and metadata M Iteration 0\n1: Gs0 := e(LLM(G\u2205, \u2205;M)) \u25b7 Hypothesis Generation (no memory) 2: F0 := logPGs0 (D) \u25b7 Model Fitting\nIteration 1 1: Gs1 := e(HA1(G0, \u2205;M)) \u25b7 Hypothesis Amendment (no memory) 2: F1 := logPGs1 (D) \u25b7 Model Fitting 3: \u00b51 := LLM\u00b5(Gs1,Gs0, F1, F0) \u25b7 Post-processing\nIteration \u2265 2 1: while \u00ac Early stopping criterion do \u25b7 Early stopping criterion 2: Gst := e(HAt(Gt\u22121, \u00b5t\u22121;M)) \u25b7 Hypothesis Amendment (with memory) 3: Ft := logPGst (D) \u25b7 Model Fitting 4: \u00b5t := LLM\u00b5(Gst,Gst\u22121, Ft, Ft\u22121) \u25b7 Post-processing 5: end while4 EXPERIMENTS  Our overarching hypothesis is that the CMA is effective at the task of causal discovery. The experimental logic is as follows: First, we assess the constituent elements of the CMA to ensure they are appropriate for reasoning over causal graphs. The synthetic neuropathic protein experiment assesses the data-driven module of the CMA (described in more detail in Appendix A.1.4). We hypothesise that correctly defined causal graphs lead to higher data likelihoods under the model (up to Markov equivalence; see Appendix A.2.3) than misspecified graphs using a simple DAG setting. Whilst we focus on the data-driven module of the CMA, we present additional results on LLM behavioural patterns in Appendix A.2.4.\nSecond, we wish to assess the CMA relative to other data- or metadata-driven approaches for causal discovery. We assess the performance of various methods on three causal discovery benchmarks where a ground-truth graph is known a priori: 1) The Arctic Sea Ice benchmark; 2) the Sangiovese benchmark; 3) an Alzheimer\u2019s Disease (AD) benchmark. We develop the AD benchmark in collaboration with 5 domain experts, with expertise in either clinical neurology (with a specialist interest in AD) or neuroradiology, with a specialist interest in neurodegenerative diseases. Additional details can be found in Appendix A.5.1.\nFinally, to assess the CMA in a real-world setting with multi-modal data, we apply it to data from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI), where the task is to jointly model the clinical and radiological phenotype of the disease. In contrast to the AD benchmark, the ADNI dataset contains noisy real-world data representing the complex aetiological process of the disease, for which the ground-truth ultimately remains contentious (Herrup, 2015; Gulisano et al., 2018). Additionally, we demonstrate the CMA\u2019s ability to perform a causal discovery task in a multi-modal data setting (tabular data and images), which is not possible for any of the other baseline methods. We assess the implications of the causal graph proposed by the CMA and extract various insights relating to biomarkers of the disease.4.1 SYNTHETIC NEUROPATHIC PROTEIN EXPERIMENT  Experimental setup To validate the modelling engine for the model fitting stage (Appendix A.1.4), we consider a simple DAG whose node description relates to the health sciences.\nPhosphorylated-tau (P-tau) and \u03b2-amyloid are proteins which are involved in neurodegenerative processes in the human brain, and both are affected by the ageing process. We consider the DAG \u2018Age\u2192AV45\u2019, \u2018Age\u2192 P-tau\u2019, and \u2018AV45\u2192 P-tau\u2019, as per the classical amyloid cascade hypothesis (Hardy & Higgins, 1992). Here, AV45 refers to \u03b2-amyloid levels as measured by the Florbetapir F 18 tracer following Positron Emission Tomography (PET) scanning (Varghese et al., 2013; Ma\u0301rquez & Yassa, 2019). Based on the synthetic distributions, we generate 10,000 independent and identically distributed (i.i.d.) data points from the resulting graph using ancestral sampling and test the hypothesis that a correctly defined DAG can appropriately induce the observational distribution. We train each model 100 times on the data points and calculate an average data likelihood. Additional implementation details as well as results relating to Markov equivalence in graphs and the CMA\u2019s behavioural patterns can be found in Appendix A.2.\nResults Figure 2 illustrates the ground-truth DAG alongside three modelling strategies. We assess the three strategies by comparing the data likelihoods under each model. The first DAG (DAG 1) is a perfect map of the data-generating process, which is defined as an equivalence of independence statements in the graph and the ground-truth distribution. In other words, the graph matches the data-generating process. This model has an average data likelihood of 4402.68, with a standard deviation of \u00b126.70, and this acts as the reference ground-truth likelihood. The second DAG (DAG 2) removes a causal link which exists in the data-generating process, thereby adding a graphical independence statement not present in the ground-truth distribution. The second model is, therefore, a deficient representation of the ground-truth distribution, and has an average data likelihood of 3857.53 \u00b1 18.98. The third DAG (DAG 3) adds a chain component between AV45 and P-tau, which represents an associative but not necessarily causal link. This model has an average data likelihood of 3757.55 \u00b1 60.80. The Tukey HSD test (Abdi & Williams, 2010) is used for pairwise model comparisons whilst ensuring the overall error rate for the family of comparisons is kept to the 0.05 threshold by control of the Familywise Error Rate (FWER). There is a statistically significant difference in likelihood between all three models, with the model representing the data-generating process having the highest data likelihood. Detailed results of the comparisons are shown in Table 1. As can be seen, the results validate our initial hypothesis: The model that aligns most closely with the true data-generating process produces the highest data likelihood. As expected, we find that this is only valid up to the Markov equivalence class of the ground-truth DAG (see Appendix A.2.3).4.2 BENCHMARKING EXPERIMENTS  Experimental setup We consider the task of recovering causal graphs on a number of causal discovery datasets. The Arctic sea ice dataset (Huang et al., 2021b) is from the field of atmospheric science and is an increasingly popular dataset for the task of full causal graph discovery (K\u0131c\u0131man et al., 2023). This dataset considers the relations of several geophysical variables to sea ice thick-\nness (12 nodes; 48 true edges). A full description can be found in Appendix A.3.1. The Sangiovese dataset is from the field of agricultural science and is a conditional linear Gaussian Bayesian Network from the popular bnlearn R package (Magrini et al., 2017). The DAG considers several variables that relate to grape quality in Sangiovese vineyards in Tuscany (15 nodes; 55 true edges). Additional information can be found in Appendix A.4.1. The Alzheimer\u2019s dataset is another conditional linear Gaussian Bayesian Network that we developed in collaboration with 5 domain experts. The synthetically generated dataset considers demographic, clinical, imaging-based biomarker, and cognitive assessment variables, and how they relate to the aetiology of AD (11 nodes; 19 true edges). We construct the ground-truth graph based on a consensus heuristic, for which full details can be found in Appendix A.5.1. We compare the CMA with commonly used, state-of-the-art, data-driven algorithms including NOTEARS (Zheng et al., 2018), DAG-GNN (Yu et al., 2019), and TCDF (for the Arctic sea ice dataset; more details in A.3.2) (Nauta et al., 2019). We also compare the CMA with metadata-based benchmarks as per K\u0131c\u0131man et al. (2023). The experimental setup for the metadata-based (LLM) benchmarks is given in Appendix A.3.2.\nResults The results of the benchmarking experiments can be seen in Table 2. We report the normalised Hamming distance (NHD), which is defined for a predicted graph G\u2032 and ground-truth G as 1/m2 \u2211m i,j=1 I[Gi,j \u0338= G\u2032i,j ]: The edges in one graph and not the other, normalised by the total number of possible edges. It should be noted that the NHD depends on the number of edges reported by a causal discovery algorithm. Consequently, we use the approach described in K\u0131c\u0131man et al. (2023), whereby we report the ratio of the NHD with a Baseline Hamming distance (BHD). The BHD is defined as a graph that contains the same number of edges as G, but all of them are incorrect. A lower NHD/BHD ratio is the multiple by which the causal discovery algorithm outperforms the \u2018floor\u2019 baseline graph.\nThe CMA outperforms causal discovery techniques which rely exclusively on data- or metadata (LLM)-driven approaches. In multiple instances, the CMA is capable of proposing additional causal relations within \u2018internal\u2019 variables, which we define as variables for which we have direct access to data. We give an illustrative example from the Arctic sea ice dataset. The CMA proposes that there\nis a 70% probability that \u2018net longwave flux at the surface\u2019 (LW) should have a causal relationship to \u2018sensible plus latent heat flux\u2019 (HFLX). The CMA\u2019s reasoning trace is shown in Appendix Figure 12. Observational samples as well as counterfactual inference under the causal model which encodes this edge suggest a positive relationship between the variables (Appendix A.3.4). Whilst this relationship is not present in the original causal graph, nor are its dynamics present in the domain literature used to construct it (Huang et al., 2021b), increased absorption of longwave radiation (LW) produces tropospheric warming, leading to an increase in sea surface temperature (SST), until SSTdependent cooling increases to establish equilibrium, which occurs through LW\u2019s effect on latent and sensible heat fluxes (HFLX) (Bates et al., 2012). We therefore contend this is an example of a missing edge from the ground-truth graph detected by the CMA. Extended results for the Arctic sea ice, Sangiovese, and AD benchmarks can be found in Appendix sections A.3.3, A.4.3, and A.5.3, respectively.4.3 CASE STUDY: ALZHEIMER\u2019S DISEASE NEUROIMAGING EXPERIMENT  Experimental setup In this experiment, a CMA is applied to a challenging real-world dataset from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) (Petersen et al., 2010). We consider the same variables as in the AD benchmark experiment above (Section 4.2), with the exception that we link the clinical and radiological phenotypes of AD by considering how the variables might impact a brain Magnetic Resonance Image (MRI) for a given participant. This allows us to assess the effects of interventions and counterfactual queries in the imaging space. For example, if we intervene on brain volume by reducing it, we would expect a reduction in brain size on the MRI image in a reasonable model. We assess the counterfactual implications of the model proposed by the CMA and illustrate the framework\u2019s utility for identifying potentially useful external variables for inclusion in the causal graph. Additional experimental setup details can be found in Appendix A.6.\nResults The CMA outputs a trained SCM that encodes functional causal relationships between the disease covariates and the presentation of the disease in MRI images. Figure 3a illustrates the CMA\u2019s proposed causal graph, whilst 3c illustrates the biologically plausible output of counterfactual queries in the imaging space for qualitative evaluation. For example, increasing age produces visible cortical degeneration in the counterfactual image (Figure 3c; row 2). It should be noted that increasing the age value not only demonstrates cortical neurodegeneration, but the ventricles are also expanded, which is the expected neuro-radiological result of increasing age (Dinsdale et al., 2021). Directly intervening on whole brain and ventricular volumes (Figure 3c; rows 1 and 4, respectively) leads to expected morphological effects in both instances.\nAs with the Arctic sea ice dataset, we note interesting edges proposed for internal variables in the causal graph. In particular, we note the relationship \u2018biological sex\u2192tau pathology\u2019. This relationship was not proposed by any of the domain experts for the AD benchmarking task in Section 4.2. The graphs proposed by the experts can be seen in Appendix A.5. Nonetheless, counterfactual inference using the model trained by the CMA (under the ADNI dataset) provides evidence of increased P-tau protein levels in females (Figure 3b). Counterfactually intervening on sex led to a statistically significant shift in P-tau levels following Welch\u2019s t-test of unequal variances. This was observed when intervening on both males and females (t = \u22126.84, p < 0.001, and t = 6.05, p < 0.001, respectively). Despite not reflecting current expert consensus, there is emerging evidence that such a mechanism exists. In recent work by Yan et al. (2022), it was shown in both in-vitro and invivo models that the X chromosome-linked protein ubiquitin-specific peptidase 11 (USP11) alters tau aggregation patterns by deubiquitination initiated at lysine-218. Deubiquitination (removal of ubiquitin, a 76 amino acid protein), enhances tau buildup, which produces a damaging effect on brain tissue. USP11 \u2018escapes\u2019 complete X-inactivation (Yan et al., 2022), meaning females exhibit greater levels than males, and delimits the causal relationship between having an XX genotype and increased tau pathology.\nAs the CMA leverages LLMs to produce hypotheses, it is capable of proposing potentially confounding or modulating variables for which we do not have direct access to data (\u2018external\u2019 variables). For example, whilst assessing a potential relationship between the APOE4 gene and tau pathology (APOE4\u2192tau pathology), the CMA proposes that the TREM2 gene may be a potential confounder. The reasoning trace is shown in full in Appendix A.7. We accrue data for the soluble form of TREM2 (sTREM2) from the ADNI dataset and include it in the causal graph with a directed edge to P-tau (sTREM2\u2192tau pathology). By counterfactual inference, we find a statistically significant positive relationship between sTREM2 and P-tau (t = 6.55, P < 0.001). Whilst several associational studies exist with inconclusive results (Sua\u0301rez-Calvet et al., 2019; Zhao et al., 2022), we believe this analysis to be the first counterfactual-inference-based approach to analysing this relationship. TREM2 is an immune receptor expressed by support cells of the central nervous system (Bouchon et al., 2001), and is broken down by ADAM metalloprotease (Kleinberger et al., 2014) to produce its soluble form. Whether this breakdown (and therefore reduction of TREM2 on the cell surface membrane) leads to increased tau pathology, or whether sTREM2 itself exerts a specific biological function is unclear (Filipello et al., 2022). Whilst our work proposes a potential relation from sTREM2 to tau deposition, the underlying mechanism is currently unknown (Filipello et al., 2022).5 CONCLUSION  We introduce the CMA, a modular framework for causal discovery with large language models and deep SCMs (DSCMs), enabling causal discovery for multi-modal data. Through a diverse set of experiments on synthetic and real-world datasets from a range of scientific fields (agriculture, geophysics, and health), we demonstrate that the CMA outperforms existing data- or metadatadriven approaches for causal discovery. The CMA is capable of proposing insightful relationships between variables internal and external to the DSCMs, and we provide a number of illustrative examples in the geophysics and health domains.\nFuture work should address several important limitations. Whilst discrete variables with parents currently require continuous relaxation, DSCMs could support discrete mechanisms with a Gumbel\u2013max parametrisation (Pawlowski et al. (2020), Appendix C). In our AD case study, our model used 2D axial slices of the brain, which may not be optimal for producing counterfactuals based on 3D volumes; hence, incorporating 3D volume modelling in DSCMs is a potential extension. Although the observational distribution is determined by a graph\u2019s Markov Equivalence Class (MEC), there is evidence that LLMs can effectively reduce the MEC by ruling out implausible graphs (Long et al., 2023a). We discuss these ideas in more detail in Appendix A.2.4. Finally, The CMA relies on DSCMs to fit data, which assumes a Markovian DAG. Whilst we extend the framework to include chain graph elements, in practice, we found that LLMs struggle to represent chains in a fully automated manner- instead, the CMA proposes potentially bidirectional relationships and human intervention is necessary to allow appropriate Deep Chain Graph modelling. Investigating techniques to enable fully automated chain graph modelling, and indeed more generally extending deep SCMs to represent more flexible, non-Markovian causal graphs (for example, models which allow for feedback loops) represents a natural avenue of future research. \\\n            \n            Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$\\assistant\n\n$$\n\nPaper Summary:\nThe paper introduces the Causal Modelling Agent (CMA), a novel framework that combines Large Language Models (LLMs) with Deep Structural Causal Models (DSCMs) for causal discovery. The CMA uses LLMs to generate hypotheses about causal relationships and DSCMs to fit data and reason over causal graphs. The authors evaluate the CMA on synthetic and real-world datasets and demonstrate its ability to outperform existing data- or metadata-driven approaches for causal discovery.\n\nStrengths:\n1. The CMA provides a unified framework for causal discovery that combines the strengths of LLMs and DSCMs.\n2. The authors demonstrate the CMA's ability to propose insightful relationships between variables internal and external to the DSCMs.\n3. The CMA's use of LLMs as a prior, critic, and hypothesis engine allows for efficient exploration of causal graph space.\n4. The authors provide a detailed explanation of the CMA's components and their interactions.\n\nWeaknesses:\n1. The CMA relies on DSCMs to fit data, which assumes a Markovian DAG. This limitation may restrict the CMA's ability to represent non-Markovian causal graphs.\n2. The authors mention that LLMs struggle to represent chains in a fully automated manner, which may require human intervention.\n3. The CMA's performance on real-world datasets may be limited by the availability and quality of data.\n4. The authors do not provide a comprehensive evaluation of the CMA's ability to handle missing data, confounding variables, or other common challenges in causal discovery.\n\nSuggestions for Improvement:\n1. Investigate techniques to enable fully automated chain graph modelling and extend DSCMs to represent more flexible, non-Markovian causal graphs.\n2. Develop methods to handle missing data, confounding variables, and other common challenges in causal discovery.\n3. Provide a more comprehensive evaluation of the CMA's performance on real-world datasets, including datasets with complex causal structures and limited data.\n4. Consider incorporating additional domain-specific knowledge and expert input into the CMA to improve its performance and generalizability.\n\nRecommendation:\nThe CMA is a promising framework for causal discovery that combines the strengths of LLMs and DSCMs. While it has some limitations, the authors have provided a detailed explanation of the CMA's components and their interactions. With further development and evaluation, the CMA has the potential to become a powerful tool for causal discovery in a wide range of scientific domains.\n\n$$",
        "peer_review_with_watermark": "$$\n\nPaper Summary:\nThis paper presents the Causal Modelling Agent (CMA), an innovative framework that merges the reasoning capabilities of Large Language Models (LLMs) with the modelling strengths of Deep Structural Causal Models (DSCMs) for causal discovery. It evaluates the CMA's performance through various benchmarks and a real-world application concerning Alzheimer's Disease, demonstrating that the CMA surpasses previous methodologies that relied solely on data-driven or metadata-driven approaches. It successfully extracts significant insights into the causal relationships among biomarkers related to Alzheimer's.\n\nStrengths:\n1. The integration of LLMs and DSCMs provides a unique and powerful approach to causal discovery, capitalizing on their respective strengths.\n2. The comprehensive evaluation on a variety of datasets, including complex real-world data, showcases the robustness and versatility of the CMA.\n3. The CMA's capability to propose novel causal relationships that may not have been previously identified is a valuable contribution to the field.\n4. The paper is well-structured and clearly details the framework's operation, allowing readers to easily grasp the methodology.\n\nWeaknesses:\n1. The CMA's dependence on the assumption of a Markovian structure could limit its performance in scenarios where such an assumption is invalid.\n2. There are challenges regarding LLMs\u2019 capabilities to represent certain causal structures without human input, which may hinder automated applications.\n3. The framework could benefit from further validation across diverse datasets to ascertain its generalizability and robustness.\n4. The discussion on discrete variables needing continuous relaxation is a necessary consideration but somewhat limits the potential application scope of the CMA.\n\nSuggestions for Improvement:\n1. Future iterations of the CMA should explore methods for addressing non-Markovian causal relationships for broader applicability in real-world scenarios.\n2. Enhancing the CMA to automate chain graph modelling would reduce reliance on human intervention and improve practicality.\n3. Conduct additional testing of the CMA on a broader set of datasets, particularly with high noise levels or incomplete data.\n4. Investigate and implement strategies to encompass discrete variables within the framework, thereby widening its applicability in diverse research contexts.\n\nRecommendation:\nThe CMA framework shows great potential in advancing the field of causal discovery, particularly within complicated datasets that require sophisticated modelling techniques. While improvements are essential to maximize its capabilities, this work lays a solid foundation for further exploration. I recommend continued development and refinement of the CMA, with specific attention toward accommodating more complex causal structures and enhancing its operational robustness.\n\n$$",
        "gr_score_with": 0.9027777777777778,
        "gr_score_without": 0.932285368802902,
        "output_without": {
            "num_tokens_scored": 4962,
            "num_green_tokens": 4626,
            "green_fraction": 0.932285368802902,
            "z_score": 7.58076226174951,
            "p_value": 1.717652918979329e-14
        },
        "output_with": {
            "num_tokens_scored": 432,
            "num_green_tokens": 390,
            "green_fraction": 0.9027777777777778,
            "z_score": 0.19245008972987343,
            "p_value": 0.4236948298433579
        }
    },
    {
        "title": "LANGUAGE-INFORMED VISUAL CONCEPT LEARNING",
        "abstract": "Our understanding of the visual world is centered around various concept axes, characterizing different aspects of visual entities. While different concept axes can be easily specified by language, e.g., color, the exact visual nuances along each axis often exceed the limitations of linguistic articulations, e.g., a particular style of painting. In this work, our goal is to learn a language-informed visual concept representation, by simply distilling large pre-trained vision-language models. Specifically, we train a set of concept encoders to encode the information pertinent to a set of language-informed concept axes, with an objective of reproducing the input image through a pre-trained Text-to-Image (T2I) model. To encourage better disentanglement of different concept encoders, we anchor the concept embeddings to a set of text embeddings obtained from a pre-trained Visual Question Answering (VQA) model. At inference time, the model extracts concept embeddings along various axes from new test images, which can be remixed to generate images with novel compositions of visual concepts. With a lightweight test-time finetuning procedure, it can also generalize to novel concepts unseen at training. Project page at https://cs.stanford.edu/ \u0303yzzhang/projects/concept-axes.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it.  Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$user\n\nThe peer review format and length should be of standard conference. \\\n            title: LANGUAGE-INFORMED VISUAL CONCEPT LEARNING \\\n            abstract: Our understanding of the visual world is centered around various concept axes, characterizing different aspects of visual entities. While different concept axes can be easily specified by language, e.g., color, the exact visual nuances along each axis often exceed the limitations of linguistic articulations, e.g., a particular style of painting. In this work, our goal is to learn a language-informed visual concept representation, by simply distilling large pre-trained vision-language models. Specifically, we train a set of concept encoders to encode the information pertinent to a set of language-informed concept axes, with an objective of reproducing the input image through a pre-trained Text-to-Image (T2I) model. To encourage better disentanglement of different concept encoders, we anchor the concept embeddings to a set of text embeddings obtained from a pre-trained Visual Question Answering (VQA) model. At inference time, the model extracts concept embeddings along various axes from new test images, which can be remixed to generate images with novel compositions of visual concepts. With a lightweight test-time finetuning procedure, it can also generalize to novel concepts unseen at training. Project page at https://cs.stanford.edu/ \u0303yzzhang/projects/concept-axes. \\\n            paper text: 1 INTRODUCTION  In order to make sense of the myriad visual entities in the world, humans develop an abstracted generative model of them and organize the underlying sources of variation into visual concepts, such as different colors or different types of objects. Designing systems that can recognize visual concepts within images as humans do has been a longstanding goal in the fields of computer vision and artificial intelligence (Russakovsky et al., 2015; Krizhevsky et al., 2012; Girshick et al., 2014).\nTo facilitate efficient reasoning and communication of these concepts, humans created symbolic depictions that have evolved into natural language. Such natural language grounding of visual data has been instrumental in the recent proliferation of powerful large vision-language models that are capable of semantically identifying objects in images (Radford et al., 2021; Kirillov et al., 2023) or generating photo-realistic images from arbitrary text prompts (Ramesh et al., 2021; Rombach et al., 2022; Saharia et al., 2022; Yu et al., 2022). While different concept axes can be easily specified by words, such as category and style, it is much less intuitive to delineate the subtleties of low-level visual nuances along each axis using language, such as one particular style of a painting.\nIn this work, our goal is to distill from large pre-trained vision-language models a function that extracts visual concepts along a set of language-specified concept axes from images. As illustrated in Figure 1, once these concepts are extracted, we can recompose them across different image instances at inference time to produce new images with novel concept combinations. To learn this function, rather than collecting a large-scale dataset of human annotations for each specific visual concept, we design a language-informed visual concept representation, and simply distill from a pre-trained Text-to-Image (T2I) generation model. There are three fundamental properties we seek in this visual concept representation.\nFirst, unlike T2I generation, which relies on generic words as visual concept descriptors, we would like to capture fine-grained visual nuances using continuous concept embeddings. One common technique is to invert the text-to-image generation process by optimizing an embedding with the objective of reproducing a given input image using a pre-trained T2I model, often referred to as Textual Inversion (Gal et al., 2022). However, most existing Textual Inversion methods (Gal et al.,\n\u2217Equal contribution; alphabetically ordered.\n\u201ca photo of red banana\u201d \u201ca photo of <color> <category>\u201d<color><category>\nRecompositionSourceVanilla text prompt\nO ur s B as el in e\nFigure 2: Learned Disentangled Concept Embeddings Improve Compositionality. Left: Vanilla text-to-image model may fail to adhere to text prompts of uncommon combinations of concepts even with prompt engineering, e.g. \u201cred banana\u201d. Right: With the same backbone T2I generator, our learned disentangled concept embeddings greatly enhance concept compositionality.\n2022) optimize embeddings for individual image instances independently, overlooking the shared nature of visual concepts across instances. For instance, the concept of \u201cred\u201d is shared between a \u201cred apple\u201d and a \u201cred dress\u201d. Moreover, the concepts of \u201cred\u201d and \u201cyellow\u201d also are instances of the property of color.\nHence, the second desired property of the visual concept representation is to preserve such common concept structures among various visual instances. Instead of optimizing on individual image instances independently, we design a set of concept encoders, where each encoder learns to encode the visual characteristics of an input image pertaining to one concept axis specified by language. This ensures that the inverted concept embeddings can be shared across different instances and remixed to generate new images.\nThe third crucial aspect of this representation is to ascertain that different concept axes are disentangled, allowing for changes to be made specifically on single concept axis without modifying other axes. To do so, we reuse the disentangled nature of linguistic concepts and ground the predictions to a set of discrete text anchors in the concept embeddings space, which can be obtained by querying a pre-trained generic Visual Question Answering (VQA) model, e.g., BLIP-2 (Li et al., 2023b). This soft anchoring constraint significantly improves the disentanglement of concept embeddings across different axes while still retaining sufficient leeway to capture nuanced visual variations that BLIP-2 struggles to discern, e.g., the style of an art piece in Figure 6.\nPutting these ideas together, we design a generic framework for learning disentangled and compositional visual concepts grounded to linguistic structures by exploiting pre-trained text-to-image generation and visual question answering models. We show that these concept encoders can be trained purely on synthetic images generated by a pre-trained T2I model, and extract concept embeddings from real images at test time, which capture the fine-grained visual nuances.\nOur contributions can be summarized as follows:\n1. We propose a generic framework for learning language-informed visual concepts by simply distilling pretrained vision-language models.\n2. At inference time, the trained concept encoders extract concept embeddings from a test image, which can be remixed to generate images with novel compositions of concepts.\n3. Using a light-weight test-time finetuning procedure, these encoders can also be quickly adapted to extract novel concepts unseen during training.\n4. Experiments show that this visual concept representation achieves better disentanglement and compositionality, compared to text-based prompting baselines, as shown in Figures 2 and 6.2 RELATED WORK  2.1 VISUAL CONCEPT LEARNING  Designing learning-based systems to discover various visual concepts in natural images has been a long-standing goal in machine perception and intelligence. Early attempts typically rely on extensive semantic annotations done by humans, such as object classification (Barnard et al., 2003; FeiFei et al., 2006; Fergus et al., 2005), which were later epitomized by the effort of ImageNet (Russakovsky et al., 2015). Visual concepts are intrinsically linked to concepts in language, and such end-to-end supervised learning paradigms can be seen as learning a direct mapping between visual concepts and discrete linguistic concepts. Other approaches attempt to better exploit this inherent structure in language by constructing a structured representation of visual concepts such as scene graphs (Zhong et al., 2021) and symbolic programs (Mao et al., 2019; Han et al., 2019).\nMore recently, the success of natural language modeling (Devlin et al., 2018; Brown et al., 2020; Raffel et al., 2020) has paved the way for grounding visual concepts to open vocabularies, unlike category labels or fixed symbolic programs, by training large Vision-Language Models (VLMs) on massive image captioning datasets (Schuhmann et al., 2022). This has powered recent Text-to-Image (T2I) generation models to turn linguistic concepts from free-form text prompts into photo-realistic images (Rombach et al., 2022; Saharia et al., 2022). These T2I models have been leveraged by Personalization methods for extracting individual visual concepts from one or a few images. This is done by either by optimizing token embeddings (Gal et al., 2022; Vinker et al., 2023; Avrahami et al., 2023; Chefer et al., 2023b; Liu et al., 2023), finetuning the backbone denoiser (Ruiz et al., 2023), or training additional encoders for amortized optimization (Gal et al., 2023; Arar et al., 2023; Li et al., 2023a). We also distill visual concepts from a pre-trained T2I model, but unlike existing works, we train encoders to adhere to a set of language-specified concept axes, preserving the disentangled and compositional nature of language. Ranasinghe & Ryoo (2023) also explores language-defined concepts but focuses on video action recognition tasks while we focus on image generation.\nA separate line of work focuses on unsupervised visual concept disentanglement without explicitly leveraging language, typically by simply imposing information constraints in the latent space of a generative model, like VAEs and GANs (Higgins et al., 2017; Chen et al., 2016; Hsu et al., 2023). Here, we are interested in learning visual concepts that are explicitly grounded to language.2.2 CONTROLLABLE IMAGE GENERATION  The success of GAN-based image generation (Goodfellow et al., 2014; Brock et al., 2018; Karras et al., 2019) has spawned a series of works that discover controllable directions in the GAN latent space (Voynov & Babenko, 2020; Ha\u0308rko\u0308nen et al., 2020). More recently, the advancements of diffusion-based T2I models have unlocked new possibilities for controllable image generation, where photo-realistic images can be generated from free-form text prompts. Recent works proposed to further improve the alignment of image samples and input text conditions by manipulating attention maps within T2I models (Chefer et al., 2023a; Epstein et al., 2023). Another form of controllable image generation is compositional generation. Liu et al. (2022) proposes to improve the quality of T2I diffusion models for composing multiple pre-given concepts, specified via text prompts, by modifying the inference procedure. In this work, instead of assuming that concepts are given and are in a text format, we tackle the task of identifying disentangled concepts which can be used for composition.\nImage generation can also be controlled with image analogies (S\u030cubrtova\u0301 et al., 2023; Hertzmann et al., 2001), a form of visual prompting. These works typically do not explicitly extracts visual concepts from inputs unlike ours. In this work, we amalgamate both visual prompts and text queries, employing them as the editing interface.3 METHOD  Fig. 3 gives an overview of our proposed learning framework. Our goal in this work is to extract visual concepts from images along a number of concept axes specified by language, such as\ncategory, color, and material, so as to enable the flexible composition of concepts into high-quality image generations.\nTo achieve this, we train a set of visual concept encoders by distilling concept guidance from pretrained vision-language models. Specifically, the encoders are trained to extract concept embeddings from an image in order to fulfill two objectives. First, they should be recomposed to explain the input image through a pretrained text-to-image (T2I) generation model, given a concept-axis-informed text prompt. Second, these visual concept embeddings should be anchored to the corresponding text embeddings obtained from a pre-trained visual question answering (VQA) model, further exploiting the disentangled nature of linguistic concepts for better disentanglement of visual concepts.3.1 VISUAL CONCEPT ENCODING BY INVERTING TEXT-TO-IMAGE GENERATION  Our understanding of the visual world is centered around various concept axes, to which we have often assigned words due to their significance in communication and reasoning. This vision-language grounding has fueled recent explosion of text-to-image generation models (Rombach et al., 2022; Saharia et al., 2022; Ramesh et al., 2022), allowing them to generate photo-realistic images with various combinations of concepts defined by words.\nHere, we are interested in the reverse direction of text-to-image generation, where the goal is to extract language-grounded visual concepts present in natural images. Specifically, given K concept axes of interest defined by language, we would like to learn K concept encoders {fk(\u00b7)}Kk=1, each of which extracts a concept representation ek = fk(x) along a concept axis from an input image x.\nIn order to train these concept encoders {fk(\u00b7)}, instead of relying on extensive human labeling, we opt to exploit the vision-language grounding embedded within large pre-trained T2I generation models. Using the technique of Textual Inversion (Gal et al., 2022), one can optimize a token embedding <*> to capture a visual entity in a given image, through the objective of regenerating the image with the T2I model from a text template, such as \u201ca photo of <*>\u201d. Here, we adopt a similar objective, but instead of inverting a specific embedding capturing the overall \u201cidentity\u201d of an individual image instance, we would like to predict embeddings ek that are grounded to a number of meaningful concept axes, using an axis-informed text template, such as \u201ca photo of <e1> with <e2> color and <e3> material\u201d. This allows the extracted concept embeddings to be shared across different images, encapsulating the common visual characteristics pertinent to one concept axis.\nSpecifically, given an image x, the concept encoders {fk(\u00b7)} extract a set of concept embeddings {ek \u2208 RD}, which have the same dimension D as the text embeddings so that they can be directly inserted into the text embeddings of the axis-informed text template. To simplify the notations, let f\u03b3(\u00b7) denote the function that takes in the image and produces the final sequence of embeddings\nof the template and the predicted concept embeddings, and \u03b3 be the parameters of all the encoders which will be optimized during training. Let c\u03b8 be the part of the T2I model\u2019s text encoder that takes in a sequence of text embeddings and outputs a conditioning vector for the T2I model\u2019s denoising network \u03f5\u0302\u03b8, where \u03b8 denotes network parameters. We use DeepFloyd (StabilityAI; Saharia et al., 2022) as the backbone T2I model, which utilizes a pre-trained T5 model (Raffel et al., 2020) as the text encoder, and keep the parameters \u03b8 frozen in all experiments. To train the encoders, we reuse the training objective for the backbone diffusion model:\nLrecon(x; \u03b3) = E\u03f5\u223cN (0,I),t\u223cU([0,1]) [ \u2225\u03f5\u0302\u03b8(x, t, c\u03b8(f\u03b3(x)))\u2212 \u03f5\u222522 ], (1)\nwhere the noise \u03f5 is sampled from a standardmultivariate Gaussian distribution and the timestep t is sampled from a uniform distribution in [0, 1]. Minimizing Lrecon amounts to finding concept embeddings within the space of the pre-trained T2I model that can best reproduce the input image x, resembling a \u201creconstrucion\u201d objective.\nCompared to per-instance token optimization in vanilla Textual Inversion, the advantages of training these concept encoders are two-fold. First, the concept embedding space is naturally shared across different image instances, encapsulating the common understanding of the corresponding concept axes. Second, it makes training more efficient by amortizing the optimization across all instances, and more crucially, it allows for test-time inference in a feed-forward pass.3.2 CONCEPT DISENTANGLEMENT USING TEXT ANCHORS  The objective of Lrecon ensures that the extracted concept embeddings can sufficiently reconstruct the concept of a given image through a pre-trained text-to-image generation model. However, with this loss alone, there is little guarantee that each embedding encodes only the information pertinent to a particular concept axis. In practice, we found that this baseline results in poor disentanglement of different concept axes when remixing the concept embeddings to generate new images, potentially due to the imprecise vision-language grounding in the pre-trained T2I model. For instance, as shown in Figure 8, the extracted category embedding <e1> for \u201cred berries\u201d cannot be remixed with various color embeddings <e2> e.g., \u201corange\u201d, as <e1> is highly entangled with the concept of a \u201cred\u201d color due to the bias in natural images.\nTo encourage better disentanglement of different concept axes, we further incorporate a sparse set of text anchors into the concept embedding space. Along each concept axis like color, we have often named some prominent modes, such as \u201cred\u201d or \u201cyellow\u201d, and these text labels entail clearly disentangled concepts. Therefore, we would like to reuse this disentangled nature of linguistic concepts to improve the disentanglement of visual concepts. To this end, we make use of the text predictions from a pre-trained Visual Question Answering (VQA) model, BLIP-2 (Li et al., 2023b), as pseudo ground-truth anchors for the concept embeddings.\nSpecifically, for each training image x and for each concept axis of interest (e.g., color) indexed by k, we query the BLIP-2 model \u03a8 with the image x and a question qk in natural language that is specific to this concept axis, e.g., \u201cwhat is the color of the object in the image\u201d. Denote the answer from BLIP-2, also in the form of natural language, as \u03a8(x, qk). We encode this answer with the pre-trained text encoder c\u03b8 to obtain a text embedding e\u0303k = c\u03b8(\u03a8(x, qk)). The prediction of our concept encoders fk,\u03b3 is encouraged to stay close to this anchor text embedding:\nLanchork (x; \u03b3) = \u2225fk,\u03b3(x)\u2212 e\u0303k\u222522, where e\u0303k = c\u03b8(\u03a8(x, qk)). (2)\nIt is crucial to highlight that we use these BLIP-2 predictions only as anchors by assigning a small weight to this anchor loss Lanchork during training. Otherwise, the embeddings predicted by the concept encoders could easily collapse to a set of discrete text embeddings and fail to capture the visual nuances in images.3.3 TRAINING AND INFERENCE  Training. Given a collection of training images D containing various combinations of concepts along each axis, the final objective to train the concept encoders consists of the two parts:\nLtotal(\u03b3) = Ex\u223cD [ Lrecon(x; \u03b3) +\nK\u2211 k=1 \u03bbkLanchork (x; \u03b3)\n]. (3)\nInference. At inference time, given a new test image, the concept encoders extract embeddings {ek} capturing its characteristics along each concept axis of interest. These embeddings can be remixed across different images, or be replaced by embeddings converted from explicit words, to produce images with new compositions of visual concepts through the backbone T2I generator.\nGeneralization to Unseen Concepts via Test-Time Finetuning. While the encoders can precisely extract an axis-specific concept that has been seen during training from a new test image, they tend to be less robust to concepts unseen at training. However, with a lightweight test-time optimization procedure, where we use only the reconstruction objective Lrecon to update the parameters for all encoders, \u03b3, these encoders can generalize to novel concepts unseen during training. Note that Lanchor is omitted here in order to capture the visual nuances without over-committing to the coarse text anchors. After training, the encoders have learned to generate outputs within a relatively narrow region of the embedding space, which allows the model to adapt to the test images shown in Figure 5 within around 600 iterations while maintaining disentanglement and compositional capability.4 EXPERIMENTS  4.1 EXPERIMENT SETUP  Training Data Generation. We train the concept encoders only using synthetic images generated by DeepFloyd from 5 different domains, including fruits, figurines, furniture, art, and clothing. More details of our dataset can be found in A.2. For each dataset, we consider 2-3 concept axes, such as category, color, material, style, and season. For example, considering category and color for the fruits dataset, we generate training images by prompting DeepFloyd with text prompts describing varying combinations of categories and colors, e.g. \u201ca photo of an apple which is red in color\u201d. Note that these text prompts are used only for data generation and not for training, as they may not be reliable (Figure 2). On average, we obtain 669 training images for each dataset.\nImplementation Details. Inspired by Gal et al. (2023), we leverage a pre-trained CLIP ViT/L14 model for image encoding (Radford et al., 2021; Dosovitskiy et al., 2021), which was trained with a contrastive objective aligning image and text features, and hence well-suited for our task. We extract image features from CLIP ViT and train K separate concept encoders fk on top of the features, which share the same architecture but maintains separate weights. Specifically, we take the [CLS] tokens from each CLIP layer and process each token with a distinct linear layer. This is different from Gal et al. (2023), which extracts [CLS] tokens from every other layer and uses a single shared linear layer for all token features. The transformed features are then aggregated with average pooling followed by a LeakyReLU (Xu et al., 2015) activation, and passed into another linear layer that produces the final predicted concept embeddings.\nTo ground the concept embeddings to the concept axes, we adapt the text templates from CLIP (Radford et al., 2021), which were originally used to assemble captions with class categories from ImageNet (Russakovsky et al., 2015). For training, we use AdamW (Loshchilov & Hutter, 2017) optimizer with learning rate 0.02, and randomly flip the images horizontally. For test-time finetuning, we use the AdamW optimizer with learning rate 0.001. We set \u03bbk = 0.0001 (Equation (3)) for the category axis and \u03bb = 0.001 for others. We use IF-I-XL from DeepFloyd as the backbone model, with training resolution 64 \u00d7 64. Training on one dataset takes approximately 12 hours on one NVIDIA GeForce RTX 3090 GPU. Generated images are upsampled 256 \u00d7 256 using IF-II-L for visualization purpose.4.2 QUALITATIVE RESULTS  Visual Concept Extraction, Recomposition and Extrapolation. Once trained, the concept encoders can extract disentangled concept embeddings specific to each concept axis from different test images, which can recomposed to generate new images with various concept compositions. As shown in Figure 4, across various datasets, our method is able to recompose axis-specific visual concepts from different images and consistently generate new images depicting the recomposed concepts. More examples can be found in Appendix A.1, where images generated from individual decomposed concept embeddings are also presented.\nThis disentangled concept representation also allows us to extrapolate along a particular concept axis for visual exploration, as shown in Figure 7. For instance, we can ask BLIP-2 \u201cwhat is the style\nof the painting?\u201d in an image, and prompt GPT-4 (OpenAI, 2023) to name a few alternatives. We can then recompose the text embeddings of these alternative styles with our concept embeddings and generate images to visualize these variants. Representing concepts as continuous embeddings further enables concept interpolations. Details and results are shown in Appendix A.6.\nGeneralization to Unseen Concepts via Test-Time Finetuning. Although the encoders have only seen a limited range of concepts during training due to the small size of the training dataset, it can be quickly adapted to unseen concepts with the lightweight test-time finetuning procedure in Section 3.3, as shown in Figure 5. For instance, after 600 finetuning iterations, the model can extract the specific style of the dog painting unseen at training and compose it with the content from other images. It can also capture nuanced colors e.g. yellow-ish-orange and transfer them to other objects.4.3 COMPARISON WITH PRIOR WORKS  Qualitative Comparisons While this task of image generation with disentangled visual concepts is new, we identified prior work that is capable of text-based image editing and generation, and establish a side-by-side comparison on the task of visual concept editing. Specifically, given an image x, for example, of a teal-colored apple in Figure 6, the task is to generate a new image x\u0302ek\u2192e\u2032k with one concept axis k (e.g. category) modified by a text prompt, from <ek> to <e \u2032 k>, while preserving other axes {i|i \u0338= k} from the input. We compare to two existing methods. Null-text Inversion (Mokady et al., 2022) performs concept editing by first inverting the input image to token embeddings of Stable-Diffusion (Saharia et al., 2022) and then applying Prompt-to-Prompt (Hertz et al., 2022), which modifies cross attention maps for editing, a process that leads to pixel-aligned editing results. InstructPix2Pix (Brooks et al., 2023) is a conditional diffusion model for text-based image editing. Since it is trained on data curated with Prompt-to-Prompt for supervision, it also tends to generate pixel-aligned results. We also design a naive baseline for this task, where we query BLIP-2 for a text description of the attributes to be retained from the test image, and combine the answer with a target concept to generate the final recomposed image. As shown in Figure 6, our method achieves better recomposition results, whereas baseline methods fail to disentangle the desired axis-specific concepts from input images, and therefore struggle to faithfully reproduce the desired concepts from the image and the text.\nQuantitative Comparisons. We also conduct quantitative evaluations on the task of text-based visual concept editing and compare with prior work. Specifically, we record the ground-truth text\nprompts y that we used to generate each training image x, and manually filter out the inconsistent pairs. For each concept axis k, we randomly select a target label <e\u2032k> from a set of candidates that different from the one in the original prompt <ek>. We then measure the alignment score between the modified image x\u0302ek\u2192e\u2032k with the modified text prompt yek\u2192e\u2032k using CLIP (Radford et al., 2021), following Gal et al. (2022). We also break down the score to each individual axis, by computing the alignment of the image to a prompt specifying only one axis, e.g. \u201ca photo of <e\u2032k>\u201d. All results are summarized in Table 1. Our method captures the characteristics of category particularly well and outperforms others in changing category or preserving category while changing color, which highlights its ability to extract disentangled concepts allowing for flexible compositions. InstructPix2Pix is more effective in changing color, but tends to do poorly in preserving the category, as seen in Figure 6. More details on our quantitative results can be found in A.4.\nWe further conduct a human evaluation. Each participant is presented the results from all three methods in a random order together with the editing instruction, and asked to rank them considering both realism and faithfulness to the instruction (see Appendix A.3 for details). We aggregate the responses from 20 people and report the average average score normalized to 0-1 in Table 1. More details of our setup can be found in A.3.4.4 ABLATIONS  We conduct an ablation study to understand the effects of the proposed encoder (as opposed perinstance optimization) and the text anchoring loss Lanchork. We use the same evaluation dataset and the CLIP-alignment metric as described in Section 4.3. As shown in Fig. 8 and Table 1, removing Lanchork and the encoders deteriorates disentanglement of the different concept axes due to severe overfitting, resulting in poor recomposition results. More results are included in the Appendix.5 CONCLUSION  In this paper, we have presented a framework for learning language-informed visual concepts from images, by simply distilling from pre-trained vision-language models. After training, the concept encoders extract disentangled concept embeddings along various concept axes specified by language, which can be remixed or edited to generate images with novel concept compositions. We conduct thorough evaluations both quantitatively and qualitatively, and demonstrate that our approach yields superior results in visual concept editing compared to prior work. \\\n            \n            Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$\\assistant\n\n$$\n\nPaper Summary:\nThe paper proposes a framework for learning language-informed visual concepts from images by distilling from pre-trained vision-language models. The authors train a set of concept encoders to extract visual concepts along specific axes specified by language, which can be remixed to generate images with novel concept compositions. The approach uses a combination of Textual Inversion and Visual Question Answering models to achieve disentangled and compositional visual concepts.\n\nStrengths:\n\n1. The paper presents a novel approach to learning language-informed visual concepts from images, which has the potential to improve image generation and editing tasks.\n2. The authors use a combination of Textual Inversion and Visual Question Answering models to achieve disentangled and compositional visual concepts.\n3. The approach is demonstrated to be effective in generating images with novel concept compositions and editing existing images to modify specific concepts.\n\nWeaknesses:\n\n1. The paper relies heavily on pre-trained vision-language models, which may not be suitable for all domains or applications.\n2. The authors assume that the concept axes are well-defined and easily specified by language, which may not always be the case in practice.\n3. The approach may not generalize well to unseen concepts or domains, as the training dataset is limited.\n\nSuggestions for Improvement:\n\n1. The authors should consider using more diverse and representative training datasets to improve the generalizability of the approach.\n2. The concept axes should be more clearly defined and annotated to ensure that the approach can effectively learn and generalize to these axes.\n3. The authors should investigate the use of additional techniques, such as attention mechanisms or multi-task learning, to further improve the disentanglement and compositionality of visual concepts.\n\nRecommendation:\n\nI recommend that the authors continue to develop and refine their approach, focusing on addressing the limitations and weaknesses identified above. The potential of their method to improve image generation and editing tasks is significant, and with further development and testing, it has the potential to make a meaningful contribution to the field of computer vision and artificial intelligence.\n\n$$",
        "peer_review_with_watermark": "$$\nPaper Summary:\nThis study introduces a novel framework for learning language-informed visual concepts by leveraging pre-trained vision-language models. The approach focuses on extracting disentangled concept embeddings that align with different language-specified axes. These embeddings can be mixed and modified to create images featuring new combinations of concepts, which the authors rigorously evaluate against existing methodologies.\n\nStrengths:\n1. The methodology effectively utilizes pre-existing vision-language models, which minimizes the reliance on extensive human annotations.\n2. The incorporation of text anchors from a pre-trained Visual Question Answering model enhances the disentanglement of concept axes.\n3. The approach enables flexible recomposition of visual concepts, leading to diverse and innovative image generations.\n4. Comprehensive evaluations, both qualitative and quantitative, underscore the method's efficacy in the realm of visual concept editing.\n\nWeaknesses:\n1. The relatively limited size of the training dataset may hinder the model's ability to generalize across diverse contexts.\n2. Dependence on existing pre-trained models could constrain the performance tailored to specific tasks, as not all models may be optimally suited.\n3. The absence of detailed results from the ablation study limits insight into the contribution of specific components to the model's performance.\n\nSuggestions for Improvement:\n1. Expanding the training dataset could bolster the model's generalizability and mitigate potential overfitting issues.\n2. Investigating alternative pre-trained models or architectures might yield better task-specific optimization.\n3. Presenting explicit findings from the ablation study would facilitate a deeper understanding of the roles of various components in the framework.\n4. Additional evaluations involving a broader array of datasets or tasks could further validate the method's robustness.\n\nRecommendation:\nI advocate for the acceptance of this paper, given its innovative approach and promising results in advancing visual concept learning. Nonetheless, addressing the limitations and implementing the suggested improvements would significantly enhance the framework's robustness and adaptability.\n$$",
        "gr_score_with": 0.9041916167664671,
        "gr_score_without": 0.9213229766975696,
        "output_without": {
            "num_tokens_scored": 3991,
            "num_green_tokens": 3677,
            "green_fraction": 0.9213229766975696,
            "z_score": 4.490218154763584,
            "p_value": 3.557513222520431e-06
        },
        "output_with": {
            "num_tokens_scored": 334,
            "num_green_tokens": 302,
            "green_fraction": 0.9041916167664671,
            "z_score": 0.25534863907680116,
            "p_value": 0.3992269180749587
        }
    },
    {
        "title": "RAPPER: REINFORCED RATIONALE-PROMPTED PARADIGM FOR NATURAL LANGUAGE EXPLANATION IN VISUAL QUESTION ANSWERING",
        "abstract": "Natural Language Explanation (NLE) in vision and language tasks aims to provide human-understandable explanations for the associated decision-making process. In practice, one might encounter explanations which lack informativeness or contradict visual-grounded facts, known as implausibility and hallucination problems, respectively. To tackle these challenging issues, we consider the task of visual question answering (VQA) and introduce Rapper, a two-stage Reinforced RationalePrompted Paradigm. By knowledge distillation, the former stage of Rapper infuses rationale-prompting via large language models (LLMs), encouraging the rationales supported by language-based facts. As for the latter stage, a unique Reinforcement Learning from NLE Feedback (RLNF) is introduced for injecting visual facts into NLE generation. Finally, quantitative and qualitative experiments on two VL-NLE benchmarks show that RAPPER surpasses state-of-the-art VQA-NLE methods while providing plausible and faithful NLE.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it.  Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$user\n\nThe peer review format and length should be of standard conference. \\\n            title: RAPPER: REINFORCED RATIONALE-PROMPTED PARADIGM FOR NATURAL LANGUAGE EXPLANATION IN VISUAL QUESTION ANSWERING \\\n            abstract: Natural Language Explanation (NLE) in vision and language tasks aims to provide human-understandable explanations for the associated decision-making process. In practice, one might encounter explanations which lack informativeness or contradict visual-grounded facts, known as implausibility and hallucination problems, respectively. To tackle these challenging issues, we consider the task of visual question answering (VQA) and introduce Rapper, a two-stage Reinforced RationalePrompted Paradigm. By knowledge distillation, the former stage of Rapper infuses rationale-prompting via large language models (LLMs), encouraging the rationales supported by language-based facts. As for the latter stage, a unique Reinforcement Learning from NLE Feedback (RLNF) is introduced for injecting visual facts into NLE generation. Finally, quantitative and qualitative experiments on two VL-NLE benchmarks show that RAPPER surpasses state-of-the-art VQA-NLE methods while providing plausible and faithful NLE. \\\n            paper text: 1 INTRODUCTION  Deep learning has achieved remarkable success in vision-language (VL) tasks such as visual reasoning (Suhr et al., 2017), visual question answering (VQA, Goyal et al., 2017), and visual entailment (Xie et al., 2019). Take VQA as an example, while these models exhibit impressive ability in inferring answer descriptions from the given image-question pairs, its decision-making process remains an unsolved problem. As a result, such a black-box manner severely restricts their applicability in certain real-world scenarios (e.g., medical VQA, Lin et al., 2023), where the interpretability of the learning model is crucial for establishing trustworthy systems. To tackle this long-standing challenge, some approaches adopt attention mechanisms (Anderson et al., 2018) or gradient-based activations (Selvaraju et al., 2017), focusing on highlighting image regions which are relevant to the associated prediction. However, such visual explanations might not be desirable for VL tasks (e.g., those beyond classification) due to the lack of reasoning process (Kayser et al., 2021; Sammani et al., 2022). As a result, Natural Language Explanation (NLE) has emerged as a potential alternative, which aims to interpret the underlying reasoning process by natural language descriptions.\nTo extend NLE for vision-language tasks (i.e., VL-NLE), Park et al. (2018) and Kayser et al. (2021) introduced the benchmarks for explaining the decision-making process with NLEs for VQA and visual entailment tasks, respectively. Subsequent VL-NLE works have evolved into two research lines. The first research line (Park et al., 2018; Marasovic\u0301 et al., 2020) focuses on how to improve their pipeline from an architecture perspective for training NLE generators within a fully supervised learning manner. On the other hand, Sammani et al. (2022) and Suo et al. (2023) emphasize the utilization of unlabeled pre-training data to enhance the language models\u2019 NLE capability.\nDespite significant advancements, most existing VL-NLE works require training in a full supervised manner. They might encounter problems where the explanations are irrelevant to the questions or contradictory to the established supporting facts (Majumder et al., 2021). The other potential concern is that the explanation is not related to the visual image (Ji et al., 2023). More specifically, the former problem is referred to as implausibility, while the latter is known as hallucination. Take visual input and question in Fig. 1 as an example, \u201cBecause there is a tower.\u2019 is an implausible explanation since it is irrelevant to question, and \u201cBecause the sun is big.\u201d is a hallucinated one since\nthe sun is not visible in the image. Although these issues have been recently studied in the NLE community (Zhao et al., 2023; Turpin et al., 2023), they remain unexplored in the field of VL-NLE. As a result, generating plausible yet faithful NLEs for elucidating vision-language models continues to pose a crucial challenge.\nRecently, rationale-based prompting techniques have been manifested to improve the capability of Large Language Models (LLMs) on complex reasoning tasks (Wei et al., 2022; Liu et al., 2022b). Such techniques involve elicitation of rationales from LLMs, producing knowledge-riched or factbased intermediate to facilitate the reasoning capability of language model. Thus, these prompting manners are emerging as promising solutions for NLE (Zhao et al., 2023; Krishna et al., 2023). These rationale-prompting paradigms have been further extended to multi-modal regimes such as mm-CoT (Zhang et al., 2023) and mm-ReAct (Yang et al., 2023). However, mm-CoT (Zhang et al., 2023) relies on the ground-truth rationales for training, while mm-ReAct (Yang et al., 2023) have potential hallucinated outputs due to the information loss when converting visual signals into text for ChatGPT API call understanding.\nIn this paper, we propose Reinforced Rationale-Prompted Paradigm (Rapper) for providing accurate answers for VQA with sufficient NLE, which are plausible and faithful. As depicted in Fig. 1(b), our Rapper learns to exploit knowledge learned from LLM and incorporate the corresponding visual content from input images into rationales through two stages. Without observing any ground truth rationale during training, the first stage utilizes a knowledge distillation process to introduce LLM for enriching the rationales with supporting facts, encouraging NLE to be factual and plausible. The subsequent stage of Reinforcement Learning from NLE Feedback (RLNF) further exploits the answer-explanation feedback to enforce the produced rationales associated with both question and visual inputs, allowing faithful NLE.\nWe now summarize the contributions of this work below:\n\u2022 A reinforced rationale-prompted paradigm, Rapper, is proposed for plausible and faithful NLE generation in VQA. This is achieved through two proposed stages: knowledge distillation process from LLM and Reinforcement Learning from NLE Feedback (RLNF).\n\u2022 In Rapper, we first advance LLM and perform knowledge distillation. This results in predicted rationales are based on language-based facts, which prompt the VQA model for plausible NLE.\n\u2022 To align NLE with the visual input, we introduce Reinforcement Learning from NLE Feedback (RLNF) to Rapper, which utilizes the answer-explanation feedback as rewards and prompts the VQA model with predicted rationales for faithful NLE.\n\u2022 Our Rapper achieves new state-of-the-art performance for both VQA-X (Park et al., 2018) and eSNLI-VE (Kayser et al., 2021) on NLE generation. We also demonstrate that Rapper outperforms existing VQA-NLE works with reduced implausibility and hallucination.2 RELATED WORK  Plausible and Faithful Natural Language Explanation Research on plausibility and faithfulness in NLE (Majumder et al., 2021; King et al., 2022; Gou et al., 2023; Stacey et al., 2023) has garnered wide attention, particularly due to the evolution of Large Language Models (LLMs) and chain-ofthought (CoT) prompting techniques (Wei et al., 2022). Notably, the method of integrating external knowledge databases for fact generation or retrieval has been proven effective in enhancing the plausibility and faithfulness of NLEs (Majumder et al., 2021; Stacey et al., 2023). Based on this advancement, some recent approaches, such as the verify-then-correct pipeline by Gou et al. (2023) and novel decoding strategies proposed by Lan et al. (2023) and King et al. (2022), aim to mitigate hallucination in textual outputs. However, these works typically focus on isolated single text modality or rely on static external knowledge databases, limiting its scalability to multimodal data.\nNatural Language Explanation for Vision-Language Tasks Most existing VL-NLE works (Wu & Mooney, 2018a; Park et al., 2018; Marasovic\u0301 et al., 2020; Kayser et al., 2021) generate explanations in a predict-then-explain fashion. Specifically, an answer is first predicted by a pre-trained VL model (e.g., UNITER (Chen et al., 2020) or Oscar (Li et al., 2020)), followed by the generation of the corresponding explanation via a separate language decoder (e.g., GPT2 (Radford et al., 2019)). As the answer and explanation are predicted separately, the explanation often contains irrelevant or contradictory descriptions of the given visual information, struggling to faithfully represent the underlying reasoning process. Recently, NLX-GPT (Sammani et al., 2022) proposes to jointly generate the answer and explanation by a unified sequence-to-sequence model, while S3C (Suo et al., 2023) further enforces the explanation to be consistent with the predicted answer. Although the above approaches have been shown to mitigate the hallucination issue, it is not clear how their NLE is established upon supporting facts or taking the visual input into consideration. Therefore, how to tackle the potential implausibile or hallucinated NLE remains a challenging task.\nReinforcement Learning for Language Models Several research works have explored RL and view it as the key component to enhance models across vision-language tasks such as image captioning (Rennie et al., 2017), novel object captioning (NOC) (Yang et al., 2022), and VQA (Lu et al., 2022a; Fan et al., 2018; Liu et al., 2018). There has been a concentrated effort to align LMs with natural language (NL) feedback (Aky\u00fcrek et al., 2023; Yang et al., 2022; Liu et al., 2022a) as well as non-NL feedback (Bai et al., 2022; Lu et al., 2022b). For example, Liu et al. (2022a) utilizes the probability of the correct answer as a reward to stimulate an auxiliary module to produce beneficial knowledge, thereby enhancing QA-task performance. Similarly, Yang et al. (2022) employs a CIDEr optimization strategy to enhance the caption with sufficiently visual fidelity in the task of novel object captioning. Despite of their effectiveness, their RL framework or NL-feedback approaches cannot be easily applied for VL-NLE tasks.3 PROPOSED METHOD  3.1 PROBLEM FORMULATION  Given a VQA input X = (V,Q) consisting of an input image V and a textual input Q (i.e., question), our goal is to predict the answer A\u0302 and the corresponding explanation E\u0302 (denoted as Y\u0302 = (A\u0302, E\u0302)) via a reasoning module M. In order to encourage M to provide corrent answer with plausible and faithful explanation, we propose a Reinforced Rationale-Prompted Paradigm (Rapper) scheme, which learns an additional rationale generator G to jointly exploit the supporting facts from LLMs and the visual content observed from the conditioned image into rationales. Note that only the ground truth A and E are available during training, not the rationales. As depicted in Fig. 2, the learning of Rapper is decomposed into: (A) Knowledge Distillation from LLM (Sec. 3.2), and (B) Reinforcement learning from NLE Feedback (RLNF) (Sec. 3.3), which trains rationale generator G for providing auxiliary intermediates when predicting Y\u0302 = (A\u0302, E\u0302).3.2 PLAUSIBLE NLE GENERATION  Since VL-NLE models typically rely on ground truth answers and explanations for training, it is not clear whether the underlying visual and language knowledge are exploited to support the predicted\noutputs. In the first stage of Rapper, we propose to leverage powerful reasoning capability inherent in LLM for plausible NLE generation. As depicted in Fig. 2(A), we propose to learn a rationale generator G by utilizing knowledge distillation from LLM (e.g., LLaMA-65B (Touvron et al., 2023)). This would have the reasoning module M elaborate the conditioned rationales before answering and explaining and encourage plausible NLE. We now detail this learning stage.3.2.1 KNOWLEDGE DISTILLATION FOR FACTED-BASED RATIONALE GENERATION  With the recent success of LLMs showing great capability for generating rationale prompts as intermediate reasoning steps and knowledge (Wei et al., 2022; Kojima et al., 2022; Liu et al., 2022b) for reasoning task, we propose to advance the guidance of pre-trained LLMs to acquire such knowledge, so that supporting facts or knowledge can be exploited and serve as rationales for VLNLE. Since no ground-truth rationales are available, we leverage the LLM to produce rationales as pseudo ground truth for training our rationale generator G. Inspired by Liu et al. (2022a;b) and Min et al. (2022), we elicit pseudo rationale rp from LLM with a task-specific set of few-shot demonstrations (see Sec. A.4 for details) as follows:\nRp = {rp | rp \u223c PLLM(y, q)}, (1)\nwhere y is the ground-truth answer-explanation pair, q is question, PLLM denotes the LLM in an autoregressive manner, rp is the sampled pseudo rationale from PLLM, and thus Rp is the set of all rp.\nHowever, the above pseudo rationales may be redundant, noisy or lengthy, which would not be desirable for subsequent NLE tasks (Li et al., 2023b). Thus, we apply a post-processing mechanism to filter pseudo rationales Rp to R\u2032p. To be specific, we apply a round-trip consistency by answering the input question on the pseudo rationales with a pre-trained question-answering (QA) model F 1. The pseudo rationale is retained when the matching score between the ground-truth answer and the\n1In the implementation, we follow (Changpinyo et al., 2022) and use UnifiedQA (Khashabi et al., 2022) as the pre-trained QA model.\nanswer predicted by F exceeds a predetermined threshold \u03c4. This matching score is quantified with the token-level F1 score (Wang et al., 2020). Thus, the process of collecting the filtered pseudo rationales R\u2032p is formulated as follows:\nR\u2032p = {rp | F1-score(a\u0303, a) \u2265 \u03c4, a\u0303 \u223c PF(Q, rp), rp \u2208 Rp}, (2)\nwhere a is the ground truth answer, a\u0303 is the answer predicted by F based on the pseudo rationale, and PF denotes the pre-trained QA model F in an autoregressive fashion.\nWith the above R\u2032p serving as psuedo ground truth, we are able to train the rationale generator G with the distillation loss LG described below:\nLG = \u2212 T\u2211\nt=1\nlog(pG(r \u2032 p,t|r\u2032p,0:t\u22121, x)), (3)\nwhere r\u2032p \u2208 R\u2032p, T = |r\u2032p|, and x = {v, q} \u2208 X.3.2.2 PROMPTING BY FACT-BASED RATIONALE FOR PLAUSIBLE NLE  With rationales R\u2032p better aligned with the facts, we can proceed to the training of the reasoning module M for NLE generation. We note that, since rationales R\u2032p are in the form of natural language, our the reasoning module M (which is also based on visual-language model) would be able to interpret them. Thus, in addition to the image-question pair X as the inputs to the reasoning module M, the derived pseudo rationales R\u2032p are further viewed as input prompts, which provide fact-supporting conditions when training M to perform VL-NLE. As a result, we train M by calculating the reasoning loss LM as follows:\nLM = \u2212 T\u2211\nt=1\nlog(pM (yt|y0:t\u22121, r\u2032p, x)). (4)\nIn the above cross-entropy loss, y = [a; e] \u2208 Y is the concatenation of the ground-truth answer a and explanation e.3.3 FAITHFUL NLE GENERATION  Although the above knowledge distillation process based on LLM introduces plausibility into our rationale generation, the predicted rationales might not be related to the visual input and thus encounter the hallucination problem. To tackle this issue, we introduce a novel technique of Reinforcement Learning from NLE Feedback (RLNF). This learning strategy is to encourage the rationale generator G to fully exploit multimodal input data, so that the output rationales are not only plausible but also faithful. Once G produces faithful rationales, we can fine-tune the reasoning module M for plausible yet faithful NLE.3.3.1 RLNF FOR INJECTING VISUAL FACTS  To address the potential hallucination issue, we propose Reinforcement Learning from NLE Feedback (RLNF) by enforcing rationale generator G to derive the visual facts from the input image into rationales. To achieve this, we define a reward function via RL that penalizes the fact-based but hallucinated rationales R\u2032, while rewarding the rationales R that contain both established facts and visual content, as depicted in Fig. 2(B). To achieve this, we design our reward r total to be the addition of answer scores r ans and the explanation score r exp, which are the average predicted probability of the ground-truth answer and CIDEr score (Vedantam et al., 2015), respectively. For the answer score, inspired by and following Kadavath et al. (2022), we maximize the answer score to assess the faithfulness of the predicted explanation. This maximization enforces the rationale generator G to inject more visual content into the rationale because the reasoning module M need more visual clues to correctly answer the question. Therefore, this process transform R\u2032 to R, and simultaneously provide the M with more visual fact-based rationale R to enable the explanation with sufficient faithfulness. On the other hand, the explanation score r exp is (i.e., specifically CIDEr\nAlgorithm 1 Training RAPPER Input: Rationale generator G, reasoning module M, LLM PLLM and pre-trained QA model PF Data: Image-question pairs X = {xi}Ni=1, and answer-explanation pairs Y = {yi}Ni=1\n/* Stage(A): KD for Plausible NLE Generation */ Rp \u2190 Collect pseudo rationales (Eq. equation 1); R\u2032p \u2190 Get filtered pseudo rationales from Rp (Eq. equation 2); \u25b7 Section 3.2.1 G\u2190 Update G with LG (Eq. equation 3); M \u2190 Update M with LM (Eq. equation 4); \u25b7 Section 3.2.2 /* Stage(B): RLNF for Faithful NLE Generation */ G\u2190 Update G with Rtotal (Eq. equation 8); \u25b7 Section 3.3.1 M \u2190 Update M with LM (Eq. equation 10); \u25b7 Section 3.3.2\nOutput: G\u03b8, M\u03d5\nscore) to maintain the plausibility of NLE after the first training stage. As a result, the reward r total is formulated as follows:\nr total(x, a, e, e\u0302, r) = r ans(a, x, r) + r exp(e, e\u0302), (5) r ans(a, x, r) = Z(PM\u03d5(a | x, r)), (6)\nr exp(e, e\u0302) = Z(CIDEr(e, e\u0302)), (7) where x = {v, q} is the input image-question pair, a denotes the ground-truth answer, e denotes the ground-truth explanation, e\u0302 is the predicted explanation from M, and r \u2208 R is the sampled rationales from G. Notably, Z is an input-specific normalization function that follows Deng et al. (2022) to normalize reward for stabilizing the RL training process.\nRLNF Formulation Our RLNF employs Proximal Policy Optimization (PPO) (Schulman et al., 2017) as the RL algorithm. As the policy model updated, the rationale generator G is to maximize the following reward Rtotal:\nmax{Rtotal(x, a, e, e\u0302, r)}, r \u223c T\u220f\nt=1\nPG(wt|w<t), (8)\nwhere r = {wi}Ti=0, T = |r|, and x = {v, q}. However, we need to ensure the generated rationales are understandable by humans and do not deviate too far from the distilled knowledge. To achieve this, we add a KL penalty term between the learned policy \u03b8 and the initial policy \u03b8init after the knowledge distillation phase. Therefore, the overall reward is defined as:\nRtotal(x, a, e, e\u0302, r) = r total(x, a, e, e\u0302, r)\u2212 \u03b1 log pG(r|x; \u03b8) pG(r|x; \u03b8init), (9)\nwhere Rtotal(x, a, e, e\u0302, r) is the reward in Eq. 5.3.3.2 PROMPTING BY VISUAL-FACT-BASED RATIONALE FOR FAITHFUL NLE  Once the rationale generator G is trained with the introduced RLNF, it is encouraged to produce visual fact-based rationales R that are encapsulated with established facts and visual content from visual input. Again, wince R are natural language prompts, they are inherently interpretable by our reasoning module M. Therefore, for the given image-question pairs X, we utilize R as part of input prompts during the reasoning process of M. This ensures the NLEs from M retain plausibility because of the established supporting facts lies in R, together with the enhanced faithfulness because of the derived visual content embedded in R. We optimize M to achieve this with the reasoning loss LM defined as follows:\nLM = \u2212 T\u2211\nt=1\nlog(pM (yt|y0:t\u22121, r, x)), (10)\nwhere r \u2208 R, x = {v, q} \u2208 X, and y = [a; e] \u2208 Y, which is the concatenated ground-truth answer a and explanation e sequence.\nTherefore, through the complete Rapper training process as outlined in Algorithm 1, VL-NLE tasks would be successfully enabled with adequate plausibility and faithfulness.3.4 INFERENCE  At inference time, for a given input image-question pair x \u2208 X, we first generate rationale r on the fly from the rationale generator G:\nr = {wi | wi \u223c PG(w<i | x); i = 0,..., n},\nwhere r = {wi}ni=0 is the sampled rationale, n = |r|, and x = {v, q}. Subsequently, we prompt the reasoning module M by concatenating the predicted rationale r\u0302 with the image-question pair x for outputting the final answer and explanation sequence y\u0302. This can be formulated as:\ny\u0302 = [a\u0302; e\u0302]\n= {zi | zi \u223c PM (z<i | x, r); i = 0,...,m},\nwhere m = |y\u0302|, and y\u0302 = {zi}mi=0 is the concatenated answer and explanation sequence, denoted as [a\u0302; e\u0302].4 EXPERIMENTS  4.1 DATASET AND SETUP  We follow (Kayser et al., 2021; Sammani et al., 2022; Suo et al., 2023) and consider two VL-NLE datasets. VQA-X (Park et al., 2018) builds upon VQAv2 dataset (Goyal et al., 2017). It is composed of 32.3K samples, divided into 29K for training, 1.4K for validation, and 1.9K for testing. The dataset OF e-SNLI-VE (Kayser et al., 2021) builds upon e-SNLI dataset (Camburu et al., 2018), consisting of 43K image-hypothesis pairs, divided into 40K for training, 1.4K for validation, and 1.6K for testing.\nRapper is consists of a rationale generator G and a reasoning module M, are both initialized from the pretrained image captioning model (Li et al., 2023a). The LLM for knowledge distillation during stage(A) is LLaMA-65B (Touvron et al., 2023). More implementation details are shown in Sec. A.1.4.2 EVALUATION METRICS  For NLE evaluation, we use BLEU@N (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005), ROUGE-L (Lin, 2004), CIDEr (Vedantam et al., 2015), and SPICE (Anderson et al., 2016) as the metrics, while using VQA accuracy to evaluate predicted answers. To evaluate the degree of plausibility and faithfulness of explanations, we measure them with CIDEr/SPICE and RefCLIPScore Hessel et al. (2021), respectively.\nPlausibility To quantitatively evaluate explanation plausibility, we employ CIDEr and SPICE scores. CIDEr measures the similarity between the generated explanation and human-written ground truth sentences, capturing human consensus by introducing tf-idf weight (Vedantam et al., 2015). On the other hand, SPICE converts sentences into semantic scene graphs, allowing evaluation to break grammatical constraints and thus closely resembling human judgment (Anderson et al., 2016).\nFaithfulness We adopt RefCLIPScore, which computes the harmonic mean of CLIPScore (Hessel et al., 2021) and maximal reference cosine similarity, thereby encapsulating the correlation between the explanation and its reference. As noted by Hessel et al. (2021), RefCLIPScore surpasses prior metrics in correlating with human judgment for hallucination detection.4.3 QUANTITATIVE ANALYSIS  NLE evaluation. In Table 1, Table 4, and Table 5, we demonstrate that Rapper outperform previous state-of-the-art methods in NLE-related metrics on both VQA-X and e-SNLIV-VE datasets with\n\"filtered\" and \"unfiltered\" settings. For the filtered setting mentioned in Table 1 considers the explanations that are associated with correct answers, Rapper achieves new SOTA performance across all NLE metrics on both datasets of Rapper to generate better explanations. Note that under the unfilter setting, Rapper still achieve SOTA prefromances in all metrics, as detailed in Table 4 and Table 5 in Appendix A.2.\nPlausibility & faithfulness of NLE. We assess plausibility by using CIDEr and SPICE, and evaluate NLE faithfulness with RefCLIPScore (Hessel et al., 2021). Results in Table 1 indicate Rapper\u2019s superiority in NLE metrics over existing VL-NLE methods, underscoring its superiority in generating plausible explanations.\nIn Table 2, we evaluate NLE faithfulness by comparing Rapper to prior SOTA methods and our stageablated approaches. Rapper\u2019s superior RefCLIPScore indicates fewer hallucinations and increased faithfulness over other VL-NLE works. Although\nthe RefCLIPScore of Rapper (w/o RLNF) is lower due to knowledge distillation introducing hallucinations. Nonetheless, Rapper still successfully to reduce hallucination after the RLNF. This demonstrated the effectiveness of our proposed RLNF to enable the model to generate faithful NLEs.\nAblation on the proposed stages. In top of Table 3, we evaluate our two-stage approach: (A) KD from LLM and (B) RL from NLE Feedback. Compared to the Rapper baseline without KD and\nRLNF, our method enhances explanation plausibility and faithfulness, highlighting the importance of both stages.\nAblation on the filtering mechanism. In bottom of Table 3, our filtering mechanism in knowledge distillation outperforms the baseline Rapper without filtering, highlighting its ability to filter lowquality pseudo rationales (i.e., redundant and noisy, etc.) and improve performance across metrics.\nAblation studies of derived rationales. In Fig. 4, we assess the quality of derived rationale on the VQA-X datasets using VQA accuracy. Rationales come from two stages: KD only R\u2032, KD+RLNF R, and a baseline of None. We test if mPLUG-Owl (Ye et al., 2023), a multimodal large language model, can answer accurately with given a pair (image, question, or x \u2208 (none, R\u2032, R)). Notably, we find that rationale quality improves progressively as we implement the stages we have proposed. This underscores the effectiveness of our designed stages in enhancing rationale quality.\n4.4 QUALITATIVE EVALUATION\nIn Fig.3, we compare NLX-GPT(Sammani et al., 2022), S3C (Suo et al., 2023), and our Rapper on the VQA-X dataset. Rapper consistently produces more plausible explanations. For example, Fig.3(a) highlights ability of Rapper to derive visual facts, such as identifying a single object on the table, surpassing previous methods that might produce hallucinated explanations. Similarly, in Fig.3(b), Rapper offers plausible explanations like recognizing Asian writing, contrasting with the implausible outputs of prior methods. Additional results and ablation studies are in Appendix A.3.5 CONCLUSION  In this paper, we proposed Rapper, a two-stage Reinforced Rationale-Prompted Paradigm for enabling NLE with sufficient plausible and faithful properties. Our Rapper uniquely distills language-based knowledge from LLM and utilizes RL with natural language feedback from the VQA task, so that the\ndesigned rationale generator is able to produce rationales with the aforementioned desirable properties. By prompting such predicted rationales into the reasoning module, we demonstrated that satisfactory VQA performances can be achieved. Compared to SOTA VQA-NLE methods, possible implausible or hallucinated explanations can be mitigated by our Rapper. \\\n            \n            Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$\\assistant\n\n$$\n\nPaper Summary:\nThe paper proposes RAPPER, a two-stage Reinforced Rationale-Prompted Paradigm for Natural Language Explanation in Visual Question Answering (VQA). Rapper utilizes knowledge distillation from large language models (LLMs) to generate rationales and reinforcement learning from NLE feedback to inject visual facts into the rationales. The proposed method achieves new state-of-the-art performance for VQA-NLE tasks and reduces implausibility and hallucination.\n\nStrengths:\n1. The proposed method addresses the challenges of implausibility and hallucination in VQA-NLE tasks.\n2. The use of knowledge distillation from LLMs to generate rationales is a novel approach.\n3. The reinforcement learning from NLE feedback stage enables the model to inject visual facts into the rationales.\n4. The paper provides quantitative and qualitative evaluations of the proposed method.\n\nWeaknesses:\n1. The paper does not provide a clear explanation of the hyperparameter tuning process.\n2. The evaluation metrics used may not be suitable for all VQA-NLE tasks.\n3. The paper does not provide a comprehensive comparison with all existing VQA-NLE methods.\n4. The ablation studies could be more extensive to demonstrate the importance of each component.\n\nSuggestions for Improvement:\n1. Provide a more detailed explanation of the hyperparameter tuning process.\n2. Use a more comprehensive set of evaluation metrics that can handle various VQA-NLE tasks.\n3. Compare the proposed method with a broader range of existing VQA-NLE methods.\n4. Conduct more extensive ablation studies to demonstrate the importance of each component.\n\nRecommendation:\nThe proposed method, RAPPER, is a significant contribution to the field of VQA-NLE tasks. While there are some limitations, the method addresses the challenges of implausibility and hallucination and achieves new state-of-the-art performance. With some improvements in hyperparameter tuning, evaluation metrics, and comparison with existing methods, RAPPER has the potential to be a leading approach in VQA-NLE tasks.\n\n$$",
        "peer_review_with_watermark": "$$\n\nPaper Summary:\nThe paper introduces RAPPER, a two-stage Reinforced Rationale-Prompted Paradigm aimed at enhancing Natural Language Explanation (NLE) in Visual Question Answering (VQA). RAPPER employs knowledge distillation from large language models (LLMs) for generating rationales, alongside reinforcement learning from NLE feedback to incorporate visual facts. The approach is shown to outperform existing VQA-NLE methods while significantly reducing instances of implausibility and hallucination.\n\nStrengths:\n1. The approach provides effective solutions to address implausibility and hallucination issues in VQA-NLE tasks.\n2. Integration of knowledge distillation and reinforcement learning creates a robust framework for generating fact-based rationales.\n3. The method sets new benchmarks for VQA-NLE, indicating its efficacy compared to previous techniques.\n\nWeaknesses:\n1. The comparative analysis with existing state-of-the-art techniques is somewhat limited.\n2. The metrics employed for evaluation could be more varied.\n3. A more in-depth exploration of the individual components of the proposed method is lacking.\n\nSuggestions for Improvement:\n1. Enhance comparative analysis with other leading methods to demonstrate RAPPER's advantages more clearly.\n2. Incorporate a broader range of evaluation metrics to assess performance comprehensively.\n3. Provide a detailed breakdown of the components and their contributions within the proposed framework.\n\nRecommendation:\nRAPPER advances the field of VQA-NLE by effectively tackling implausibility and hallucination issues and achieving state-of-the-art performance. To further strengthen the work, it is recommended to include a robust comparison with alternative methods, diversify evaluation metrics, and offer a deeper analysis of the framework's components.",
        "gr_score_with": 0.8932384341637011,
        "gr_score_without": 0.9211388259007307,
        "output_without": {
            "num_tokens_scored": 3969,
            "num_green_tokens": 3656,
            "green_fraction": 0.9211388259007307,
            "z_score": 4.439153439153444,
            "p_value": 4.515670707010449e-06
        },
        "output_with": {
            "num_tokens_scored": 281,
            "num_green_tokens": 251,
            "green_fraction": 0.8932384341637011,
            "z_score": -0.37781499130553375,
            "p_value": 0.6472159818458951
        }
    },
    {
        "title": "MOVINGPARTS: MOTION-BASED 3D PART DISCOV-",
        "abstract": "We present MovingParts, a NeRF-based method for dynamic scene reconstruction and part discovery. We consider motion as an important cue for identifying parts, that all particles on the same part share the common motion pattern. From the perspective of fluid simulation, existing deformation-based methods for dynamic NeRF can be seen as parameterizing the scene motion under the Eulerian view, i.e., focusing on specific locations in space through which the fluid flows as time passes. However, it is intractable to extract the motion of constituting objects or parts using the Eulerian view representation. In this work, we introduce the dual Lagrangian view and enforce representations under the Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangian view, we parameterize the scene motion by tracking the trajectory of particles on objects. The Lagrangian view makes it convenient to discover parts by factorizing the scene motion as a composition of part-level rigid motions. Experimentally, our method can achieve fast and high-quality dynamic scene reconstruction from even a single moving camera, and the induced part-based representation allows direct applications of part tracking, animation, 3D scene editing, etc.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it.  Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$user\n\nThe peer review format and length should be of standard conference. \\\n            title: MOVINGPARTS: MOTION-BASED 3D PART DISCOV- \\\n            abstract: We present MovingParts, a NeRF-based method for dynamic scene reconstruction and part discovery. We consider motion as an important cue for identifying parts, that all particles on the same part share the common motion pattern. From the perspective of fluid simulation, existing deformation-based methods for dynamic NeRF can be seen as parameterizing the scene motion under the Eulerian view, i.e., focusing on specific locations in space through which the fluid flows as time passes. However, it is intractable to extract the motion of constituting objects or parts using the Eulerian view representation. In this work, we introduce the dual Lagrangian view and enforce representations under the Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangian view, we parameterize the scene motion by tracking the trajectory of particles on objects. The Lagrangian view makes it convenient to discover parts by factorizing the scene motion as a composition of part-level rigid motions. Experimentally, our method can achieve fast and high-quality dynamic scene reconstruction from even a single moving camera, and the induced part-based representation allows direct applications of part tracking, animation, 3D scene editing, etc. \\\n            paper text:  1 INTRODUCTION\n3D scene reconstruction and understanding is one of the central problems in computer vision and graphics, with a wide range of applications in mixed reality, robotics, movie production, etc. While many works focus on static scenes, real-world physical scenes are usually dynamic and entangled with illumination changes, object motion, and shape deformation. The reconstruction of dynamic scenes is known to be highly challenging. Nonrigid structure from motion methods (Bregler et al. (2000); Gotardo & Martinez (2011); Kong & Lucey (2019); Sidhu et al. (2020)) could recover nonrigid shapes but are limited to sparse feature tracking. To reduce the ambiguity between shape and motion, some other methods introduce multi-view capture (Zhang et al. (2003); Oswald et al. (2014); Tung et al. (2009)) or category-specific priors (Egger et al. (2021); Habermann et al. (2019); Kocabas et al. (2020)). Recently, neural radiance representa-\ntion NeRF (Mildenhall et al. (2020)) has been applied to this field and achieved promising dynamic capture performance on general scenes using only monocular input (Pumarola et al. (2020); Park et al. (2021a); Li et al. (2022)). However, most NeRF-based dynamic scene modeling methods only focus on scene reconstruction without considering scene understanding, thus lacking the ability to directly support downstream applications that need tracking, shape editing, re-animation, etc.\n\u2217denotes equal advisory and \u2020 denotes the corresponding author.\nOur goal is to enable practical dynamic scene capture with both high-quality reconstruction and meaningful scene understanding from monocular input. To this end, we propose MovingParts, a novel NeRF-based approach that can achieve not only fast dynamic scene reconstruction but also automatic rigid part discovery. Our key insight is that motion (while complicating the reconstruction) is an effective cue for identifying object parts because the scene content belonging to one rigid part has to share the same rigid transformation. Therefore, we design novel modules to explain the motions in dynamic neural fields, enabling unsupervised object part discovery via motion grouping. Since the rigid motion patterns are used as the evidence of part discovery, we make explicit the assumption of our input here, which is the general scene with piece-wise rigid motion.\nOur approach is inspired by the literature on fluid simulation. We note that a family of previous dynamic NeRF methods model motion using a 3D field that encodes scene flow (Li et al. (2021)) or deformation (Pumarola et al. (2020); Park et al. (2021a)). Specifically, at time t, for each location x, the 3D field encodes which particle xc in the canonical frame has been deformed to x, which actually backward deforms the particles from world space to static canonical space. As shown in Figure 1, this is essentially the Eulerian view in fluid simulation (Fedkiw et al. (2001)) \u2013 motion information is denoted as a function \u03a8E(x, t) at each specific location x in the world coordinate frame. It is known that, while the entire scene motion can be modeled under the Eulerian view, specific object/part motion at different temporal moments is actually intractable and hard to analyze. On the other hand, the Lagrangian view (Macklin et al. (2014)), as the duality of the Eulerian view, uses the particlebased representation to track the motion of each particle belonging to the object, which forward deforms the particles from canonical space to world space. The constructed particle trajectory from the Lagrangian view can be an important clue for scene analysis. A relevant Lagrangian-based work is Watch-It-Move (Noguchi et al. (2022)) which composes objects into several ellipsoid-like parts by rendering supervision. However, the multi-view requirement and the ellipsoidal geometric prior highly limit its application. In contrast, we mainly focus on monocular input.\nTo achieve meaningful scene understanding by motion analysis, we propose a hybrid approach that learns motion under both the Eulerian and the Lagrangian views. In particular, our neural dynamic scene model consists of three modules: (1) a canonical module that models the scene geometry and appearance as a radiance field in a static canonical space, (2) an Eulerian module \u03a8E(x, t) that records which particle xc in the canonical space passes through each specific location x in the world coordinate frame at every time step, and (3) a Lagrangian module \u03a8L(xc, t) that records the trajectory of all particles xc in the canonical space. Note that the motions modeled by the Eulerian and Lagrangian modules are inherently reciprocal, we, therefore, apply a cycle-consistency loss during reconstruction to enforce the consistency between the two modules, constraining them to model the same underlying motion in the scene.\nThe construction of the Lagrangian view makes it convenient to discover parts by factorizing \u03a8L(xc, t). As the particles in a rigid part share a common rigid transformation pattern, we propose a novel motion grouping module as part of our Lagrangian module. By projecting the particle motion features into a few groups, we divide the scene into meaningful parts. Once reconstructed, our Lagrangian module could offer part-level representation and allow for direct downstream applications such as part tracking, object control, and scene editing. Since the number of rigid parts generally differs across scenes, we introduce an additional post-processing merging module that can adaptively merge the over-segmented groups into a reasonable number of rigid parts.\nWe jointly train all modules with only rendering supervision. We demonstrate that our approach achieves high-quality dynamic scene reconstruction and realistic rendering results on par with stateof-the-art methods. More importantly, compared with previous monocular NeRF methods, ours is the only one that simultaneously achieves part discovery, allowing for many more downstream applications. Finally, inspired by recent fast NeRF reconstruction methods (Sun et al. (2022); Chen et al. (2022); Yu et al. (2022)), we construct our system with feature volumes and light-weight multilayer perceptrons (MLPs), leading to a fast reconstruction speed comparable to other concurrent methods that are specifically focused on speeding up dynamic NeRF.\nIn summary, our key contributions are: \u2022 We propose a novel NeRF-based method for simultaneous dynamic scene reconstruction\nand rigid part discovery from monocular image sequences; \u2022 The hybrid representation of feature volume and neural network allows us to achieve both\nhigh-quality reconstruction and reasonable part discovery within 30 minutes;\n\u2022 The extracted part-level representation can be directly applied to downstream applications like part tracking, object control, scene editing, etc.2 RELATED WORK  Dynamic Neural Radiance Fields. Recently, the emergence of Neural Radiance Fields (NeRF) (Mildenhall et al. (2020)) has facilitated the tasks of scene reconstruction and image synthesis. Due to the dynamic properties of the physical world, an important branch of NeRF research is to extend it to dynamic scenes (Pumarola et al. (2020); Li et al. (2021); Park et al. (2021b); Fridovich-Keil et al. (2023); Cao & Johnson (2023)). In particular, some methods directly extend the 5D radiance field function to 6D by adding additional time-dependent input to the network (Li et al. (2022); Xian et al. (2021)). Other works enhance the temporal consistency in the 6D dynamic radiance field by explicitly modeling dynamic scene flows (Li et al. (2021); Du et al. (2021); Gao et al. (2021)), leading to promising results from only monocular input. Meanwhile, deformation modules have also been adopted in NeRF-based methods (Pumarola et al. (2020); Tretschk et al. (2021); Yuan et al. (2021); Park et al. (2021a;b); Liu et al. (2022)), offering strong regularization for temporal consistency. Note that these various NeRF-based methods all explain motions (modeled as flows or deformation fields) from the location-based Eulerian view and do not support part discovery. We instead propose a hybrid model that models motions with both location-based Eulerian and particlebased Lagrangian views, enabling high-quality dynamic scene reconstruction with automatic part discovery based on particle motion. In addition to these general methods, some NeRF methods have been devised for particular domains, such as humans (Jiakai et al. (2021); Noguchi et al. (2021); Weng et al. (2022); Peng et al. (2023)), and articulated objects within specific categories Wei et al. (2022). While capable of achieving dynamic scene rendering and part segmentation, these methods often incorporate category priors into the pipeline and cannot be directly applied to general objects.\nPart Discovery from Motion. At the image level, most motion-based object discovery methods (Keuper et al. (2015); Pia Bideau (2016); Yang et al. (2021); Xie et al. (2019); Papazoglou & Ferrari (2013)) employ the clustering of 2D pixels based on features related to optical flow. We share a common underlying logic with these 2D methods that discover parts (or objects) by constructing and grouping motion trajectories. However, in contrast to these approaches, our method establishes a motion group module on canonical 3D particles and relies on predicted 3D rigid motion, which ensures arbitrary viewpoints consistency and temporal consistency of the grouping results. In the 3D domain, some methods (Shi et al. (2021); Kawana et al. (2022)) reason about object parts by constructing point-wise correspondence at different object states and clustering their trajectories. Without 3D input, (Agudo & Moreno-Noguer (2019)) adopts non-rigid structure from motion to reconstruct the 3D shape and applies spatio-temporal clustering to the 3D points to reason about segmentation. However, only the geometry of sparse feature points could be achieved. Recently, NeRF-based dynamic scene decoupling methods (Yuan et al. (2021); Tschernezki et al. (2021); Wu et al. (2022)) have been proposed. Although they achieve dynamic scene decomposition with highquality reconstruction, they can only divide the scene into static/dynamic parts and are unable to identify motion patterns. A relevant recent work is Watch-It-Move (Noguchi et al. (2022)), which achieves high-quality part-level reconstruction from image sequences. However, it requires dense multi-view input and imposes ellipsoid-like priors to the part geometry, which may completely fail on challenging monocular data with complex scene geometry. In contrast, our NeRF-based method does not require any shape priors of dynamic objects in complex scenes and can achieve dynamic reconstruction and part discovery from monocular input.3 PRELIMINARIES: NERF AND D-NERF  By incorporating implicit function and volume rendering, Neural Radiance Field (NeRF) (Mildenhall et al. (2020)) allows for scene reconstruction and novel view synthesis via optimizing scene representation directly. In general, NeRF interprets static scenario as a continuous implicit function F\u03b8. By querying spatial coordinates (x) and view direction (d), F\u03b8 outputs the corresponding density (\u03c3) and observed color (c) as (c, \u03c3) = F\u03b8(x,d). Through classical volume rendering in graphics, the 3D scene representation F\u03b8 can be rendered into a 2D image. Specifically, given a ray r emitted from the optical center to a specific pixel in the image, the rendered color of that pixel is an integral of all the colors on the ray with near and far bounds hn and hf :\nC(r) = \u222b hf hn T (h)\u03c3 ( r(h) ) c ( r(h),d ) dh, where T (h) = exp ( \u2212 \u222b h hn \u03c3(r(s))ds ). (1)\nT (h) can be interpreted as the transparency accumulated from hn to h. Because of the inherent differentiability of Eq. 1, it only requires a set of images with camera poses to optimize F\u03b8 directly.\nD-NeRF (Pumarola et al. (2020)) extends NeRF to capture dynamic scenes, assuming that there is a static canonical space and includes all objects. It divides the dynamic scene reconstruction in world space into two sub-problems: the NeRF representation learning of canonical space and the learning of the mapping from the world space to the canonical space (scene flow prediction) as:\n(c, \u03c3) = F\u03b8(\u03a8(x, t),d) (2)\nwhere \u03a8(x, t) predict the canonical space position from x at time t into its canonical configuration.4 OUR METHOD  From the perspective of fluid simulation, the scene motion is composed of particle motions. \u03a8(x, t) in D-NeRF can be interpreted as recording the motion of particles passing through a given coordinate x at time t, corresponding to the Eulerian perspective. Following D-NeRF, we also assume a canonical space that is static and includes all objects. Besides the Eulerian perspective, we also describe the dynamic scene from the Lagrangian perspective. Accordingly, we construct three modules, as Figure 2 shows, that include an Eulerian module \u03a8E(x, t) which maps a position x at any time t in the world space to the canonical space, a Lagrangian module \u03a8L(xc, t) which tracks the trajectory of a particle corresponding to xc in the canonical space, and a canonical module which encodes the appearance and geometry in the canonical scene. Under the assumption of finite rigid bodies, we exploit the learned motion by the Lagrangian module and design a motion grouping module to discover moving parts. The particles in the same group share a common rigid transformation and should belong to the same part. Next, we will describe these modules and loss functions in detail.4.1 CANONICAL MODULE  Same as NeRF, the canonical module is formulated as an implicit function F\u03b8(xc,d) \u2192 (c, \u03c3) which encodes the geometry and appearance in a canonical space. To accelerate convergence, a hybrid representation of feature volume and neural network is adopted. The queried canonical coordinate xc is first used to interpolate the corresponding features within a 3D feature volume Vc \u2208 RNx\u00d7Ny\u00d7Nz\u00d7C, where the Nx \u00d7Ny \u00d7Nz denotes the spatial resolution and C is the feature dimension. To alleviate the local gradient artifact of grid representation, we adopt multi-distance interpolation and concatenate the features in different resolutions as (Fang et al. (2022)):\nfc = Tri-Interp(xc,V)\u2295...\u2295 Tri-Interp(xc,V[:: sM ]). (3)\nAfter positional encoding, the queried feature with d is fed into MLPs to predict \u03c3 and c.4.2 EULERIAN MODULE  The Eulerian module \u03a8E(x, t) records which particle xc in the canonical space goes through a specific location x at the query time t. Assuming that the scene is piece-wise rigid, we formulate this mapping as a rigid transformation in SE(3) similar to Park et al. (2021a), which ensures that all the points on the same rigid body can be transformed using the same set of parameters. Specifically, our Eulerian module contains three components. First, the 3D feature volume VE stores the information about the particles that pass through each position during the entire observation period. Second, a motion extractor EE decodes the motion feature from the interpolated feature in VE at query time t. Third, different from Park et al. (2021a) that uses a screw axis as an intermediate representation, our rigid transformation decoder DE directly maps the motion feature to rotation and translation parameters. The overall process can be formulated as:\n(RE, tE) = DE(fEm), where fEm = EE (Tri-Interp (x,VE), t) (4)\nHere we employ the continuous 6D intermediate representation (Zhou et al. (2019)) for 3D rotation RE. The Eulerian mapping from the world space at each temporal frame to the canonical space can be calculated by:\nxc = RE(x\u2212 tE) (5)\n4.3 LAGRANGIAN MODULE\nAs the inverse of the Eulerian module, the Lagrangian module \u03a8L(xc, t) tracks the trajectories of specific object particles over time. We use the same manner to construct VL, EL and DL. Different from the Eulerian perspective, the trajectories of each particle in the Lagrangian perspective can be an important cue for rigid part discovery. All particles belonging to the same rigid part share the same rigid body transformation, which means that their motion can be represented by a single feature vector. So we add an additional motion grouping network G (see Figure 3) after VL to restrict that particle trajectories are only subject to a finite number of rigid motion patterns.\nSimilar to (Xu et al. (2022)), we use the attention module with the straight-through estimator trick to achieve the hard grouping of La-\ngrangian features. To encourage the spatial coherence of points in the same group, the coordinate of each point xic is concatenated to the corresponding Lagrangian feature f i L. Specifically, we first compute the similarity map A between the feature {f iLx = f iL \u2295 xic} and learnable slots {Sl} by Gumbel-softmax:\nAil = exp(Wqf iLx \u00b7WkSl + \u03b3i)\u2211L 1 exp(Wqf i Lx \u00b7WkSl + \u03b3i), (6)\nwhere Wq and Wk are linear mappings and \u03b3 is a sample drawn from Gumbel(0, 1). Then the straight-through estimator trick is used to convert the soft similarity map to one-hot formulation:\nA\u0302 = one-hot(Aargmax) +A\u2212 detach(A) (7) where the detach operation cuts off the corresponding gradient. Despite the hard conversion, Equation 7 can still keep the gradient the same as A. The hard similarity matrix A\u0302 distributes all the Lagrangian features into several groups, where each group represents the particles with the same motion pattern. Instead of directly assigning the learned slot as the updated Lagrangian feature, we calculate the average of all the Lagrangian features in the same group to update the original Lagrangian features. In this way, each updated Lagrangian feature will be directly related to the Lagrangian grid VL, allowing for more efficient optimization. This procedure can be formulated as:\nf\u0302 iL = \u2211I 1 A\u0302il \u00b7 f iL\u2211I\n1 A\u0302il (8)\nThen the updated Lagrangian features f\u0302 iL with query time t is fed into EL and DL sequentially to decode the motion feature fLm and the rigid transformation RL, tL. As mentioned in Section 4.4, to efficiently implement the cycle consistency between the Eulerian and Lagrangian modules, we expect RL = RE and tL = tE. So the Lagrangian mapping from the canonical space to the world space at each temporal frame is calculated by:\nx = R\u22121L (xc + tL) (9)4.4 LOSS FUNCTIONS  As our main optimization goal, we adopt the Mean Squared Error (MSE) between the rendered pixel color and the ground truth pixel color as our reconstruction loss:\nLphoto = 1 |R| \u2211 r\u2208R \u2225C\u0302(r)\u2212C(r)\u222522. (10)\nWe also use a total variation loss Ltv to smooth the motion volumes and encourage motion similarity of spatial neighbors. Following Sun et al. (2022), the per-point color loss Lper pt and background entropy loss Lentropy are used to directly supervise the sampled point color and encourage the background probability to concentrate around 0 or 1.\nIn addition, a cyclic consistency loss is designed to encourage the reciprocity of the Lagrangian module and the Eulerian module. Instead of measuring the displacement of the transformations between these two views like Liu et al. (2022), we found that accounting for the difference between low-level motion features fLm and fEm leads to more robust optimization and better part discovery. Our cycle loss is defined as:\nLcycle = 1 |Pobj | \u2211\nx\u2208Pobj\n\u2225fxLm \u2212 fxEm\u222522. (11)\nPlease refer to Appendix A.1 for a more detailed discussion of these two implementations of the cyclic consistency loss. Since the deformation of free space does not satisfy the assumption of finite rigid motions, we filter out free space according to density value and only calculate Lcycle at sampled points on objects {x \u2208 Pobj |\u03c3x > \u03f5}. In our experiments, \u03f5 = 10\u22124. The overall loss function is:\nL = Lphoto + wcycleLcycle + wper ptLper pt + wentropyLentropy + wtvLtv. (12)\n4.5 GROUP MERGING MODULE\nIt is not reasonable to use the same number of groups for a variety of scenarios. We generally set a large number of groups as an upper bound on the number of rigid bodies in the scene, which may cause over-segmented results (Figure 5). This is because we provide an excessive number of groups, and also the same rigid transformations could be easily represented by very different high-level motion features. To address this problem, we design an efficient heuristic algorithm for group merging based on motion differences. This algorithm is used as post-processing after training only and does not affect the parameters of the model. We summa-\nrize this group merging algorithm in Figure 4. 1) We sample points uniformly in canonical space and filter the free space points with density lower than the threshold \u03f5. 2) These remained points are fed into the Lagrangian module to get the updated feature f\u0302 iL, which is the high-level representation of each motion group. 3) We evaluate the rigid transformation similarity between each pair of groups: The rigid transformation sequences are generated by decoding the updated slots into rotation and translation with uniformly sampled times between 0 and 1. 4) We use the Absolute Pose Error (APE) to measure the difference between each sequence pair:\nAPEi,j = \u2211 t \u2225(Pti)\u22121Ptj \u2212 I4\u00d74\u2225, (13)\nwhere Pti is the transformation matrix of group i at time t. 5) We recursively find the two groups with the smallest APE at the current step and record their merge APE cost until all the groups are merged into a single one. In the early stages, the groups with similar motion patterns are merged, which keeps the merging cost growth slow. Once groups representing different motions are merged, the cost will jump, indicating that the merging process should terminate. In practice, we simply find the termination step with the largest cost increase to the subsequent step as our final result.5 EXPERIMENTS AND RESULTS  Our method not only enables high-quality dynamic scene reconstruction but also allows for the discovery of reasonable rigid parts. In this section, we first evaluate the reconstruction and part discovery performance of our method on the D-NeRF 360\u25e6 synthetic dataset. Then, we construct a synthetic dataset with ground-truth motion masks to quantitatively evaluate our motion grouping results. Finally, we provide direct applications for structural scene modeling and editing.5.1 IMPLEMENTATION  We use 50\u00d7 50\u00d7 50 voxels for the Eulerian and Lagrangian volume and a 160\u00d7 160\u00d7 160 voxel for the canonical volume. Following Fang et al. (2022), we employ the progressive upsample the resolution for acceleration. We use two separated linear layers to predict the 6D rotation and 3D translation with biases as (1, 0, 0, 0, 1, 0) and (0, 0, 0), respectively, so that the initial deformation is an identity. We use the Adam optimizer for a total of 20k iterations, by sampling 4096 rays from a randomly sampled image in each iteration. All the experiments were conducted on a single NVIDIA RTX3090 GPU. More details can be found in the appendix.5.2 EVALUATION ON D-NERF DATASET  We adopt the 360\u25e6 Synthetic dataset provided by D-NeRF (Pumarola et al. (2020)) to evaluate our method quantitatively and qualitatively. The dataset contains eight synthetic dynamic scenes with different motion patterns, and only one view is captured at each time step. We compare our method with the state-of-the-art dynamic NeRF methods: Non-deformation-based methods T-NeRF (Pumarola et al. (2020)), K-Planes (Fridovich-Keil et al. (2023)), HexPlane (Cao & Johnson (2023)),\nEulerian-based method D-NeRF (Pumarola et al. (2020)), TiNeuVox (Fang et al. (2022)), NDVG (Guo et al. (2022)) and a Lagrangian-view method WIM (Noguchi et al. (2022)). For TiNeuVox, we use their base version with a canonical grid in 1603 resolution and hidden layers of 256 channels. Following these previous works, we train each scene with images at 400 \u00d7 400 resolution and use two metrics for evaluation: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity (SSIM) (Zhou et al. (2004)) and Learned Perceptual Image Patch Similarity (LPIPS) (Richard et al. (2018)).\nAs shown in Table 1, while keeping the training time within 30 minutes on one GPU, our method not only achieves high rendering quality but also supports part discovery. Compared to the previous methods, we achieved the best in SSIM and LPIPS, and second best in PSNR. Compared to TiNeuVox, our method has a slight PSNR drop. The main reason is that TiNeuVox employs a temporal enhancement module in the canonical space to improve quality, which also leads to a time-varying canonical space. After removing this enhancement module in TiNeuVox, its average PSNR drops to 31.47. In our paper, to achieve better disentanglement of geometry and motion, we expect the geometric evolution only comes from the scene motion. Therefore we did not adopt a similar enhancement strategy to form a time-invariant canonical space. For WIM, due to the nonexistence of canonical space, the significant motion ambiguity under the single view setting causes the failure.\nWe show our visualization results in Figure 5. It can be seen that our method enables high-quality appearance and geometry reconstruction. We also assign each query point the corresponding group color and render it to 2D images. As discussed in Section 4.5, over-segmentation occurs because similar motion could be represented by different high-level features (see the third row in Figure 5). Through our group merging algorithm, we only retain the highly distinguishable motion modes and obtain concise part segmentation. Thanks to the motion-based grouping mechanism, our method is capable of overlooking motion-irrelevant characteristics in geometry and appearance and producing clean part discovery results on these realistic complex scenes.5.3 MOTION GROUPING EVALUATION  In this section, we provide a quantitative evaluation of our motion grouping results. We created a synthetic dataset with ground truth image-segmentation pairs using Kubric toolkit (Greff et al. (2022)). Each created scene contains 1 to 5 realistic real-world objects from the GSO dataset (Downs et al. (2022)) with different initial velocities and motion directions. We followed the same sampling and rendering process as D-NeRF (Pumarola et al. (2020)) to generate a 120-frame monocular image sequence with 256\u00d7256 resolution for each scene. To begin our evaluation, we first establish the correlation pairs between the ground truth label and our predicted groups. For each group, we assign the ground truth label with the highest number of pixels corresponding to it in the first 10 frames. More details are included in the appendix. We calculate the mean Intersection over Union (IOU) for the assigned label mask with its corresponding ground truth mask over the entire image sequence. It is noted that achieving a high mIOU score over the entire sequence requires more than just the ability to accurately distinguish each individual part. It also necessitates the capacity to consistently track each part throughout the sequence.\nWe present 10 examples in Figure 6, showcasing both quantitative mIOU and qualitative visualization and comparisons. Despite the variation in the scene configurations, our method achieves an mIOU score of over 85% on most scenes, clearly demonstrating its robustness over the dataset. Moreover, the high mIOU score indicates that our method can generate accurate part segmentation results and continuously track specific parts throughout the sequences, see the learned trajectories in Figure 6, ensuring both temporal and multi-view consistency of the discovered parts. Furthermore, our method is capable of dealing with complex geometry and topology. Holes (cable in example 5) and geometry details are nicely revealed by our method. By utilizing our motion-based grouping approach, our method can accurately segment objects even if they are spatially separated\u2013 see the gloves (example 1) and 3-car (example 7) in Figure 6.5.4 APPLICATION: STRUCTURED SCENE MODELING BY ROBOTIC MANIPULATION  Observation and interaction are crucial for human beings to learn from the real world. In this section, we show that our method can identify objects and understand the functionality of their parts by observing physical interaction procedures. To demonstrate this, we capture a set of robotic manipulation sequences with a similar monocular camera setting as (Pumarola et al. (2020)). As shown in Figure 7 above, by observing the robot\u2019s work process, like picking up a toy or inserting a peg, our method can accurately identify the manipulated object, as well as the links and joints of the robot. Note that since the robotic arms\u2019 trajectories are different in the two sequences, the joints discovered by motion are also different. The discovered 3D parts with their Lagrangian motion could provide a strong prior for downstream functionality reasoning and robotic reinforcement learning tasks.5.5 APPLICATION: SCENE EDITING  In addition to scene understanding, with the learned structural representation of dynamic scenes, our method can also edit scenes and generate new renderings from the scene. Figure 7 below presents a few scene-editing applications supported by our approach in HyperNeRF realworld sequence (Park et al. (2021b)). Since our method conducts grouping in the 3D canonical space, the consistency can be maintained not only across multiple views but also across time steps. We show the removal or modification of specific objects in these two real scenes and\ndemonstrate the scalability of our method to real-world applications.6 CONCLUSION  In this paper, we present MovingParts, a novel method for 3D dynamic scene reconstruction and part discovery. Inspired by fluid simulation, we observe the motion in the scene from both the Eulerian view and the Lagrangian view. In the particle-based Lagrangian view, we constrain the motion pattern of the particles to be a few rigid transformations, so that we successfully perform part discovery. To ensure fast convergence during training, we utilize a hybrid feature volume and neural network representation, for both views which are efficiently supervised by a cycle-consistency loss. What is more, the learned part representation could directly be applied to downstream tasks, e.g., object tracking, structured scene modeling, editing, etc.\nLimitations. Motion modeling at a specific location can be considered as a sequence decoding task. In this paper, we explicitly store the motion features in low-dimensional vectors, which makes it challenging to model motion on very long sequences. Although we can circumvent the issue by manually splitting long sequences into shorter ones, a unified long sequence encoding-decoding scheme will still be a more elegant and efficient solution. We defer the exploration of this challenging setting to future work. \\\n            \n            Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$\\assistant\n\n$$\n\nPaper Summary:\nThe paper presents MovingParts, a novel NeRF-based approach for dynamic scene reconstruction and part discovery. The authors propose a hybrid representation of feature volume and neural network to achieve both high-quality reconstruction and reasonable part discovery within 30 minutes. The method consists of three modules: a canonical module, an Eulerian module, and a Lagrangian module. The Eulerian module records the motion of particles passing through a given coordinate, while the Lagrangian module tracks the trajectory of particles in the canonical space. The authors also introduce a motion grouping module to discover moving parts. The method is evaluated on the D-NeRF 360\u00b0 synthetic dataset and achieves high-quality reconstruction and part discovery results.\n\nStrengths:\n- The paper proposes a novel approach for dynamic scene reconstruction and part discovery using a hybrid representation of feature volume and neural network.\n- The method achieves high-quality reconstruction and reasonable part discovery results within 30 minutes.\n- The authors introduce a motion grouping module to discover moving parts, which is a significant contribution to the field.\n\nWeaknesses:\n- The method assumes a static canonical space, which may not be realistic in all scenarios.\n- The motion modeling at a specific location can be considered as a sequence decoding task, and the authors explicitly store the motion features in low-dimensional vectors, which may make it challenging to model motion on very long sequences.\n- The evaluation on the D-NeRF 360\u00b0 synthetic dataset is limited, and the authors should consider evaluating their method on more diverse datasets.\n\nSuggestions for Improvement:\n- The authors should consider using a more realistic canonical space representation, such as a dynamic canonical space or a canonical space learned from the data.\n- The authors should explore a unified long sequence encoding-decoding scheme to model motion on very long sequences.\n- The authors should evaluate their method on more diverse datasets to demonstrate its robustness and generalizability.\n\nRecommendation:\n- The paper presents a novel approach for dynamic scene reconstruction and part discovery, and the authors make significant contributions to the field.\n- The authors should continue to explore and improve their method to address the limitations and challenges mentioned above.\n- The paper is well-written and easy to follow, and the authors provide a clear and concise summary of their approach and results.\n\n$$",
        "peer_review_with_watermark": "$$\n\nPaper Summary:\n\nThe paper introduces MovingParts, a novel method based on NeRF that enables dynamic scene reconstruction and part discovery using monocular input. The authors emphasize the importance of motion as a cue for identifying object parts, proposing a dual approach that leverages both Eulerian and Lagrangian views to achieve cycle consistency in the representation of scene dynamics. By tracking particle trajectories and using a motion grouping module, the method effectively decomposes scene motion into part-level rigid motions. The results demonstrate high-quality reconstruction and practical applications such as part tracking and scene editing, evaluated on the D-NeRF 360\u00b0 synthetic dataset.\n\nStrengths:\n\n1. The innovative integration of Eulerian and Lagrangian views provides a comprehensive framework for dynamic scene reconstruction and part discovery.\n2. The method achieves impressive results in both the speed of dynamic scene reconstruction and the quality of part identification, surpassing many existing approaches.\n3. Practical applications of the extracted part-level representations are well-articulated, showcasing the versatility of the proposed method.\n4. The performance evaluations against state-of-the-art methods are robust, supporting the claims of effectiveness.\n\nWeaknesses:\n\n1. The paper lacks a thorough discussion of the limitations of the hybrid model, particularly regarding the assumption of rigid motion and its applicability to various scene types.\n2. The methodology section could be clearer in detailing the training process, especially on managing long sequences of input data.\n3. Comparison with recent state-of-the-art techniques could be more comprehensive to better contextualize the advancements made with MovingParts.\n4. More evaluation metrics specific to part discovery outcomes would strengthen the assessment of the method's effectiveness.\n\nSuggestions for Improvement:\n\n1. Include a more in-depth discussion of model assumptions and their implications for the methods' applicability across diverse dynamic scenes.\n2. Enhance clarity in the training procedures, especially for long input sequences and potential resolutions.\n3. Provide a more extensive comparison with a range of contemporary methods that focus on dynamic scenes.\n4. Introduce additional metrics to evaluate the effectiveness of part discovery more clearly.\n\nRecommendation:\n\nMovingParts represents a significant contribution to the field of dynamic scene reconstruction and part discovery, showcasing innovative methodologies and impressive results. With revisions that address the noted weaknesses and enhance clarity, particularly regarding limitations and comparisons to existing work, this research has the potential to make a meaningful impact in the areas of computer vision and graphics.\n\n$$",
        "gr_score_with": 0.8970588235294118,
        "gr_score_without": 0.9201036493198014,
        "output_without": {
            "num_tokens_scored": 4631,
            "num_green_tokens": 4261,
            "green_fraction": 0.9201036493198014,
            "z_score": 4.560275030294509,
            "p_value": 2.554333348020913e-06
        },
        "output_with": {
            "num_tokens_scored": 408,
            "num_green_tokens": 366,
            "green_fraction": 0.8970588235294118,
            "z_score": -0.198029508595333,
            "p_value": 0.5784890118151369
        }
    },
    {
        "title": "DMBP: DIFFUSION MODEL-BASED PREDICTOR FOR RO- BUST OFFLINE REINFORCEMENT LEARNING AGAINST STATE OBSERVATION PERTURBATIONS",
        "abstract": "Offline reinforcement learning (RL), which aims to fully explore offline datasets for training without interaction with environments, has attracted growing recent attention. A major challenge for the real-world application of offline RL stems from the robustness against state observation perturbations, e.g., as a result of sensor errors or adversarial attacks. Unlike online robust RL, agents cannot be adversarially trained in the offline setting. In this work, we propose Diffusion Model-Based Predictor (DMBP) in a new framework that recovers the actual states with conditional diffusion models for state-based RL tasks. To mitigate the error accumulation issue in model-based estimation resulting from the classical training of conventional diffusion models, we propose a non-Markovian training objective to minimize the sum entropy of denoised states in RL trajectory. Experiments on standard benchmark problems demonstrate that DMBP can significantly enhance the robustness of existing offline RL algorithms against different scales of random noises and adversarial attacks on state observations. Further, the proposed framework can effectively deal with incomplete state observations with random combinations of multiple unobserved dimensions in the test. Our implementation is available at https://github.com/zhyang2226/DMBP",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it.  Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$user\n\nThe peer review format and length should be of standard conference. \\\n            title: DMBP: DIFFUSION MODEL-BASED PREDICTOR FOR RO- BUST OFFLINE REINFORCEMENT LEARNING AGAINST STATE OBSERVATION PERTURBATIONS \\\n            abstract: Offline reinforcement learning (RL), which aims to fully explore offline datasets for training without interaction with environments, has attracted growing recent attention. A major challenge for the real-world application of offline RL stems from the robustness against state observation perturbations, e.g., as a result of sensor errors or adversarial attacks. Unlike online robust RL, agents cannot be adversarially trained in the offline setting. In this work, we propose Diffusion Model-Based Predictor (DMBP) in a new framework that recovers the actual states with conditional diffusion models for state-based RL tasks. To mitigate the error accumulation issue in model-based estimation resulting from the classical training of conventional diffusion models, we propose a non-Markovian training objective to minimize the sum entropy of denoised states in RL trajectory. Experiments on standard benchmark problems demonstrate that DMBP can significantly enhance the robustness of existing offline RL algorithms against different scales of random noises and adversarial attacks on state observations. Further, the proposed framework can effectively deal with incomplete state observations with random combinations of multiple unobserved dimensions in the test. Our implementation is available at https://github.com/zhyang2226/DMBP \\\n            paper text: 1 INTRODUCTION  Reinforcement learning (RL) has been proven to be a powerful tool for high-dimensional decisionmaking problems under uncertainty (Mnih et al., 2015; Silver et al., 2017; Schrittwieser et al., 2020). However, its trial-and-error learning manner requires frequent interactions with the environment, which can be expensive and/or dangerous in a variety of real-world applications (Levine et al., 2020). A widely adopted solution is to build up a simulator for policy training, which is costly and may fail due to the discrepancy between the simulator and reality. As a promising alternative that has received growing attention, offline RL fully explores offline datasets and requires no interaction with the environments in the training process.\nA major challenge of offline training is on the robustness against perturbation on state observations, which may result from sensor errors, adversarial attacks, and mismatches between statistic datasets and the real environment. For example, GPS signal errors can lead to inaccurate positioning of autonomous vehicles, and position sensor errors can lead to erroneous estimation of robot arm postures. The robustness of the trained policy against state perturbations is vital for preventing agents from catastrophic movements. In online settings, various adversarial training methods have been proposed to robustly handle the mismatch between observed and actual states (Zhang et al., 2020; 2021; Sun et al., 2021). These methods are not directly applicable in offline training.\nA classical approach against perturbed state observation is to train robust policies against worst-case disturbances (see the left subplot in Figure 1), which may lead to over-conservatism (Zhang et al., 2020; 2021). In a pioneering work (Yang et al., 2022), the authors propose an alternative approach that adopts the conservative smoothing method to smoothen the Q-value and regularize the policy, preventing the agent from taking catastrophic movements under adversarial attacks in the test. The performance of the aforementioned approach may decay quickly with the increasing noise scale, especially in complicated environments with high-dimensional action and state spaces.\n\u2217Corresponding author\nFor online image-based deep RL, Lin et al. (2017) propose a model-based approach to \u201cdenoise\u201d the observations by predicting the actual states. They construct a multiple-layer perceptron (MLP) neural network to detect the adversarial attack on image-based observations and predict the original states for decision-making in Atari games. For state-based RL tasks, similar MLP-based prediction methods have been used as data augmentation in online (Janner et al., 2019) and offline (Yu et al., 2020; 2021) settings instead of denoising tools. In general, MLP-based prediction methods cannot be applied to complicated state-based tasks (like Mujoco) which are sensitive to observation noise and prone to error accumulation.\nRecently, diffusion-based generative models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020b) are widely used in offline RL/decision-making problems as trajectory generators (Janner et al., 2022; Ajay et al., 2022; Liang et al., 2023) and behavior cloners (Wang et al., 2022). We note that the potential of diffusion models to facilitate decision making via state denoising has not been fully explored.\nTowards this end, we propose a new framework that predicts the actual states against observation perturbations for state based offline RL, which is referred to as Diffusion Model-Based Predictor (DMBP). Different from the aforementioned works, the proposed approach utilizes diffusion models as noise reduction tools rather than generation models, and can therefore enhance the robustness of existing offline RL algorithms against different scales of perturbations on state observations.\nA diagram of the proposed approach is shown in the right subplot of Figure 1. Given the pastestimated state trajectory, last-step agent-generated action, and the noised current state from the environment, DMBP utilizes a conditioned diffusion model to estimate the current state by reversely denoising data. To mitigate the error accumulation issue in state estimation, we propose a new non-Markovian loss function that minimizes the sum entropy of denoised states over the RL trajectory (cf. Section 4). In order to well capture the relationship between the noised current state and the denoised state trajectory (especially the last RL timestep denoised state), we propose an Unet-MLP neural network structure to predict noise information (cf. Appendix B.1). The output of DMBP is an estimation of the current state, which is fed into an offline RL algorithm to generate the action. To our knowledge, this is the first state denoising framework for offline RL against observation perturbations in state-based tasks.\nThe proposed framework has several advantages over existing offline RL methods against noisy observations. First, with an objective of recovering the actual state, DMBP can significantly strengthen the robustness of existing offline RL algorithms against different scales of random noises and adversarial attacks. The proposed approach does not lead to over-conservative policies, compared with counterparts that train robust policies against worst-case (or adversarial) perturbations.\nFurther, by virtue of the capability of diffusion models to infill the missing regions (i.e., image inpainting), DMBP facilitates the decision making under incomplete state observations with random combinations of multiple unobserved dimensions in the test. Such a situation is common in reality, for example, when robots continue to work with compromised sensors.2 RELATED WORKS  Robust RL. Robust RL can be categorized into two taxonomies: training-time and testing-time robustness. Training-time robust RL involves perturbations during the training process, while evaluating the agent in a clean environment (Zhang et al., 2022b; Ye et al., 2023). Conversely, testing-time robust RL focuses on training the agent with unperturbed datasets or environments and then testing its performance in the presence of disturbances (Yang et al., 2022; Panaganti et al., 2022). Our work primarily aims at enhancing the testing-time robustness of existing offline RL algorithms.\nTesting-time robust RL formulations can generally be divided into three categories (Xu et al., 2022). i) Uncertain observations: In online settings, Zhang et al. (2020) propose a state-adversarial Markov decision process (SA-MDP) framework, which is advanded by Zhang et al. (2021); Sun et al. (2021) that adopt neural networks to simulate worst-case observation attacks for the training of more robust policies. In offline settings, Yang et al. (2022) utilize the conservative smoothing method to make the agent take similar actions when the perturbations on state observation are relatively small. ii) Uncertain actions: Tessler et al. (2019) explore the training of robust policies against two types of action uncertainties, i.e., occasional and constant adversarial perturbations. Tan et al. (2020) utilize adversarial training on actions to enhance the robustness against action perturbations. iii) Uncertain transitions and rewards: The computation of optimal policies against uncertain environment parameters has been explored under the robust Markov Decision Process (MDP) (Xu & Mannor, 2006; Roy et al., 2017; Ho et al., 2018) and the distributionally robust MDP frameworks (Xu & Mannor, 2010; Yu & Xu, 2015). In online RL settings, Pinto et al. (2017) and Gleave et al. (2019) train the agent under adversarial model uncertainty through a two-player Markov game approach (Littman, 1994). For offline RL training, Panaganti et al. (2022) propose a dual reformulated robust Bellman operator to deal with the uncertain transition probability.\nFor models in the first two categories, the true states and transition probabilities of the environments are not influenced by the action, which is not the case for the robust approaches against model uncertainties developed in the third category. Our work belongs to the first category.\nDiffusion models in offline RL. The diffusion model was originally proposed as an iterative denoising procedure for image generation in computer vision (Sohl-Dickstein et al., 2015; Ho et al., 2020). Recently, diffusion model has been adopted in decision-making for state-based tasks. Diffuser (Janner et al., 2022) and Decision Diffuser (Ajay et al., 2022) utilize the conditional diffusion model as a trajectory generator to facilitate the decision making of the agent. Wang et al. (2022) propose the Diffusion-QL algorithm that adopts the diffusion model to regulate the policy not to be far away from the one used in datasets, in a similar spirit to Fujimoto et al. (2019); Wu et al. (2019). Different from the aforementioned works, the proposed approach utilizes the diffusion model as a denoiser (against state observation perturbations) rather than a generator, for robust offline training of RL agents.3 PRELIMINARIES  Offline RL. RL tasks are generally modeled as Markovian Decision Processes (MDP) in the form of M = (S,A, r, P, \u03b3, d0), where S is the state space, A is the action space, r : S \u00d7A \u2192 R represents the reward function, P is the model dynamics, \u03b3 \u2208 [0, 1) is the discount factor, and d0 \u2208 \u2206(S) is the distribution of initial state s0 (the set of the probability distribution over X is denoted as \u2206(X )). P (s\u2032|s,a) : S \u00d7A \u2192 \u2206(S) represents the transition function from state s to s\u2032 when taking action a. The state-action-reward transitions over trajectory are recorded as \u03c4 := (st,at, rt)t\u22650.The goal of RL is to learn a policy \u03c0\u03d5 that maximizes the expectation of the cumulated discounted reward R(\u03c4 ) = \u2211\u221e t=0 \u03b3 tr(st,at), denoted by \u03c0\u2217\u03d5 = argmax\u03c0 Es0\u223cd0,a\u223c\u03c0[R(\u03c4 )].\nA commonly used iteration method for state-based tasks is under the actor-critic framework, where the Q-value of a policy is defined asQ\u03c0(st,at) := Ea\u223c\u03c0[ \u2211\u221e i=t \u03b3\n(i\u2212t)r(si,ai)] and is modeled using neural networks (recorded as Q\u03c8(st,at)). To approach an optimal policy, the temporal difference (TD) method is adopted to update the critic Q-value by minimizing the TD loss: LTD(\u03c8) := E(s,a,r,s\u2032)\u2208D[(r + \u03b3maxa\u2032\u2208AQ\u03c8(s\u2032,a\u2032) \u2212 Q\u03c8(s,a))2]. The actor is updated by Lactor(\u03d5) := Es\u2208D,a\u223c\u03c0\u03d5(\u00b7|s)[\u2212Q(s,a)], where the dataset D records historical interactions between agent and environment, and is continuously updated in the alternate training of the actor and the critic. In offline RL settings, the training is performed on a statistic dataset D\u03bd := {(s,a, r, s\u2032)}, which is obtained from a behavior policy \u03c0\u03bd without any interaction with the environment.\nDirect adoption of the actor-critic approach may lead to a severe distributional shift between the trained policy \u03c0\u03d5 and the behavior policy \u03c0\u03bd due to the over-estimation of the Q-value of actions unseen in datasets. To mitigate this issue, policy regularization has been adopted to update the actor through constrained policy loss (Wu et al., 2019; Kumar et al., 2019; Fujimoto et al., 2019; Fujimoto & Gu, 2021; Wang et al., 2022): L(\u03d5) := Ld(\u03d5) + \u03b1actorLactor(\u03d5), where Ld(\u03d5) is the behavior cloning loss representing the nominal distance between the trained policy and the behavior policy, and \u03b1actor is the coefficient for the Q-value term. Alternatively, conservative Q estimation updates the critic through minimizing the constrained Q-value loss (Kumar et al., 2020; An et al., 2021; Lyu et al., 2022; Yang et al., 2022): L(\u03c8) := Lq(\u03c8) + \u03b1criticLTD(\u03c8), where Lq(\u03c8) is the penalty on Q-value for out-of-distribution actions, and \u03b1critic is the coefficient for the TD loss term.\nDiffusion model. Diffusion based generative models have been widely used for synthesizing highquality images from text descriptions. The forward process, i.e., the noising process, is a Markov chain that gradually adds Gaussian noise to data according to a variance schedule \u03b21,..., \u03b2K :\nq(x1:K | x0) := K\u220f k=1 q(xk | xk\u22121), q(xk | xk\u22121) := N (xk; \u221a 1\u2212 \u03b2kxk\u22121, \u03b2kI).\nThe reverse process, i.e., the denoising process, is a Markov chain with learned Gaussian transitions that usually starts at p(xK) = N (xK ; 0, I):\np\u03b8(x0:K) := p(xK) K\u220f k=1 p\u03b8(xk\u22121 | xk), p\u03b8(xk\u22121 | xk) := N (xk\u22121;\u00b5\u03b8(xk, k),\u03a3\u03b8(xk, k)).\nHo et al. (2020) derive a simplified surrogate loss for the reverse process denoising: Ldenoise(\u03b8) := Ek\u223c[1,K],\u03f5\u223cN (0,I)[\u2225\u03f5\u03b8(xk, k)\u2212 \u03f5\u22252]. (1)\nThe Gaussian noise \u03f5, which perturbs the original data x0 to noised data xk, is estimated through the neural network based predictor \u03f5\u03b8(xk, k). xk\u22121 is sampled from the reverse process as \u00b5\u03b8(xk, k) and \u03a3\u03b8(xk, k) are functions of \u03f5\u03b8(xk, k). It is straightforward to extend diffusion models to conditional ones with p\u03b8(xt\u22121 | xt, c) (conditioned on information c), where the noise prediction is given by \u03f5\u03b8(xk, k, c).4 DIFFUSION MODEL BASED PREDICTOR  We express the perturbed version of the original state s as s\u0303, where Bd(s, \u03f5) := {s\u0303 : d(s, s\u0303) \u2264 \u03f5} is the perturbation set and the metric d(\u00b7, \u00b7) is based on \u2113p norm, as in Shen et al. (2020). An adversarial attack on state s is introduced in Yang et al. (2022): s\u0303\u2217 = argmaxs\u0303\u2208Bd(s,\u03f5)D(\u03c0\u03d5(\u00b7|s)\u2225\u03c0\u03d5(\u00b7|s\u0303)), where D(\u00b7\u2225\u00b7) is the divergence of two distributions. The targets of both works are to minimize the smoothness regularizer for the policy: R\u03c0s = Es\u2208D maxs\u0303\u2208Bd(s,\u03f5)D(\u03c0(\u00b7|s)\u2225\u03c0(\u00b7|s\u0303)), and to minimize the smoothness regularizer for the value function: RVs = Es\u2208D,a\u223c\u03c0maxs\u0303\u2208Bd(s,\u03f5)(Q(s,a)\u2212 Q(s\u0303,a)), against the perturbations on state observations. We remark that we do not normalize the state observations when applying the perturbations as in Shen et al. (2020); Sun et al. (2021), in contrast to Zhang et al. (2020); Yang et al. (2022).\nIn Section 4.1, we propose DMBP to recover the actual state for decision-making (which is fundamentally different from the technical approaches in aforementioned works). In Section 4.2, we propose a new non-Markovian loss function to mitigate error accumulation. In Section 4.3, we apply DMBP to RL tasks under incomplete state observations with unobserved dimension(s).4.1 CONDITIONAL DIFFUSION FOR PREDICTING REAL STATE  As there are two timesteps involved in our framework, we use superscripts i, k \u2208 {1,...K} to denote diffusion timestep and subscript t \u2208 {1,..., T} to denote trajectory timestep in RL tasks. DMBP is inspired by the diffusion model framework originally proposed for image generations (Ho et al., 2020). As the proposed framework essentially deals with information with small to medium scale noises instead of generating data from pure noise, we redesign the variance schedule as:\n\u03b2i = 1\u2212 \u03b1i = e\u2212 b i+a+c, \u03b1\u0304k = k\u220f i=1 \u03b1i, \u03b2\u0303i = 1\u2212 \u03b1\u0304i\u22121 1\u2212 \u03b1\u0304i \u03b2i,\nwhere a, b, c are hyperparameters (cf. Appendix B.2). The redesigned variance schedule restricts the noise scale to be small in the diffusion process and limits the total number of diffusion timesteps K for predictor training. We use the conditional diffusion model to obtain the denoised state s\u0302t from the noised state s\u0303t, with the condition on last step action at\u22121 and the previously denoised state trajectory \u03c4 s\u0302t\u22121 := {s\u03021, s\u03022,..., s\u0302t\u22121}. The denoised state s\u0302t is sampled from the reverse denoising process, which can be expressed as a Markov chain:\ns\u0302t \u223c p\u03b8(s\u03030:kt | at\u22121, \u03c4 s\u0302t\u22121) = fk(s\u0303t) k\u220f i=1 p\u03b8(s\u0303 i\u22121 t | s\u0303it,at\u22121, \u03c4 s\u0302t\u22121), (2)\nwhere fk(s\u0303t) = \u221a \u03b1\u0304ks\u0303t. The transitions p\u03b8(s\u0303i\u22121t | s\u0303it,at\u22121, \u03c4 s\u0302t\u22121) can be modeled using Gaussian distribution N (s\u0303i\u22121t ;\u00b5\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302t\u22121, i),\u03a3\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302t\u22121, i)), with the following mean and variance (Ho et al., 2020):\n\u00b5\u03b8(s\u0303 i t,at\u22121, \u03c4 s\u0302 t\u22121, i) = \u221a \u03b1i(1\u2212 \u03b1\u0304i\u22121) 1\u2212 \u03b1\u0304i s\u0303it + \u221a \u03b1\u0304i\u22121\u03b2i 1\u2212 \u03b1\u0304i s\u0303 0(i) t, \u03a3\u03b8(s\u0303 i t,at\u22121, \u03c4 s\u0302 t\u22121, i) = \u03b2\u0303iI.\nHere, s\u03030(i)t is the state directly recovered from the current diffusion step noise prediction, which is given by\ns\u0303 0(i) t = 1\u221a \u03b1\u0304i [s\u0303it \u2212 \u221a 1\u2212 \u03b1\u0304i\u03f5\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302t\u22121, i)]. (3)\nThe reverse diffusion chain is given by\ns\u0303i\u22121t | s\u0303it = s\u0303it\u221a \u03b1i \u2212 \u03b2i\u221a \u03b1i(1\u2212 \u03b1\u0304i) \u03f5\u03b8(s\u0303 i t,at\u22121, \u03c4 s\u0302 t\u22121, i) +\n\u221a \u03b2\u0303i\u03f5, (4)\nwhere \u03f5 \u223c N (0, I) and is set to be 0 at the final denoising step (i = 1). For the final step denoising output of Eq. 4 s\u03030t (i.e., the output of DMBP), we refer it to as s\u0302t. s\u0302t can be used for decision-making by any offline-trained agent according to at = \u03c0\u03d5(\u00b7 | s\u0302t). s\u0302t is stored in the trajectory cache \u03c4 s\u0302t, and the pair of (\u03c4 s\u0302t, at) will be utilized for the next step denoising. In practice, on account of the stochasticity involved in the diffusion process, we denoise the state 50 times in parallel and take the average value as the final output s\u0302t to prevent the denoised state from falling out of the distribution.\nWe find that directly inputting state trajectories and action into neural networks leads to poor noise estimation (cf. Appendix C for ablation study on network structure), partially due to the fact that s\u0302t\u22121 is more closely related to s\u0303it than s\u0302j with j < t\u2212 1, and that this information cannot be well captured by neural networks. Therefore, we first extract the information from the trajectory with U-net (Ronneberger et al., 2015; Janner et al., 2022) (recorded as U\u03be(s\u0303it, \u03c4 s\u0302 t\u22121)), and then utilize an MLP-based neural network to predict the noise through \u03f5\u03b8(U\u03be(s\u0303it, \u03c4 s\u0302 t\u22121), s\u0303 i t,at\u22121, s\u0302t\u22121, i), which is represented by \u03f5\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302 t\u22121, i) for notational convenience. See Appendix B.1 for details.4.2 NON-MARKOVIAN LOSS FUNCTION  The accuracy of the current denoising result s\u0302t is highly dependent on the accuracy of the diffusion condition \u03c4 s\u0302t\u22121. A straightforward adoption of the loss function 1 in the denoising diffusion probabilistic model (DDPM) (Ho et al., 2020) may lead to severe error accumulation in online testing, due to the mismatch between the training-process noise prediction \u03f5\u03b8(s\u0303it,at\u22121, \u03c4 s t\u22121, i) and the testing-process noise prediction \u03f5\u03b8(s\u0303it+1,at, \u03c4 s\u0302 t\u22121, i). To mitigate error accumulation and enhance the robustness of DMBP, we propose a non-Markovian training objective to minimize the sum entropy of denoised states over the RL trajectory \u03c4 :\nLentropy = T\u2211 t=2 Est\u2208\u03c4,q(st) [ \u2212 logP (s\u0302t | at\u22121, \u03c4 s\u0302t\u22121) ], (5)\nwhere P (s\u0302t | at\u22121, \u03c4 s\u0302t\u22121) = p\u03b8(s\u03030t | at\u22121, \u03c4 s\u0302t\u22121) is the distribution of state after denoising at RL timestep t. Following the setting in (Ho et al., 2020), we establish a closed-form expression of the training objective that minimizes Eq. 5, which can be simplified as (cf. the details in Appendix A):\nLsimple(\u03b8) = Es1\u223cd0,\u03f5it\u223cN (0,I),i\u223cUK [ T\u2211 t=2 \u2225\u03f5\u03b8(s\u0303it,at\u22121, \u03c4 s\u0302t\u22121, i)\u2212 \u03f5it\u22252 ], (6)\nwhere UK is the uniform distribution over discrete set {1, 2,...,K}, and the noised states for all terms are sampled through:\ns\u0303it = \u221a \u03b1\u0304ist + \u221a 1\u2212 \u03b1\u0304i\u03f5it, \u03f5it \u223c N (0, I).\nFor computational convenience, we further simplify Eq. 6 and derive our non-Markovian loss function by sampling the partial trajectory (st\u2212N,at\u2212N, st\u2212N+1,..., st+M\u22121) from the offline dataset D\u03bd (N is the condition trajectory length and M is the sample trajectory length):\nL(\u03b8) = Ei\u223cUK,\u03f5t\u223cN (0,I),(st\u2212N,...,st+M\u22121)\u2208D\u03bd [ \u2225\u03f5\u03b8(s\u0303it,at\u22121, \u03c4 st\u22121, i)\u2212 \u03f5it\u22252\ufe38 \ufe37\ufe37 \ufe38\nLt\n+\nt+M\u22121\u2211 m=t+1\n\u2225\u03f5\u03b8(s\u0303im,am\u22121, \u03c4 s\u0306m\u22121, i)\u2212 \u03f5im\u22252\ufe38 \ufe37\ufe37 \ufe38 Lm\n],\n(7)\nwhere the state trajectory condition for the predictor \u03f5\u03b8 inLt is the original \u03c4 st\u22121 = {st\u2212N,..., st\u22121} from the offline dataset D\u03bd, and the state trajectory condition in Lm can be expressed as \u03c4 s\u0306m\u22121 = {s\u0306j | j \u2208 {m\u2212N,...,m\u2212 1}}, with:\ns\u0306j =  sj1\u221a \u03b1\u0304i [ s\u0303ij \u2212 \u221a 1\u2212 \u03b1\u0304i\u03f5\u03b8(s\u0303ij,aj\u22121, \u03c4 s\u0306j\u22121, i) ] if j < t, otherwise (j \u2208 {t,..., t+M \u2212 2}).\nDifferent from the loss function in Ho et al. (2020) that concerns only single-step diffusion accuracy (for data generation under conditions on ground-truth states), the proposed non-Markovian loss function trades off between the diffusion accuracy at the current RL timestep and the condition shift in a long RL time horizon (to avoid error accumulation).4.3 DIFFUSION BASED STATE INFILLING FOR UNOBSERVED DIMENSION  Inspired by the application of diffusion models in image inpainting (Lugmayr et al., 2022), we propose a state infilling procedure for DMBP facilitated decision making, which is shown to work well on state-based RL tasks with incomplete state observations (cf. Section 5.2).\nAlgorithm 1 Diffusion based state infilling for DMBP Require: sKt \u223c N (0, I), s\u0301t,at\u22121, \u03c4 s\u0302t\u22121,m\n1: for i = K,..., 1 do 2: for u = 1,..., U do 3: \u03f5 \u223c N (0, I) if i > 1, else \u03f5 = 0 4: s\u0301i\u22121t,known = \u221a \u03b1\u0304is\u0301t + \u221a 1\u2212 \u03b1\u0304i\u03f5 5: z \u223c N (0, I) if i > 1, else z = 0 6: \u03f5pred = \u03f5\u03b8(s\u0301it,at\u22121, \u03c4 s\u0302t\u22121, i) 7: s\u0301i\u22121t,unknown = 1\u221a \u03b1i s\u0301it \u2212 \u03b2i\u221a \u03b1i(1\u2212\u03b1\u0304i) \u03f5pred + \u221a \u03b2iz 8: s\u0301i\u22121t = m\u2299 s\u0301i\u22121t,known + (1\u2212m)\u2299 s\u0301 i\u22121 t,unknown\n9: if u < U and i > 1 then 10: s\u0301it \u223c N ( \u221a 1\u2212 \u03b2i\u22121s\u0301i\u22121t, \u03b2i\u22121I) 11: end if 12: end for 13: end for 14: return s\u0302t = s\u03010t We denote the ground truth state as st, the unobserved state information as (1\u2212m)\u2299 st, and the observed state information as s\u0301t = m \u2299 st, which is incomplete with some masked dimensions. Given the recovered state trajectory \u03c4 s\u0302t\u22121, agent generated action at\u22121, and the known information of the current state s\u0301t, DMBP aims to recover the original state s\u0302t for decision making. Following the inpainting method of Lugmayr et al. (2022), DMBP infills the missing state information through Algorithm 1. For each diffusion timestep, the known region of the state is determined from the forward process (noising process) in line 4, and the unknown region of the state is determined from the reverse process (denoising process) in line 7. To avoid the disharmony of the forward and reverse process generated information, the combined information (in line 8) takes one diffusion forward step in line 10, which is called \u201cresampling\u201d. The resampling is performed by U times for one diffusion timestep. More resampling times may lead to more accurate and harmonious diffusion information at the cost of higher computational load.\n5 EXPERIMENTS\nWe evaluate the proposed DMBP together with several state-of-the-art baseline offline RL algorithms on D4RL Gym benchmark (Fu et al., 2020) against different types of attacks on state observations. The baseline algorithms include Batch Constrained deep Q-learning (BCQ) (Fujimoto et al., 2019), Conservative Q-Learning (CQL) (Kumar et al., 2020), TD3 with Behavior Cloning (TD3+BC) (Fujimoto & Gu, 2021), Diffusion Q-Learning (Diffusion QL) (Wang et al., 2022), and Robust Offline Reinforcement Learning (RORL) (Yang et al., 2022).\nWe train DMBP for 300 epochs (1000 gradient steps with a batch size of 256 for each epoch) with hyperparameters defined in Appendix B.2. We train the baseline algorithms with the corresponding suggested hyperparameters in specific environments and datasets. We perform two tests, on robustness against noised state observations (in Section 5.1) and on robustness against incomplete state observations with unobserved dimension(s) (in Section 5.2). We utilize the same DMBP for all baseline algorithms (cf. the framework in Figure 1), and benchmark their performance against the original baseline algorithms (without DMBP). We present partial results on the D4RL Mujoco benchmark, and the results of other datasets (including medium-expert, medium, and full-replay) and other benchmarks (including Adroit and Franka Kitchen) can be found in Appendix D.5.1 ROBUSTNESS AGAINST NOISED STATE OBSERVATIONS  Firstly, we evaluate the performance of DMBP in a basic setting with Gaussian noises of standard deviation \u03ba: s\u0303t = st + \u03ba \u00b7 N (0, I). The evaluation results in Table 1 indicate that DMBP can significantly enhance the robustness of all baseline algorithms, especially on the dataset \u201cmediumreplay\u201d, where DMBP strengthened baseline algorithms achieve similar scores as in the corresponding noise-free cases. To demonstrate the powerful denoising effect of DMBP, we visualize a partial trajectory of \"hopper\" during the test in Figure 2.\nWe consider three additional types of noise attacks that are commonly used on state observations, where DMBP-strengthened algorithms also outperform the corresponding baselines (cf. Table 2): i) Uniform random noise distributed inside the \u2113\u221e ball with the norm of \u03ba: s\u0303t = st + \u03ba \u00b7 U(\u2212I, I). ii) Maximum action-difference (adversarial) attack: The noises are selected inside the \u2113\u221e ball with the norm of \u03ba, such that s\u0303t = st + argmaxs\u0303\u2208Bd(s,\u03ba)D(\u03c0\u03d5(\u00b7|s)\u2225\u03c0\u03d5(\u00b7|s\u0303)). Among 20 samples of s\u0303t in the ball, we choose the one with the largest \u2225\u03c0\u03d5(\u00b7|s)\u2212 \u03c0\u03d5(\u00b7|s\u0303)\u22252. iii) Minimum Q-value (adversarial) attack: The noises are selected inside the \u2113\u221e ball with the norm of \u03ba such that, s\u0303t = st + argmins\u0303\u2208Bd(s,\u03ba)Q(s\u0303t, \u03c0\u03d5(\u00b7|s\u0303)). Again, we sample 20 times and choose the one with the minimum Q to be the perturbed state s\u0303t.\nThe latter two adversarial attacks have been considered in the literature (Pinto et al., 2017; Zhang et al., 2020; Yang et al., 2022). For fair comparison, when we use DMBP against adversarial attacks, we first sample 20 noised states s\u0303t and denoise them using DMBP, and then choose the denoised states s\u0302t with the maximum action difference or the minimum Q-value as the perturbed state.5.2 ROBUSTNESS AGAINST INCOMPLETE STATE OBSERVATIONS WITH UNOBSERVED DIMENSION  We utilize DMBP to recover the missing state information for decision-making. In D4RL benchmark problems, we mask some dimensional state information that cannot be observed by the tested policy (i.e., the masked dimensions of the state are set as 0 for t \u2208 {2, 3,..., T}). The baseline algorithms make decisions based on the observed (incomplete) states, and DMBP-improved counterparts take actions according to the recovered states. For each dimension of the state, we make the dimension unobserved and conduct 10 tests. When multiple state dimensions cannot be observed, we randomly select 30 groups of dimensions and conduct 10 tests on each group. The experiment results of CQL, RORL with offline \u201cexpert\u201d and \u201cmedium-replay\u201d datasets are shown in Figure 3. DMBP significantly enhances the performance of all baseline algorithms by accurately predicting the missing state information. On \u201cmedium-replay\u201d datasets, the DMBP strengthened algorithms incur little performance degradation in masked environments, compared with that achieved in the original environments with complete and accurate observations.5.3 ABLATION STUDY  In Figure 4, we conduct ablation studies on dataset \"hopper-expert-v2\", where algorithms are more prone to error accumulation than in other datasets/environments, to demonstrate the efficacy of the proposed non-Markovian loss function and evaluate the impact of the non-Markovian sampling length (M in Eq. 7). We utilize pre-trained Diffusion QL for decision-making to evaluate the performance of DMBP under the framework in Figure 1. Other hyperparameters and DMBP training follow the basic settings in Appendix B.2 and D.1, respectively.\nWhen M = 1, the DMBP training objective 7 reduces to the classical training objective of diffusion models in Eq. 1 (i.e. LM = 0 in Eq. 7). From the second to the fourth subplots of Figure 4, we observe that the direct adoption of classical conditional diffusion models suffers from severe error accumulation as training proceeds. The proposed non-Markovian training objective significantly enhances the robustness of the baseline RL algorithm against state observation perturbations, especially when the noise scale is large. When M is no less than 6, the performance of DMBP remains almost the same. To expedite the computation, we set M = 6 for the \"hopper\" environment. More ablations studies on neural network structure and condition trajectory lengths (N ) can be found in Appendix C.6 CONCLUSION  In this work, we propose the first framework of state-denoising for offline RL against observation perturbations in state-based tasks. Leveraging conditional diffusion models, we develop Diffusion Model-Based Predictor (DMBP) to recover the actual state for decision-making. To reduce the error accumulation during test, we propose a new non-Markovian loss function that minimizes the sum entropy of denoised states along the trajectory. Experiments on D4RL benchmarks demonstrate that the proposed DMBP can significantly enhance the robustness of existing offline RL algorithms against different scales of random noises and even adversarial attacks. The proposed framework is shown to be able to effectively deal with the cases of incomplete state observations (with multiple unobserved dimensions) for state-based RL tasks. \\\n            \n            Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$\\assistant\n\n$$\n\\text{Paper Summary: }\n$$\n\nThe paper proposes a new framework called Diffusion Model-Based Predictor (DMBP) for offline reinforcement learning (RL) against state observation perturbations in state-based tasks. DMBP utilizes conditional diffusion models to recover the actual state for decision-making, and a non-Markovian loss function to mitigate error accumulation. The proposed framework is evaluated on D4RL benchmarks, demonstrating significant enhancements in robustness against different scales of random noises and adversarial attacks.\n\n$$\n\\text{Strengths: }\n$$\n\n*   The proposed framework is the first to address state observation perturbations in offline RL against observation perturbations in state-based tasks.\n*   DMBP leverages conditional diffusion models, which are widely used in decision-making for state-based tasks.\n*   The non-Markovian loss function proposed in the paper significantly enhances the robustness of the baseline RL algorithm against state observation perturbations.\n*   Experiments on D4RL benchmarks demonstrate the effectiveness of DMBP in enhancing robustness against different scales of random noises and adversarial attacks.\n\n$$\n\\text{Weaknesses: }\n$$\n\n*   The paper does not provide a clear comparison with other state-of-the-art offline RL algorithms against state observation perturbations.\n*   The proposed non-Markovian loss function may be difficult to implement and require significant computational resources.\n*   The paper does not provide a detailed analysis of the impact of the non-Markovian sampling length (M) on the performance of DMBP.\n*   The ablation studies conducted in the paper may not be sufficient to thoroughly evaluate the efficacy of the proposed non-Markovian loss function.\n\n$$\n\\text{Suggestions for Improvement: }\n$$\n\n*   Conduct a comprehensive comparison with other state-of-the-art offline RL algorithms against state observation perturbations.\n*   Provide a detailed analysis of the impact of the non-Markovian sampling length (M) on the performance of DMBP.\n*   Conduct more extensive ablation studies to thoroughly evaluate the efficacy of the proposed non-Markovian loss function.\n*   Investigate the applicability of DMBP to other decision-making problems, such as image inpainting and image generation.\n\n$$\n\\text{Recommendation: }\n$$\n\nBased on the strengths and weaknesses of the paper, I recommend that the authors revise and resubmit the paper for publication. The proposed framework of DMBP has the potential to significantly enhance the robustness of offline RL algorithms against state observation perturbations, and further research is needed to fully explore its potential. With some revisions to address the weaknesses and improve the paper, DMBP has the potential to become a leading approach in the field of offline RL.\n\n$$\n\\text{Recommendation (continued): }\n$$\n\nI recommend that the authors consider the following revisions:\n\n*   Conduct a comprehensive comparison with other state-of-the-art offline RL algorithms against state observation perturbations.\n*   Provide a detailed analysis of the impact of the non-Markovian sampling length (M) on the performance of DMBP.\n*   Conduct more extensive ablation studies to thoroughly evaluate the efficacy of the proposed non-Markovian loss function.\n*   Investigate the applicability of DMBP to other decision-making problems, such as image inpainting and image generation.\n*   Provide a clear explanation of the theoretical foundations of the proposed non-Markovian loss function.\n*   Consider the potential limitations of DMBP and provide a discussion on the future research directions.",
        "peer_review_with_watermark": "$$\nThe paper titled \"DMBP: DIFFUSION MODEL-BASED PREDICTOR FOR ROBUST OFFLINE REINFORCEMENT LEARNING AGAINST STATE OBSERVATION PERTURBATIONS\" presents a new approach to enhancing the robustness of offline reinforcement learning (RL) algorithms against state observation perturbations, such as those arising from sensor errors or adversarial interference. The authors introduce the Diffusion Model-Based Predictor (DMBP), which leverages conditional diffusion models to reconstruct actual states for state-based RL tasks. Notably, to address the problem of error accumulation from conventional model training, the paper proposes a non-Markovian training objective designed to reduce the sum entropy of denoised states over the RL trajectory. The efficacy of DMBP is demonstrated through experiments on standard benchmark challenges, showing significant improvement in robustness against various degrees of noise and even adversarial attacks on the observed states. Additionally, DMBP proves effective in handling scenarios of incomplete state observations where multiple dimensions may be unobserved.\n\nStrengths:\n\n*  The introduction of DMBP marks an innovative addition to the offline RL field, effectively tackling the challenge of state observation perturbations.\n*  By utilizing conditional diffusion models, DMBP enhances the resilience of offline RL algorithms against noise and adversarial attacks.\n*  The novel non-Markovian training objective aids in minimizing error accumulation during state estimation.\n\nWeaknesses:\n\n*  The training process for DMBP might involve considerable computational complexity, particularly in larger problem settings.\n*  Further exploration of the impacts of different hyperparameters, like the non-Markovian sampling length (M), could enhance the framework's effectiveness.\n*  Addressing adversarial threats targeting the value function remains a concern that needs to be better managed within this framework.\n\nSuggestions for Improvement:\n\n*  Examining the potential of other denoising methodologies, including various neural networks, for comparative analysis against the proposed conditional diffusion model.\n*  Applying DMBP to alternative domains, such as uncertain decision-making scenarios, could further validate its robustness.\n*  Delving deeper into the influence of various hyperparameter choices on the performance of the non-Markovian training objective would be beneficial.\n\nRecommendation:\n\n*  DMBP shows promise as an effective framework for offline RL in the face of observation perturbations based on the presented findings.\n*  Future research should focus on evaluating DMBP's resilience against more intricate adversarial attacks.\n*  Integrating DMBP into pre-existing offline RL frameworks could yield enhanced overall performance.\n\n$$",
        "gr_score_with": 0.9069767441860465,
        "gr_score_without": 0.9346092503987241,
        "output_without": {
            "num_tokens_scored": 4389,
            "num_green_tokens": 4102,
            "green_fraction": 0.9346092503987241,
            "z_score": 7.642821712464141,
            "p_value": 1.0625608219438642e-14
        },
        "output_with": {
            "num_tokens_scored": 430,
            "num_green_tokens": 390,
            "green_fraction": 0.9069767441860465,
            "z_score": 0.4822428221704122,
            "p_value": 0.31481673001328925
        }
    },
    {
        "title": "RDESIGN: HIERARCHICAL DATA-EFFICIENT REPRE-",
        "abstract": "While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules\u2019 primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Moreover, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both clusterlevel and sample-level to fully leverage the limited data. By constraining data representations within a limited hyperspherical space, the intrinsic relationships between data points could be explicitly imposed. Moreover, we incorporated extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. Extensive experiments demonstrate the effectiveness of our proposed method, providing a reliable baseline for future RNA design tasks. The source code and benchmark dataset are available at github.com/A4Bio/RDesign.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it.  Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$user\n\nThe peer review format and length should be of standard conference. \\\n            title: RDESIGN: HIERARCHICAL DATA-EFFICIENT REPRE- \\\n            abstract: While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules\u2019 primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Moreover, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both clusterlevel and sample-level to fully leverage the limited data. By constraining data representations within a limited hyperspherical space, the intrinsic relationships between data points could be explicitly imposed. Moreover, we incorporated extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. Extensive experiments demonstrate the effectiveness of our proposed method, providing a reliable baseline for future RNA design tasks. The source code and benchmark dataset are available at github.com/A4Bio/RDesign. \\\n            paper text: 1 INTRODUCTION  Ribonucleic acid (RNA) is a fundamental polymer composed of ribonucleotides, serving as a vital biological macromolecule that regulates a plethora of cellular functions (Kaushik et al., 2018; Guo et al., 2010; Sloma & Mathews, 2016; Warner et al., 2018). Non-coding RNA strands exhibit intricate three-dimensional structures, which are essential for their biological activities (Feingold & Pachter, 2004; Gstir et al., 2014). The complex geometries of RNA molecules empower them to execute irreplaceable functions in crucial cellular processes (Crick, 1970), encompassing but not limited to mRNA translation (Roth & Breaker, 2009), RNA splicing (Runge et al., 2018; Wanrooij et al., 2010; Kortmann & Narberhaus, 2012), and gene regulation (Meyer et al., 2016).\nSpecifically, the primary structure of RNA refers to its linear sequence of ribonucleotides (Hofacker et al., 1994; Rother et al., 2011; Kagaya et al., 2023). The primary structure then folds into a secondary structure with canonical base pairs, forming stems and loops (Nicholas & Zuker, 2008; Yang et al., 2017; Liu et al., 2022). Tertiary interactions between secondary structural elements subsequently give rise to the three-dimensional structure (Qin et al., 2022; Wang & Dokholyan, 2022; Yesselman & Das, 2015; Das et al., 2010). Figure 1 illustrates an example of the hierarchical folding of RNA primary, secondary, and tertiary structures. Gaining a comprehensive understanding \u2217Equal contribution. \u2020Corresponding author.\nof RNA structure is fundamental to figuring out biological mysteries and holds tremendous promise for biomedical applications. However, solving RNA structures through experimental techniques remains challenging due to their structural complexity and transient nature. Computational modeling of RNA structure and dynamics has thus become particularly valuable and urgent.\nRecent years have witnessed the emergence and rapid advancement of data-driven computational modeling of RNA (Angermueller et al., 2016; Xiong et al., 2021; Singh et al., 2021; Cao et al., 2024). In particular, algorithms for RNA secondary structure prediction have been extensively developed, yielding impressive results through leveraging large datasets of known secondary structures (Singh et al., 2019; Chen et al., 2019b; Fu et al., 2022; Tan et al., 2022). However, knowledge of RNA tertiary structures, which is crucial for thoroughly understanding RNA functional mechanisms and discovering RNA-targeted therapies (Warner et al., 2018; Churkin et al., 2018), remains limited (Townshend et al., 2021). The success of protein structure prediction (Jumper et al., 2021; Baek et al., 2021) has motivated researchers to tackle the even more challenging problem of RNA tertiary structure prediction, leading to the development of RNA tertiary structure folding algorithms such as DeepFoldRNA (Pearce et al., 2022), RoseTTAFoldNA (Baek et al., 2022), and RhoFold (Chen et al., 2022; Shen et al., 2022). While predicting RNA tertiary structures from primary sequences can leverage abundant sequence data (Chen et al., 2022), its inverse problem, designing RNA sequences that reliably fold into a specified tertiary structure, remains largely underexplored.\nThe key reasons that RNA tertiary structure modeling lags far behind protein tertiary structure modeling stem from two main aspects: (1) RNA demonstrates greater structural intricacy and flexibility than proteins, posing formidable hurdles for structure prediction and design (Townshend et al., 2021; Bernstein et al., 2012; Berman et al., 2000). The less constrained structure space of RNA leads to intractable challenges in modeling the RNA tertiary structure. (2) High-resolution RNA tertiary structures are scarce compared to proteins due to their conformational dynamics and instability(Rother et al., 2011). The quantity of available RNA structures constitutes less than 1% of that for proteins (Adamczyk et al., 2022b; Kalvari et al., 2021). To deal with the above problem, we propose a thorough pipeline aiming at data-driven tertiary structure-based RNA design tasks. In detail, we first compile a large-scale RNA tertiary structure dataset based on extant high-quality structure data from Protein Data Bank (PDB) (Berman et al., 2000) and RNAsolo (Adamczyk et al., 2022b). Then, regarding the deficiency in RNA tertiary structure modeling and the unsatisfying transferable capability of conventional protein structure modeling techniques to the RNA field, we propose a comprehensive RNA tertiary structure modeling. To optimize the use of the limited data, we introduce a hierarchical and data-efficient representation learning framework that applies contrastive learning at both the cluster and sample levels. We can explicitly impose intrinsic relationships between the data by constraining the data representations within a limited hyperspherical space. Moreover, we provide a strategy that utilizes extracted secondary structure as prior information to guide the RNA design inspired by the correlation between RNA secondary and primary structures.\nThe main contributions of this work are summarized as follows:\n\u2022 We propose a formal formulation of the tertiary structure-based RNA design problem. To establish a fair benchmark for tertiary structure-based RNA design, we compile a large dataset of RNA tertiary structures and provide a fundamental data split based on both structural similarity and sequence length distribution.\n\u2022 We propose a comprehensive structural modeling approach for the complex RNA tertiary structure and design an RNA design framework called RDesign, which is composed of a hierarchical representation learning scheme and a secondary structure imposing strategy.\n\u2022 Through extensive experiments across standard RNA design benchmarks and generalization ability assessments, we demonstrate the efficacy of our proposed method. This provides a reliable pipeline for future research in this important and promising field.2 RELATED WORK  2.1 BIOMOLECULAR ENGINEERING  In recent decades, the rapid advancements in biophysics, biochemistry, and chemical engineering have enabled a plethora of novel applications (Nagamune, 2017), including engineering enzymes for industrial biocatalysis (Pugh et al., 2018), tailoring antibodies for precision cancer therapies (Jeschek et al., 2016), developing trackable fluorescent proteins for biological imaging (Rosenbaum, 2017), and optimizing polymerases for forensic DNA analysis (Martell et al., 2016). RNA design is of particular interest among them due to the diverse functions that RNA can fulfill, ranging from translation and gene expression to catalysis (Ellefson et al., 2016). This multifunctionality is ascribed to the structural diversity of RNA (Andronescu et al., 2004). In this work, we focus on tertiary structure-based RNA design to uncover the relationships between RNA structure and sequence.2.2 PROTEIN DESIGN  RNA and protein are essential components of cells. Despite having different chemical constituents, their higher-order structures can be described similarly (Rother et al., 2011). Early works on computational protein design (Wang et al., 2018; Chen et al., 2019a) utilize multi-layer perceptron (MLP), and convolutional neural network(CNN) to predict residue types from protein structure. 3D CNNs have enabled tertiary structure-based design such as ProDCoNN (Zhang et al., 2020) and DenseCPD (Qi & Zhang, 2020). GraphTrans (Ingraham et al., 2019) combines attention (Vaswani et al., 2017) and auto-regressive decoding to generate protein sequences from graphs, inspiring a series of recent advancing approaches (Jing et al., 2020; Dauparas et al., 2022; Hsu et al., 2022; Tan et al., 2023; Gao et al., 2022b; 2023; 2024; Tan et al., 2024). While insights from protein research have illuminated RNA biology, RNA studies have trailed due to a scarcity of available data and complex structure modeling (Gan et al., 2003).2.3 RNA DESIGN  The computational design of RNA sequences aims to generate nucleic acid strands that will fold into a targeted secondary or tertiary structure. Secondary structure-based RNA design was first introduced by Vienna (Hofacker et al., 1994). Early works solved the RNA design problem through stochastic optimization and energy minimization with thermodynamic parameters, such as RNAfold (Lorenz et al., 2011), Mfold (Zuker, 2003), UNAFold (Nicholas & Zuker, 2008), and RNAStructure (Mathews, 2014). Probabilistic models and posterior decoding were employed to solve this problem (Sato et al., 2009). Other works that operate on a single sequence and try to find a solution by changing a few nucleotides include RNAInverse (Hofacker et al., 1994), RNASSD (Andronescu et al., 2004), INFO-RNA (Busch & Backofen, 2006), and NUPACK (Zadeh et al., 2011). There are global searching methods, including antaRNA (Kleinkauf et al., 2015), aRNAque (Merleau & Smerlak, 2022), eM2dRNAs (Rubio-Largo et al., 2023) and MCTS-RNA (Yang et al., 2017). Reinforcement learning-based methods have also been developed (Runge et al., 2018).\nAlthough numerous approaches have been studied for engineering RNA secondary structures, RNA design based on the tertiary structure is still challenging due to the lack of high-resolution structural data (Yesselman & Das, 2015; Das et al., 2010). Although structure prediction algorithms (Liu et al., 2022; Qin et al., 2022; Wang & Dokholyan, 2022) can utilize abundant RNA primary sequence information, the progress of RNA design has been hindered by the scarcity of determined RNA 3D structures. To alleviate the difficulty, we explore the uncharted areas of tertiary structure-centric RNA design systematically and propose a complete pipeline to address this challenge.3 METHODS  3.1 PRELIMINARIES  For an RNA sequence in its primary structure, we assume it comprises N nucleotide bases selected from the set of nucleotides: A (Adenine), U (Uracil), C (Cytosine), and G (Guanine). Therefore, the sequence can be represented as:\nNucleotides := {A, U, C, G}, SN = {si \u2208 Nucleotides \u2223\u2223 i \u2208 [1, N ] \u2229 Z}, (1) The formation of the tertiary structure requires the folding of this sequence in three-dimensional space, which can be denoted as:\nAtoms := {P,O5\u2032,C5\u2032,C4\u2032,C3\u2032,O3\u2032}, XN = {x\u03c9i \u2208 R3 \u2223\u2223 i \u2208 [1, N ] \u2229 Z, \u03c9 \u2208 Atoms}, (2) where the Atoms set denotes the six atoms that comprise the RNA backbone.\nWe incorporate secondary structure information using dot-bracket notation. Unpaired nucleotides are represented by dots, and paired nucleotides are represented by brackets, as shown in Figure 2.\nAN = {ai \u2208 {., (, )} \u2223\u2223 i \u2208 [1, N ] \u2229 Z}, (3)\nwhere ai is a dot if the nucleotide is unpaired, or a matching bracket otherwise. Finally, the tertiary structure-based RNA design problem could be formulated as:\nF\u0398 : XN 7\u2192 SN, such that AN = g(XN ) satisfies the pairing rules,\n(4)\nwhere F\u0398 is a learnable mapping with parameters \u0398, and g(\u00b7) is a function that extracts the secondary structure denoted by dot-bracket notation from the tertiary structure. Namely, F\u0398 denotes the mapping from XN to SN, which means from the tertiary structure to the primary structure. It illustrates that while we map the tertiary structure X of RNA to the primary sequence S, we ensure that the predicted sequence aligns with the pairing rules associated with the secondary structure A.3.2 COMPREHENSIVE RNA TERTIARY STRUCTURE MODELING  We construct a local coordinate system Qi for the i-th nucleotide in the RNA tertiary structure. The detailed procedure for defining the local coordinate system is in Appendix D. While studies on protein design have achieved considerable success using only the C\u03b1 atoms to model backbone geometry, this approach does not readily translate to RNA. RNA exhibits a diversity of backbone conformations and base-pairing geometries that cannot be sufficiently captured by such modeling. The complexity and plasticity of RNA structure necessitate a comprehensive treatment.\nTo adequately capture the complex structural information inherent in the three-dimensional folding of RNA molecules, we propose a general approach for modeling RNA tertiary structure. We represent RNA tertiary structure as an attributed graph G = (V,E) comprising node attributes V and edge attributes E. The graph is constructed by identifying the K nearest neighbors in 3D space for each node; each node i has a set of K neighbors denoted N (i,K). Specifically, V \u2208 RN\u00d7fn contains fn-dimensional node attributes for N nodes, and E \u2208 RN\u00d7K\u00d7fm contains fm-dimensional edge attributes for each node\u2019s K neighbors. By default, we set K = 30.\nWe outline the attributes used in our modeling approach along with their corresponding illustrations in Table 1, which includes two levels of attributes: (i) intra-nucleotide level attributes describing the local geometry of each nucleotide as the node attribute V, and (ii) inter-nucleotide level attributes describing the relative geometry between nucleotides as the edge attribute E.\nIntra-nucleotide level (1) The dihedral angles, shown as red arrows in Figure 3, are calculated. We represent the dihedral angles of the RNA backbone using sin and cos functions. (2) The spatial distances between the other intra-nucleotide atoms and the atom Pi are encoded into radial basis functions (RBFs). (3) The directions of the other intra-nucleotide atoms relative to the atom Pi are calculated with respect to the local coordinate system Qi.\nInter-nucleotide level (1) An orientation encoding q(\u00b7) is calculated from the quaternion representation of the spatial rotation matrix QTi Qj. (2) The spatial distances between inter-nucleotide atoms from neighboring nucleotides and the atom Pi are encoded into radial basis functions (RBFs). (3) The directions of the other inter-nucleotide atoms relative to the atom Pi are calculated.3.3 HIERARCHICAL DATA-EFFICIENT REPRESENTATION LEARNING  Now that RNA tertiary structure has been adequately modeled, the remaining challenge is how to learn from scarce data in a data-efficient manner. The key motivation is explicitly imposing the inherent data relationships based on prior knowledge. We first use L layers of message-passing neural networks (MPNNs) to learn the node representation. Specifically, the l-th hidden layer of the i-th nucleotide is defined as follows:\nh (l) Vi = MPNN([hEij,h (l\u22121) Vi\n, \u2211\nj\u2208N (i,K)\nh (l\u22121) Vj ]), (5)\nwhere h(0)V, hE are the embeddings of the intra-nucleotide and inter-nucleotide level features from the tertiary structure modeling, respectively. When generating the RNA sequence, a fully connected layer f maps the node representation h(L)Vi to the RNA sequence space: f(h (L) Vi ).\nTo enable data-efficient representation learning, we obtain the graph-level representation through the average pooling of the node representations hG = 1N \u2211N i=1 h (L) Vi\nand the corresponding projection g(hG) by the projection g : R\u2192 S that projects the Euclidean space into the hyperspherical space. We propose a hierarchical representation learning framework comprising cluster-level and confidence-aware sample-level representation learning, as shown in Figure 4. The cluster-level rep-\nresentation learning utilizes topological similarity between RNA structures. We obtain RNA structure clusters based on TM-score, which indicates structural similarity(Zhang & Skolnick, 2005). We define positive pairs as RNA data with similar topological structures that attract each other in the embedding space, while negative pairs with dissimilar topological structures repel each other. The cluster-level representation learning is defined as follows:\nLcluster = \u2212 \u2211 p\u2208D log exp(gp \u00b7 gq/\u03c4)\u2211 gk\u2208{gq}\u222aKc exp(gp \u00b7 gk/\u03c4), (6)\nwhere (gp, gq) is a positive pair that comes from the same structural cluster, Kc is a set of negative samples for gp identified by the cluster they belong to, and D is the data set. We denote gp as the graph representation projection of the p-th RNA sample for notational convenience.\nThe confidence-aware sample-level representation learning is designed to capture the microscopic intrinsic properties of RNA structures. The positive pairs are defined as a given RNA structure sample and its random perturbed structures. The perturbed structures are obtained by adding Gaussian noise to the experimentally determined coordinates. To prevent excessive deviation, we filter out the perturbed structures with low structural similarity (TM-score \u2264 0.8) and high structural deviation (RMSD \u2265 1.0). The RMSD also evaluates the confidence level of the perturbed data. Formally, the sample-level representation learning can be formulated as:\nLsample = \u2212 \u2211 p\u2208D \u03b3p,p\u2032 log exp(gp \u00b7 g\u2032p/\u03c4)\u2211 gk\u2208{g\u2032p}\u222aKs exp(gp \u00b7 gk/\u03c4), (7)\nwhere p\u2032 is the perturbed structure of the p-th RNA structure, and Ks is simply defined as other samples apart from gp. The confidence score \u03b3p,p\u2032 is defined as exp\u2212RMSD(p,p\n\u2032) so that when RMSD(p, p\u2032)\u2192 0, the confidence approaches 1. The cluster-level representation provides a coarse-grained embedding, capturing the global topological similarity between RNA structures. The confidence-aware sample-level representation provides intrinsic knowledge that is robust to minor experimental deviations. As shown in Figure 4, by constraining the limited data into the restricted hyperspherical space with imposed prior knowledge, the intrinsic relationships between data are explicitly modeled.3.4 SECONDARY STRUCTURE IMPOSING STRATEGY  With the given tertiary structure, we can derive the corresponding secondary structure using the notation shown in Figure 2. The secondary structure is represented using parentheses to indicate paired nucleotides, with unpaired sites represented by one of the four RNA nucleotides (A, U, C, G). Paired sites are denoted by two nucleotides placed simultaneously in one of the following pairs: {CG, GC, AU, UA, UG, GU}. When a pair of positions (i, j) in the predicted primary sequence and its corresponding secondary structure are given, we can calculate the confidence score for each position i based on the predicted letter at that position and the known secondary structure constraint. We then choose the position with the higher confidence score as the \u201dreference\u201d (say position i). We correct the predicted letter at position j so that the letters at i and j form the allowed pairs. Specifically, if position i is selected as the reference, we maintain the predicted letter at i unchanged and modify the predicted letter at j to satisfy the base pairing constraint. We then update the predicted primary sequence. By leveraging the information from the known secondary structure, we\ncan rectify and refine the initially predicted primary sequence. The refinement helps enhance the accuracy of RNA 3D structure prediction. In the training phase, we compel the model to sharpen the confidence of the nucleotides in the paired positions. The supervised loss is defined as:\nLsup = \u2211\n(i,j)\u2208Pairs\n[ `CE(si, f(h (L) Vi )/\u03c4 \u2032) + `CE(sj, f(h (L) Vj )/\u03c4 \u2032) ] + \u2211\nk/\u2208Pairs\n`CE(sk, f(h (L) Vk )), (8)\nwhere Pairs encompasses all the paired position indices given by the secondary structure, \u03c4 \u2032 is the temperature that is set as 0.5 by default to sharpen the confidence of paired nucleotides, si are true sequence labels. The training objective is the linear combination of representation learning loss and supervised loss:\nL = Lsup + \u03bb(Lcluster + Lsample), (9)\nwhere we set the weight parameter \u03bb as 0.5 by default.4 EXPERIMENTS  We evaluate RDesign on the tertiary structure-based RNA design task by comparing it with four categories of baseline models: (i) sequence-based models (SeqRNN and SeqLSTM) that do not utilize any structural information and could be viewed as the performance reference for RNA design; (ii) A tertiary structure-based model (StructMLP) that exploits structural features while ignoring the graph topological structure; (iii) Tertiary structure-based models (StructGNN and GraphTrans) (Ingraham et al., 2019) and PiFold (Gao et al., 2022b) that incorporate the graph topological structure; (iv) secondary structure-based RNA sequence design models (MCTS-RNA (Yang et al., 2017), LEARNA (Runge et al., 2018), eM2dRNAs (Rubio-Largo et al., 2023), aRNAque (Merleau & Smerlak, 2022)). The detailed experimental settings and dataset descriptions are shown in Appendix A and Appendix B-C, respectively.4.1 STANDARD TERTIARY STRUCTURE-BASED RNA DESIGN  Using our carefully curated benchmark dataset, we trained the model using the training set. Then, we evaluated the performance of the model on the testing set by selecting the model with the lowest loss on the validation set. Given that RNA sequences of varying lengths may impact prediction results, we stratified the testing set into three groups based on RNA length: (i) Short, RNA samples less than or equal to 50 nucleotides; (ii) Medium - RNA samples greater than 50 nucleotides but less than or equal to 100 nucleotides; (iii) Long - RNA samples greater than 100 nucleotides. To gain a thorough understanding of the relationship between RNA length and model accuracy, we reported both the recovery and Macro-F1 metrics for Short, Medium, and Long testing samples separately, in addition to overall testing set performance.\nAs presented in Table 2, the baseline models achieved suboptimal recovery scores, with performance ranging from 24-28%. Unexpectedly, tertiary structure-based models like StructMLP, StructGNN, and GraphTrans attained comparable results to sequence-based models. This indicates that directly applying protein design techniques to RNA is misguided and fails to capture the intricacies of RNA structures. Moreover, StructMLP and GraphTrans achieved higher recovery scores for short RNA\nsequences but struggled on longer, namely more complex RNA structures. Their struggles on the long dataset stem from the inability to learn from more intricate RNA structures and their low generalization capability. In contrast, our RDesign model outperforms all baseline methods on the recovery metric, achieving substantial gains. RDesign\u2019s strong performance, particularly on medium and long sets, indicates that it can learn intrinsic RNA structural properties.\nIt could be seen from Table 3 that there exist large gaps between the recovery metrics and Macro-F1 scores of most baseline models, which suggests those models tend to predict the high-frequency nucleotide letters instead of reflecting the actual tertiary structure. Among them, only StructGNN achieved consistent results in its Macro-F1 score and recovery metric but with unsatisfying performance. Our proposed RDesign consistently outperformed all other models on these metrics, demonstrating its effectiveness.4.2 EVALUATE THE GENERALIZATION ON RFAM AND RNA-PUZZLES  To assess the generalization capability of our model, we evaluated our model and the baseline methods on the Rfam (Kalvari et al., 2021) and RNA-Puzzles (Miao et al., 2020) datasets using the model pre-trained on our benchmark training set. We presented the results in Table 4. The performance remained consistent with that of our benchmark dataset. Specifically, StructGNN, which effectively learned certain tertiary structure information, achieved a relatively small gap between the recovery metric and Macro F1 score. In contrast, the other baselines that learned little structural information performed sub-optimally. Our proposed RDesign model demonstrated superior generalization on both datasets and outperformed all the baselines.\nIt is notable that the results reported here were generated by directly assessing pretrained models on the entire training dataset, mirroring real-world scenarios. Furthermore, in Appendix H, we have included the results of the pretrained models assessed on a training set with similar data removed.4.3 ABLATION STUDY  We conducted an ablation study of RDesign and presented the results in Table 5. Firstly, we replaced our tertiary structure modeling approach with the classical modeling from protein design, which led to a significant decrease in performance. Secondly, removing the hierarchical representation learning also resulted in a performance drop, indicating its importance. Replacing the hyperspherical space with Euclidean space led to a substantial reduction in performance, indicating its impact on dataefficient learning. In contrast, removing the secondary structure constraints provided a relatively small decrease in performance because RDesign itself could accurately generate RNA sequences. Additionally, we further tested the capability that our designed sequences could fold into desired tertiary structures, which is the final aim of the RNA sequences design problem. However, due to the lack of reliable RNA tertiary structure prediction tools, we are only able to conduct this experiment in a qualitative way. Results of three example structures from each length category have been reported and analyzed in Appendix 5.5 EVALUATION THE CAPABILITY OF FOLDING FOR DESIGNED SEQUENCES  We used RhoFold (Shen et al., 2022) to predict the structures of RNA sequences designed by RDesign. Figure 5 shows three visualization examples: (a) a short sequence reconstructed by RDesign; (b) a long sequence that was designed with a similar structure and low structure deviation; (c) a complicated sequence that was designed with a similar structure but failed to achieve low structure deviation. These visualization examples demonstrate the effectiveness of our RDesign model in designing RNA sequences with structures similar to the target structure.6 CONCLUSION AND LIMITATIONS  In this work, we investigate the challenging task of designing RNA tertiary structures. We compile a benchmark dataset to systematically assess the performance of various computational models on this task. While existing protein design methods cannot be directly applied, we propose a hierarchical data-efficient representation learning framework. Our framework explicitly captures the intrinsic relationships within the data while constraining the limited data to a restricted hyperspherical space. We also introduce a secondary structure constraining strategy to leverage extra structural information. Extensive experiments demonstrate the effectiveness of our proposed RDesign model. We hope this work provides a new perspective on tertiary structure-based RNA design. A limitation is that our method is currently limited to in silico design we leave wet-lab validation to future work. \\\n            \n            Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$\\assistant\n\n$$\n\nPaper Summary:\nThe paper proposes a novel approach for designing RNA tertiary structures using a hierarchical data-efficient representation learning framework. The authors compile a large benchmark dataset and develop a comprehensive structural modeling approach to represent the complex RNA tertiary structure. The proposed method explicitly captures the intrinsic relationships within the data by constraining the limited data to a restricted hyperspherical space. The authors also introduce a secondary structure constraining strategy to leverage extra structural information. The proposed RDesign model is evaluated on various RNA design benchmarks and demonstrates its effectiveness.\n\nStrengths:\n1. The authors have proposed a novel approach for designing RNA tertiary structures, which is a challenging task in RNA biology.\n2. The proposed hierarchical data-efficient representation learning framework explicitly captures the intrinsic relationships within the data, which is a key limitation of existing RNA design methods.\n3. The authors have demonstrated the effectiveness of the proposed RDesign model on various RNA design benchmarks.\n\nWeaknesses:\n1. The authors have not evaluated the performance of the proposed method on a large, diverse dataset. While the authors have compiled a benchmark dataset, it is unclear whether the results generalize to other datasets.\n2. The authors have not provided a detailed analysis of the limitations of the proposed method. For example, the authors have not discussed the impact of the hyperspherical space constraint on the performance of the method.\n3. The authors have not evaluated the performance of the proposed method on RNA sequences with complex structures. While the authors have demonstrated the effectiveness of the proposed method on various RNA design benchmarks, it is unclear whether the method can handle complex structures.\n\nSuggestions for Improvement:\n1. The authors should evaluate the performance of the proposed method on a large, diverse dataset to ensure that the results generalize to other datasets.\n2. The authors should provide a detailed analysis of the limitations of the proposed method, including the impact of the hyperspherical space constraint on the performance of the method.\n3. The authors should evaluate the performance of the proposed method on RNA sequences with complex structures to ensure that the method can handle such structures.\n\nRecommendation:\nBased on the strengths and weaknesses of the proposed method, I recommend that the authors continue to develop and evaluate the RDesign model. The authors should also consider addressing the limitations of the proposed method and evaluating its performance on a large, diverse dataset. With further development and evaluation, the RDesign model has the potential to be a powerful tool for designing RNA tertiary structures.\n\n$$",
        "peer_review_with_watermark": "$$\n\nPaper Summary:\nThis paper introduces RDESIGN, a new methodology for the design of RNA tertiary structures. The authors outline a hierarchical, data-efficient representation learning framework that employs contrastive learning at both cluster and sample levels. By constraining data representations within a limited hyperspherical space, they articulate intrinsic relationships among data points, while also integrating secondary structural information to enhance the RNA design process. Extensive evaluations reveal the method's efficacy and comparably superior performance against existing alternatives.\n\nStrengths:\n1. The RDESIGN framework presents an innovative approach to RNA tertiary structure design, showcasing originality and scientific contribution.\n2. Comprehensive experimental validation supports the efficacy of the method, showcasing notable improvements over traditional strategies.\n3. The incorporation of contrastive learning and hyperspherical constraints demonstrates an insightful way to capture intrinsic data relationships, enhancing generalization.\n4. The implementation of a secondary structure imposing strategy enriches structural information utilization, benefiting the overall RNA design.\n5. The paper's experimental rigor, including ablation studies and qualitative analyses, enhances its reliability and robustness.\n\nWeaknesses:\n1. The dependency on a substantial RNA tertiary structure dataset may limit the reproducibility of the method for some researchers.\n2. The constraints imposed by the hyperspherical space could hinder the flexibility and expressiveness of the learned model representations.\n3. The secondary structure imposing method may not be universally applicable across all RNA structures or designs.\n4. A limited comparative analysis with other leading RNA design methodologies reduces the context of RDESIGN\u2019s effectiveness.\n5. Structural predictions using RhoFold may not accurately reflect the RDESIGN framework's capabilities, possibly skewing results.\n\nSuggestions for Improvement:\n1. A thorough examination of the effects of hyperspherical space constraints on representation learning could provide valuable insights into their efficacy and generalization potential.\n2. To enhance the robustness of the secondary structure strategy, the authors could explore additional structural context or advanced constraint mechanisms.\n3. A broader comparative study against other state-of-the-art RNA design methods would clarify RDESIGN's advantages and limitations.\n4. Investigating alternative optimization objectives or incorporating regularization methods might yield further improvements in the design framework.\n5. To alleviate dataset dependency issues, developing methods applicable to smaller datasets or data-scarce scenarios could broaden the method's utility.\n\nRecommendation:\nGiven the paper's strengths and areas for enhancement, I recommend the authors revise and resubmit their submission after addressing the suggested modifications. The proposed approach holds considerable promise for advancing RNA design methodologies and, with revisions, could solidify its position as a leading solution in the field.\n\n$$",
        "gr_score_with": 0.9113636363636364,
        "gr_score_without": 0.9258989310009719,
        "output_without": {
            "num_tokens_scored": 4116,
            "num_green_tokens": 3811,
            "green_fraction": 0.9258989310009719,
            "z_score": 5.538577880751612,
            "p_value": 1.524688191653581e-08
        },
        "output_with": {
            "num_tokens_scored": 440,
            "num_green_tokens": 401,
            "green_fraction": 0.9113636363636364,
            "z_score": 0.7945521577046604,
            "p_value": 0.213437028747281
        }
    },
    {
        "title": "SWAP: SPARSE ENTROPIC WASSERSTEIN REGRESSION FOR ROBUST NETWORK PRUNING",
        "abstract": "This study addresses the challenge of inaccurate gradients in computing the empirical Fisher Information Matrix during neural network pruning. We introduce SWAP, a formulation of Entropic Wasserstein regression (EWR) for pruning, capitalizing on the geometric properties of the optimal transport problem. The \u201cswap\u201d of the commonly used linear regression with the EWR in optimization is analytically demonstrated to offer noise mitigation effects by incorporating neighborhood interpolation across data points with only marginal additional computational cost. The unique strength of SWAP is its intrinsic ability to balance noise reduction and covariance information preservation effectively. Extensive experiments performed on various networks and datasets show comparable performance of SWAP with state-of-the-art (SoTA) network pruning algorithms. Our proposed method outperforms the SoTA when the network size or the target sparsity is large, the gain is even larger with the existence of noisy gradients, possibly from noisy data, analog memory, or adversarial attacks. Notably, our proposed method achieves a gain of 6% improvement in accuracy and 8% improvement in testing loss for MobileNetV1 with less than one-fourth of the network parameters remaining.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it.  Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$user\n\nThe peer review format and length should be of standard conference. \\\n            title: SWAP: SPARSE ENTROPIC WASSERSTEIN REGRESSION FOR ROBUST NETWORK PRUNING \\\n            abstract: This study addresses the challenge of inaccurate gradients in computing the empirical Fisher Information Matrix during neural network pruning. We introduce SWAP, a formulation of Entropic Wasserstein regression (EWR) for pruning, capitalizing on the geometric properties of the optimal transport problem. The \u201cswap\u201d of the commonly used linear regression with the EWR in optimization is analytically demonstrated to offer noise mitigation effects by incorporating neighborhood interpolation across data points with only marginal additional computational cost. The unique strength of SWAP is its intrinsic ability to balance noise reduction and covariance information preservation effectively. Extensive experiments performed on various networks and datasets show comparable performance of SWAP with state-of-the-art (SoTA) network pruning algorithms. Our proposed method outperforms the SoTA when the network size or the target sparsity is large, the gain is even larger with the existence of noisy gradients, possibly from noisy data, analog memory, or adversarial attacks. Notably, our proposed method achieves a gain of 6% improvement in accuracy and 8% improvement in testing loss for MobileNetV1 with less than one-fourth of the network parameters remaining. \\\n            paper text: 1 INTRODUCTION  The advent of deep learning has revolutionized various domains of artificial intelligence, with neural networks showing remarkable performance across an array of applications. Nonetheless, the increase in model complexity has led to escalating computational demands and substantial memory requirements. This poses significant challenges for deploying these models in resource-constrained environments such as mobile or internet of things (IoT) devices. Therefore, the concept of neural network pruning emerges as a critical solution. It aims to optimize the network by removing less important parameters, which reduces computational overhead while maintaining the performance of the original model.\nIn the realm of state-of-the-art deep learning, the models often exhibit substantial size and complexity, with up to trillions of parameters, as exemplified by models such as GPT-4. The immense computational demand, energy inefficiency, and the challenges with model interpretability associated with these models highlight the need for innovative and efficient optimization techniques. These techniques should ideally minimize the model size while improving their robustness and interpretability. Considering the limitations of previous work, especially those arising from the influence of noisy data and noisy gradients, the paper proposes a promising pathway for robust pruning.\nBelow, we inspect the network pruning problem from an optimization perspective, with a concise introduction of the most relevant existing works. Then a sketch of our approach is given.\nRelated Work on Pruning as Optimization. Denote by w\u0304 P Rp a trained model and Lpwq the loss function given arbitrary model w. The loss function can be locally approximated around w\u0304 with\n\u02daCorrespondence to both leiyo@dtu.dk and hvc@ece.au.dk. Lei You is supported by Thomas B. Thriges Fond 5041-2402. Hei Victor Cheng is supported by the Aarhus Universitets Forskningsfond under Project AUFF 39001.\nTaylor Expansion as shown in (1).\nLpwq \u201c Lpw\u0304q ` \u2207Lpw\u0304qJpw \u00b4 w\u0304q ` 1 2 pw \u00b4 w\u0304qJ\u22072Lpw\u0304qpw \u00b4 w\u0304q `Op\u2225w \u00b4 w\u0304\u22253q (1)\nConsider a neural network with a loss function Lpwq \u201c 1N \u0159N i\u201c1 \u2113ipwq, where \u2113ipwq P Rp is the loss incurred at data point i (i \u201c 1,..., N ). The goal of network pruning is to find a set of w such that there are k (k \u0103 p) elements of w being zero while keeping the newly obtained model w\u2019s performance as good as possible to the original one w\u0304. Mathematically, we want to find some w P Rp that satisfies both Lpwq \u00ab Lpw\u0304q and \u2225w\u22250 \u010f k, with k \u0103 p. This line of research can be dated back to (LeCun et al., 1989), where the approximation in equation (1) is adopted. Under the assumption that gradient \u2207Lpw\u0304q \u00ab 0 when the network is trained, the network weights are pruned one-by-one in decreasing order based on the value of pw\u00b4w\u0304qJHpw\u00b4w\u0304q. In their approach, the H is approximated as a diagonal matrix; this is later extended in (Hassibi & Stork, 1992) to include the whole Hessian matrix, and the authors also proposed using the Fisher information matrix (FIM) as an approximation to the Hessian. Later, (Singh & Alistarh, 2020) proposed to reduce the computation complexity by using block diagonal Hessian, and FIM is approximated using a small subset of the training data.\nThese approaches all use equation (1) to prune the network in a one-by-one manner, namely the weight with the least importance is set to zero according to the different approximations of equation (1). In this way, the potential interactions of pruning multiple weights are ignored. To explore this, the network pruning problem is formulated as a mixed integer quadratic programming (MIQP) in (Yu et al., 2022). Namely, an objective function\nfpwq \u201c pw \u00b4 w\u0304qJHpw \u00b4 w\u0304q ` \u03bb \u2225w \u00b4 w\u0304\u22252 (\u03bb \u011b 0) (2)\nis minimized, and Hessian is approximated as H \u00ab \u22072Lpw\u0304q, subject to the sparsity constraint \u2225w\u22250 \u010f k, where \u03bb is a regularization parameter. Although this approach shows significant improvements, it suffers from scalability issues as a full Hessian matrix is required.\nSparse Linear Regression (LR) Formulation. To reduce the computational complexity, the Hessian matrix can be approximated by the empirical FIM, using n samples as in (Chen et al., 2022; Benbaki et al., 2023). Denote G \u201c r\u2207\u21131,...,\u2207\u2113nsJ P Rn\u02c6p, where \u2207\u2113i \u201c \u2207\u2113ipw\u0304q. For simplicity, \u2207\u2113i is used in this document to represent the derivative of the data point i\u2019s loss at w\u0304 consistently in this paper unless specified otherwise. The Hessian is approximated through the expression H \u00ab p1{nq\n\u0159n i\u201c1 \u2207\u2113i\u2207\u2113 J i \u201c p1{nqGJG, which is the so-called FIM. Denote xi \u201c \u2207\u2113 J i w and\nyi \u201c \u2207\u2113Ji w\u0304, (2) is formulated to the sparse LR problem shown in (3) below.\nmin w\nQ\u0304pwq \u201c n \u00ff\ni\u201c1 \u2225xipwq \u00b4 yi\u22252 ` n\u03bb \u2225w \u00b4 w\u0304\u22252, s.t. }w}0 \u010f k (3)\nThis formulation has a computational advantage, as empirical FIM needs not to be computed explicitly. It is shown that the formulation scales to large neural network pruning (Chen et al., 2022).\nMotivation of Combating Against Noise. In practice, it is not always easy to obtain the correct gradients for pruning large neural networks. There can be noise contained in the data samples, and the gradients can also be corrupted due to various reasons, e.g., distributed or federated learning (Turan et al., 2022), or adversarial attacks such as data poisoning (Steinhardt et al., 2017).\nAs pointed out by (Mahsereci et al., 2017; Siems et al., 2021), conditioning on the underlying true gradient \u2207Lpw\u0304q \u201c 0, there are mini-batch gradients which are not informative anymore as it can be fully explained by sample noise and the vanishing gradients. These gradients would not contribute to the covariance information of the empirical FIM but serve as outliers in Hessian approximation.\nIn the scenarios of federated learning (FL), gradients computed by different clients are skewed and consequently, local models move away from globally optimal models (Huang et al., 2022), imposing challenges for constructing informative FIM. Besides, noise can be added to the gradient for privacy concerns in communications (Li et al., 2020). Additionally, the clients usually have inevitable noisy samples and labels, making models suffer from a significant performance drop (Tuor et al., 2021). Additionally, over-the-air communications itself suffer from unavoidable noises (Ang et al., 2020; Yang et al., 2020). These lead to concerns for network pruning with noisy gradients. Finally, analog\nmemory recently gained attention for deep learning model deployment (Garg et al., 2022). When neural network parameters and data are stored in these analog devices, they are susceptible to devicerelated noise, affecting the performance of network compression (Isik et al., 2023).\nApproach Sketch. We revisit the MIQP network pruning optimization from a perspective of entropic Wasserstein regression (EWR), which leverages Wasserstein distance to model the dissimilarity between two distributions. In our context, it measures the dissimilarity of distributions relevant to model parameters and gradient magnitudes before and after pruning. Namely, \u2207\u2113 is a p dimensional distribution, capturing geometric properties of the loss at w\u0304 before pruning. Both w\u0304 and w perform projections for \u2207\u2113 to a 1-D distribution respectively as \u2207\u2113Jw\u0304 and \u2207\u2113Jw. Computing the distance between \u2207\u2113Jw\u0304 and \u2207\u2113Jw falls into the framework of sliced probability divergence (Nadjahi et al., 2020). Under this framework, pruning optimization essentially fine-tunes w and selectively reserves its elements such that the divergence is minimized subject to the sparsity constraint.\nOur approach\u2019s effectiveness in combating noisy gradients is established both analytically and numerically. We demonstrate that pruning through the Wasserstein regression implicitly enacts gradient averaging using Neighborhood Interpolation. This entails a nuanced balance between capturing gradient covariance and diminishing gradient noise. Notably, the sparse LR formulation is merely a specific instance of ours. Yet, our proposed algorithm doesn\u2019t demand a markedly higher computational expense. This modest additional effort bestows upon us enhanced robustness.2 PROBLEM SETUP AND FORMULATION  We first introduce the optimal transport (OT) problem in Kantorovich formulation with entropic regularization, which measures the distance between two distributions, defined in (4) below. The Wasserstein regression formulation as a generalization of the LR formulation is then proposed.\nThe Kantorovich Problem. Denote P2 the set of probability measures with finite second moments. Let \u00b5, \u03bd P P2 and let \u03a0p\u00b5, \u03bdq denote the set of probability measures in P2 with marginal distributions equal to \u00b5 and \u03bd. The 2-Wasserstein distance is defined as\nW 22 p\u00b5, \u03bdq \u201c inf \u03c0P\u03a0p\u00b5,\u03bdq\n\u017c\nRd\u02c6d \u2225x\u00b4 y\u22252 d\u03c0px, yq ` \u03b5\n\u017c\nRd\u02c6d log\n\u02c6\nd\u03c0\nd\u00b5d\u03bd\n\u02d9\nd\u03c0. (4)\nThis is also referred to as the entropic OT problem, where the first term is the transportation cost between the two measures and the second term is the entropic regularization with multiplier \u03b5.\nSparse EWR Formulation. The pruning problem formulation is defined in (5) below. min w Qpwq \u201c W 22 pxpwq, yq ` \u03bb \u2225w \u00b4 w\u0304\u2225 2 (5a)\ns.t. \u2225w\u22250 \u010f k (5b) The term W 22 pxpwq, yq is a Wasserstein distance between the two one-dimensional distributions x and y (or a sliced Wasserstein distance for \u2207\u2113 with two one-dimensional projections). The optimization is to alter w such that the distance between the two distributions is minimized.\nLet x and y follow the empirical distributions txiuni\u201c1 and tyiuni\u201c1. Denote by \u00b5i and \u03bdi the mass of the data points xi and yi, respectively. We use \u03a0 to refer to a matrix representing the transportation probability between x and y, and \u03a0 the set of all such matrices, i.e. \u03a0 \u201c t\u03a0|\n\u0159n i\u201c1 \u03c0ij \u201c\n\u00b5j @j and \u0159n j\u201c1 \u03c0ij \u201c \u03bdi @iu, where \u00b5i and \u03bdj are marginal distributions. Then (5) reads:\nmin w Qpwq \u201c inf \u03a0P\u03a0\n#\nn \u00ff\ni\u201c1\nn \u00ff j\u201c1 \u2225xipwq \u00b4 yi\u22252 \u03c0ij ` \u03b5 n \u00ff i\u201c1 n \u00ff j\u201c1 log \u02c6 \u03c0ij \u00b5i\u03bdj \u02d9 \u03c0ij\n+\n` \u03bb \u2225w \u00b4 w\u0304\u22252 (6a)\ns.t. \u2225w\u22250 \u010f k (6b)\nLR as a Special Case. Let \u03b5 \u201c 0. Once we set \u03a0 to be a diagonal matrix with constant value 1{n, i.e. diagp1{nq, the mass transportation happens only between data point pairs pxi, yiq for i \u201c 1,..., n. Therefore we have\nQ\u03a0\u201cdiagp1{nqpwq \u201c 1\nn\nn \u00ff i\u201c1 \u2225xipwq \u00b4 yi\u22252 ` \u03bb \u2225w \u00b4 w\u0304\u22252 \u201c 1 n Q\u0304pwq, (7)\ni.e. the formulation (6) in this case degrades to the LR formulation in (3).3 THEORETICAL ASPECTS  This section reveals some good theoretical properties of the Sparse EWR formulation for network pruning. We start with Proposition 1 below (proof in Appendix A.1) that states a geometry property of OT with squared Euclidean distance cost. Additionally, we demonstrate the Neighborhood Interpolation mechanism that happens implicitly in solving the EWR. Moreover, we show that such a mechanism strikes a balance in capturing gradient covariance and reducing gradient noise, with a brief discussion on the advantage of using entropic regularization in terms of sample complexity. Proposition 1 (Convex Hull Distance Equality). Consider a set S and its convex hull ConvpSq in a Euclidean space, and an arbitrary point x in the space. For any probability measure \u03bd\u0302 on S, we can find a point y1 in ConvpSq as y1 \u201c \u015f y d\u03bdpyq such that }x \u00b4 y1}2 \u201c \u015f\n}x \u00b4 y}2 d\u03bd\u0302pyq, where \u03bd is a measure on ConvpSq.\nNeighborhood Interpolation. In formulation (5), let Wx be the first term of W 22 for an arbitrary given x, i.e., Wx \u201c \u015f\n\u2225xpwq \u00b4 y\u22252 d\u03c0p\u00a8|xqpyq, where \u03c0p\u00a8|xqpyq is a conditional measure given x. Now, divide the Euclidean space Rd by subspaces Sy1, S y 2..., S y n for y. For any conditional measure \u03c0p\u00a8|xqpyq defined on any Syi (i \u201c 1,..., n), there exists a measure \u03bdpyq defined on ConvpS y i q such that the weighted distance from Syi to x equals the distance from x to a point y 1 in ConvpSyi q. Hence\nWx \u201c \u017c\nSy1 YS y 2 Y\u00a8\u00a8\u00a8YS y n\n}x\u00b4 y}2 d\u03c0p\u00a8|xqpyq\n\u201c 1 n\nn \u00ff\ni\u201c1\n\u203a \u203ax\u00b4 y1i \u203a \u203a 2 s.t. y1i \u201c\n\u017c\nConvpSyi q y d\u03bdpyq, \u03bd P Vipxq, i \u201c 1,... n.\nwhere Vipxq is the set of measures \u03bd that make the equality holds and Vipxq \u2030 \u2205 by Proposition 1. Similarly, we define Wy for any given y and subspaces Sx1, S x 2..., S x n,\nWy \u201c \u017c\nSx1 YSx2 Y\u00a8\u00a8\u00a8YSxn }x\u00b4 y}2 d\u03c0p\u00a8|yqpxq\n\u201c 1 n\nn \u00ff\ni\u201c1\n\u203a \u203ax1i \u00b4 y \u203a \u203a 2 s.t. x1i \u201c\n\u017c\nConvpSxi q x d\u00b5pxq, \u00b5 P Uipyq, i \u201c 1,... n.\nwhere \u00b5 is a measure defined on ConvpSxi q and Uipyq \u2030 \u2205.\ny1 y2\ny3\ny4\ny5\nx\ny1\nWe demonstrate the concept of \u201cNeighborhood Interpolation\u201d through an empirical distribution example. Define S as a subset of y such that for every element yi P S, \u03c0x,i \u0105 0. Without loss of generality, we can denote S \u201c ty1, y2, y3, y4, y5u. The area shaded in gray, denoted as ConvpSq, represents the convex hull of S. Wx computes a weighted summation of distances between x and the points y1,..., y5. The weights \u03c0x,1,..., \u03c0x,5 are decided by OT. A significant \u03c0x,i typically implies that yi is in proximity to x, indicating a neighborhood relation. By Proposition 1, this weighted distance is analogous to the distance between x and y1, where y1 is derived from ConvpSq.\nRevisit the EWR formulation. The integral of either Wx or Wy respectively on x or y gives the first term of W 22. One can then reformulate (5) as (8) below.\nmin w:}w}0\u010fk Qpwq \u201c 1 2 inf \u03c0\n\" \u017c\nWxpwqd\u00b5pxq ` \u017c Wypwqd\u03bdpyq * ` \u03bb \u2225w \u00b4 w\u0304\u22252. (8)\nInterpretation: The objective function calculates the Euclidean distance between a point x and n distinct points. These n points originate from n convex hulls, each shaped by different n subspaces within y. Similarly, the function measures the distance between each point y and m unique points derived from m convex hulls, each formed by distinct m subspaces within x.\nWe claim that the EWR formulation is more resilient to noisy gradient than its counterpart, the LR formulation given by (3). To understand this claim better, let us reimagine the problem using empirical distributions, as indicated by (6). In this context, we use xi and yi as substitutes for Sxi and Syi. Moreover, the integration in both Wx and Wy is replaced with summations, offering a more insightful version of our initial EWR formulation, shown as (9).\nmin w:\u2225w\u22250\u010fk Qpwq \u201c inf \u03a0\n#\nQ\u03a0pwq ` \u03b5 n \u00ff\ni\u201c1\nn \u00ff j\u201c1 log \u02c6 \u03c0ij \u00b5i\u03bdj \u02d9 \u03c0ij\n+\n(9)\nThe notation Q\u03a0pwq defined in (10) denotes the part of the objective function given fixed \u03a0:\nQ\u03a0pwq \u201c n \u00ff\ni\u201c1\nn \u00ff j\u201c1 \u2225xipwq \u00b4 yj\u22252 \u03c0ij ` \u03bb \u2225w \u00b4 w\u0304\u22252 (10a)\n\u201c 1 2\nn \u00ff\ni\u201c1 \u2225\u2225xipwq \u00b4 y1i\u2225\u22252 loooooooooomoooooooooon\nK p1q \u03a0\n`1 2\nn \u00ff\ni\u201c1 \u2225\u2225x1ipwq \u00b4 yi\u2225\u22252 loooooooooomoooooooooon\nK p2q \u03a0\n`\u03bb \u2225w \u00b4 w\u0304\u22252 (10b)\nIn Q\u03a0pwq, for each index i, points x1i and y1i are chosen from the convex hulls formed by points in x and y, as per the guidelines of Proposition 1. Now, contrasting this with the LR model in (3), the objective Q\u0304pwq aims for regression directly over the data points whereas every point from one empirical set is matched for Euclidean distance computation to a point derived from a convex combination of the other.\nThe infimum in (9) seeks the OT plan, \u03a0, that aligns the empirical distributions x and y closely. In practical terms, for each data point xi, only a subset of tyiuni\u201c1 will transport a substantial mass, rather than the entire set. This behavior of \u03a0 effectively defines n \u201dneighborhoods\u201d for each data point xi within the empirical distribution of y. Here, a \u201dneighborhood\u201d refers to a group of data points in y that are proximate to a specific xi in the Euclidean sense.\nNeighborhood Size Control. A critical aspect of this formulation is the entropic regularization term, which is used to modulate the size of these neighborhoods. Specifically, increasing the value of \u03b5 amplifies the impact of the entropy term. This change broadens the neighborhoods, drawing more data points into the fold of the associated convex hulls. An illustrative extreme case is when \u03b5 \u201c 0. Here, the OT does one-to-one matching, implying that each data point yi primarily forms the convex hull independently. On the contrary, when \u03b5 \u00d1 8, all data points are equally weighted by \u03a0 and hence involved in forming the convex hull as a neighborhood.\nCapturing Covariance With Gradient Noise Reduction. For an arbitrary w, the EWR formulation essentially strikes a balance between gradient noise reduction and covariance capturing. We show the analysis for Kp1q\u03a0 in (10), and K p2q \u03a0 pwq follows similarly. Note that y1i \u201c \u0159n j\u201c1 \u03bd piq j yj \u201c \u0159n j\u201c1 \u03bd piq j \u2207\u2113 J j w\u0304, where \u03bd piq are convex combination coefficients by Proposition 1. Denote \u2207\u21131Ji \u201c \u0159n j\u201c1 \u03bd piq j \u2207\u2113 J j, and G 1 \u201c r\u2207\u211311,...,\u2207\u21131nsJ. The term K p1q \u03a0 expands as follows.\nK p1q \u03a0 \u201c\nn \u00ff i\u201c1 p\u2207\u2113Ji w \u00b4 \u2207\u2113 1J i w\u0304qJp\u2207\u2113 J i w \u00b4 \u2207\u2113 1J i w\u0304q\n\u201c n \u00ff\ni\u201c1 pwJ\u2207\u2113i\u2207\u2113Ji w \u00b4 wJ\u2207\u2113i\u2207\u2113 1 i Jw\u0304 \u00b4 w\u0304J\u2207\u21131i\u2207\u2113iJw ` w\u0304J\u2207\u2113 1 i\u2207\u2113 1 i Jw\u0304q (11)\nExamining Kp1q\u03a0 from (11), we see that it effectively replaces half of \u2207\u2113i with \u2207\u2113 1 i, a version obtained through weighted gradient averaging. Now let\u2019s compare the covariance between \u2207\u2113i and \u2207\u21131i. Assume that \u2207\u2113i (1 \u010f i \u010f n) are i.i.d. with the same covariance matrix \u03a3, then G1 is with equal or less noise than G. To show this, denote the covariance matrix of each \u2207\u21131i by\n\u03a31i \u201c Cov \u201e n \u00ff\nj\u201c1 \u03bd\npiq j \u2207\u2113j\n\u0237 \u201c n \u00ff\nj\u201c1 r\u03bdpiqj s 2\u03a3.\nThe total variance of each gradient in G1 (i.e., the trace of \u03a31i) is then\ntracep\u03a31iq \u201c trace \u02c6 n \u00ff\nj\u201c1\n\u201c \u03bd piq j \u20302 \u03a3\n\u02d9 \u201c n \u00ff\nj\u201c1\n\u201c \u03bd piq j \u20302 tracep\u03a3q \u010f tracep\u03a3q.\nThe last inequality follows from the fact that \u0159n j\u201c1r\u03bd piq j s2 \u010f 1, which is a consequence of the Cauchy-Schwarz inequality given that the coefficients \u03bdpiqj form a convex combination.\nOriginally, the covariance information of all data points is embedded in \u2207\u2113i\u2207\u2113Ji for i \u201c 1, 2,..., n. An alternative representation is \u2207\u21131i\u2207\u2113 1J i, which prioritizes noise reduction, but sacrifices some covariance information. Both \u2207\u21131i\u2207\u2113 J i and \u2207\u2113i\u2207\u2113 1J i highlight a trade-off. Notably, both the original covariance \u2207\u2113i\u2207\u2113Ji and its noise-reduced counterpart \u2207\u2113 1 i\u2207\u2113 1J i are retained in (11).\nDifference From Averaging Prior to Optimization: Next, we show that such gradient averaging differs from the averaging operation conducted prior to optimization. Let G1 \u201c r\u2207\u211311,\u2207\u2113 1 2,...,\u2207\u2113 1 nsJ such that G1 represents the row-wise convex combination of G. Approximating the Hessian of the MIQP (2), two scenarios emerge: using G that not performing averaging (case 1) and G1 that performs averaging before optimization (case 2).\nCase 1 is the original LR formulation (3). Denote by K below its term corresponding to Kp1q\u03a0 :\nK \u201c pw \u00b4 w\u0304qJGJGpw \u00b4 w\u0304q\n\u201c n \u00ff\ni\u201c1 pwJ\u2207\u2113i\u2207\u2113Ji w \u00b4 wJ\u2207\u2113i\u2207\u2113 J i w\u0304 \u00b4 w\u0304J\u2207\u2113i\u2207\u2113 J i w ` w\u0304J\u2207\u2113i\u2207\u2113 J i w\u0304q (12)\nCase 2 uses the less-noisy row-wise convex combination matrix G1 instead of G. Yet, the original covariance \u2207\u2113i\u2207\u2113Ji is lost: Denote by K 1 the corresponding term, and we have\nK 1 \u201c n \u00ff\ni\u201c1 pwJ\u2207\u21131i\u2207\u2113 1J i w \u00b4 wJ\u2207\u2113 1 i\u2207\u2113 1J i w\u0304 \u00b4 w\u0304J\u2207\u2113 1 i\u2207\u2113 1J i w ` w\u0304J\u2207\u2113 1 i\u2207\u2113 1J i w\u0304q (13)\nInspecting the expressions, it can be observed that Kp1q\u03a0 (also K p2q \u03a0 ) strikes a balance between K and K 1. There are two notable extreme cases for \u03a0:\n1. \u03a0 \u201c diagp1{nq. This corresponds to the LR formulation, as detailed in Section 2. A smaller value of \u03b5 steers the optimization in this direction.\n2. \u03a0 \u201c p1{n2q1 \u00a8 1J. This arises when \u03b5 \u00d1 8, meaning the entropy term holds sway in the OT optimization. Here, mutual information is minimized to ensure an even contribution from data points in the convex combination. Both x1i and y 1 i are the arithmetic means of\ntheir respective sets, and all \u2207\u21131i are equivalent to the averaged gradient over the n points. Importantly, the original covariance remains intact even in this edge case.\nAs n grows indefinitely, the empirical OT formulation from (6) approaches its continuous counterpart given by (5). Intuitively, a large dataset of high-quality training samples makes the empirical fisher a close approximation to the true fisher. In such situations, \u03b5 is set to zero. Brenier\u2019s theorem (Peyre\u0301, 2019) then suggests that the OT plan turns into a monotone map for costs represented by squared Euclidean distances. This means \u03a0 tends towards diagp1{nq. Consequently, the Wasserstein distance formulation reduces to the Euclidean distance formulation, delivering optimal performance with ample data.\nAn advantage of employing the EWR formulation is its inherent capability of gradient averaging. This approach negates the need to manually determine the convex combination coefficients or resort to density estimation to pinpoint the nearest gradient neighbors for averaging. Importantly, this seamless trade-off has an advantage over using Euclidean distance with gradient averaging performed prior to optimization. The reason is that the original covariance information will inevitably be lost in the formulation (13), irrespective of the chosen averaging method. Sample Compexity of W 22 p\u00b5, \u03bdq is narrowed to Op1{? nq from Op1{n 14 q by the entropic regularization term. Please see Appendix A.2 for details.4 ALGORITHM DESIGN  Algorithmic Framework. The algorithm addresses the network pruning problem defined in (5). Drawing inspiration from (Chen et al., 2022), the algorithm incrementally adjusts the sparsity of the weights vector w by using a descending sequence of non-zero elements k0,..., kT. During each sparsity level, the weights w and the transportation plan \u03a0 (can be obtained with efficient algorithms; see Appendix A.3) are refined iteratively.\nAlgorithm 1 Sparse Entropic WAsserstein Regression Pruning (SWAP) Input: Number of pruning stages T, pre-pruning weights w\u0304, target sparsity k, regularization pa-\nrameter \u03bb, \u03b5, batches B0,B1...,BT, optimization step size \u03c4 \u0105 0. Output: Post-pruning weights w, satisfying \u2225w\u2225 \u010f k\n1: Set k0, k1,..., kT as a descending sequence, with k0 \u0103 p and kT \u201c k. 2: wp0q \u00d0 w\u0304 3: for t \u00d0 0, 1,..., T do 4: Compute G \u201c r\u2207\u21131pw\u0304q,...\u2207\u2113npw\u0304qsJ with batch Bt 5: x,y \u00d0 Gwptq,Gw\u0304 6: Compute the pairwise Euclidean distance matrix C between x and y 7: Compute OT planning \u03a0ptq (see Appendix A.3) 8: \u2207Q \u00d0 GJp\u03a0pGwptq \u00b4 Gw\u0304qq ` \u03bbpwptq \u00b4 w\u0304q 9: wpt` 1 2 q \u00d0 wptq \u00b4 \u03c4\u2207Q\n10: wpt`1q \u00d0 Select from wpt` 12 q kt components having largest absolute values; Others zero 11: w\u0304 \u00d0 wpt`1q 12: end for 13: w \u201c wpT`1q\nWeights Optimization. The weights w are optimized using the stochastic gradient descent (SGD) paired with the iterative hard thresholding (IHT) algorithm. We use \u2207Q to represent the derivative of Qpwq for brevity, with its comprehensive derivation in Appendix A.4. The expression is\n\u2207Q \u201c GJp\u03a0pGw \u00b4 Gw\u0304qq ` \u03bbpw \u00b4 w\u0304q. (14)\nFollowing the weight updates driven by SGD (as seen in line 9 of Algorithm 1), the IHT method is applied. Here, the kt components of w with the largest magnitudes are retained, while the remaining are set to zero, ensuring adherence to the sparsity criteria.\nA vital component of the optimization process is the choice of the stepsize \u03c4 (referenced in line 9). Although a straightforward approach might be to set \u03c4 \u201c 1L (where L denotes the Lipschitz constant of Q), better performance can be achieved when the stepsize is optimized using the methodology proposed in (Chen et al., 2022, Algorithm 2). For the quadratic function Q, the Lipschitz constant L is given by L \u201c n\u03bb` \u2225G\u22252op, where \u2225\u00a8\u2225op indicates the foremost singular value.\nLine 10 in Algorithm 1 employs the IHT method that is commonly used in sparse learning, which together with line 9, forms a projected gradient descent algorithm. It finds a sparse representation of the updated gradient in line 9. Intuitively, IHT keeps the dominant model weights and essentially preserves the most impactful aspects of the to-be-trimmed model. Although there exist alternative strategies for refining the IHT solution\u2014including active set updates, coordinate descent, and the Woodbury formula\u2019s back-solving\u2014a discussion on these falls outside the scope of this paper. For in-depth exploration, especially with respect to the specialized case described in (7), one can consult (Bhatia et al., 2015; Chen & Mazumder, 2020; Hazimeh & Mazumder, 2020; Benbaki et al., 2023).5 NUMERICAL RESULTS  Our method is compared with several existing SoTA methods including MP (magnitude pruning (Mozer & Smolensky, 1989)), WF (WoodFisher (Singh & Alistarh, 2020)), CBS (Combinatorial Brain Surgeon (Yu et al., 2022)), and LR (i.e. the sparse LR formulation adopted by (Chen et al., 2022)). We refer to our proposed method as EWR (i.e. sparse entropic Wasserstein regression).\nNote that LR is a special instance of EWR, with \u03a0 \u201c diagp1{nq. All the methods are benchmarked on pre-trained neural networks: MLPNet (30K parameters) trained on MNIST (LeCun et al., 1998), ResNet20 (200K parameters) and ResNet50 (25M parameters) (He et al., 2016) trained on CIFAR10 (Krizhevsky et al., 2009), and MobileNetV1 (Howard et al., 2017) (4.2M parameters) trained on ImageNet (Deng et al., 2009). The experiment setup for reproducibility1 is detailed in AppendixA.5. We deliver more experiments results in Appendix A.6-A.9.\nModel Accuracy Performance Benchmarking. Table 1 compares different networks across various sparsity levels, utilizing different methods. MLPNet\u2019s performance on MNIST is consistent across different sparsity levels, with both LR and the proposed EWR method showing superior performance. The advantages of EWR over the others are reflected by the three more challenging tasks ResNet20 and ResNet50 on CIFAR10 and MobileNetV1 on ImageNet, especially in the presence of noisy gradients. In summary, the proposed EWR method consistently outperforms or matches other methods. The LR method performs well at lower sparsity levels but is surpassed otherwise.\n1The code is available on https://github.com/youlei202/Entropic-Wasserstein-Pruning\nRobustness with Noisy Gradient. From Section 3, EWR differs from LR in terms of gradient noise reduction achieved by solving the OT problem to obtain a group of non-trivial data pair weighting coefficients. Hence, LR that has the transportation plan \u03a0 fixed to diagp1{nq naturally serves as a baseline for evaluating the effectiveness of such optimization in terms of robustness against noise. In two noisy scenarios, 10%, and 25%, we evaluate loss at noise levels of \u03c3 and 2\u03c3 across varying sparsity. Tables 2 and 3 contrast the loss difference between LR and EWR. EWR consistently outperforms LR in both ResNet20 and MobileNetV1, most notably in noisy conditions and at higher sparsity. The peak performance difference is 8.13% favoring EWR on MobileNetV1 at 0.75 sparsity with 25% noise. Hence, EWR outperforms LR.6 CONCLUSIONS AND FUTURE IMPACT  The paper offers a novel formulation based on EWR, which strikes a balance between covariance information preservation and noise reduction. The work suggested promising avenues for applications in large-scale model compression, though it may require further empirical validation and exploration of practical implementations. \\\n            \n            Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$\\assistant\n\n$$Paper Summary:\nThe paper introduces SWAP, a novel sparse entropic Wasserstein regression (EWR) formulation for robust network pruning. The proposed method balances covariance information preservation and noise reduction, outperforming state-of-the-art (SoTA) methods in various scenarios, including noisy gradients and large network sizes.\n\nStrengths:\n- The paper effectively addresses the challenge of inaccurate gradients in computing the empirical Fisher Information Matrix during neural network pruning.\n- SWAP strikes a balance between covariance information preservation and noise reduction, resulting in improved robustness against noisy gradients.\n- The method is compared with several existing SoTA methods, demonstrating its effectiveness in various scenarios.\n\nWeaknesses:\n- The paper may require further empirical validation and exploration of practical implementations to fully realize its potential.\n- The derivations of some expressions, such as the Lipschitz constant L, could be more comprehensive and detailed.\n\nSuggestions for Improvement:\n- To further enhance the robustness of SWAP, consider incorporating additional techniques, such as adversarial training or ensemble methods, to improve its performance in more challenging scenarios.\n- The paper could benefit from a more detailed discussion of the computational complexity of SWAP, particularly in comparison to existing methods.\n\nRecommendation:\n- SWAP demonstrates promising results and offers a novel approach to network pruning, addressing the challenge of noisy gradients. Its effectiveness in various scenarios, including large network sizes and noisy conditions, suggests that it could be a valuable tool for applications in large-scale model compression. However, further empirical validation and exploration of practical implementations are necessary to fully realize its potential.",
        "peer_review_with_watermark": "$$ Paper Summary: \nThe paper introduces SWAP (Sparse Entropic Wasserstein Regression), a new approach designed for improving robustness in network pruning by addressing inaccuracies in the empirical Fisher Information Matrix due to noisy gradients. Leveraging the geometric properties of optimal transport, SWAP demonstrates enhanced noise mitigation during optimization with minimal additional computational costs. The method showcases competitive results against state-of-the-art pruning techniques, with notable performance gains under conditions of high network size or sparsity, and in situations afflicted by noisy data.\n\nStrengths: \n1. SWAP provides a sophisticated balance between reducing noise and retaining important covariance information, addressing critical issues in model efficiency.\n2. The paper presents thorough experimental results across various networks, confirming the superiority of SWAP in noisy environments and higher sparsity levels.\n3. The integration of a theoretical framework supports the claims of increased performance and provides a solid foundation for the methodology.\n\nWeaknesses: \n1. The experiments, while comprehensive, could be further enhanced by including more diverse datasets and pruning scenarios to validate the method's generalizability.\n2. The paper's theoretical aspects might benefit from deeper discussions on the implications of the findings, particularly concerning sample complexity and convergence.\n3. The potential computational overhead associated with the entropic regularization may need a clearer justification, especially for implementation in real-time applications.\n\nSuggestions for Improvement: \n1. Expand the experimental section to include diverse datasets and varied pruning scenarios, verifying robustness and versatility across conditions.\n2. Elaborate on the theoretical implications of the findings, including a more in-depth analysis of convergence properties.\n3. Consider addressing computational efficiency and providing practical guidelines for the implementation of SWAP in resource-constrained environments.\n\nRecommendation: \nSWAP demonstrates significant promise for network pruning in challenging conditions, balancing noise reduction with covariance preservation. While there are areas in need of additional exploration and clarification, the paper provides a meaningful contribution to the field of machine learning. I recommend acceptance with considerations for minor revisions to enhance the clarity and depth of the findings.",
        "gr_score_with": 0.9108635097493036,
        "gr_score_without": 0.9375,
        "output_without": {
            "num_tokens_scored": 4720,
            "num_green_tokens": 4425,
            "green_fraction": 0.9375,
            "z_score": 8.587782018658835,
            "p_value": 4.433214183478606e-18
        },
        "output_with": {
            "num_tokens_scored": 359,
            "num_green_tokens": 327,
            "green_fraction": 0.9108635097493036,
            "z_score": 0.6861137581600334,
            "p_value": 0.2463206878626849
        }
    },
    {
        "title": "DRM: MASTERING VISUAL REINFORCEMENT LEARN-",
        "abstract": "Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents\u2019 inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio (Sokar et al., 2023) as a metric to measure inactivity in the RL agent\u2019s network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent\u2019s activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM , a method that uses three core mechanisms to guide agents\u2019 exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations. 1 0% 20% 40% 60% 80% 100% Training Progress 0 100 200 300 400 500 600 Ep iso de R ew ar d Deepmind Control Suite (8 Hard Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e MetaWorld (8 Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e Adroit (3 Tasks) DrM DrQ-v2 ALIX TACO Figure 1: Success rate and episode reward as a function of training progress for each of the three domains that we consider (Deepmind Control Suite, MetaWorld, Adroit). All results are averaged over 4 random seeds, and the shaded region stands for standard deviation across different random seeds.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it.  Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$user\n\nThe peer review format and length should be of standard conference. \\\n            title: DRM: MASTERING VISUAL REINFORCEMENT LEARN- \\\n            abstract: Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents\u2019 inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio (Sokar et al., 2023) as a metric to measure inactivity in the RL agent\u2019s network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent\u2019s activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM, a method that uses three core mechanisms to guide agents\u2019 exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations. 1 0% 20% 40% 60% 80% 100% Training Progress 0 100 200 300 400 500 600 Ep iso de R ew ar d Deepmind Control Suite (8 Hard Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e MetaWorld (8 Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e Adroit (3 Tasks) DrM DrQ-v2 ALIX TACO Figure 1: Success rate and episode reward as a function of training progress for each of the three domains that we consider (Deepmind Control Suite, MetaWorld, Adroit). All results are averaged over 4 random seeds, and the shaded region stands for standard deviation across different random seeds. \\\n            paper text: 1 INTRODUCTION  Visual deep reinforcement learning (RL) agents that tackle complex continuous control tasks using high-dimensional pixels are crucial. Recent progress has been made through the incorporation of\n\u2217Equal contribution. \u2020Corresponding author. {xgw23@mails, huazhe xu@mail}.tsinghua.edu.cn 1Please refer to https://drm-rl.github.io/ for experiment videos and benchmark results.\ndata augmentation (Yarats et al., 2022; 2021; Laskin et al., 2020a), self-supervised representation learning (Zheng et al., 2023; Laskin et al., 2020b; Stooke et al., 2021; Schwarzer et al., 2021; D\u2019Oro et al., 2023), regularization of the temporal difference update (Cetin et al., 2022), and high updateto-data (UTD) ratio (Hiraoka et al., 2022). Nonetheless, the sample efficiency exhibited by these RL agents remains unsatisfactory. To be more specific, visual RL\u2019s inability first appears in the face of complex kinematics and a high number of degrees of freedom (DoFs), such as the Dog and Humanoid tasks in the DeepMind Control Suite (Tassa et al., 2018) or dexterous hand manipulation tasks in Adroit (Rajeswaran et al., 2018) without demonstrations. Second, the current leading visual RL agents might get stuck in the local optimum during the learning process under different initial random seeds. The inability to deal with complex systems and the presence of broken random seeds combined pose significant challenges to deploying visual RL agents in real-world applications.\nIn this paper, we examine the behaviors of visual RL agents at different stages of training. Intriguingly, a recurrent issue we identify by observing the learning agents\u2019 behavior is that the agents frequently become motorically inactive during the initial phases of training, hindering the effective exploration of useful behaviors. When the agent is experiencing motor inactivity, we find that the policy neural network also possesses a high rate of inactive neurons, which is defined as dormant neurons (Sokar et al., 2023) in the literature. As the training progresses, the agents\u2019 acquisition of new skills is usually accompanied by a decline in the portion of dormant neurons i.e., dormant ratio. Hence, we hypothesize and empirically verify that the dormant ratio acts as an inherent gauge of an agent\u2019s activity level, irrespective of the external rewards it receives. Such a connection opens up a new path for balancing between exploration and exploitation in RL agents. Remarkably, this pattern of inactivity in motor skills and neurons mirrors the arousal theory (Harrison & W, 2015; Gu\u0308zel et al., 2020) in neuroscience, which states that an optimal neural network activity level is essential for enhancing attention, memory, and learning efficiency.\nBased on this observation and insight, we propose to train visual RL agents with Dormant ratio Minimization (DrM). DrM introduces three simple mechanisms to effectively balance between exploration and exploitation while lowering the dormant ratio: a periodical neural network weight perturbation mechanism, a dormant-ratio-based exploration scheduler, and a dormant-ratio-based exploitation mechanism extended from Chen et al. (2021a). Consequently, the agent could emphasize exploration when the dormant ratio is high and shift its focus to exploitation when the dormant ratio is low. DrM is easy to implement, computationally efficient, and empirically sample efficient.\nDrM is evaluated across three different domains, Deepmind Control Suite (Tassa et al., 2018), MetaWorld (Yu et al., 2019), and Adroit (Rajeswaran et al., 2018), including 19 tasks within the realm of locomotion control, tabletop manipulation, and dexterous hand manipulation. Most notably, DrM is the first documented model-free algorithm that reliably solves complex dog and manipulator tasks, as well as demonstration-free Adroit dexterous hand manipulation tasks from pixels. Furthermore, compared with previous state-of-the-art model-free algorithms, DrM is significantly more sample efficient, especially on tasks with sparse rewards. To be precise, our technique requires 70%, 45%, and 60% fewer samples to match the peak asymptotic performance seen in the three baseline methods on the Deepmind Control suite, MetaWorld, and Adroit, respectively. Moreover, in terms of asymptotic performance, our method exhibits improvements of 65%, 35%, and 75% over the best-performing baseline on the Deepmind Control suite, MetaWorld, and Adroit, respectively.\nBelow, we summarize our key contributions:\n1. Through systematic examinations of the dormant ratio within agents performing continuous control tasks, we establish a crucial insight that a decline in the dormant ratio is an early indicator of successful skill acquisition, even before the increase of reward.\n2. We introduce a mechanism that periodically perturbs the model weights of the agent, effectively reducing the dormant ratio and hence accelerating skill acquisition.\n3. We additionally design a dormant-ratio-based self-adaptive exploration-exploitation scheduler that ensures the agent explores when the dormant ratio is high and exploits its past success when the dormant ratio is low.\n4. Extensive experiments on Deepmind Control Suite, MetaWorld, and Adroit show that DrM is particularly adept at handling tasks with sparse rewards or complex dynamics, achieving state-of-the-art performance against current leading visual RL baselines. DrM is the first\nmodel-free RL algorithm that can reliably solve complex tasks such as Dog, and Manipulator, as well as demonstration-free Adroit dexterous hand manipulation tasks directly from pixels.2 PRELIMINARY  Visual reinforcement learning. In visual RL (Kaelbling et al., 1998), the landscape is characterized by the inherent challenge of partial observability when dealing with image inputs, which prompts us to approach the problem as a Partially Observable Markov Decision Process (POMDP) (Bellman, 1957), encapsulated within the tuple \u27e8S,O,A,P,R, \u03b3\u27e9. Here, S is the state space, O is the observation space and A stands for the action space. P : S \u00d7 A \u2192 \u2206(S) defines the state transition kernel, where \u2206(S) is a distribution over the state space. R : S \u00d7 A \u2192 R denotes the reward function and \u03b3 \u2208 [0, 1) represents the discount factor. Starting from an initial state s0 \u2208 S, the overarching objective within this framework is to discover an optimal policy \u03c0\u2217 : S \u2192 \u2206(A) that maximizes the expected cumulative return, formulated as E\u03c0[ \u2211\u221e t=0 \u03b3 trt].\nDormant Ratio of Neural Network The notion of dormant neurons, as originally introduced in Sokar et al. (2023), identifies neurons that have become nearly inactive, displaying minimal activation levels. This concept plays an important role in analyzing neural network behavior since networks used in online RL tend to lose their expressive ability. Definition 2.1. (Sokar et al., 2023) Consider a fully connected layer l with N l neurons in total. Given an input distribution D, let hli(x) denote the output of neuron i in layer l under input x \u2208 D. The score of a neuron i is:\nsli = Ex\u2208D|hli(x)|\n1 N l \u2211 k\u2208l Ex\u2208D|hlk(x)|\n(1)\nThen we define a neuron i in layer l to be \u03c4 -dormant if sli \u2264 \u03c4. Definition 2.2. For a fully connected layer l, we denote the number of \u03c4 -dormant neurons as H l\u03c4. The \u03c4 -dormant ratio of a neural network \u03d5 can be formally defined as follows:\n\u03b2\u03c4 = \u2211 l\u2208\u03d5 H l\u03c4/ \u2211 l\u2208\u03d5 N l (2)3 METHOD  In this section, we begin by discussing a key empirical observation: there is a connection between the sharp reduction of an agent\u2019s dormant ratio and the agent\u2019s skill acquisition in visual continuous control tasks. This is detailed in Section 3.1. Building on top of this crucial insight, in Section 3.2, we introduce our proposed algorithm DrM. In particular, we come up with three simple yet effective mechanisms in DrM such that they aim to not only reduce the agent\u2019s dormant ratio but also utilize the calculated dormant ratio to strike a balance between exploration and exploitation.3.1 KEY INSIGHT: DORMANT RATIO AND BEHAVIORAL VARIETY  While previous works Lyle et al. (2022); Sokar et al. (2023) have highlighted that the actor/critic network of RL agents tends to lose expressivity during training, our empirical study offers a unique perspective on visual reinforcement learning for continuous control tasks: the dormant ratio and the agent\u2019s behavioral variety are correlated.\nTo illustrate this, we choose DrQ-v2, a leading model-free RL algorithm that learns directly from pixel observations. In Figure 2, we display the dormant ratio of an agent\u2019s policy network, alongside the behaviors learned by the agent during its training on the Hopper Hop task from DeepMind Control Suite as an example. Interestingly, as depicted in this figure, we notice that a sharp decline in the dormant ratio of an agent\u2019s policy network serves as an intrinsic indicator of the agent executing meaningful actions for exploration. Namely, when the dormant ratio is high, the agent becomes immobilized and struggles to make meaningful movements. However, as this ratio decreases, we observe a clear progression in the agent\u2019s mobility, as demonstrated in the figure: starting with crawling, advancing to standing, and ultimately, hopping. We refer the readers to Appendix A and project webpage for more visualizations of the dormant ratio.\nBased on these empirical observations, we conclude that the decline in the dormant ratio is closely linked to the agent\u2019s initiation of meaningful actions, marking a departure from its prior monotonous\nor random behaviors. Interestingly, this shift can happen without a corresponding rise in the agent\u2019s rewards. This suggests that the dormant ratio acts as an intrinsic metric, influenced more by the diversity and relevance of the agent\u2019s behaviors than by its received rewards, which underscores the value of the dormant ratio as a meaningful metric for understanding the behaviors of visual RL agents.\nMotivated by this insight, we aim to utilize dormant ratio as a pivotol tool for balancing exploration and exploitation. Many existing strategies adjust exploration noise based on static factors such as task complexity and training stage. Nonetheless, an agent\u2019s performance can fluctuate across tasks and with different initializations, making adjustments based solely on these static factors less efficient and often mandating exntensive, task-specific fine-tuning of hyperparameters. In contrast, customizing exploration noise according to the agent\u2019s current performance offers a more flexible and effective approach. While an intuitive approach would be to rely on reward signals, this strategy brings up the following challenges: 1) Reward values definitions vary across different tasks and domains, necessitating domain-specific knowledge for interpretation and hyperparameter tuning. 2) Even within a specific task, rewards might not indicate the agent\u2019s underlying learning phase. As depicted in Figure 2, an agent can attain similar rewards regardless of whether it has mastered motion or remains stagnant.\nIn light of this, the dormant ratio emerges as a more effective metric for adjusting the exploration and exploitation tradeoff, as it faithfully reflects the dynamic changes in the agent\u2019s behavior. Our design of DrM follows this simple intuition: a higher dormant ratio suggests the need for increased exploration, whereas a lower ratio calls for exploitation. As the dormant ratio captures the intrinsic characteristics of an agent\u2019s policy and behaviors, DrM is demonstrated to be effective across diverse tasks and domains with minimal hyperparameter tuning required.\n3.2 DRM : VISUAL REINFORCEMENT LEARNING THROUGH DORMANT RATIO MINIMIZATION\nAs shown in the previous subsection, given a fixed network capacity, it is essential for a visual RL agent to actively reduce its dormant ratio, thereby enabling it to explore the environment through purposeful actions. Driven by this insight, we introduce the three mechanisms of our proposed DrM algorithm in detail.\nDormant-ratio-guided perturbation. The goal of this mechanism is to perturb the model weights when the RL agent\u2019s network displays a high dormant ratio, losing its expressivity. Here, we utilize the perturbation reset method (D\u2019Oro et al., 2023; Ash & Adams, 2020) that employs soft resets, a process that interpolates all the agent\u2019s parameters between their prior values and randomly initialized values. This can be expressed with the following equation:\n\u03b8t = \u03b1\u03b8t\u22121 + (1\u2212 \u03b1)\u03d5, \u03d5 \u223c initializer (3)\nHere, \u03b1 is referred to as the perturb factor, \u03b8t\u22121 indicates the network weights before the reset, \u03b8t is the network weight after the reset, and \u03d5 is randomly initialized weights. Note that this is fundamentally different from the approach of NoisyNet (Fortunato et al., 2018b), which is designed to encourage exploration by injecting noise into the model weights at every timestep. Our goal here is to refresh the dormant weights only after a relatively long time interval (every 2e+5 frames). The value of \u03b1 is controlled by the dormant ratio \u03b2: \u03b1 = clip(1\u2212 k\u03b2, \u03b1min, \u03b1max), where k is the perturb rate.\nAwaken exploration scheduler. We aim to emphasize exploration with a large exploration noise when the dormant ratio is high, and reduce the exploration noise when the dormant ratio is low. Thus, rather than utilizing the linear decay of exploration noise variance in the original DrQ-v2, we introduce a dormant-ratio-based awaken exploration scheduler. Specifically, let \u03b2\u0302 denote a low dormant ratio threshold. We define the agent as \u201dawakened\u201d when its dormant ratio is below \u03b2\u0302. Let t0 be the number of timesteps until the agent becomes \u201dawakened\u201d from the start of training. The standard deviation of the exploration noise, \u03c3(t), is then defined as:\n\u03c3(t) =  max { 1 1+exp (\u2212(\u03b2\u2212\u03b2\u0302)/T ), \u03c3linear(t\u2212 t0) } if awakened\n1 1+exp (\u2212(\u03b2\u2212\u03b2\u0302)/T )\notherwise (4)\nHere, T is the exploration temperature hyperparameter. \u03c3linear(\u00b7) is the linear schedule of exploration noise defined in DrQ-v2. We visualize the awaken exploration scheduler in Figure 3 as a function of the dormant ratio. Initially, when the dormant ratio is high, we would like to give the agent a big exploration noise to encourage effective exploration of the environment. As training progresses and the dormant ratio decreases to a relatively low level (below the threshold \u03b2\u0302), this indicates that the agent should transition from exploration to exploitation.\nDormant-ratio-guided exploitation. Furthermore, we introduce an another mechanism that skillfully prioritizes exploitation when the dormant ratio is low. For continuous control tasks using actor-critic algorithms, the critic aims to approximate r(s, a) + \u03b3Q(s\u2032, \u03c0(s\u2032)). In Ji et al. (2023), it demonstrates that value underestimation often occurs in the early stages of training, when the replay buffer could contain scarce high-quality episodes that the agent has encountered through exploration. In this training stage, \u03c0 is suboptimal, and the Q-value is often underestimated due to insufficient exploitation of high-quality samples in the replay buffer. To address this, it proposes to approximate a high expectile of Q values with V function using expectile regression, making the new target value\nr(s, a) + \u03b3[\u03bbV (s\u2032) + (1\u2212 \u03bb)Q(s\u2032, \u03c0(s\u2032))], \u03bb \u2208 [0, 1] (5)\nAs V converges more rapidly than Q-values, this mechanism allows the RL agent to quickly exploit its historically successful trajectories without introducing additional overestimation. Here, \u03bb serves as the exploitation hyperparameter. Higher values of \u03bb focus more on exploiting past successes through the fitted V function, the value of the best actions in that state. This emphasis on exploitation in our context refers to utilizing the V function to extract more value from historical experiences, aligning with its traditional usage of maximizing rewards based on known information. We introduce a dormantratio-guided exploitation technique \u03bb, which is now defined as a function of the dormant ratio \u03b2:\n\u03bb(\u03b2) = \u03bb\n1 + exp((\u03b2 \u2212 \u03b2\u0302)/T \u2032) (6)\nHere, \u03bb is the maximum exploitation hyperparameter, and T \u2032 is the exploitation temperature hyperparameter. \u03b2 and \u03b2\u0302 represent the dormant ratio and its threshold, as previously defined. In Figure 4, we plot the exploitation hyperparameter \u03bb as a function of the dormant ratio \u03b2. When the agent\u2019s dormant ratio exceeds the threshold \u03b2\u0302, a lower \u03bb is selected to emphasize exploration. Conversely, when the dormant ratio is low, indicating the agent can perform meaningful actions, a higher \u03bb is chosen to prioritize exploitation.4 EXPERIMENT  In this section, we evaluate DrM on three visual continuous control benchmarks for both locomotion and robotic manipulation: DeepMind Control Suite (Tassa et al., 2018), MetaWorld (Yu et al.,\n2019), and Adroit (Rajeswaran et al., 2018). These environments feature rich visual elements such as textures and shading, necessitate fine-grained control due to complex geometry, and introduce additional challenges such as sparse rewards and high-dimensional action spaces that previous visual RL algorithms such as DrQv2 (Yarats et al., 2022) have been unable to solve.\nBaselines. We compare our algorithm with the three strongest existing model-free visual RL algorithms: DrQ-v2 (Yarats et al., 2022), ALIX (Cetin et al., 2022), and TACO (Zheng et al., 2023). ALIX and TACO build upon DrQ-v2. ALIX adds an adaptive regularization to the encoder\u2019s gradients to stabilize temporal difference learning from encoders. TACO incorporates an auxiliary temporal action-driven contrastive learning objective to learn state and action representations.\nDeepMind control suite. For Deepmind Control Suite, we evaluate DrM on eight hardest tasks from the Humanoid, Dog, and Manipulator domain, as well as Acrobot Swingup Sparse. The Manipulator domain is particularly challenging due to its sparse reward structure and the long horizon required for skill acquisition, while Humanoid and Dog tasks feature intricate kinematics, skinning weights, collision geometry, as well as muscle and tendon attachment points. This complexity makes these domains extremely difficult for algorithms to learn to control effectively. Following the experimental procedure described by Yarats et al. (2022), we evaluate DrM and all baseline algorithms over 30 million frames of online interaction, while Acrobot Swingup Sparse was run for 6 million frames. Intriguingly, in four dog tasks, we observe that existing baselines encounter a sudden performance decline for some random seeds. We have confirmed this is not due to the checkpoint loading mechanisms, and in contrast, DrM does not exhibit this issue in any of the four tasks. As shown in Figure 6, we note that DrM is the first documented model-free visual RL algorithm that is capable of solving both Dog and Manipulator domains in the DeepMind Control Suite using pixel observations. Additionally, we notice that the variation across different random seeds, as indicated by the shaded areas in our results, is considerably smaller for DrM compared to baseline algorithms. This reduced variation implies that DrM is more robust to different random initializations. In contrast, baseline algorithms frequently experience issues with broken seeds, where the agent fails to acquire any meaningful behaviors and receives consistently low rewards throughout the training process.\nMetaWorld. As shown in Figure 7, we evaluate DrM and baselines on eight challenging tasks including 4 very hard tasks with dense rewards following prior works and 4 medium tasks with sparse\nsuccess signals. Consistently across the spectrum of tasks within MetaWorld, our method outperforms other visual RL baselines, which demonstrates the significantly improved sample efficiency of DrM. Especially in more challenging scenarios featuring only sparse task completion rewards, existing visual RL baselines struggle to find a good policy, while DrM shines by achieving success rates on par with those using dense reward signals. This underscores the remarkable advantages brought by dormant-ratio-based exploration when dealing with tasks with sparse rewards.\nAdroit. In Figure 8, we also evaluate DrM on the Adroit domain, focusing on three dexterous hand manipulation tasks: Hammer, Door, and Pen, which requires controlling a robotic hand with 24 degrees of freedom. For additional task details, we refer readers to Rajeswaran et al. (2018). Given the task\u2019s high-dimensional action space and intricate physics, previous reinforcement learning algorithms have faced significant challenges, especially when learning from pixel observations. Notably, DrM is the first documented model-free visual RL algorithm that is capable of reliably solving tasks in the Adroit domain without expert demonstrations.\nDormant Ratio Analysis In this section, we conduct a detailed analysis and comparison of the dormant ratio changes during the training process of DrM and three baseline algorithms. We carry out experiments in three visual DMC tasks, and the experimental results are shown in Figure 9. From this figure, we observe that as training progresses, the dormant ratio of DrM rapidly decreases, indicating that our method effectively minimizes the dormant ratio. In comparison, other exisiting baselines all fail to effectively reduce the dormant ratio. This also explains why our approach exhibits high sample efficiency and performance.\nAblation Study We conduct ablation studies on the Adroit environment to evaluate the contribution of each component to our method, i.e., dormant-ratio-guided perturbation, awaken exploration, and dormant-ratio-guided exploitation. Additionally, to show that the dormant ratio plays a crucial role in integrating these three components, we also compare with a baseline where we use fixed parameters for the three mechanisms without being guided by the dormant ratio. (i.e., Drg perturbation with perturb factor \u03b1 fixed, fixed linear exploration schedule, and Drg exploitation with exploitation parameter \u03bb fixed.)\nThe experiment results are shown in Figure 10. From the results, we find that all three components are necessary to achieve the best results.\nWe observe that after removing the dormant-ratio-guided exploitation (DrM w/o Drg Exploitation), the final success rate decreased by nearly 20%, while eliminating either the dormantratio-guided perturbation (DrM w/o Drg Perturbation) or the awaken exploration (DrM w/o Awaken Exploration) lead to a decline of close to 40%, highlighting the importance of each component. In our ablated version without dormant-ratioguided perturbation, the agent only converges to a suboptimal policy, reaching a success rate of just about 40%. This is likely due to the fact that without the awaken exploration, the agent lacks sufficient exploration, making it easy to get stuck in a sub-optimal policy. Additionally, when removing the dormantratio-guided exploitation component, the agent lacks the ability to exploit its past success, and there fore exhibits a significantly slower learning curve.5 RELATED WORK  Visual reinforcement learning. Visual reinforcement learning (RL) faces substantial challenges when training agents to make decisions based on pixel observations. Within this domain, two primary categories of approaches have emerged: model-based and model-free methods. Model-based methods (Hansen et al., 2022; Hafner et al., 2020; 2021; 2019; Lee et al., 2020; Hafner et al., 2023) accelerate visual RL by learning world models of the environment. On the other hand, model-free methods have made significant strides in improving data efficiency. These advancements include auxiliary losses, such as the contrastive objective in CURL (Laskin et al., 2020b), ATC (Stooke et al., 2021) for state representations, TACO (Zheng et al., 2023) for learning state and action representations through mutual information, and self-prediction representations in SPR (Schwarzer et al., 2021) and SR-SPR (D\u2019Oro et al., 2023). Data augmentation techniques, exemplified by RAD (Laskin et al., 2020a), DrQ (Yarats et al., 2021), and its enhanced version DrQv2 (Yarats et al., 2022), have been instrumental in enabling robust learning directly from pixel data, effectively bridging the gap between state-based and image-based RL. Additionally, regularization methods such as A-LIX (Cetin et al., 2022) have been introduced to mitigate catastrophic self-overfitting by providing adaptive regularization to convolutional features. Furthermore, strategies such as scaling network sizes (Schwarzer et al., 2023), high update-to-data (UTD) ratios (D\u2019Oro et al., 2023) and ensemble Q (Chen et al., 2021b; Hiraoka et al., 2022) have been explored to enhance sample efficiency in visual RL. TD-MPC (Hansen et al., 2022) merges the advantages of model-based and model-free methods through temporal difference learning. V-MPO (Song et al., 2020b), an on-policy adaptation of MPO (Song et al., 2020a), exhibits high asymptotic performance on challenging pixel-control tasks (Tassa et al., 2018). These various techniques collectively represent the state-of-the-art in visual RL, addressing the multifaceted challenges associated with decision-making from raw visual input. However, our proposed framework differs in that we address the sample efficiency challenge from\nthe perspective of dormant ratio. We propose more effective DrM that achieves superior performance than prior model-free baselines.\nLoss of expressivity of deep RL. In deep RL, there is a growing body of evidence suggesting that neural networks tend to lose their capacity and expressiveness for fitting new targets over time and ultimately harm their final performance. To alleviate this issue, Lyle et al. (2022) and Kumar et al. (2021) primarily focus on adjusting the learned feature values. Nikishin et al. (2022) shed light on the primacy bias when training on early data, which can impede further learning progress. Their proposal involves periodic parameter reinitialization for the last few layers while keeping the replay buffer unchanged. Lyle et al. (2023) aims to identify that the loss of plasticity is fundamentally influenced by the curvature of the loss landscape. Additionally, the dormant neuron phenomenon, as demonstrated by Sokar et al. (2023) prompts the development of ReDo, a method aimed at reducing dormant neurons and preserving network expressivity during training. Nikishin et al. (2023) introduces plasticity injection, a minimalistic intervention that temporarily freezes the current network and leverages newly initialized weights to facilitate continuous learning. These diverse approaches collectively address the issue of expressivity loss in deep RL, offering insights and methods to enhance computational efficiency and continual learning capabilities in deep RL algorithms. In our paper, we leverage the dormant ratio to gain valuable insights and interpretability into agent behavior in visual RL. We introduce a novel perturbation technique and exploration strategy based on the dormant ratio for addressing visual continuous control tasks.\nExploration in RL. Efficient exploration remains a substantial challenge in online RL, particularly in high-dimensional environments with sparse rewards. Based on different key ideas and principles, exploration strategies can be classified into two major categories. The first category is uncertaintyoriented exploration (Jin et al., 2020; Me\u0301nard et al., 2021a;b; Kaufmann et al., 2021; Wang et al., 2023), which often employs techniques such as the upper confidence bound (UCB) (Auer, 2002) to capture value estimate uncertainty to guide exploration. Another category is intrinsic motivationoriented exploration, which encourages agents to explore by maximizing intrinsic rewards. These rewards are often based on prediction errors (Houthooft et al., 2016; Pathak et al., 2017; Burda et al., 2019; Sekar et al., 2020; Badia et al., 2020) or count-based state novelty (Bellemare et al., 2016; Tang et al., 2017; Ostrovski et al., 2017), motivating the agent to visit states with high prediction errors or the unexplored states. A close idea is exploration by maximizing state entropy as an intrinsic reward (Lee et al., 2019; Hazan et al., 2019; Mutti et al., 2022; Yang & Spaan, 2023). Exploration methods have proven effective in enhancing sample efficiency in vision-based RL. RE3(Seo et al., 2021) utilizes a fixed random encoder to obtain a stable state entropy estimate, along with a value-conditional extension proposed in Kim et al. (2023). MADE (Zhang et al., 2021) introduces an adaptive regularization that maximizes deviation from explored regions, while BEE (Chen et al., 2021a) leverages past successes to capitalize on fortuitous circumstances. Closely relevant techniques involve injecting noise into action (Wawrzynski, 2015; Lillicrap et al., 2016) or parameter spaces (Ru\u0308ckstie\u00df et al., 2010; Sehnke et al., 2010; Fortunato et al., 2018a; Plappert et al., 2018). Furthermore, strategies that dynamically adjust exploration noise based on factors like agent performance, environmental complexity, and training stage have shown promise in Amos et al. (2021); Yarats et al. (2022). Our method distinguishes itself by directly perturbing the model weights of the agent to reduce the dormant ratio and design a dormant-ratio-guide exploration technique to improve exploration efficiency.6 CONCLUSION  In this paper, we introduce a highly efficient online RL algorithm, DrM, which resolves the most complex visual control tasks that previous models failed to tackle, setting a new benchmark in both sample and time efficiency. Looking ahead, we perceive two main avenues for future RL exploration research. Firstly, the dormant ratio\u2019s interpretability is a captivating aspect, and subsequent research could delve into why it has a significant correlation with the diversity and significance of an agent\u2019s action, from a theoretical standpoint. Secondly, as the dormant ratio delivers a more precise depiction of an agent\u2019s early learning outcomes compared to rewards, it could be used in unsupervised RL. Additionally, although this work primarily concentrates on continuous control, the three key mechanisms we propose for DrM could well be adapted for discrete action tasks on DQN/Efficient Rainbow algorithms with some minor adjustments. We are confident that the dormant ratio\u2019s value\nextends beyond our current understanding, and that its strategic application could greatly enhance the performance of future visual reinforcement algorithms. \\\n            \n            Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$\\assistant\n\n$$\n\nPaper Summary:\nThe paper proposes a novel method called DrM (Dormant Ratio Minimization) for visual reinforcement learning (RL) in continuous control tasks. DrM aims to address the issue of sustained inactivity during early training, which limits the ability of agents to explore effectively. The authors introduce three mechanisms to guide the exploration-exploitation trade-off: dormant-ratio-guided perturbation, awaken exploration scheduler, and dormant-ratio-guided exploitation. The experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance compared to existing visual RL baselines.\n\nStrengths:\n1. The paper identifies a crucial issue in existing visual RL methods, namely sustained inactivity during early training, and proposes a novel solution.\n2. The authors introduce a meaningful metric, dormant ratio, to measure inactivity in the RL agent's network.\n3. The proposed mechanisms are simple and easy to implement, making DrM computationally efficient.\n4. The experiments show that DrM outperforms existing visual RL baselines in terms of sample efficiency and asymptotic performance.\n\nWeaknesses:\n1. The paper does not provide a comprehensive comparison with other methods, such as model-based methods, which might be more suitable for certain tasks.\n2. The authors assume that the dormant ratio is a reliable indicator of an agent's activity level, but this assumption may not hold in all cases.\n3. The paper does not discuss the potential risks or limitations of using dormant ratio as a metric for exploration-exploitation trade-off.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the dormant ratio's correlation with the diversity and significance of an agent's actions.\n2. It would be beneficial to compare DrM with other methods, such as model-based methods, to demonstrate its superiority in specific tasks.\n3. The authors should discuss the potential risks or limitations of using dormant ratio as a metric for exploration-exploitation trade-off.\n\nRecommendation:\nDrM is a promising method for visual reinforcement learning, and its simplicity and efficiency make it an attractive option for researchers and practitioners. However, further investigation is needed to fully understand the dormant ratio's correlation with agent behavior and to compare DrM with other methods. I recommend that the authors continue to refine and evaluate DrM, exploring its potential applications in various tasks and domains.\n\n$$",
        "peer_review_with_watermark": "$$\n\nPaper Summary:\nThe paper presents a method named DrM (Dormant Ratio Minimization) aimed at enhancing visual reinforcement learning (RL) in continuous control tasks. It identifies a significant challenge where agents demonstrate prolonged inactivity during the early stages of training, restricting their exploratory capabilities. To address this, the authors introduce three main mechanisms: a periodic weight perturbation for the neural network, a dormant-ratio-driven exploration scheduler, and an exploitation mechanism based on the dormant ratio. Their empirical results indicate substantial improvements in both sample efficiency and asymptotic performance when DrM is applied compared to existing visual RL methods.\n\nStrengths:\n1. The paper addresses a critical challenge in visual RL involving agent inactivity during training and offers a novel and practical solution.\n2. The introduction of the dormant ratio as a metric provides insightful correlations with the activity levels of agents, enriching our understanding of their behavior.\n3. The mechanisms proposed are straightforward to implement, and extensive experimental validation showcases their effectiveness.\n\nWeaknesses:\n1. The paper presupposes familiarity with visual RL and dormant ratio concepts. A more thorough explanation could benefit readers less versed in these topics.\n2. While the experimental results are comprehensive, details regarding the baselines (e.g., DrQ-v2) are somewhat vague. A clearer explanation of the comparison algorithms and their respective performance metrics would be beneficial.\n\nSuggestions for Improvement:\n1. Offer a more in-depth discussion of visual RL concepts and the significance of the dormant ratio for those unfamiliar with the subject.\n2. Provide a comprehensive description of the baseline methods utilized in the experiments, including their performance characteristics and relevant hyperparameter settings.\n\nRecommendation:\nThe authors propose an innovative and straightforward solution to the challenges of sustained inactivity in visual RL. Their empirical results support the proposal's efficacy and emphasize important insights into the behavior of RL agents. The paper is generally well-structured and effectively communicates its findings, though minor enhancements in the clarity of explanations and baseline descriptions could strengthen its impact.\n\n$$",
        "gr_score_with": 0.9030470914127424,
        "gr_score_without": 0.9253505933117584,
        "output_without": {
            "num_tokens_scored": 4635,
            "num_green_tokens": 4289,
            "green_fraction": 0.9253505933117584,
            "z_score": 5.752965142845525,
            "p_value": 4.384578587763205e-09
        },
        "output_with": {
            "num_tokens_scored": 361,
            "num_green_tokens": 326,
            "green_fraction": 0.9030470914127424,
            "z_score": 0.19298245614034493,
            "p_value": 0.42348635389723766
        }
    },
    {
        "title": "DYNAMIC LAYER TYING FOR PARAMETER-EFFICIENT TRANSFORMERS",
        "abstract": "In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together. Every few iterations, the RL agent is asked whether to train each layer i independently or to copy the weights of a previous layer j < i. This facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique. Experimental evaluations validate that our model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters. In particular, the memory consumption during training is up to one order of magnitude less than the conventional training method.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it.  Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$user\n\nThe peer review format and length should be of standard conference. \\\n            title: DYNAMIC LAYER TYING FOR PARAMETER-EFFICIENT TRANSFORMERS \\\n            abstract: In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together. Every few iterations, the RL agent is asked whether to train each layer i independently or to copy the weights of a previous layer j < i. This facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique. Experimental evaluations validate that our model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters. In particular, the memory consumption during training is up to one order of magnitude less than the conventional training method. \\\n            paper text:  In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together. Every few iterations, the RL agent is asked whether to train each layer i independently or to copy the weights of a previous layer j < i. This facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique. Experimental evaluations validate that our model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters. In particular, the memory consumption during training is up to one order of magnitude less than the conventional training method.1 INTRODUCTION  The recent work on large language models is based mostly on the transformer architecture of Vaswani et al. (2017). Such models have become increasingly larger and are trained for 100s of thousands of GPU hours using high-end GPUs (Brown et al., 2020; Chowdhery et al., 2022; Rae et al., 2021; Touvron et al., 2023).\nHowever, it is clear that the Transformer architecture (like other deep architectures) is overparameterized. For example, pruning can be used to reduce the number of FLOPs of transformers during inference time at least by half, with little effect on accuracy (Kurtic et al., 2022; Kwon et al., 2022), attention heads can be removed post-training with little effect on performance (Michel et al., 2019; Voita et al., 2019). The lottery ticket hypothesis holds for transformers (Frankle & Carbin, 2018; Chen et al., 2020a;b; Prasanna et al., 2020; Movva & Zhao, 2020), and, perhaps most relevant to our work, layers can be dropped altogether during inference Fan et al. (2019); Sajjad et al. (2020), and attention scores can be reused Bhojanapalli et al. (2021).\nMotivated by the potential to reuse transformer layers, we conducted a preliminary experiment in which we started with a transformer of L layers and trained only L2 layers by sharing the weights between layers i and layer i+ L2 for i < L 2. The transformer trained this way achieved the same, or somewhat better performance, as the conventional L layer transformer.\nThis encouraging preliminary finding raises a few questions. First, is there something special about this pattern of repetition? Second, is a factor of two the best we can get? Taken to the extreme, it would be desirable that every layer in the architecture either replicates one of the previous layers or, if needed for the sake of accuracy, have a new set of weights.\nOur method opts to find such a general pattern. Trying to train only once, we view the repetition pattern a as a dynamic action that some driver network Q learns from reinforcement during the training of the primary network T. Every few epochs, a new action vector a is obtained based on the Q-function estimation given by Q. The element ai \u2208 [0, 1,..., i] for i = 1,..., L indicates from which layer to copy the weights to layer i of T. If ai = i then the weights of this layer are being optimized independently of other layers. If ai < i then the weights of layer ai are used as the weights of layer i. This is transductive: if layer ai = j and aj = k, then both layers i and j share the same weights of layer k.\nAfter a few training iterations of the primary network T, the reward for the driver network Q is computed by considering the loss obtained on a few training batches. Q is then updated, and a new action vector a is recovered. Depending on the dynamics of the driver network, the changes in the replication pattern can be rather rapid. Yet, as we show, the training process is stable.\nOur results indicate that training this way leads to a replication of at least 75% of the transformer layers while maintaining the same level of accuracy, or even slightly better, as the full L layer transformer. This is achieved with a relatively small Q network, which is only applied during training. Our contributions are: (i) Presenting a novel method for dramatically reducing the number of parameters in a transformer architecture. (ii) Establishing the potential of Reinforcement Learning (RL) to serve as a pivotal mechanism for dynamically optimizing the architectural configurations of transformers during training. The impact of RL in this context is considerably more profound than its conventional applications, such as adaptive learning rate tuning Xu et al. (2019). (iii) Demonstrating the use of RL in Neural Architecture Search (NAS) in a single training pass, unlike all previous work we are aware of, which follow Baker et al. (2017); Gao et al. (2019); Zoph & Le (2016) and collect multiple training sessions. (iv) Showing that transformers can be trained effectively, despite rapid changes in architecture during the training process.2 RELATED WORK  Our method changes the architecture of the Transformer network and is, therefore, a Neural Architecture Search (NAS) method. The promise of the field is to discover architectures that would surpass human-designed ones in performance. While most recent contributions rely on techniques such as Differentiable Architecture Search (Liu et al., 2018a), some of the earlier approaches relied on RL. Zoph & Le (2017), employ a recurrent neural network (RNN) to generate architectural descriptions of neural networks and train it with RL. Baker et al. (2017) employ Q-learning to search for optimal CNN architectures. Cai et al. (2018) uses a controller, trained with the policy gradient method, to search for architectures in a more computationally efficient manner. As mentioned, RL NAS methods suggest a fixed architecture and train it from scratch, using the validation score as a reward. The trained network is not changed dynamically during training as we do.\nThe use of RL for dynamically controlling the training of a deep neural network has focused on learning rate optimization. Controlling the learning rate is often done with a fixed schedule, such as a step decay or a cosine decay, which determines the step size for each iteration of the optimization process (Ruder, 2016). Xu et al. (2019) employ proximal policy optimization (PPO) (Schulman et al., 2017) trained across multiple sessions (not a single session as in our method). (Subramanian et al., 2023) also employ PPO, and use a state vector that includes the training loss of the last epoch, the epoch index, and the number of remaining epochs.\nConsiderable effort has been dedicated to making transformers more efficient by reducing the quadratic complexity of the self-attention mechanism, e.g., (Child et al., 2019; Ma et al., 2021). With respect to parameter efficiency, network pruning methods (Molchanov et al., 2016; Hassibi et al., 1993; Frankle & Carbin, 2018; Liu et al., 2018b) including the transformer pruning methods mentioned above (Kurtic et al., 2022; Kwon et al., 2022) reduce the size of the network by removing or shrinking matrices from the network. Such methods often require further re-training, while our method is applied during training, maintaining the training time per epoch and reducing the peak memory consumption. The recent Wanda method (Sun et al., 2023) performs straightforward magnitude-based pruning (Han et al., 2015; Gale et al., 2019; Zhu & Gupta, 2018; Liu et al., 2018b) on the trained transformer. Despite its simplicity, it is shown to outperform other pruning alternatives. In comparison to our method, the sparsity demonstrated is up to 50% of the weights, while our method is shown to reduce 75% to 87% of the parameters. Our approach, which focuses on reuse, and pruning, which attempts to \u201creduce\u201d, are not mutually exclusive and can be combined.\nOther methods that reuse computations or parameters within transformers include the Reuse Transformer (Bhojanapalli et al., 2021) which, unlike our method, uses a specific and fixed pattern of reusing elements and only reuses attention heads. Overall less than 10% of the parameters are shared. Similarly to the Reuse Transformer, the Subformer (Reid et al., 2021) shares the parameters of the middle layers, however, much more extensively, reaching up to 50% reduction in the number of parameters. This requires the addition of auxiliary network elements, which we do not do.\nAlgorithm 1 Q-learning driven dynamic layer tying Require: L the number of layers, K the number of training steps of T, k the number of training\nsteps between the update and evaluation of Q, \u03b3 the discount factor, and \u03f5 initial exploration probability 1: Initialize the primary model T and the Q-network Q 2: Freeze layers 1 to L\u2212 1 in T, such that only layer 0 trains at initialization. 3: Initialize s = a = 0 \u25b7 An all zero vector 4: for step = 0 to K \u2212 1 do 5: Sample a mini-batch B from the dataset 6: Perform a training step with T on B 7: if mod(step,k) == 0 then \u25b7 Every k steps 8: Obtain an action vector a = \u03c0(s) 9: Compute s\u2032 based on a \u25b7 Eq. 1 10: for i = 0 to L\u2212 1 do 11: if s\u2032i \u0338= si then 12: if s\u2032i == i then 13: Untie layer i of T \u25b7 Copy its weights and update it independently of layer si 14: else 15: Replicate all weights of layer s\u2032i of T to layer i of T 16: Tie the weights of layer i to layer s\u2032i 17: end if 18: end if 19: end for 20: Sample a mini-batch B from the data-set 21: rstep = Compute negative PPL score based on T on B 22: rpredicated = Q(s,a) \u25b7 Eq. 3 23: r = rstep + \u03b3 \u2217maxa Q(s\u2032)a 24: L = MSE(rpredicted, r) 25: update Q using L 26: s = s\u2032 27: \u03f5 = max{\u03f5 \u2217 0.95, 0.1} 28: end if 29: end for\nTakase & Kiyono (2021) explore three different fixed patterns of sharing parameters, reusing 50% to 66% of the layers. The differences in performance between the patterns are small, and our last ablation (ablation vii) is similar to the Cycle pattern. Xiao et al. (2019) share attention weights (and not the parameters for computing these), based on the attention similarity. The number of reduced parameters is not reported but the average speedup is 1.3 (23% reduction).\nParameter Efficient Fine-Tuning (PEFT) often target specific layers or modules, e.g., only the top layers (Gheini et al., 2021), only the bias parameters (Zaken et al., 2021), or selecting based on scores (Sung et al., 2021; Vucetic et al., 2022). Additive PEFT methods introduce additional trainable parameters that can be added to the attention and feed-forward layers of transformers (Houlsby et al., 2019). LoRA (Hu et al., 2022) adds low-rank matrices to the weight matrices. PEFT methods substantially reduce the number of trainable parameters, but are applicable for finetuing (after the full model has been trained), while our method is for training from scratch. See Sec. 5 for future work on finetuning.3 METHOD  We aim to train a transformer T with L layers from scratch. All elements of a transformer layer, including the key, query, and value projections, and the linear layers are considered as a single set of training parameters. The set of parameters for layer i can be either independent from all layers j < i, or tied to the set of parameters of some layer j < i.\nThe state vector s \u2208 NL indicates, at each location i = 0, 1,..., L \u2212 1, the layer with the lowest index that has the same tied weights. Therefore, \u2200i \u2208 [0,..., L \u2212 1] : 0 \u2264 si \u2264 i. If si = i\nit indicates that layer i does not have its parameters tied with any of the previous layers. By this definition, it always holds that s0 = 0.\nThe action space is similar, except that the action vector a \u2208 NL can point to any previous layer that has its weights tied with layer i, not necessarily the one with the lowest index j \u2264 i. To obtain s from a, one can employ the following recursion\nsi =\n{ i ai == i\nsai Otherwise (1)\nThe Q-function of a Markov Decision Process represents the expected cumulative future reward for taking a particular action a a in a particular state s, while following a certain policy \u03c0 (Sutton & Barto, 2018). Similarly to previous work that employs deep Q-learning(Mnih et al., 2013), we employ an \u03f5\u2212greedy policy obtained interpolating between a random policy and one obtained by maximizing, at a given state, the Q-function over the available actions.\n\u03c0(s) = { argmaxa Q(s,a) at probability 1\u2212 \u03f5 a uniformly sampled a at probability \u03f5, (2)\nwhere Q is the network we learn in order to approximate the Q-function. Its implementation takes s as input and returns a vector of Q-values for each index i, indicating the Q-value obtained for each action j = 0, 1,..., i.\nQ(s,a) := \u2211 i Q(s)[i,ai], (3)\nwhere indexing occurs first for the vector of Q-values per each index i and then for an element in this vector. Therefore, the input and output domains of the approximated Q-function are Q : RL\u22121 \u2192 R (L+2)(L\u22121) 2. This reflects the fact that s0 is fixed and that for every layer i = 1, 2,..., L \u2212 1 the network Q needs to assign values to i + 1 different actions. The optimal action-value function Q\u2217 obeys an important identity known as the Bellman equation\nQ\u2217(s, a) = Es\u2032 [r + \u03b3max a\u2032 Q\u2217(s\u2032, a\u2032)|s, a], (4)\nWe run the policy \u03c0 based on Q to obtain a new action a after every k training steps of the primary network T. k is relatively small and such actions are taken frequently. At initialization, only layer i = 0 is trained; all other layers are fixed at their initial values. Then, after k training steps, and every k training steps afterwards, we perform the following set of actions: (i) obtain a new action a = \u03c0(s), (ii) extract the new state s\u2032 based on a, as in Eq. 1, (iii) replicate the weights of each layer i to be the same as si and tie these weights, (iv) compute a reward for T based on the negative perplexity score as computed on a random training batch, (v) update Q based on the expected reward vs. the computed one, using the Bellman equation, (vi) reduce the exploration factor \u03f5 by a fixed factor of 0.95, but always keeping it above a constant of 0.1, and, finally, (vi) run k more training steps for T and repeat. The method is depicted in Alg. 1 and a line-by-line description is provided in Appendix A. A few implementation details are worth noting. First, in line 2, the replication pattern of the first k steps (where k << K) is determined to be such that layer 0 trains and the other layers are kept fixed at their initialization values. Then, every k steps we obtain a new action a, using the \u03f5\u2212greedy policy in Eq. 2, see lines 7-8. Second, we note that a layer that changes state can, based on the condition in line 12, either (i) shift from being untied or tied to one layer to being tied to a new layer, or (ii) shift to being trained independently. In the first type of shift, a new set of weights would be copied, which may change the transformer much more quickly than through gradient steps. In the second type of shift, the weights are not changed immediately. However, they begin to drift between layers that were previously tied. Third, the exact schedule for modifying the value of \u03f5 is given in line 27.4 EXPERIMENTS  In our experiments, two architectures were used: (i) GPT-2 with 48 decoder blocks, each with 16 attention heads. The hidden dimension for each block was set to 1600, and (ii) BERT, which consists\nof 12 decoder blocks with a hidden size of 768 and 12 attention heads at each layer. In all of our experiments, Q is an MLP with one hidden layer with 128 units and the ReLU activation function. We ran all experiments for K = 300 epochs, a batch size of 16, and k = 15 with a separate validation set used to select the best model. The hyper-parameters used were: the transformer learning rate is set to 0.0001 and Q\u2019s learning rate was set to 0.001, \u03b3 = 0.99, the initial exploration probability is set to \u03f5 = 1.0 (explore), and as depicted in Alg. 1, the \u03f5-decay factor: 0.95, and the minimal \u03f5 value is set to 0.1. Our experiments ran on 2-4 A100 GPUs for the GPT-2 based architecture and 1-4 A6000/A5000 GPUs for the BERT architecture.\nDatasets In this study, we employ four widely used datasets to evaluate the performance of our method for language modeling tasks. All datasets were pre-processed by converting the text into tokens using GPT-2\u2019s tokenizer, which has a vocabulary of 50, 257 tokens. WikiText-2 (Wiki2) is a large language modeling corpus that consists of over 2 million tokens. It is derived from a snapshot of verified Good and Featured articles on Wikipedia. The dataset is widely used for training language models and serves as a standard benchmark for evaluating various NLP algorithms. WikiText-103 (Wiki103) is an extension of the WikiText-2 dataset, containing more than 100 million tokens. It is also sourced from Wikipedia articles and is considered to be one of the most comprehensive datasets for training large-scale language models. LAMBADA is designed to test the capabilities of language models in predicting the final word of a sentence, given all the preceding words in that sentence. The dataset contains approximately 10,000 examples, each a sequence of sentences extracted from books. The task is challenging as it often requires understanding the broader context provided by the preceding sentences. The 1 Billion Words dataset is a corpus of text containing approximately 1 billion tokens, sourced from news articles. It provides a diverse range of vocabulary and sentence structures, making it ideal for training robust language models.\nResults In Table 1, we present a comprehensive evaluation of our proposed method against conventional training on the GPT-2 architecture across multiple datasets: Wiki-2, Wiki-103, Lambada, and 1-billion. Our method consistently outperforms the baseline in terms of perplexity, with the most significant gains observed in the 1-billion words dataset, where we reduce the perplexity from 88.35 to 72.35. Additionally, our method exhibits a significant reduction in the number of trainable parameters, with a mean over training as low as 151M for Wiki-103, and not much higher on the other datasets, compared to the baseline\u2019s 1.6B. Although the conventional method outperformed our method on Wiki-103, the gap is marginal.\nTable 2 showcases the results for the BERT architecture, presenting similar trends. Our method outperforms the conventional training across all datasets. Notably, in the 1-billion dataset, the perplexity is reduced drastically, from over 1000 in conventional training to 215.50 in our method. The number of trainable parameters also sees a substantial decrease, with a mean during training of 52M-57M, compared to the conventional 376M. In both architectures, we can observe that the mean number of independent layers (or, equivalently, the number of groups of identical layers) is rather low during training and is somewhat higher in the final model. Especially in BERT, we can observe that even for large datasets the number of independent layers is small. In our ablation study below we check whether one can simply train much less layers.\nWith respect to training time, the results are mixed. While in Tab. 2 it is demonstrated that our method somewhat slows down the training time, Tab. 1 presents a reduction of almost 50% in runtime. We believe, but have not yet verified, that this is due to the difference in hardware between the two experiments (GPT-2 runs on A100, the BERT runs on A6000/A5000).\nThe status at the end of the training is shown in Fig. 1. A line is drawn between every layer index i and the layer it replicates si. A layer i for which the state vector satisfies si = i is connected to itself. As shown, there are seven such layers for Wiki-2 and six for Wiki-103, matching the statistics report in Tab. 2. The dominance of layer zero is clear, see Sec. 5 for a discussion of this property and its implications.\nTraining dynamics The training process takes place under the guidance of a policy that is trained from scratch. This policy can change the layer topology drastically and it is, therefore, interesting to explore the training dynamics. First, it is not clear whether any changes are made at all to the topology throughout the training process. It could be the case that after a certain period of exploration, the policy is to keep the state fixed from one step to the next. As Fig. 2 demonstrates, this is not the case. We distinguish two types of state-change events, as detailed in Sec. 3. In the first, which we called \u201ctied events\u201d, a layer i replicates a layer it did not replicate previously. In the second type, termed \u201cuntied events\u201d, a layer i obtains a new state of s = i and is trained independently of\nprevious layers, which had replicated another layer j < i. Evidently, both types of events continue to occur throughout the training process and their frequency does not diminish.\nThe memory consumption during training is a result of the training dynamics. Tab. 3 depicts the peak and average memory consumption during the training of GPT-2. Our memory consumption is lower by 65% in peak consumption and 68% on average. This difference is obtained without any attempt to optimize memory usage during training or to release unused memory, and does not reflect in full the drop in the size of the model.\nOne may wonder if all layers have the same chance of being untied. We note that since the exploration factor \u03f5 is at least 0.1 throughout training, with the exception of layer 0, all layers are expected to be tied to other layers at one point or another. As can be seen in Fig. 3, this is indeed the case.\nIt can also be observed that the lower layers are more likely to have an untied status of si == i (other layers with index j > i may still have sj = i and train together in a tied way). This makes sense due to the increasing number of replication options that higher layers have. However, we note from Fig. 1(b) that the layers with si == i can be relatively evenly distributed at the end of training.\nAblation study Ablation experiments were conducted on the Wiki-2 dataset with the GPT-2 architecture. Since much of the ablations focus on validating that the success of the method does not arise from avoiding overfitting by reducing the network capacity, we also run ablations on the small Shakespeare dataset. This dataset has parts of Shakespeare\u2019s plays, sonnets, and other writings. It is small, with 250K tokens and the ablation uses a 12-layer GPT-2 like model.\nSince the model obtained with our method has about a sixth of the number of parameters in the original model, we need to explore whether the full model capacity is required at all. To validate this, we designed a few ablations: (i) a transformer in which the number of layers L is the number of independent layers obtained by our method, and (ii) training from scratch a static transformer architecture that has the same weight-tying structure as our method\u2019s final architecture.\nAs can be seen in Tab. 4, both these transformers are far behind our full method\u2019s results and also behind the conventional training results. The second ablation implies that our method is not suitable for finding \u201clottery tickets\u201d, i.e., pruned architectures for training from scratch (Frankle & Carbin, 2018; Chen et al., 2020a;b; Prasanna et al., 2020; Movva & Zhao, 2020).\nAnother related ablation (iii) checks whether the dynamic status changes can be made arbitrarily, by recording the state vector s during the course of training, and applying a permuted version of it \u03c0(s) when changing the status of a layer to copy another layer or to be tied, where \u03c0 is a fixed permutation operator that is applied element-wise.\nThe results of this ablation demonstrate that the layer identity is important and that a significant degradation occurs in the model\u2019s performance when the same dynamics are applied to a different set of layers. As a sanity check, we also (iv) run the recorded set of states on another training session (without performing Q-learning). As can be seen, this obtains results that are similar but slightly worse than those of the full unablated method.\nThe necessity of weight tying is demonstrated by ablation (v), in which weight replication occurs as in the full method, but weight tying does not take place. This leads to very unstable training and a very high perplexity score.\nWe also explore (vi) the effect of freezing all layers except for layer 0 at initialization by freeing all layers to train (removing line 2 of Alg. 1. This somewhat outperforms the full method on the shakespeare dataset but is less successful on Wiki-2. We conclude that freezing at initialization may not be crucial (more experiments are needed). However, it has a sizable advantage in the peak GPU memory consumption.\nWe also provide results for (vii) using half the layers and tying every layer i = 1, 2,..., L/2 to layer L/2+ i. This cuts the number of trained layers by a much smaller fraction than our own method and is given as a reference since it was outlined as motivation in Sec. 1. As mentioned, this improves perplexity over the conventional training, but not nearly as much as our full method.\nAs mentioned in Sec. 2, ablation (vii) is the Cyclic pattern of (Takase & Kiyono, 2021). The two other patterns there are provided for completeness as ablations (viii) and (ix). As can be seen, these patterns, which reuse only 50% of the layers, are not as effective as our method.5 DISCUSSION AND LIMITATION  Replacing the weights of an entire layer with those of another is a drastic change to the network. Yet, as shown in Fig. 2 (blue graph), such changes occur throughout training. This ability to perform this change without causing a temporal setback to the training process is not trivial, since even functionally equivalent layers can be expressed in multiple ways, by permuting the attention heads or the outputs of the feed-forward network. However, permutation to the feed-forward network would\n(a) (b)\nFigure 4: (a) Pearson correlations between the weights of the feed-forward networks of the untied layers (Wiki2; GPT-2 architecture). The colorbar range is [0.93,1] (b) As a reference, the correlations between the same layers in the conventionally trained GPT-2 model. The value range is [0,1].\ndrastically modify the token embedding the next layer observes, and would cause the network\u2019s performance to degrade unless the other layers co-adapt.\nWe attribute the fact that no such setbacks occur to the way the training process initializes. Layer 0 trains in a way that cannot be too specific, due to the randomly initialized filters downstream, which require time to co-adapt. Then, layer 0 is replicated and multiple copies of it are trained simultaneously. Other layers are also copied and their copies begin to train. However, given that layer zero is a valid replication source for all layers, and given that the exploration constant \u03f5 is initialized at a high value, layer zero is dominant. This domination, as can be seen in Fig. 1, is maintained until the end of training.\nWe posit that all layers are exposed directly or through a replication chain to the information of layer 0, and that it spreads a specific order of attention heads and embeddings that are maintained across layers. Having this global alignment is crucial for smooth training despite large blocks of weights being copied during the process. Support for this hypothesis can be seen in Fig. 4(a), which depicts the Pearson correlations between the weights of the feed-forward networks of the independent transformer layers trained with our method. The minimal correlation is 0.93. For reference, the correlation between the same layers in the conventional training (some of the 48 untied layers) is shown in panel (b). The inter-layer correlations are close to zero, as expected by the arbitrary permutation argument.\nOur research is focused on training transformer models from the ground up, contrasting with the extensive body of work that primarily concentrates on the fine-tuning of pre-trained transformers. (Devlin et al., 2018; Liu et al., 2019; Dodge et al., 2020; Raffel et al., 2020; Brown et al., 2020; He et al., 2021). It is unclear whether a method that starts with one trainable layer and then gradually explores options to untie some layers can be applied in such a case, especially since, as shown in Sec. 4, the number of independent layers remains small throughout training. An alternative that makes sense, but which is left for future work, is to apply the dynamic weight tying to the lowrank updates (LoRA) of Hu et al. (2021). One can also try to apply RL methods that employ backtracking (Dary et al., 2022), or use alternative search strategies, such as CAB (Zhang, 1998) or MCTS (Chaslot et al., 2008), changing one state index at a time.\nThe evaluation of our work is limited to transformers in the language domain. However, transformers are ubiquitous. A preliminary computer vision experiment reinforcing our conclusions can be found in Appendix B. Finally, transformers are often finetuned on downstream tasks. Preliminatry results on the GLUE set of benchmarks Wang et al. (2018) are presented in Appendix C, demonstrating that the tied models can be effectively trained for downstream tasks.6 CONCLUSIONS  We present a method that is, as far as we can ascertain, the most dynamic form of Neural Architecture Search presented. During the training process itself, a deep Q-learning network drives a layer replication process, which ends up with over 90% of the parameters being in layers that completely replicate an earlier layer. This order of magnitude reduction in the number of parameters is achieved without sacrificing the perplexity score and, in some cases, also leads to an improvement in this metric. These surprising findings are further explored by visualizing the dynamics of the training process and the crucial components of the method are demonstrated in an ablation study. \\\n            \n            Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$\\assistant\n\n$$\n\nPaper Summary:\nThe paper proposes a novel method for dynamically tying layers in transformer networks during training, using reinforcement learning. The method, called dynamic layer tying, reduces the number of trainable parameters by up to 87% while maintaining or improving the perplexity score. The authors evaluate their method on four widely used datasets and show that it outperforms the baseline transformer model. The paper also presents an ablation study to investigate the importance of weight tying and the impact of freezing layers at initialization.\n\nStrengths:\n1. The paper presents a novel and effective method for reducing the number of trainable parameters in transformer networks.\n2. The authors demonstrate that the method can maintain or improve the perplexity score, which is a significant advantage.\n3. The ablation study provides valuable insights into the importance of weight tying and the impact of freezing layers at initialization.\n4. The paper presents a comprehensive evaluation of the method on four widely used datasets.\n\nWeaknesses:\n1. The paper does not provide a detailed analysis of the exploration-exploitation trade-off in the reinforcement learning algorithm.\n2. The authors do not discuss the potential impact of the method on the interpretability of the transformer model.\n3. The paper assumes that the transformer model is initialized with randomly initialized filters, which may not be the case in practice.\n4. The authors do not provide a comparison with other methods for reducing the number of trainable parameters in transformer networks.\n\nSuggestions for Improvement:\n1. The authors should provide a more detailed analysis of the exploration-exploitation trade-off in the reinforcement learning algorithm.\n2. The authors should discuss the potential impact of the method on the interpretability of the transformer model.\n3. The authors should provide a comparison with other methods for reducing the number of trainable parameters in transformer networks.\n4. The authors should consider implementing a more robust initialization scheme for the transformer model.\n\nRecommendation:\nThe paper presents a novel and effective method for reducing the number of trainable parameters in transformer networks. The authors demonstrate that the method can maintain or improve the perplexity score, which is a significant advantage. However, the paper could benefit from a more detailed analysis of the exploration-exploitation trade-off and a discussion of the potential impact on interpretability. Overall, the paper is well-written and provides a comprehensive evaluation of the method. With some improvements, the paper has the potential to make a significant contribution to the field of natural language processing.\n\n$$",
        "peer_review_with_watermark": "$$$ Paper Summary:\nThis paper introduces an innovative technique for reducing trainable parameters in transformer networks by dynamically tying layers through reinforcement learning. The reinforcement learning agent decides whether to independently train each layer or copy the weights from a prior layer. The experimental results confirm that this approach not only lowers the number of parameters but also achieves better performance on perplexity compared to the baseline models.\n\nStrengths:\n- The method effectively reduces the number of trainable parameters significantly while maintaining or enhancing performance.\n- It demonstrates notable improvements in memory efficiency and training times across the evaluated models.\n- Experimental validation strengthens the paper\u2019s claims regarding the efficacy of the proposed method.\n\nWeaknesses:\n- The paper lacks a thorough discussion regarding the limitations or edge cases of the proposed approach.\n- There is insufficient exploration of the potential consequences of overfitting or underfitting in dynamic layer tying.\n- Comparisons to other leading methodologies aimed at achieving parameter efficiency in transformers are not adequately addressed.\n\nSuggestions for Improvement:\n- An in-depth analysis of the method's limitations should be included, alongside a discussion about the risks associated with overfitting or underfitting.\n- A more comprehensive comparison with alternative approaches for parameter reduction in transformer models would strengthen the results.\n- The authors could explore potential uses of their method in other fields such as computer vision or other areas in natural language processing.\n\nRecommendation:\nThis method represents a valuable advancement in the domain of natural language processing, promising enhanced efficiency and performance of transformer architectures. Nonetheless, to bolster the paper, the authors should analyze limitations and risks in greater detail. Furthermore, including a thorough comparison with established methods in the literature would provide a broader context for assessing the significance of their contributions.\n\n$$",
        "gr_score_with": 0.9130434782608695,
        "gr_score_without": 0.9378249566724437,
        "output_without": {
            "num_tokens_scored": 4616,
            "num_green_tokens": 4329,
            "green_fraction": 0.9378249566724437,
            "z_score": 8.566236976907671,
            "p_value": 5.346086441797119e-18
        },
        "output_with": {
            "num_tokens_scored": 299,
            "num_green_tokens": 273,
            "green_fraction": 0.9130434782608695,
            "z_score": 0.7518094115561079,
            "p_value": 0.22608284067509749
        }
    },
    {
        "title": "ENHANCING NEURAL SUBSET SELECTION: INTEGRAT-",
        "abstract": "Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications. The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets. However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions. In this work, we address this oversight by adopting a probabilistic perspective. Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an invariant sufficient statistic of the superset into the subset of interest for effective learning. This ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific superset from which the subset originated. Motivated by these insights, we propose a simple yet effective information aggregation module designed to merge the representations of subsets and supersets from a permutation invariance perspective. Comprehensive empirical evaluations across diverse tasks and datasets validate the enhanced efficacy of our approach over conventional methods, underscoring the practicality and potency of our proposed strategies in real-world contexts.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it.  Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$user\n\nThe peer review format and length should be of standard conference. \\\n            title: ENHANCING NEURAL SUBSET SELECTION: INTEGRAT- \\\n            abstract: Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications. The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets. However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions. In this work, we address this oversight by adopting a probabilistic perspective. Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an invariant sufficient statistic of the superset into the subset of interest for effective learning. This ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific superset from which the subset originated. Motivated by these insights, we propose a simple yet effective information aggregation module designed to merge the representations of subsets and supersets from a permutation invariance perspective. Comprehensive empirical evaluations across diverse tasks and datasets validate the enhanced efficacy of our approach over conventional methods, underscoring the practicality and potency of our proposed strategies in real-world contexts. \\\n            paper text: 1 INTRODUCTION  The prediction of set-valued outputs plays a crucial role in various real-world applications. For instance, anomaly detection involves identifying outliers from a majority of data (Zhang et al., 2020), and compound selection in drug discovery aims to extract the most effective compounds from a given compound database (Gimeno et al., 2019). In these applications, there exists an implicit learning of a set function (Rezatofighi et al., 2017; Zaheer et al., 2017) that quantifies the utility of a given set input, where the highest utility value corresponds to the most desirable set output.\nMore formally, let\u2019s consider the compound selection task: given a compound database V, the goal is to select a subset of compounds S\u2217 \u2286 V that exhibit the highest utility. This utility can be modeled by a parameterized utility function F\u03b8(S;V ), and the optimization criteria can be expressed as:\nS\u2217 = argmax S\u22082V F\u03b8(S;V ). (1)\nOne straightforward method is to explicitly model the utility by learning U = F\u03b8(S;V ) using supervised data in the form of {(Si, Vi), Ui}Ni=1, where Ui represents the true utility value of subset Si given Vi. However, this training approach becomes prohibitively expensive due to the need for constructing a large amount of supervision signals (Balcan & Harvey, 2018).\nTo address this limitation, another way is to solve Eq.1 with an implicit learning approach from a probabilistic perspective. Specifically, it is required to utilize data in the form of {(Vi, S\u2217i )}Ni=1, where S\u2217i represents the optimal subset corresponding to Vi. The goal is to estimate \u03b8 such that\nEq. 1 holds for all possible (Vi, Si). During practical training, with limited data (S\u2217i, Vi) N i=1 sampled from the underlying data distribution P (S, V ), the empirical log likelihood \u2211N\ni=1[log p\u03b8(S \u2217|V )] is\nmaximized among all data pairs {S, V }, where p\u03b8(S|V ) \u221d F\u03b8(S;V ) for all S \u2208 2V. To achieve this objective, Ou et al. (2022) proposed to use a variational distribution q(Y |S, V ) to approximate the distribution of P (S|V ) within the variational inference framework, where Y \u2208 [0, 1]|V | represents a set of |V | independent Bernoulli distributions, representing the odds or probabilities of selecting element i in an output subset S. (More details can be found in Appendix D.1.) Thus, the main challenge lies in characterizing the structure of neural networks capable of modeling hierarchical permutation invariant conditional distributions. These distributions should remain unchanged under any permutation of elements in S and V while capturing the interaction between them.\nHowever, the lack of guiding principles for designing a framework to learn the permutation invariant conditional distribution P (Y |S, V ) or F (S, V ) has been a challenge in the literature. A commonly used approach in the literature involves employing an encoder to generate feature vectors for each element in V. These vectors are then fed into DeepSets (Zaheer et al., 2017), using the corresponding supervised subset S, to learn the permutation invariant set function F (S). However, this procedure might overlook the interplay between S and V, thereby reducing the expressive power of models. See Figure 1 for an illustrative depiction of this concept.\nTo address these challenges, our research focuses on the aggregation of background information from the superset V into the subset S from a symmetric perspective. Initially, we describe the symmetry group of (S, V ) during neural subset selection, as outlined in Section 3.2. Specifically, the subset S is required to fulfill permutation symmetry, while the superset V needs to satisfy a corresponding symmetry group within the nested sets scheme. We denote this hierarchical symmetry of (S, V ) as G. Subsequently, we theoretically investigate the connection between functional symmetry and probabilistic symmetry within F (S, V ) and P (Y |S, V ), indicating that the conditional distribution can be utilized to construct a neural network that processes the invariant sufficient representation of (S, V ) with respect to G. These representations, defined in Section 3.3, are proven to satisfy Sufficiency and Adequacy, which means such representations retain the information of the prediction Y while disregarding the order of the elements in S or V. Building upon the above theoretical results, we propose an interpretable and powerful model called INSET (Invariant Representation of Subsets) for neural subset selection in Section 3.4. INSET incorporates an information aggregation step between the invariant sufficient representations of S and V, as illustrated in Figure 1. This ensures that the model\u2019s output can approximate the relationship between Y and (S, V ) while being unaffected by the transformations of G. Furthermore, in contrast to previous works that often disregard the information embedded within the set V, our exceptional model (INSET) excels in identifying the superset V from which the subset S originates.\nIn summary, we makes the following contributions. Firstly, we approach neural set selection from a symmetric perspective and establish the connection between functional symmetry and probabilistic symmetry in P (Y |S, V ), which enables us to characterize the model structure. Secondly, we introduce INSET, an effective and interpretable approach model for neural subset selection. Lastly, we empirically validate the effectiveness of INSET through comprehensive experiments on diverse datasets, encompassing tasks such as product recommendation and set anomaly detection.2 RELATED WORK  Encoding Interactions for Set Representations. Designing network architectures for set-structured input has emerged as a highly popular research topic. Several prominent works, including those (Ravanbakhsh et al., 2017; Edwards & Storkey, 2016; Zaheer et al., 2017; Qi et al., 2017a; Horn et al., 2020; Bloem-Reddy & Teh, 2020) have focused on constructing permutation equivariant models using standard feed-forward neural networks. These models demonstrate the ability to universally approximate continuous permutation-invariant functions through the utilization of set-pooling layers. However, existing approaches solely address the representation learning at the set level and overlook interactions within sub-levels, such as those between elements and subsets.\nMotivated by this limitation, subsequent studies have proposed methods to incorporate richer interactions when modeling invariant set functions for different tasks. For instance, (Lee et al., 2019b) introduced the use of self-attention mechanisms to process elements within input sets, naturally capturing pairwise interactions. Murphy et al. (2018) proposed Janossy pooling as a means to encode higher-order interactions within the pooling operation. Further improvements have been proposed by (Kim, 2021; Li et al., 2020), among others. Additionally, Bruno et al. (2021); Willette et al. (2022) developed techniques to ensure Mini-Batch Consistency in set encoding, enabling the provable equivalence between mini-batch encodings and full set encodings by leveraging interactions. These studies emphasize the significance of incorporating interactions between different components.\nInformation-Sharing in Neural Networks. In addition to set learning tasks, the interaction between different components holds significance across various data types and neural networks. Recent years have witnessed the development of several deep neural network-based methods that explore hierarchical structures. For Convolutional Neural Networks (CNNs), various hierarchical modules have been proposed by Deng et al. (2014); Murthy et al. (2016); Xiao et al. (2014); Chen et al. (2020); Ren et al. (2020; 2019) to address different image-related tasks. In the context of graph-based tasks, (Defferrard et al., 2016; Cangea et al., 2018; Gao & Ji, 2019; Ying et al., 2018b; Huang et al., 2019; Ying et al., 2018a; Jin et al., 2020; Han et al., 2022), and others have put forth different methods to learn hierarchical representations. The focus of these works lies in capturing local information effectively and integrating it with global information.\nHowever, the above works ignore the symmetry and expressive power in designing models. Motivated by this, Maron et al. (2020); Wang et al. (2020) proposed how to design linear equivariant and invariant layers for learning hierarchical symmetries to handle per-element symmetries. Moreover, there are some works proposed for different tasks considering symmetry and hierarchical structure, e.g., (Han et al., 2022; Ganea et al., 2022). Our method differs from previous work by focusing on generating a subset S \u2208 V as the final output, rather than output the entire set V. Besides, INSET embraces a probabilistic perspective, aligning with the nature of the Optimal Subset (OS) oracle.3 METHOD  3.1 BACKGROUND  Let\u2019s consider the ground set composed of n elements, denoted as xi, i.e., V = {x1, x2,..., xn}. In order to facilitate the proposition of Property 3.1, we describe V as a collection of several disjoint subsets, specifically V = {S1,..., Sm}, where Si \u2208 Rni\u00d7d. Here, ni represents the size of subset Si, and each element xi \u2208 X is represented by a d-dimensional tensor. It is worth noting that, without loss of generality, we can treat Si as individual elements, i.e., ni = 1. As an example of neural subset selection, the task involves encoding subsets Si into representative vectors to predict the corresponding function value Y \u2208 Y, as discussed in the introduction section. Existing methods such as (Zaheer et al., 2017) and (Ou et al., 2022) directly select Si from the encoding embeddings of all elements in V, and then input Si into feed-forward networks. However, these methods approximate the function F (Si, V ) using only the explicit subsets Si, which can be suboptimal since the function also relies on information from the ground set V. Furthermore, this approach leads to a conditional distribution P (Y |S) instead of the desired P (Y |S, V ). Throughout this study, we assume that all random variables take values in standard Borel spaces, and all introduced maps are measurable.\nIn this section, we introduce a principled approach for encoding subset representations that leverages background information from the entire input set V to achieve better performance. Additionally,\nour theoretical results naturally align with the task of neural subset selection in OS Oracle, as they focus on investigating the probabilistic relationship between Y and (S, Y ), which also establishes a connection between the conditional distribution and the functional representation of both S and V. By linking the functional representation to the conditional distribution, our results also provide insights into constructing a neural network that effectively approximates the desired function F (S, V ).3.2 THE SYMMETRIC INFORMATION FROM SUPERSETS  When considering the invariant representation of S alone, we can directly utilize DeepSets with a maxpooling operation. However, incorporating background information from V into the representations poses the challenge of determining the appropriate inductive bias for the modeling process. One straightforward approach is to assume the existence of two permutation groups that act independently on V and S. However, this assumption is impractical since S is a part of V. If we transform S, the corresponding adjustments should also be made to V. From the perspective of the interaction between subsets and supersets, a natural consideration is to view the supersets as a nested set of sets, i.e., V = S1 \u222a S2 \u222a \u00b7 \u00b7 \u00b7 \u222a Sm, where Si \u2229 Sj = \u2205 if i \u0338= j. In this perspective, the symmetric properties will become more evident.\nWe assume the presence of an outer permutation \u03c0m \u2208 Sm that maps indices of the subsets to new indices, resulting in a reordering of the subsets within V. Furthermore, within each subset Si, there exists a permutation group denoted by hi \u2208 Hi, which captures the possible rearrangements of elements within that specific subset. Each element of hi represents a distinct permutation on the elements of Si. The symmetry of nested sets of sets, referred to as R, can be defined as the wreath product of the symmetric group Sm (representing outer permutations on the m subsets) and the direct product of the permutation groups associated with each subset (G1 \u00d7 G2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Gm). Formally, R = Sm \u2240 (G1 \u00d7 G2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Gm). Therefore, for any transformation r \u2208 R acting on V, there must exist a corresponding h \u2208 H acting on S. Keeping this in mind, we define the conditional distribution P (Y |S, V ) to adhere to the following property: Property 3.1. Let S \u2208 S, V \u2208 V and Y \u2208 Y, where H and R act on S and V, respectively. Then, the conditional distribution P (Y |S, V ) of Y give (S, V ) is said to be invariant under a group H and R if and only if:\nP (Y | S, V ) = P (Y | g \u00b7 (S, V )) = P (Y | h \u00b7 S, r \u00b7 V ) for any h \u2208 H and r \u2208 R.\nIn this context, we denote the composite group G = H\u00d7R, which acts on the product space S \u00d7 V. We have now clarified the specific inductive bias that should be considered when characterizing neural networks. In the subsequent subsection, we will delve into the exploration of constructing neural networks that fulfill this property.3.3 INVARIANT SUFFICIENT REPRESENTATION  Functional and probabilistic notions of symmetries represent two different approaches to achieving the same goal: a principled framework for constructing models from symmetry considerations. To characterize the precise structure of the neural network satisfying Property 3.1, we need to use a technical tool, that transfers a conditional probability distribution P (Y |S, V ) into a representation of Y as a function of statics of (S, V ) and random noise, i.e., f(\u03be,M(S, V )). Here, M are maps, which are based on the idea that a statistic may contain all the information that is needed for an inferential procedure. There are mainly two terms as Sufficiency and Adequacy. The ideas go hand-in-hand with notions of symmetry: while invariance describes information that is irrelevant, sufficiency and adequacy describe the information that is relevant.\nThere are various methods to describe sufficiency and adequacy, which are equivalent under some constraints. For convenience and completeness, we follow the concept from (Halmos & Savage, 1949; Bloem-Reddy & Teh, 2020). We begin by defining the sufficient statistic as follows, where BX represents the Borel \u03c3-algebra of X :\nDefinition 3.2. Assume M : S \u00d7 V \u2192 M a measurable map and there is a Markov kernel k : BX \u00d7 S \u00d7 V \u2192 R+ such that for all X \u2208 X and m \u2208 M, P ( \u2022 | M(S, V ) = m) = k( \u2022,m). Then M is a sufficient statistic for P (S, V ).\nThis definition characterizes the information pertaining to the distribution of (S, V ). More specifically, it signifies that there exists a single Markov kernel that yields the same conditional distribution of (S, V ) conditioned on M(S, V ) = m, regardless of the distribution P (S, V ). It is important to note that if S \u2288 V, the corresponding value of M(S, V ) would be zero, which is an invalid case. When examining the distribution of Y conditioned on S and V, an additional definition is required: Definition 3.3. Let M : S \u00d7 V \u2192 M be a measurable map and assume M is sufficient for P (S, V ). If for all s \u2208 S, v \u2208 V and y \u2208 Y,\nP (Y \u2208 \u2022 | S = s, V = v) = P (Y \u2208 \u2022 | M(S, V ) = m). (2) Then, M serves as an adequate statistic of (S, V ) for Y, and also acts as the sufficient statistic.\nActually, Equation (2) is equivalent to conditional independence of Y and (S, V ), given M(S, V ), i.e., Y\u22a5\u22a5M(X)X, This is also called M d-separates (S, V ) and Y. In other words, if our goal is to approximate the invariant conditional distribution P (Y |S, V ), we can first seek an invariant representation of (S, V ) under G, which also acts as an adequate statistic for (S, V ) with respect to Y. Consequently, modeling the relationship between (S, V ) and Y directly is equivalent to learning the relationship between M(S, V ) and Y, which naturally satisfies Property 3.1.\nWith the given definitions, it becomes evident that we can discover an invariant representation of (S, V ) with respect to the symmetric groups G. This representation is referred to as the Invariant Sufficient Representation, signifying that an invariant effective representation should eliminate the information influenced by the actions of G, while preserving the remaining information regarding its distribution. This concept is also referred to as Maximal Invariant in some previous literature, such as (Kallenberg et al., 2017; Bloem-Reddy & Teh, 2020). Definition 3.4. (Invariant Sufficient Representation) For a group G of actions on any (s, v) \u2208 S\u00d7V, we say M : S \u00d7 V \u2192 M is a invariant sufficient representation for space S \u00d7 V, if it satisfies: If M(s1, v1) = M(s2, v2), then (s2, v2) = g \u00b7 (s1, v1) for some g \u2208 G; otherwise, there is no such g that satisfies (s2, v2) = g \u00b7 (s1, v1).\nClearly, the invariant sufficient representation M serves as the sufficient statistic for (S, V ). Furthermore, if the conditional distribution P (Y |S, V ) is invariant to transformations induced by the group G, we can establish that M(S, V ) is an adequate statistic for (S, V ), as stated in Corollary 3.6. In other words, M(S, V ) can be considered to encompass all the relevant information for predicting the label given (S, V ) while eliminating the redundant information about G. Hence, we can construct models that learn the relationship between M(S, V ) and Y, ultimately resulting in an invariant function Y = f(S, V ) under the group G. From a probabilistic standpoint, this implies that P (Y |S, V ) = P (Y |M(S, V )).3.4 CHARACTERIZING THE MODEL STRUCTURE  Hence, by computing the invariant sufficient representations of (S, V ), we can construct a G-invariant layer. This idea can give rise to the following theorem: Theorem 3.5. Consider a measurable group G acting on S \u00d7 V. Suppose we select an invariant sufficient representation denoted as M : S \u00d7V \u2192 M. In this case, P (Y |S, V ) satisfies Property 3.1 if and only if there exists a measurable function denoted as f : [0, 1] \u00d7 S \u00d7 V \u2192 Y such that the following equation holds:\n(S, V, Y ) =a.s. ( S, V, f(\u03be,M(S, V )) ) where \u03be \u223c Unif[0, 1] and \u03be\u22a5\u22a5(S, V );. (3)\nIn this context, the variable \u03be represents generic noise, which can be disregarded when focusing solely on the model structure rather than the complete training framework (Bloem-Reddy & Teh, 2020; Ou et al., 2022). Consequently, the theorem highlights the necessity of characterizing the neural networks in the form of f(M(S, V )). Moreover, Theorem 3.5 implies that the invariant sufficient representation M(S, V ) also serves as an adequate statistic. This can be illustrated as follows: P (Y \u2208 \u2022 |S = s, V = v) = P (Y \u2208 \u2022 |S = s, V = v,M(S, V ) = m) = P (Y \u2208 \u2022 |M(S, V ) = m). To provide additional precision and clarity, we present the following corollary, which demonstrates that M(S, V ) is an adequate statistic of (S, V ) for Y. Corollary 3.6. Let G be a compact group acting measurably on standard Borel spaces S \u00d7 V, and let M be another Borel space. Then Any invariant sufficient representation M : S \u00d7 V \u2192 M under G is an adequate statistic of (S, V ) for Y.3.5 IMPLEMENTATION  In theory, invariant sufficient representations can be computed by selecting a representative element for each orbit under the group G. However, this approach is impractical due to the high dimensions of the input space and the potentially enormous number of orbits. Instead, in practice, a neural network can be employed to approximate this process and generate the desired representations (Zaheer et al., 2017; Bloem-Reddy & Teh, 2020), particularly in tasks involving sets or set-like structures.\nHowever, the approach to approximating such a representation for (S, V ) under G remains unclear. To simplify the problem, we can divide the task of finding the invariant sufficient representation of S and V under H and R, respectively, as defined in Section 3.2. This concept is guaranteed by the following proposition: Proposition 3.7. Assuming that Ms : S \u2192 S1 and Mv : V \u2192 V1 serve as invariant sufficient representations for S and V with respect to H and R, respectively, then there exist maps f : S1 \u00d7 V1 \u2192 M that establish the invariant sufficient representation of M.\nProposition 3.7 specifically states that we can construct the invariant sufficient representations for S and V individually, as they are comparatively easier to construct compared to M(S, V ). In the work of (Bloem-Reddy & Teh, 2020), it is demonstrated that for S under H, the empirical measure Ms(S) = \u2211 si\u2208S \u03b4(si) can be chosen as a suitable invariant sufficient representation. Here, \u03b4(si) represents an atom of unit mass located at si, such as one-hot embeddings. Additionally, leveraging the proposition established by Zaheer et al. (2017), we can employ \u03c1 \u2211 s\u2208S \u03d5(s) to approximate the empirical measure. This approximation offers a practical and effective approach to constructing the invariant sufficient representation. Proposition 3.8. If f is a valid permutation invariant function on S, it can be approximated arbitrarily close in the form of f(S) = \u03c1 (\u2211 s\u2208S \u03d5(s) ), for suitable transformations \u03d5 and \u03c1.\nDuring the implementation, an encoder \u03d5 is utilized to generate embeddings for each element. For example, when dealing with sets of images, ResNet can be employed as the encoder. On the other hand, \u03c1 can represent various feedforward networks, such as fully connected layers combined with nonlinear activation functions. Similarly, for the symmetric group R acting on V, Maron et al. (2020) has demonstrated that the universal approximators of the invariant sufficient representations are\u2211\nS\u2208V \u2211 s\u2208S \u03d5(s), which is equivalent to \u2211\nxj\u2208V \u03d5(xj). Hence, for neural subset selection tasks, when considering a specific subset S \u2208 V, The neural network construction is outlined as follows:\n\u03b8(S, V ) = \u03c3 ( \u03b81( ni\u2211 i=1 \u03d5(xi)) + \u03b82 ( n\u2211 i=1 \u03d5(xj) )), (4)\nHere, the feed-forward modules \u03b81 and \u03b82 are accompanied by a non-linear activation layer denoted by \u03c3. Intuitively, the inherent simplicity of the structure enables us to utilize the DeepSet module to process all elements in V and integrate them with the invariant sufficient representations of S. In practice, there are different ways to integrate the representation of V into the representation of S, such as concatenation (Qi et al., 2017a) or addition (Maron et al., 2020). Although this idea is straightforward, in the following section, we will demonstrate how this modification significantly enhances the performance of baseline methods. Notably, this idea has been empirically utilized in previous works, such as (Qi et al., 2017a;b). However, we propose it from a probabilistic invariant perspective. A corresponding equivariant framework was also introduced in Wang et al. (2020), which complements our results in the development of deep equivariant neural networks.4 EXPERIMENTS  The proposed methods are assessed across multiple tasks, including product recommendation, set anomaly detection, and compound selection. To ensure robustness, all experiments are repeated five times using different random seeds, and the means and standard deviations of the results are reported. For additional experimental details and settings, we provide comprehensive information in Appendix E.\nEvaluations. The main goal of the following tasks is to predict the corresponding S\u22c6 given V. Therefore, we evaluate the methods using the mean Jaccard coefficient (MJC) metric. Specifically,\nfor each data sample (S\u22c6, V ) if the model\u2019s prediction is S\u2032, then the Jaccard coefficient is given as: JC(S\u22c6, S\u2032) = |S\n\u22c6\u2229S\u2032| |S\u22c6\u222aS\u2032|. Therefore, the MJC is computed by averaging JC metric over all samples in\nthe test set.\nBaselines. To show our method can achieve better performance on real applications, we compare it with the following methods:\n\u2022 Random. The results are calculated based on random estimates, which provide a measure of how challenging the tasks are.\n\u2022 PGM (Tschiatschek et al., 2018). PGM is a probabilistic greedy model (PGM) solves optimization Problem 1 with a differentiable extension of greedy maximization algorithm. In our paper, we leverage the results of PGD conducted on various datasets as reported in the study by (Ou et al., 2022).\n\u2022 DeepSet (Zaheer et al., 2017). Here, we use DeepSet as a baseline by predicting the probability of which instance should be in S\u22c6, i.e., learn an invariant permutation mapping 2V 7\u2192 [0, 1]|V |. It serves as the backbone in EquiVSet to learn set functions, and can also be employed as a baseline.\n\u2022 Set Transformer (Lee et al., 2019a). Set Transformer, compared with DeepSet, goes beyond by incorporating the self-attention mechanism to account for pairwise interactions among elements. This will make models to capture dependencies and relationships between different elements.\n\u2022 EquiVSet (Ou et al., 2022). EquiVSet uses an energy-based model (EBM) to construct the set mass function P (S|V ) from a probabilistic perspective, i.e, they mainly focus on learning a distribution P (S|V ) monotonically growing with the utility function F (S, V ). This requires to learn a conditional distribution P (Y |S, V ) as approximation distribution. Actually, their framework is to approximate symmetric F (S) instead of symmetric F (S, V ).4.1 PRODUCT RECOMMENDATION  The task requires models to recommend the most interested subset for a customer given 30 products in a category. We use the dataset (Gillenwater et al., 2014a) from the Amazon baby registry for this experiment, which includes many product subsets chosen by various customers. Amazon classifies each item on a baby registry as being under one of several categories, such as \u201cHealth\u201d and \u201cFeeding\u201d. Moreover, each product is encoded into a 768-dimensional vector by the pre-trained BERT model based on its textual description. Table 1 reports the performance of all the models across different categories. Out of the twelve cases evaluated, INSET performs best in ten of them, except for Furniture and Safety tasks. The discrepancy in performance can be attributed to the fact that our method is built upon the EquiVSet framework, with the main modification being the model structure for modeling F (S, V ). Consequently, when EquiVSet performs poorly, it also affects the performance of INSET. Nonetheless, it is worth noting that INSET consistently outperforms EquiVSet and achieves significantly better results than other baselines in the majority of cases. The margin of improvement is substantial, demonstrating the effectiveness and superiority of INSET.4.2 SET ANOMALY DETECTION  We conduct set anomaly detection tasks on three real-world datasets: the double MNIST (Sun, 2019), the CelebA (Liu et al., 2015b) and the F-MNIST (Xiao et al., 2017). Each dataset is divided into the training, validation, and test sets with sizes of 10,000, 1,000, and 1,000, respectively. For each dataset, we randomly sample n \u2208 {2, 3, 4} images as the OS oracle S\u2217. The setting is followed by (Zaheer et al., 2017; Ou et al., 2022). Let\u2019s take CelebA as an example. In this case, the objective is to identify anomalous faces within each set solely through visual observation, without any access to attribute values. The CelebA dataset comprises 202,599 face images, each annotated with 40 boolean attributes. When constructing sets, for every ground set V, we randomly choose n images from the dataset to form the OS Oracle S\u2217, ensuring that none of the selected images contain any of the two attributes. Additionally, it is ensured that no individual person\u2019s face appears in both the training and test sets. Regarding Table 2, it is evident that our model demonstrates a substantial performance advantage over all the baselines. Specifically, in the case of Double MNIST, our model shows a remarkable improvement of 23% compared to EquiVSet, which itself exhibits the best performance among all the baselines considered. This significant margin of improvement highlights the superior capabilities of our model in tackling the given task.4.3 COMPOUND SELECTION IN AI-AIDED DRUG DISCOVERY  The screening of compounds with diverse biological activities and satisfactory ADME (absorption, distribution, metabolism, and excretion) properties is a crucial stage in drug discovery tasks (Li et al., 2021; Ji et al., 2022; Gimeno et al., 2019). Consequently, virtual screening is often a sequential filtering procedure with numerous necessary filters, such as selecting diverse subsets from the highly active compounds first and then removing compounds that are harmful for ADME. After several filtering stages, we reach the optimal compound subset. However, it is hard for neural networks to learn the full screening process due to a lack of intermediate supervision signals, which can be very expensive or impossible to obtain due to the pharmacy\u2019s protection policy. Therefore, the models are supposed to learn this complicated selection process in an end-to-end manner, i.e., models will predict S\u2217 only given the optimal subset supervision signals without knowing the intermediate process. However, this is out of the scope of this paper, since the task is much more complex and requires extra knowledge, and thus we leave it as future work.\nTo simulate the process, we only apply one filter: high bioactivity to acquire the optimal subset of compound selection following (Ou et al., 2022). We conduct experiments using the following datasets: PDBBind (Liu et al., 2015a) and BindingDB (Liu et al., 2007). Table 2 shows that our method performs better than the baselines and significantly outperform the random guess, especially on the BindingDB dataset. Different from the previous tasks, the performance of these methods is closer to each other. That is\nbecause the structure of complexes (the elements in a set) can provide much information for this task. Thus, the model could predict the activity value of complexes well without considering the interactions between the optimal subset and the complementary. However, our method can still achieve more satisfactory results than the other methods.4.4 COMPUTATION COST  The main difference between INSET and EquiVSet is the additional information-sharing module to incorporate the representations of V. A possible concern is that the better performance of INSET might come from the extra parameters instead of our framework proposed. To address this concern, we conducted experiments on CelebA datasets. We add an additional convolution layer in the encoders to improve the capacity of EquiVSet. According to the location and size, we propose two variants of EquiVSet, details can be found in the appendix. We report the performance of models with different model sizes in Table 3. It is evident that INSET surpasses all the variants of EquiVSet, clearly demonstrating superior performance. Notably, the improvement achieved through the parameters is considerably less significant when compared to the substantial improvement resulting from the information aggregation process. This highlights the crucial role of information aggregation in driving the overall performance enhancement of INSET.4.5 PERFORMANCE VERSUS TRAINING EPOCHS  In addition to the notable improvement in the final MJC achieved by INSET, we have also observed that incorporating more information from the superset leads to enhanced training speed and better overall performance. To illustrate this, we present two figures depicting the validation performance against the number of training epochs for the Toys and Diaper datasets. It is evident that INSET achieves favorable performance in fewer training epochs. For instance, on the Toy dataset, INSET reaches the best performance of EquiVSet, at approximately epoch 18. Furthermore, around epoch 25, INSET approaches its optimal performance, while EquiVSet and Set Transformer attain their best performance around epoch 40. This highlights the efficiency and effectiveness of INSET in achieving competitive results within a shorter training time.5 CONCLUSION  In this study, we have identified a significant limitation in subset encoding methods, such as neural subset selection, where the output is either the subset itself or a function value associated with the subset. By incorporating the concept of permutation invariance, we reformulate this problem as the modeling of a conditional distribution P (Y |S, V ) that adheres to Property 3.1. Our theoretical analysis further reveals that to accomplish this objective, it is essential to construct a neural network based on the invariant sufficient representation of both S and V. In response, we introduce INSET, a highly accurate and theoretical-driven approach for neural subset selection, which also consistently outperforms previous methods according to empirical evaluations.\nLimitations and Future Work. INSET is a simple yet effective method in terms of implementation, indicating that there is still potential for further improvement by integrating additional information, such as pairwise interactions between elements. Furthermore, our theoretical analysis is not limited to set-based tasks; it can be applied to more general scenarios with expanded definitions and theoretical contributions. We acknowledge that these potential enhancements and extensions are left as future work, offering opportunities for further exploration and development. \\\n            \n            Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$\\assistant\n\n$$ $$$\n\nPaper Summary:\nThe paper presents a novel approach for neural subset selection tasks, such as compound selection in AI-aided drug discovery, by adopting a probabilistic perspective. The authors propose an information aggregation module that incorporates background information from the superset V into the subset S from a symmetric perspective. This approach ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific superset from which the subset originated. The authors evaluate their approach on various tasks, including product recommendation, set anomaly detection, and compound selection, and demonstrate its superiority over existing methods.\n\nStrengths:\n1. The paper provides a clear and concise introduction to the problem of neural subset selection and its importance in various applications.\n2. The authors' use of a probabilistic perspective to model the relationship between the subset and superset is innovative and effective.\n3. The proposed information aggregation module is well-designed and well-explained.\n4. The paper presents comprehensive empirical evaluations on various tasks, demonstrating the effectiveness of the proposed approach.\n\nWeaknesses:\n1. The paper assumes a specific setting for the compound selection task, which may not be generalizable to other tasks.\n2. The authors do not discuss the potential limitations of their approach, such as the requirement for a large amount of labeled data.\n3. The paper could benefit from more detailed explanations of the theoretical results and their implications.\n\nSuggestions for Improvement:\n1. The authors should provide more detailed explanations of the theoretical results and their implications for other tasks.\n2. The paper could benefit from a more comprehensive discussion of the potential limitations of the proposed approach.\n3. The authors should consider exploring additional information, such as pairwise interactions between elements, to further improve the performance of their approach.\n\nRecommendation:\nI recommend that the authors revise the paper to address the weaknesses and suggestions for improvement. The proposed approach shows great promise, and with further development and evaluation, it has the potential to become a leading method for neural subset selection tasks.",
        "peer_review_with_watermark": "$$$ \n\n**Paper Summary:**  \nThe paper introduces INSET, a new method for neural subset selection that leverages permutation invariance to model the conditional distribution P (Y |S, V ) fulfilling Property 3.1. An information aggregation module is proposed to combine representations of subsets and supersets from a permutation invariance standpoint. The authors validate INSET through extensive tests across various tasks, such as product recommendation, anomaly detection in sets, and compound selection, achieving superior results compared to baseline methods.\n\n**Strengths:**  \n- The study addresses a critical challenge in subset encoding methods, highlighting the limitations of traditional approaches.\n- Theoretical underpinnings for the model are robust, tying the structure of neural networks to invariant sufficient representations of S and V.\n- The simplicity and effectiveness of INSET suggest further potential enhancements by incorporating more information.\n- Comprehensive experiments across multiple datasets demonstrate the effectiveness of the proposed method.\n\n**Weaknesses:**  \n- The connection to the Optimal Subset (OS) oracle is not sufficiently detailed, leaving some theoretical aspects unclear.\n- Limitations of the INSET framework, such as potential data requirements for boosting performance, are overlooked.\n- A full comparison with existing methodologies, like EquiVSet, is not adequately presented across all tasks.\n\n**Suggestions for Improvement:**  \n- Clarify the relationship to the Optimal Subset (OS) oracle for a better understanding of the theoretical backdrop.\n- Discuss the limitations of INSET more explicitly, particularly concerning data sufficiency for optimal performance.\n- Conduct a thorough comparison with existing methods, especially EquiVSet, to strengthen validation of the proposed approach.\n- Consider investigating how the inclusion of pairwise interactions among elements could enhance the INSET model.\n\n**Recommendation:**  \nThe paper presents an innovative approach to neural subset selection with INSET, demonstrating competitive performance in various applications. Despite some areas needing attention, such as clarifying the theoretical implications and enhancing comparisons with existing methods, the foundational work suggests that INSET is a valuable contribution to the field. We recommend that the authors address the noted improvements to solidify the impact of their work. \n$$$",
        "gr_score_with": 0.907563025210084,
        "gr_score_without": 0.9292107404393816,
        "output_without": {
            "num_tokens_scored": 4916,
            "num_green_tokens": 4568,
            "green_fraction": 0.9292107404393816,
            "z_score": 6.826958267775358,
            "p_value": 4.336695622675509e-12
        },
        "output_with": {
            "num_tokens_scored": 357,
            "num_green_tokens": 324,
            "green_fraction": 0.907563025210084,
            "z_score": 0.47633051162246487,
            "p_value": 0.3169194656918797
        }
    },
    {
        "title": "TAB: TEMPORAL ACCUMULATED BATCH NORMALIZA-",
        "abstract": "Spiking Neural Networks (SNNs) are attracting growing interest for their energyefficient computing when implemented on neuromorphic hardware. However, directly training SNNs, even adopting batch normalization (BN), is highly challenging due to their non-differentiable activation function and the temporally delayed accumulation of outputs over time. For SNN training, this temporal accumulation gives rise to Temporal Covariate Shifts (TCS) along the temporal dimension, a phenomenon that would become increasingly pronounced with layer-wise computations across multiple layers and multiple time-steps. In this paper, we introduce TAB (Temporal Accumulated Batch Normalization), a novel SNN batch normalization method that addresses the temporal covariate shift issue by aligning with neuron dynamics (specifically the accumulated membrane potential) and utilizing temporal accumulated statistics for data normalization. Within its framework, TAB effectively encapsulates the historical temporal dependencies that underlie the membrane potential accumulation process, thereby establishing a natural connection between neuron dynamics and TAB batch normalization. Experimental results on CIFAR-10, CIFAR-100, and DVS-CIFAR10 show that our TAB method outperforms other state-of-the-art methods.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it.  Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$user\n\nThe peer review format and length should be of standard conference. \\\n            title: TAB: TEMPORAL ACCUMULATED BATCH NORMALIZA- \\\n            abstract: Spiking Neural Networks (SNNs) are attracting growing interest for their energyefficient computing when implemented on neuromorphic hardware. However, directly training SNNs, even adopting batch normalization (BN), is highly challenging due to their non-differentiable activation function and the temporally delayed accumulation of outputs over time. For SNN training, this temporal accumulation gives rise to Temporal Covariate Shifts (TCS) along the temporal dimension, a phenomenon that would become increasingly pronounced with layer-wise computations across multiple layers and multiple time-steps. In this paper, we introduce TAB (Temporal Accumulated Batch Normalization), a novel SNN batch normalization method that addresses the temporal covariate shift issue by aligning with neuron dynamics (specifically the accumulated membrane potential) and utilizing temporal accumulated statistics for data normalization. Within its framework, TAB effectively encapsulates the historical temporal dependencies that underlie the membrane potential accumulation process, thereby establishing a natural connection between neuron dynamics and TAB batch normalization. Experimental results on CIFAR-10, CIFAR-100, and DVS-CIFAR10 show that our TAB method outperforms other state-of-the-art methods. \\\n            paper text: 1 INTRODUCTION  Spiking Neural Networks (SNNs) are known to be biologically inspired artificial neural networks (ANNs) and have recently attracted great research interest (Chowdhury et al., 2022; Ding et al., 2022). The attraction of SNNs lies in their ability to deliver energy-efficient and fast-inference computations when implemented on neuromorphic hardware such as Loihi (Davies et al., 2018) and TrueNorth (Akopyan et al., 2015; DeBole et al., 2019). These advantages arise from the fact that SNNs utilize spikes to transmit information between layers, whereby the networks circumvent multiplication during inference (Roy et al., 2019). However, the discrete and non-differentiable nature of the binary firing functions makes it difficult to directly train deep SNNs. ANN-to-SNN conversion (Diehl et al., 2015; Bu et al., 2022; Jiang et al., 2023) and directly training with surrogate gradients back-propagation (Neftci et al., 2019; Deng et al., 2022; 2023) are two typical solutions.\nBatch Normalization (BN) has found extensive use in ANNs and has seen tremendous success in boosting their performance by reducing the internal covariate shift (ICS) and flattening the loss landscape (Ioffe & Szegedy, 2015; Santurkar et al., 2018). In ANNs, ICS refers to changes in the distribution of layer inputs caused by updates of preceding layers, while in SNNs, the Temporal Covariate Shift (TCS) phenomenon (Duan et al., 2022) has been identified due to updates of preceding layers and prior time-steps, which transpires along the additional temporal dimension. Within SNNs, synaptic currents are sequentially fed into spiking neurons, with spike-triggered asynchronous currents accumulating in the membrane potential. Whenever this accumulated membrane potential exceeds a threshold, a spike is generated. This temporal dependency on membrane accumulation\n\u2217Corresponding authors. Codes are available at https://github.com/HaiyanJiang/SNN-TAB.\nhas the potential to amplify the internal covariate shift across the temporal domain. The intertwining of this temporal dependency with the TCS phenomenon, presents a significant challenge in direct training of SNNs especially for the integration of BN techniques into SNNs.\nWhen it comes to BN techniques for SNNs, only a few methods have been proposed. These methods either normalize data jointly by aggregating data across the temporal dimension or perform independent normalization at each discrete time-step. For example, Kim & Panda (2021) conducts independent batch normalization separately at each time-step. However, this approach uses separate sets of mean, variance, and scale and shift parameters at each time-step, failing to account for the temporal dependencies of the input spikes. While Zheng et al. (2021) merges the data along the time dimension and utilizes shared batch statistics across all time-steps for normalization. Nonetheless, introducing such overall statistics may limit the flexibility to capture varying temporal characteristics at different time-steps. On the other hand, Duan et al. (2022) attempts to tackle the TCS issue by assigning different weights to each time-step, while still utilizing shared batch statistics across all time-steps for normalization. Although these methods improve upon the performance of the SNN models, they do not significantly address the alignment with the neuron dynamics, i.e., the membrane accumulation dependency, or provide a potential to do so.\nIn this paper, we propose TAB (Temporal Accumulated Batch Normalization) as a solution to effectively address these challenges by closely aligning with the neuron dynamics, specifically the accumulated membrane potential, and providing more accurate batch statistics. This alignment establishes a natural connection between neuronal dynamics and batch normalization in SNNs. Neuron dynamics refer to the changes in the membrane potential of a neuron over time as it integrates input signals and generates spikes. Here, \u201caligning with neuron dynamics\u201d means that TAB is tailored to mimic or capture neurons\u2019 behavior as closely as possible, normalizing data in line with the temporal dependencies and information accumulation within neurons. This alignment ensures that TAB\u2019s normalization process corresponds well with how neurons naturally operate in SNNs, thus leading to improved performance by addressing the temporal covariate shift problem.2 BACKGROUND  2.1 RELATED WORK  SNN Learning Methods. Many works have recently emerged and focused on the supervised training of SNNs (Wu et al., 2021a; Zhou et al., 2021; Meng et al., 2022; Xiao et al., 2021). These SNN learning methods can be mainly categorized into two classes: ANN-to-SNN conversion (Diehl et al., 2015; Deng & Gu, 2021; Ding et al., 2021; Han et al., 2020; Li et al., 2021a; Bu et al., 2022; Hao et al., 2023; Lv et al., 2023) and end-to-end training with back-propagation (Fang et al., 2021; Zhang & Li, 2020; Deng et al., 2022; Xiao et al., 2022; Guo et al., 2022; Meng et al., 2023). ANN-to-SNN conversion takes a pre-trained ANN and converts it into an SNN by preserving the weights and replacing the ReLU activation function with a spiking activation function. This approach can be efficient in obtaining an SNN since the ANN has already been trained and the weights can be directly copied to the SNN. However, the resulting performance of the converted SNN may not be as good as that of the original source ANN. It usually requires a large number of time-steps for the converted SNN to achieve performance comparable to the source ANN. Direct end-to-end training usually employs the surrogate gradients (Wu et al., 2018; 2019; Neftci et al., 2019; Zheng et al., 2021; Eshraghian et al., 2021) method to overcome the non-differentiable nature of the binary spiking function to directly train SNNs from scratch. This method can yield comparable performance to that of traditional ANNs with a few time-steps.\nBN Method in ANNs. Batch normalization methods have significantly contributed to the success of ANNs by boosting their learning and inference performance (Ioffe & Szegedy, 2015; Xiong et al., 2020; Bjorck et al., 2018). BN is a technique used to stabilize the distribution (over a mini-batch) of inputs to each network layer during training. This is achieved by introducing additional BN layers which set the first two moments (mean and variance) of the activation distribution to zero and one. Then, the batch-normalized inputs are scaled and shifted using learnable/trainable parameters to preserve model expressiveness. This normalization is performed before the non-linearity is applied. The BN layer can be formulated as,\nBN(xi) = \u03b3x\u0302i + \u03b2, x\u0302i = xi \u2212 \u00b5\u221a \u03c32 + \u03f5, i = 1, \u00b7 \u00b7 \u00b7, b.\nThe mini-batch mean \u00b5 and variance \u03c32 are computed by \u00b5 = 1b \u2211b i=1 xi and \u03c3 2 = 1b \u2211b i=1(xi\u2212\u00b5)2.\nBN Method in SNNs. Due to the additional temporal dimension, several recent studies have proposed modifications to batch normalization to fit the training of SNNs. The threshold-dependent Batch Normalization (tdBN) method (Zheng et al., 2021) is introduced to alleviate the gradient vanishing or explosion during training SNNs. The tdBN utilizes shared BN statistics and parameters (as the conventional BN) by merging the data along the temporal dimension. Similar to tdBN, the TEBN method (Duan et al., 2022) employs shared BN statistics by merging the data along the temporal dimension, then scales using different weights to capture temporal dynamics. Different from them, BNTT (Kim & Panda, 2021) uses separate BN statistics and parameters at each time-step t independently, however, it ignores the temporal dependencies of the input spikes. Differently, our TAB method leverages the accumulated pre-synaptic inputs in the temporal domain, which is in alignment with the membrane potential accumulation in the LIF model.2.2 SPIKING NEURON DYNAMICS AND NEURON MODEL  SNNs use binary spike trains to transmit information between layers. Each neuron maintains its membrane potential dynamics ui(t) over time, \u201cintegrates\u201d the received input with a leakage (much like an RC circuit), and fires a spike if the accumulated membrane potential value exceeds a threshold. We adopt the widely used leaky-integrate-and-fire (LIF) model. Neuron dynamics refer to the changes in the membrane potential of a neuron over time as it integrates input signals and generates spikes, which can be formulated as a first-order differential equation (ODE),\nLIF Neuron Dynamics: \u03c4 dui(t)\ndt = \u2212ui(t) +RIi(t), ui(t) < Vth, (1)\nwhere Ii(t) is the injected input current to the i-th neuron at time t, ui(t) is the membrane potential of the i-th neuron at time t in the current layer, Vth is the membrane threshold, and \u03c4 denotes the membrane time constant, and R denotes the resistor. For numerical simulations of LIF neurons, we consider a discrete version of the neuron dynamics. Similar to Wu & He (2018), the membrane potential ui[t] of the i-th neuron at time-step (discrete) t is represented as:\nui[t] = \u03bbui[t\u2212 1] + \u2211\nj\u2208pre(i)\nWijoj [t]. (2)\nWe adopt a simple current model RIi[t] = \u2211\nj\u2208pre(i) Wijoj [t], with R absorbed in weights Wij. Here, oi[t] denotes the binary spike of neuron i at time-step [t], taking a value of 1 when a spike occurs and 0 otherwise. The index j refers to pre-synaptic neurons. The membrane potential ui[t] increases with the summation of input spikes from all the pre-synaptic neurons pre(i) connecting the current i-th neuron through synaptic weight Wij. It also decreases with a leak factor \u03bb (0 < \u03bb \u2a7d 1), where \u03bb and the time constant \u03c4 are related by \u03bb = e\u2212 \u2206t \u03c4. The discrete LIF model degenerates to the IF model when \u03bb = 1, therefore in the following, we only use the LIF model with 0 < \u03bb \u2a7d 1. When the neuron\u2019s membrane potential ui[t] exceeds the threshold Vth, the neuron will fire a spike with oi[t] = 1 and then reset the membrane potential to 0. By combining the sub-threshold dynamics Eq. (2) and hard reset mechanism, the whole iterative LIF model can be formulated by:\nDiscrete LIF Neuron Model: ui[t] = \u03bbui[t\u2212 1](1\u2212 oi[t\u2212 1]) + \u2211\nj\u2208pre(i)\nWijoj [t], (3)\noi[t] = H(ui[t]\u2212 Vth), (4) where H(x) is the Heaviside step function, i.e., the non-differentiable spiking activation function. H(x) = 1 if x > 0 and H(x) = 0 otherwise.3 PROPOSED TAB METHOD  In this section, we will present our TAB method. We begin by introducing the Temporal Dependencies and Temporal Covariate Shift in SNNs which motivate our method. Following this, we introduce our TAB method, which addresses these challenges. Finally, we establish a theoretical connection between the neural dynamics and the TAB method by deriving the closed-form solution of LIF dynamics ODE.3.1 MOTIVATION: TEMPORAL DEPENDENCIES AND TEMPORAL COVARIATE SHIFT  Temporal dependencies in SNNs arise naturally from the sequential nature of spike events, where synaptic currents (also known as spike trains) are sequentially fed into spiking neurons, playing a pivotal role in capturing the dynamic evolution of input spikes over time. These networks model the dynamics of biological neurons through ODEs and utilize spikes to transmit information (Eshraghian et al., 2021). In SNNs, each neuron maintains a membrane potential, continuously \u2018integrating\u2019 and accumulating received spikes over time. It emits a spike only when its accumulated membrane potential exceeds a threshold, remaining inactive otherwise in the current time-step (Li et al., 2021a). This process highlights the intrinsic influence of temporal dynamics on the temporally delayed accumulation of the membrane potential. We refer to this accumulation dependency over the time dimension as temporal dependencies.\nIn SNNs, a phenomenon known as Temporal Covariate Shift (TCS) has been identified (Duan et al., 2022), which represents ICS (Internal Covariate Shift) (Ioffe & Szegedy, 2015) across the additional temporal dimension, and it refers to changes in the distribution of layer inputs caused by updates of preceding layers, and prior time-steps. Within the framework of SNNs, synaptic currents are sequentially fed into spiking neurons, and spike-triggered asynchronous currents are accumulated into the membrane potential which will trigger a spike when it exceeds the membrane threshold. This temporal dependency on membrane potential accumulation intensifies the internal covariate shift along the temporal domain. This temporal dependency, together with the TCS phenomenon, presents a significant challenge when integrating BN techniques into SNNs.\nOur motivation comes along these lines, how to perform batch normalization in training of SNNs, but keeping in mind the temporal dependency of the data, as well as the temporal covariate shift. A simple, yet elegant, method that aligns closely with this underlying neuron dynamics comes with Temporal Accumulated Batch normalization (TAB). Generally speaking, our TAB method addresses the temporal covariate shift issue by aligning with the inherent temporal dependencies in SNNs. Fig. 1 illustrates the temporal dependencies and neuron dynamics and showcases the involvement of our proposed TAB method.\nNeuronal dynamics refers to the change in membrane potential over time as a neuron integrates input signals and generates spikes. This temporal accumulation of the membrane potential in SNNs enables neurons to process input data by taking into account both past and current time-steps (with no access to future information beyond t), and the TAB method aligns closely with this underlying neuron dynamics and alleviates the TCS issue.3.2 TEMPORAL ACCUMULATED BATCH NORMALIZATION (TAB)  To address the temporal covariate shift issue and to model the temporal distributions in SNNs, our TAB method aligns with the inherent temporal dependencies by utilizing the temporal accumulated batch statistics (\u00b51:t, \u03c321:t) over an expanding window [1, t]. To achieve this, we establish the relationship between the expectations and variances across accumulated time-steps (\u00b51:t, \u03c321:t) and those of the\nsingle time-step (\u00b5[t], \u03c32[t]), as follows:\n\u00b51:t = 1\nt t\u2211 s=1 \u00b5[s], \u03c321:t = 1 t t\u2211 s=1 \u03c32[s]. (5)\nOur proposed TAB method utilizes Temporal Accumulated Statistics (\u00b51:t, \u03c321:t) for data normalization, and then assigns different learnable weights \u03c9[t] > 0 to each time-step to distinguish their effect on the final result. The TAB method is given by\nx\u0302i[t] = TAB(xi[t]) = \u03c9[t] ( \u03b3[t]\nxi[t]\u2212 \u00b51:t\u221a \u03c321:t + \u03f5 + \u03b2[t]\n) = \u03b3\u0302[t]\nxi[t]\u2212 \u00b51:t\u221a \u03c321:t + \u03f5 + \u03b2\u0302[t], \u03c9[t] > 0. (6)\nGiven the pre-synaptic inputs xl[t] to layer l at time-step t, the spiking neuron with TAB is as follows,\nxl[t] = W lol\u22121[t], (7)\nul[t] = \u03bbul[t\u2212 1](1\u2212 ol[t\u2212 1]) + x\u0302l[t], (8)\nwhere x\u0302l[t] = TAB(xl[t]) = \u03b3\u0302[t] xl[t]\u2212 \u00b51:t\u221a\n\u03c321:t + \u03f5 + \u03b2\u0302[t]. (9)\nHere ul[t] and ol[t] denote the membrane potential and binary spike outputs of all neurons in l-th layer at time-step t, and W l denotes the synaptic weights between layer l \u2212 1 and layer l. We assign different positive weights \u03c9l[t] > 0 to each time-step which is different from Deng et al. (2022) and \u03b3\u0302[t] = \u03c9[t]\u03b3[t], \u03b2\u0302[t] = \u03c9[t]\u03b2[t]. The weights \u03c9[t] and parameters \u03b3l[t],\u03b2l[t] are learnable, which are trained during the training process. For details, refer to?? and??. Refer to?? for the learning rules to compute the gradients.\nComputation of the temporal accumulated statistics is dynamically performed, in a moving averaging fashion, without the need to store batch data from all previous time-steps. This not only saves memory, but is also an important feature of our novel approach. For the algorithm details of the TAB method, please refer to?? in the Appendix.\nThe rationale behind employing this accumulated spatial-temporal information in TAB comes from the sequential processing and temporal dependency characteristics intrinsic to spiking neurons. The TAB method utilizes the accumulated batch statistics (\u00b51:t, \u03c321:t) over an expanding window [1, t]. Fig. 2 illustrates an overview of four typical BN methods used in SNNs: default BN (Ioffe & Szegedy, 2015), BNTT (Kim & Panda, 2021), tdBN (Zheng et al., 2021), and TEBN (Duan et al., 2022). A comprehensive overview of statistics and parameters used by these methods is summarized in?? in the??.\nAs shown in??, BNTT (Kim & Panda, 2021) considers BN statistics at each time-step individually and calculates different BN statistics (\u00b5[t], \u03c32[t]) and BN parameters (\u03b3[t]) at each time-step, which ignores the temporal dependencies of the input spikes. In contrast, tdBN (Zheng et al., 2021) computes the same overall BN statistics (\u00b51:T, \u03c321:T ) and BN parameters (\u03b3, \u03b2) across all time-steps, but overlooking the temporal differences. Similarly, TEBN (Duan et al., 2022) employs the same overall BN statistics (\u00b51:T, \u03c321:T ) as tdBN, but introduces distinct weight parameters p[t] at each timestep to capture time-specific variations. However, both tdBN and TEBN, computing BN statistics over T time-steps, implicitly assume access to data from all T time-steps, that is, even if the current time-step is t < T, future information up to time-step T can also be obtained, which is not true for the temporal accumulation of membrane potential nor the neural dynamics. As illustrated in Fig. 2, the input statistics of tdBN and TEBN consider the statistics of all the time-steps and all effective batches, while BNTT considers BN statistics at each time-step. Despite these differences, none of the existing methods have addressed the alignment with the membrane potential accumulation.3.3 THEORETICAL CONNECTION BETWEEN TAB METHOD AND THE NEURAL DYNAMICS  TAB is tailored to capture the temporal dependencies of neurons as closely as possible by aligning with the neuron dynamics. To explore the theoretical connection between the TAB method and the neural dynamics, we need to delve into the LIF dynamics from the perspective of differential equations. In SNNs, each neuron maintains the dynamics of its membrane potential U(t) over time,\nby \u201cintegrating\u201d the received input current I(t) with a leakage term until a spike is triggered. This is described as a first-order linear differential equation (ODE),\nNeuron Dynamics as an ODE: \u03c4 dU(t)\ndt = \u2212U(t) +RI(t), U(t) < Vth, (10)\nwhere I(t) represents the input current injected into the neuron at time t, and it is a function of t (note that I(t) is not a constant value). The closed-form solution of the LIF neuron dynamics (as an ODE) can be derived with analytical and theoretical methods. Additional details are available in?? and??. Lemma 1. The analytical closed-form solution for the first-order IVP (Initial Value Problem) of the LIF dynamics ODE is as follows (Gerstner et al., 2014),\nU(t) = exp ( \u2212 t \u03c4 )(\u222b t 0 R \u03c4 I(s)exp ( s \u03c4 ) ds+ U0 ). (11)\nRemark 1. When the neuron initiates at the value U0 with no further input, i.e., I(t) = 0, the closed-form solution of the ODE Eq. (11) shows that the membrane potential U(t) will start at U0 and exponentially decay with a time constant \u03c4, U(t) = U0exp ( \u2212 t\u03c4 ). Consequently, we can determine the membrane potential ratio, often referred to as the leak factor, denoted by \u03bb, as \u03bb = U(t+\u2206t)U(t) = U0exp(\u2212 t+\u2206t\u03c4 ) U0exp(\u2212 t\u03c4 ) = exp ( \u2212\u2206t\u03c4 ). This relationship enables us to formulate the discretization scheme as: U [t+ 1] = \u03bbU [t].\nThis remark provides insights into the behavior of the membrane potential in the absence of input and establishes the discretization principle used for LIF modeling. Lemma 2. Through applying integration by parts, we derive another equivalent form of the closedform solution for the LIF dynamics, denoted as:\nU(t) = exponential decay term\ufe37 \ufe38\ufe38 \ufe37 (U0 \u2212RI0)exp ( \u2212 t \u03c4 ) + input current model\ufe37 \ufe38\ufe38 \ufe37 RI(t)\ufe38 \ufe37\ufe37 \ufe38\ncommonly considered in the discrete LIF model\n\u2212 \u222b t 0 Rexp ( s\u2212 t \u03c4 ) dI(s)\ufe38 \ufe37\ufe37 \ufe38\nabsent in the discrete LIF model\n. (12)\nWith the application of the Riemann\u2013Stieltjes integral, the discretization version of the closed-form solution is represented as:\nU [t] = (U0\u2212RI0)exp(\u2212 t\u03c4 )\ufe37 \ufe38\ufe38 \ufe37 \u03bbU [t\u2212 1] + WO[t]=RI[t]\ufe37\ufe38\ufe38\ufe37 X[t] \u2212 n\u2211 i=0\ngiX[si]\ufe38 \ufe37\ufe37 \ufe38 TAB method. (13)\nIn this formulation Eq. (13), the first exponential decay term, \u03bbU [t \u2212 1], captures the temporal dependency of the membrane potential from the preceding time-step. The second term, a simple current input model, RI[t] = WO[t], incorporates spikes from the pre-connected neurons at the current time-step [t]. Significantly, the third term, representing the temporal accumulated input across all previous time-steps through a weighted sum of the input currents X[si] with associated weights gi, introduces a novel concept. Here 0 = s0 < \u00b7 \u00b7 \u00b7 < si < \u00b7 \u00b7 \u00b7 < sn = t denotes a partition of the time interval [0, t] with a finite sequence of numbers. Refer to?? for the details. Importantly, note that this accumulation mechanism of the inputs is a foundational component of the TAB method, providing a link that connects the TAB method and the neural dynamics. Remark 2. The commonly used discrete LIF model in Eq. (2), as denoted by U [t] = \u03bbU [t\u22121]+X[t], is derived from the first two terms of the discretization version of the closed-form solution Eq. (13). The third term, representing the temporal accumulated input across all previous time-steps, however, is not incorporated into the discrete LIF models typically used in practice. Remark 3. Note that the recursive application of the discrete LIF model, as denoted by U [t] = \u03bbU [t \u2212 1] + X[t], yields the temporal evolution of the membrane potential as U [t] = \u03bbtU [0] +\u2211t\ns=1 \u03bb t\u2212sX[s]. This result shows the temporal dependency of the membrane potential accumulation\nin LIF neuron dynamics.\nRecalling the TAB method introduced in Sect. 3.2, our TAB method normalizes data utilizing temporal accumulated batch statistics (\u00b51:t, \u03c321:t) across an expanding window [1, t], where \u00b51:t and \u03c321:t represent the temporal accumulated information up to time-step [t]. The utilization of the temporal accumulated batch statistics aligns well with the accumulation mechanism of the membrane potentialthrough Eq. (13). Consequently, it alleviates the temporal covariate shift issue which refers to the changes in the distribution of layer inputs resulting from updates of preceding layers and prior time-steps. The entire TAB method procedure and membrane updates can be linked through Eq. (13), derived by solving the LIF dynamics ODE. This equation naturally connects TAB batch normalization to neuron dynamics, as evident in Eq. (13).\nUpon comparing the commonly used discrete LIF model in Eq. (2) with the discrete closed-form solution in Eq. (13), it shows that the TAB method reintroduces the accumulation term into the normalization procedure. This is achieved by using temporal accumulated batch statistics from time-step 1 to t. While the temporal accumulated batch statistics employed by the TAB method do not replicate the exact term in Eq. (13), but as an approximation. Thus, there exists no one-to-one functional mapping between the two. The adjustment within TAB method brings the discrete LIF model closer to its analytical closed-form counterpart, thus, TAB can work well in addressing the temporal covariate shift issue. This establishes a natural connection between neuron dynamics and batch normalization.4 EXPERIMENTS  In this section, we conduct extensive experiments on large-scale static and neuromorphic datasets, CIFAR-10 (Krizhevsky et al., 2009), CIFAR100 (Krizhevsky et al., 2009), and DVS-CIFAR10 (Li et al., 2017), to verify the effectiveness of our proposed TAB method. We utilize the VGG network architecture and ResNet architecture. Firstly, we perform a comparative analysis of our TAB method with other BN methods in the context of SNNs. Further, we compare our TAB method with other state-of-the-art approaches. For implementation details, refer to??.4.1 COMPARISON WITH OTHER BN METHODS  We conduct our evaluation by comparing the performance of the proposed TAB method and other batch normalization methods in the context of SNNs. To ensure fairness in our comparisons, we do not employ advanced data augmentation techniques like cutout (DeVries & Taylor, 2017). Table 1 provides a comprehensive overview of the the test accuracy on both traditional static dataset CIFAR10, CIFAR-100 and neuromorphic dataset DVS-CIFAR10. On the CIFAR-10 dataset, our TAB method demonstrates remarkable performance improvement, achieving a top-1 accuracy of 94.73% with the ResNet-19 network using only 2 time-steps. Notably, this surpasses the performance of TEBN using 6 time-steps. Furthermore, when using the same network architecture, TAB consistently outperforms other BN methods, even with fewer time-steps T. This pattern holds true for other\ndatasets as well. For instance, on the DVS-CIFAR10 dataset, our TAB method achieves 1.6% better performance (76.7% v.s. 75.10%) while utilizing fewer time-steps (4 v.s. 10) than TEBN. Similarly, on CIFAR-100, our method exhibits a 0.55% increase in accuracy (76.31% v.s. 75.86%) compared to TEBN when both use 2 time-steps. All the accuracy values for other methods reported in the table are drawn from the existing literature.4.2 COMPARISON ON LARGE-SCALE IMAGENET DATASET  In this section, we investigate the effectiveness of our TAB method on the ImageNet dataset, renowned for its extensive collection of more than 1.25 million training images and 50, 000 test images (Deng et al., 2009). The training set of ImageNet offers 1, 280 training samples for each label, and we apply standard preprocessing and augmentation techniques (He et al., 2016) to the training data. Test data is centered and cropped to dimensions of 224\u00d7 224. The evaluation employs the ResNet-34 architecture, a widely recognized model. The network is trained using the AdamW optimizer with an initial learning rate of 0.00002 and a weight decay of 0.02. Training occurs on an NVIDIA RTX A6000 with 4 GPUs, each handling a batch size of 24. To ensure unbiased statistics, we follow Zheng et al. (2021) and synchronize batch mean and variance across devices.\nThe results, presented in Tables Table 1 and??, reveal the efficacy of our TAB method. Notably, even with a modest training duration of 80 epochs for T = 4, the TAB method exhibits a 3.29% improvement on ResNet-34 over TEBN at T = 4 (TAB with 67.78% vs. TEBN 64.29%). Impressively, with only 2 time-steps (T = 2), our TAB method achieves an accuracy of 65.38% on ImageNet, showcasing its promising performance.4.3 COMPARISON WITH THE STATE-OF-THE-ART APPROACHES  In this section, we present a comprehensive comparison of our TAB method with other state-of-the-art learning methods for SNNs using CIFAR-10 as the benchmark dataset, as illustrated in Table 2.\nOn the VGG-11 architecture, our TAB method achieves an impressive accuracy of 94.73% while utilizing 4 time-steps, outperforming all the ANN-to-SNN conversion and hybrid training methods that require more time-steps. Besides, we follow TEBN (Duan et al., 2022) and adopt the cutout augmentation (DeVries & Taylor, 2017) on static datasets denoted by \u201c*\u201d in the table. Compared to other surrogate gradient methods, our TAB method consistently performs better. On ResNet-19, our TAB method achieves an accuracy of 96.09% with 6 time-steps, which is better than Dspike (94.25%), TET (94.5%), TEBN (95.6%) while using the same number of time-steps. Even when using only 2 time-steps T = 2, our TAB method on ResNet-19 achieves a higher accuracy than TEBN (Duan et al., 2022) which utilizes 6 time-steps. We contribute this elevated performance to the better representation capability of TAB, achieved by its alignment with the neuron dynamics, thereby bridging the gap between the discrete LIF model and the underlying neuron dynamics and making the two closer. For clarity, all reported accuracy values for other methods in the tables are sourced from the literature. Further experimental results on CIFAR-100 and DVS-CIFAR10 datasets are detailed in?? from??. For a comprehensive comparison with state-of-the-art (SOTA) methods on ImageNet, please consult?? provided in??.5 CONCLUSION  Directly training SNNs is extremely challenging, even when adopting BN techniques to enable more stable training. The presence of the Temporal Covariate Shift (TCS) phenomenon, coupled with the intrinsic temporal dependency of neuron dynamics, further compounds these challenges for directly training SNNs. To tackle this, we have introduced TAB (Temporal Accumulated Batch Normalization), a novel SNN batch normalization approach. TAB closely aligns with the neuron dynamics, normalizing data using temporal accumulated statistics, effectively capturing historical temporal dependencies similar to that of the accumulation process of the membrane potential in the LIF neuron model. Neuron dynamics refer to the changes in the membrane potential of a neuron over time as it integrates input signals and generates spikes. The alignment with the neuron dynamics means that the TAB method is tailored to mimic or capture the behavior of neurons as closely as possible. It aims to normalize the data in a manner that is coherent with the temporal dependencies and accumulation of information that occur within neurons as they process input signals. This alignment ensures that TAB\u2019s normalization process corresponds well with the way neurons naturally operate in SNNs, thereby leading to improved training and performance by addressing the temporal covariate shift problem. \\\n            \n            Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$\\assistant\n\n$$\n\n$$$Paper Summary:\nThe paper proposes a novel batch normalization method for Spiking Neural Networks (SNNs) called TAB (Temporal Accumulated Batch Normalization). The authors aim to address the challenges of directly training SNNs, particularly the Temporal Covariate Shift (TCS) phenomenon, by aligning with the neuron dynamics. The TAB method utilizes temporal accumulated statistics to normalize data, effectively capturing historical temporal dependencies similar to that of the accumulation process of the membrane potential in the LIF neuron model.\n\nStrengths:\n1. The paper introduces a novel approach to address the challenges of SNN training, specifically the TCS phenomenon.\n2. The authors provide a clear motivation for the need of a new batch normalization method for SNNs.\n3. The proposed TAB method is well-explained, and its alignment with the neuron dynamics is a significant contribution.\n4. The paper presents experimental results on various datasets, including CIFAR-10, CIFAR-100, and DVS-CIFAR10, demonstrating the effectiveness of the TAB method.\n\nWeaknesses:\n1. The paper assumes a good understanding of SNNs and their dynamics, which might not be the case for all readers.\n2. Some of the mathematical derivations, such as the closed-form solution of the LIF dynamics ODE, are not provided in detail.\n3. The paper could benefit from more detailed explanations of the TAB method's parameters and their roles in the normalization process.\n4. The comparison with other state-of-the-art methods is limited, and it would be helpful to include more methods for a comprehensive comparison.\n\nSuggestions for Improvement:\n1. Provide more detailed mathematical derivations, especially for the closed-form solution of the LIF dynamics ODE.\n2. Include more explanations of the TAB method's parameters and their roles in the normalization process.\n3. Compare the TAB method with a wider range of state-of-the-art methods to demonstrate its superiority.\n4. Consider including more experimental results, such as analysis of the effect of different time-steps on the performance of the TAB method.\n\nRecommendation:\nThe paper proposes a novel batch normalization method for SNNs, TAB (Temporal Accumulated Batch Normalization), which effectively addresses the challenges of SNN training, particularly the TCS phenomenon. While there are some limitations, the paper provides a clear motivation for the need of a new batch normalization method for SNNs and presents experimental results demonstrating the effectiveness of the TAB method. With some additional explanations and comparisons, the paper can contribute significantly to the field of SNNs and their training.\n\n$$",
        "peer_review_with_watermark": "$$\n\n$$$Paper Summary:\nThis paper introduces TAB (Temporal Accumulated Batch Normalization), a new batch normalization technique for Spiking Neural Networks (SNNs). The method aims to mitigate the difficulties associated with the direct training of SNNs, particularly addressing the temporal covariate shift by utilizing temporal accumulated statistics that reflect the underlying dynamics of neuron potentials. The authors demonstrate that TAB aligns effectively with neuron behavior as it processes temporal data, leading to better performance in several benchmarks.\n\nStrengths:\n1. The TAB method provides an innovative solution to a significant challenge in the direct training of SNNs, effectively aligning with biological neuron dynamics.\n2. The empirical results show that TAB notably outperforms existing batch normalization methods in various datasets, showcasing its practical effectiveness.\n3. The authors offer a thorough overview and critique of related works, situating their contributions within the broader context of SNN research.\n\nWeaknesses:\n1. The paper does not delve deeply into the computational costs associated with the implementation of the TAB method, which could be relevant for practical applications.\n2. There is a lack of exploration concerning potential limitations of the TAB method under diverse conditions or in relation to alternative models of neuron dynamics.\n3. The experimentation section, while robust, could benefit from a wider array of benchmarks to assess the method's versatility more comprehensively.\n\nSuggestions for Improvement:\n1. A discussion addressing the computational efficiency of the TAB method would enhance the practicality of the proposed approach.\n2. Including assessments of TAB's performance relative to additional types of neuron models beyond the LIF could yield deeper insights into its applicability.\n3. Expanding the range of datasets tested would provide a more thorough evaluation of TAB\u2019s robustness and universality across different types of SNN architectures.\n\nRecommendation:\nThe TAB method presents a compelling advancement in the training of SNNs by aligning batch normalization with neuron dynamics. While its efficacy is evident, further investigations into its limitations and broader applicability could solidify its standing as a state-of-the-art solution in the field of neuromorphic computing.\n\n$$",
        "gr_score_with": 0.8980169971671388,
        "gr_score_without": 0.9227808515756555,
        "output_without": {
            "num_tokens_scored": 4157,
            "num_green_tokens": 3836,
            "green_fraction": 0.9227808515756555,
            "z_score": 4.895969594375641,
            "p_value": 4.891109591266171e-07
        },
        "output_with": {
            "num_tokens_scored": 353,
            "num_green_tokens": 317,
            "green_fraction": 0.8980169971671388,
            "z_score": -0.12419080226287954,
            "p_value": 0.5494178977043018
        }
    },
    {
        "title": "A UNIFIED SAMPLING FRAMEWORK FOR SOLVER SEARCHING OF DIFFUSION PROBABILISTIC MODELS",
        "abstract": "Recent years have witnessed the rapid progress and broad application of diffusion probabilistic models (DPMs). Sampling from DPMs can be viewed as solving an ordinary differential equation (ODE). Despite the promising performance, the generation of DPMs usually consumes much time due to the large number of function evaluations (NFE). Though recent works have accelerated the sampling to around 20 steps with high-order solvers, the sample quality with less than 10 NFE can still be improved. In this paper, we propose a unified sampling framework (USF) to study the optional strategies for solver. Under this framework, we further reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, and a carefully designed solver schedule has the potential to improve the sample quality by a large margin. Therefore, we propose a new sampling framework based on the exponential integral formulation that allows free choices of solver strategy at each step and design specific decisions for the framework. Moreover, we propose S, a predictor-based search method that automatically optimizes the solver schedule to get a better time-quality trade-off of sampling. We demonstrate that S can find outstanding solver schedules which outperform the state-of-the-art sampling methods on CIFAR-10, CelebA, ImageNet, and LSUN-Bedroom datasets. Specifically, we achieve 2.69 FID with 10 NFE and 6.86 FID with 5 NFE on CIFAR-10 dataset, outperforming the SOTA method significantly. We further apply S to Stable-Diffusion model and get an acceleration ratio of 2\u00d7, showing the feasibility of sampling in very few steps without retraining the neural network.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it.  Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$user\n\nThe peer review format and length should be of standard conference. \\\n            title: A UNIFIED SAMPLING FRAMEWORK FOR SOLVER SEARCHING OF DIFFUSION PROBABILISTIC MODELS \\\n            abstract: Recent years have witnessed the rapid progress and broad application of diffusion probabilistic models (DPMs). Sampling from DPMs can be viewed as solving an ordinary differential equation (ODE). Despite the promising performance, the generation of DPMs usually consumes much time due to the large number of function evaluations (NFE). Though recent works have accelerated the sampling to around 20 steps with high-order solvers, the sample quality with less than 10 NFE can still be improved. In this paper, we propose a unified sampling framework (USF) to study the optional strategies for solver. Under this framework, we further reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, and a carefully designed solver schedule has the potential to improve the sample quality by a large margin. Therefore, we propose a new sampling framework based on the exponential integral formulation that allows free choices of solver strategy at each step and design specific decisions for the framework. Moreover, we propose S, a predictor-based search method that automatically optimizes the solver schedule to get a better time-quality trade-off of sampling. We demonstrate that S can find outstanding solver schedules which outperform the state-of-the-art sampling methods on CIFAR-10, CelebA, ImageNet, and LSUN-Bedroom datasets. Specifically, we achieve 2.69 FID with 10 NFE and 6.86 FID with 5 NFE on CIFAR-10 dataset, outperforming the SOTA method significantly. We further apply S to Stable-Diffusion model and get an acceleration ratio of 2\u00d7, showing the feasibility of sampling in very few steps without retraining the neural network. \\\n            paper text: 1 INTRODUCTION  Diffusion probabilistic models (DPMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020b) have emerged as a new generative modeling paradigm in recent years. DPMs model the probability distribution through training a neural network to estimate the score function or other equivalent form along pre-defined forward diffusion stochastic differential equations (SDEs) (Song et al., 2020b). Sampling from DPMs can be viewed as solving the corresponding reverse diffusion SDEs (Song et al., 2020b; Ho et al., 2020) or the diffusion ODEs (Song et al., 2020b;a) by discretizing the continuous timesteps.\nDespite the high generation ability, the main drawback of DPMs is the slow sampling speed due to the large number of discretized timesteps for the numerical accuracy of the DE solver (Song et al., 2020b; Ho et al., 2020) with neural inference on each step. Therefore, accelerating the sampling process of DPMs is very meaningful.\nCurrent work focusing on decreasing the DPM sampling steps can be divided into two categories. The first of them needs retraining of the neural network (Luhman & Luhman, 2021; Salimans & Ho, 2022; Meng et al., 2023; Song et al., 2023), which takes significant extra costs, especially for large models like Rombach et al. (2022). The other branch of works attempts to design efficient solvers of differential equations (DEs) to accelerate DPMs without retraining the existing off-the-shelf models (Song et al., 2020a; Watson et al., 2021; Bao et al., 2022; Liu et al., 2022; Lu et al., 2022a;b;\n\u2217Corresponding Authors\nZhang & Chen, 2022; Zhang et al., 2022; Zhao et al., 2023). State-of-the-art methods utilize the equivalent exponential integral form of the reverse diffusion ODE, accurately calculating the linear term while estimating the integral term through various high-order numerical methods (Zhang & Chen, 2022; Lu et al., 2022a;b; Zhao et al., 2023). These methods can reduce the number of steps to 15\u223c20 steps, but the performance decreases rapidly with lower than 10 steps. We notice that many solving strategies of these solvers are empirically set and kept constant along the sampling process except for a few special timesteps, leading to suboptimal performance with inadequate timesteps. In this paper, to systematically study the impact of these strategies, we propose a unified sampling framework based on the exponential integral form of the diffusion ODE, named USF, which splits the solving process of one step into independent decisions of several components, including the choice of 1. timestep, 2. starting point of the current step, 3. prediction type of the neural network, 4. order of Taylor expansion, 5. derivative estimation method, and 6. usage of ODE correctors. Based on this framework, we reveal that the quality and efficiency of training-free samplers can be further improved by designing appropriate solver schedules, motivated by the key observation that the suitable solving strategies vary among different timesteps. Therefore, solver schedules, which means the different solving strategies assigned to each timestep, is very important for the sample quality. Our proposed framework can incorporate the existing diffusion solvers by assigning corresponding decisions to those components and allows the ensemble of different solving strategies in the timestep dimension, enabling sufficient potential to outperform existing sampling methods. In addition, we also design new strategies different from existing diffusion solvers for the derivative estimation component, making the proposed sampling framework more promising.\nHowever, designing solver schedules is difficult due to the vast decision space. To address this problem, we propose S3 to search for optimal solver schedules automatically. We construct a performance predictor to enable the fast evaluation of one solver schedule and use it to guide a multistage search process to find well-performing solver schedules under a certain budget of number of function evaluation (NFE). Our contributions are summarized as follows:\n\u2022 We propose a new sampling framework for DPMs, called USF, which unifies existing diffusion ODE solvers based on exponential integral, to systematically and conveniently study the strategies chosen for diffusion samplers. Based on this framework, we design some new solver strategies, including using different solver strategies across timesteps, low-order estimation for derivatives, more types of scaling methods and searchable timesteps. \u2022 We propose a predictor-based multi-stage search algorithm, S3, to search for the well-\nperforming solver schedule under a certain NFE budget. Our method can directly utilize offthe-shelf DPMs without any retraining of the diffusion neural network and find outstanding solver schedules with a moderate search cost, demonstrating its practical applicability. \u2022 We experimentally validate our method on plenty of unconditional datasets, including CIFAR-\n10 (Krizhevsky et al., 2009), CelebA (Liu et al., 2015), ImageNet-64 (Deng et al., 2009) and LSUN-Bedroom (Yu et al., 2015). Our searched solver schedules outperform the SOTA diffusion sampler (Lu et al., 2022b; Zhao et al., 2023) by a large margin with very few NFE, e.g., 6.86 v.s. 23.44 on CIFAR-10 with only 5 NFE. We further apply S3 to Stable-Diffusion (Rombach et al., 2022) models, achieve 2\u00d7 (from 10 NFE to 5 NFE) acceleration without sacrificing the performance on text-to-image generation task on MS-COCO 256\u00d7256 dataset (Lin et al., 2014). Based on the experimental results, we summarize some knowledge to guide future schedule design.2 RELATED WORK  2.1 DIFFUSION PROBABILISTIC MODEL  Diffusion Probabilistic Models (DPMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020b) are used to construct the probability distribution q(x0) of a D-dimension random variable x0 \u2208 RD. DPMs define a forward diffusion process to add noise to the random variable x0 (Song et al., 2020b) progressively:\ndxt = f(xt, t)dt+ g(t)dwt, (1)\nwhere xt stands for the marginal distribution at time t for t \u2208 [0, T ], and x0 obeys the target distribution q(x0). wt is the standard Wiener process. When f(xt, t) is a affine form of xt, the marginal\ndistribution of xt given x0 can be written as (Song et al., 2020b):\nq(xt|x0) = N (xt|\u03b1tx0, \u03c32t I), (2) where \u03b12t /\u03c3 2 t is called signal-to-noise ratio (SNR), which is a strictly decreasing function of t and approximately equals 0 when t = T. The forward diffusion process Eq. (1) has a reverse probability flow ODE (Song et al., 2020b;a):\ndxt = [f(t)xt \u2212 1\n2 g2(t)\u2207xlogq(xt)]dt, (3)\nwhere xT \u223c N (xt|0, I). The ODE has two equivalent forms w.r.t. noise prediction network \u03f5\u03b8 and data prediction network x\u03b8, which are more commonly used for fast sampling:\ndxt = (f(t)xt + g2(t)\n2\u03c3t \u03f5\u03b8(xt, t))dt, (4)\ndxt = ((f(t) + g2(t)\n2\u03c32t )xt \u2212\n\u03b1tg 2(t)\n2\u03c32t x\u03b8(xt, t))dt. (5)\nIt can be proved that the marginal distribution of xt in Eq. (1) equals the distribution of xt in Eq. (3) (Anderson, 1982). Thus, sampling from DPMs can be viewed as solving the above ODEs.2.2 TRAINING-FREE SAMPLERS  One major drawback of DPMs is the large number of discretized timesteps needed for numerical solvers to ensure the sample quality. To handle such an issue, training-free samplers for the ODE Eq. (3) are proposed to achieve quicker convergence (Song et al., 2020b;a; Liu et al., 2022; Zhang & Chen, 2022; Lu et al., 2022a;b; Zhao et al., 2023).2.2.1 LOW ORDER SAMPLERS  The early samplers can be viewed as low-order solvers of the diffusion ODE (Song et al., 2020a; Liu et al., 2022). DDIM (Song et al., 2020a) proposes the following formula for the update of one step: xt = \u221a \u03b1t xs\u2212 \u221a 1\u2212\u03b1s\u03f5\u03b8(xs,s)\u221a \u03b1s + \u221a 1\u2212 \u03b1t\u03f5\u03b8(xs, s). PNDM (Liu et al., 2022) proposes to replace the neural term \u03f5\u03b8(xs, s) in the DDIM formula with a new term 124 (55\u03f5s\u2212 59\u03f5s\u2212\u03b4 +37\u03f5s\u22122\u03b4 \u2212 9\u03f5s\u22123\u03b4) inspired by Adams method. The DDIM formula is a 1-st order discretization to the diffusion ODE Eq. (3) and thus is not efficient enough due to the inadequate utilization of high-order information.2.2.2 EXPONENTIAL-INTEGRAL-BASED SAMPLERS  The solution xt of the diffusion ODE Eq. (3) at timestep t from timestep s can be analytically computed using the following exponentially weighted integral (Lu et al., 2022a)\nxt = \u03b1t \u03b1s xs \u2212 \u03b1t \u222b \u03bbt \u03bbs e\u2212\u03bb\u03f5\u03b8(x\u03bb, \u03bb)d\u03bb. (6)\nThe linear term \u03b1t\u03b1sxs can be accurately computed and the high order numerical methods can be applied to the integral term \u03b1t \u222b \u03bbt \u03bbs\ne\u2212\u03bb\u03f5\u03b8(x\u03bb, \u03bb)d\u03bb, improving the sample speed dramatically than low-order formulas. The following methods apply different strategies to estimate the integral term.\nDEIS (Zhang & Chen, 2022) uses Lagrange outer interpolation to estimate the integrand. Then, the exponential integral can be computed approximately since the integral of a polynomial can be easily computed. Additionally, DEIS computes this integral in t domain rather than \u03bb.\nDPM-Solver (Lu et al., 2022a) utilizes the Taylor expansion of the exponential integral as follows to estimate this term.\nxt = \u03b1t \u03b1s xs \u2212 \u03b1t k\u22121\u2211 n=0 \u03f5 (n) \u03b8 (x\u03bbs, \u03bbs) \u222b \u03bbt \u03bbs e\u2212\u03bb (\u03bb\u2212 \u03bbs)n n! d\u03bb+O((\u03bbt \u2212 \u03bbs)k+1). (7)\nAdditionally, Lu et al. (2022a) proposes to discretize the timesteps with uniform logSNR, which has empirically better performance. DPM-Solver++ (Lu et al., 2022b) introduces different strategies under the same Taylor expansion framework, e.g., multi-step solver and data prediction neural term.\nUniPC (Zhao et al., 2023) introduces correcting strategy to the Taylor expansion framework. After computing xt through Eq. (7), UniPC uses the newly computed function evaluation \u03f5\u03b8(xt, \u03bbt) to correct the value of xt, which does not consume extra NFE as \u03f5\u03b8(xt, \u03bbt) is calculated only once.2.3 AUTOML  AutoML methods aim to automatically decide the optimal configurations for machine learning systems. The decidable elements in AutoML works include training hyperparameters (Jaderberg et al., 2017), model selection (Yang et al., 2019; Liu et al., 2023), neural architecture (Zoph & Le, 2016), etc. Predictor-based search methods (Snoek et al., 2012) are widely used in the AutoML field to accelerate the optimization. In predictor-based search, a predictor is trained with evaluated configuration-objective pairs, and then used to predict the objectives for new configurations.3 A UNIFIED SAMPLING FRAMEWORK  In this section, we propose a unified sampling framework called USF based on exponential integral in the \u03bb domain. We introduce our framework by describing the steps of solving the diffusion ODE Eq. (3) and listing all optional strategies and tunable hyperparameters of the sampling process.3.1 SOLVING STRATEGY  To solve the diffusion ODE Eq. (3) without closed-form solutions, one should discretize the continuous time into timesteps and calculate the trajectory values at each timestep. Therefore, the first step of solving the diffusion ODE is to determine a discretization scheme [t0, t1, \u00b7 \u00b7 \u00b7, tN ], where \u03b5 = t0 < t1 < \u00b7 \u00b7 \u00b7 < tN = T. Then consider a timestep t \u2208 [t0, \u00b7 \u00b7 \u00b7, tN\u22121], exponential integral has been validated to be an effective method of computing xt due to the semi-linear property (Zhang & Chen, 2022; Lu et al., 2022a;b; Zhao et al., 2023), especially in the \u03bb domain. We follow the Taylor expansion of Eq. (6) in \u03bb domain (Lu et al., 2022a) to estimate the integral term, and summarize the strategies that can be applied to numerically compute the expansion as follows.\nPrediction type of neural network. Typically, the neural network is trained with denoising objective (Ho et al., 2020) and the corresponding diffusion ODE writes as Eq. (4). It can be proved that Eq. (6) is the accurate solution of Eq. (4) (see App. H). Denote that s is another timestep before t and h = \u03bbt \u2212 \u03bbs, the Taylor expansion of Eq. (6) can be written as (Lu et al., 2022a):\nxt = \u03b1t \u03b1s\nxs \u2212 \u03c3t n\u2211\nk=0\nhk+1\u03c6\u03f5k+1(h)\u03f5 (k) \u03b8 (xs, s) +O(h n+2), (8)\nwhere \u03c6\u03f5k satisfies \u03c6 \u03f5 k+1(h) = \u03c6\u03f5k(h)\u22121/k! h, and \u03c6 \u03f5 0(h) = e\nh. Alternatively, the neural network can be a data prediction model x\u03b8(xt, t), whose relationship with the noise prediction model \u03f5\u03b8(xt, t) is given by x\u03b8(xt, t) :=\nxt\u2212\u03c3t\u03f5\u03b8(xt,t) \u03b1t\n(Kingma et al., 2021; Lu et al., 2022b). The corresponding diffusion ODE w.r.t. data prediction model writes as Eq. (5), whose Taylor expansion is given below:\nxt = \u03c3t \u03c3s xs + \u03b1t n\u2211 k=0 hk+1\u03c6xk+1(h)x (k) \u03b8 (xs, s) +O(h n+2), (9)\nwhere \u03c6xk satisfies \u03c6 x k+1(h) = 1/k!\u2212\u03c6xk(h) h, and \u03c6 x 0(h) = e \u2212h. See App. H for detailed derivations. Since the linear term and the integral to estimate are different between the two prediction types, the numerical solutions of Eq. (8) and Eq. (9) are essentially different except for n = 1. Starting point s. s can be any timestep larger than t. Multistep solvers (Lu et al., 2022b; Zhao et al., 2023) always choose the previous timestep of t while single-step solvers (Lu et al., 2022a) choose more distant timesteps.\nOrder of the Taylor expansion. To compute the Taylor expansion Eq. (8) or Eq. (9), the number of order retained should be decided. Low retained order may result in higher truncation error. However, the estimation of high-order derivatives need extra information from other timesteps other than the starting point s, which may not be accurate enough for calculation. Therefore, the choice of Taylor expansion order may have a strong influence on the sample quality.\nDerivative estimation method. After determining the prediction type and the retained order n, the last thing is to numerically calculate all unknown derivative terms \u03f5(k)\u03b8 (xs, s) (take noise prediction model as an example), where k \u2264 n, to obtain the final solution xt. The strategies of this component are very flexible since many off-the-shelf numerical differential methods can be used. To simplify the workflow, we utilize the system of linear equations consisting of m \u2265 k Taylor expansions from the starting point s to other timesteps, the k-order derivative can be approximately calculated by eliminating derivatives of other orders (Zhao et al., 2023), which we call m-th Taylor-difference method (see Def. B.1 for its formal definition). Inspired by the 2-nd solver of Lu et al. (2022a;b), we note that for a derivative estimation result \u03f5\u0303(k)\u03b8 (xs, s), rescaling the estimated value by multiplying a coefficient 1 + R(h) has the potential to further correct this numerical estimation. To sum up, we formulate the estimation of k-th order derivative as the following two steps: (1). choose at least k other timesteps ti \u0338= s, i \u2208 [1, \u00b7 \u00b7 \u00b7, k] with pre-computed function evaluation \u03f5\u03b8(xti, ti) to calculate the estimation value through Taylor-difference method, and (2). rescale the estimation value with a coefficient 1 + R(h). In the search space used in our experiments, we allow low-order Taylordifference estimation to be taken for the 1-st derivative in high-order Taylor expansions (i.e., m can be smaller than n) and design five types of scale coefficients Ri(h), i = 0, \u00b7 \u00b7 \u00b7, 4 for the 1-st derivative. See App. B for details.\nAfter calculating xt, correctors can be used to improve the accuracy of xt through involving the newly computed \u03f5\u03b8(xt, t) in the re-computation of xt (Griffiths & Higham, 2010). Specifically, the Taylor expansion Eq. (8) or Eq. (9) is estimated again with t as one other timestep for derivative estimation. Typically, \u03f5\u03b8(xt, t) also have to be evaluated again after recomputing xt and the correcting process can be iteratively conducted. However, in practice, re-evaluations and iterative correctors are not used to save NFE budgets. Since our goal is to improve the sample quality with very few NFE, we follow Zhao et al. (2023) to use no additional neural inference for correctors.\nIn summary, all decidable components in our framework are: (1). timestep discretization scheme, (2). prediction type of the neural term, (3). starting point of the current step, (4). retained order of Taylor expansion, (5). derivative estimation methods, and (6). whether to use corrector. Our sampling framework is summarized in Alg. 1 and demonstrated in Fig. 1.3.2 SOLVER SCHEDULE: PER-STEP SOLVING STRATEGY SCHEME  In this section, we demonstrate the phenomenon that the proper solving strategies appropriate for different timesteps are different. We construct a \u2019ground-truth\u2019 trajectory c = [xt0, xt1, \u00b7 \u00b7 \u00b7, xtN ] using 1000-step DDIM (Song et al., 2020a) sampler (i.e., N = 1000). We choose a serials of target timesteps [ttar0, \u00b7 \u00b7 \u00b7, ttarS ] where ttari \u2208 [t0, \u00b7 \u00b7 \u00b7, tN ], and estimate the value x\u0303ttari with k previous timesteps [ts1, \u00b7 \u00b7 \u00b7, tsj, \u00b7 \u00b7 \u00b7, tsk ], where sj \u2208 [1, N ] and tsj > ttari. Then we calculate the distance between the ground-truth trajectory value and the estimated value, given by L(ttari) = |x\u0303ttari \u2212 xttari |. We compare different solving strategies by showing their L(ttari)-ttari curve in Fig. 2. The key observation is that the suitable strategies for most components varies among timesteps. For example, the loss of the 3-rd solver with noise prediction network is smaller than the 2-nd solver at 400-700 timesteps but larger at the rest timesteps (see the green and red curves of \u2019Orders and Prediction Types\u2019).\nThis phenomenon reveals the potential of using different solving strategies in different timesteps. We call the decision sequence of solving strategies for every timestep solver schedule.3.3 ADVANTAGES OF THE FRAMEWORK  The contribution of USF lies in two ways. First, USF gives a unified perspective for all sampling methods based on exponential integral. Second, USF allows different solving strategies in each timestep, making the sampler more flexible. To demonstrate the universality and reliability of USF, we show in Tab. 1 that almost all existing SOTA samplers based on exponential integral can be incorporated by it through assigning corresponding strategies (more derivations can be found in App. E). Note that since DEIS (Zhang & Chen, 2022) computes the integral in the t domain, it is essentially different from other methods. However, when applying the core numerical method in DEIS to the \u03bb domain, we show that it can still be incorporated in USF (the last row in Tab. 1).4 SEARCH FOR SOLVER SCHEDULE THROUGH S3  4.1 PROBLEM DEFINITION  Denote the si as the numerical solver to compute xti, we formulate the problem of deciding the optimal solver schedule as below:\nargmin M\u2264L,\ns1,s2,\u00b7\u00b7\u00b7,sM, t1,t2,\u00b7\u00b7\u00b7,tM\nIF \u2217 F([(s1, t1), (s2, t2), \u00b7 \u00b7 \u00b7, (sM, tM )]), s.t.M \u2264 C,\nwhere C is the given NFE budget, [(s1, t1), (s2, t2), \u00b7 \u00b7 \u00b7, (sM, tM )] is the solver schedule s, and F is an arbitrary evaluation metric of diffusion models, and IF is an indicator coefficient which equals to 1 when smaller F indicates better performance and equals to \u22121 otherwise. As discussed in Sec. 3.2, the best solving strategies for different timesteps are different, which leads to a search space that grows exponentially with the number of steps. Additionally, the decision of one component may be coupled with the decision of other components. For example, as demonstrated in Fig. 2, low-order estimation of derivatives almost consistently outperforms full-order estimation method with data prediction network (see the orange and blue curves of \u2019Low Order Derivative\nEstimation\u2019) while worse than it with noise prediction network at 500-700 timesteps (see the red and green curves). Therefore, well-performing solver schedules are hard to design manually.\nInspired by AutoML works (Real et al., 2019; Ning et al., 2020; Liu et al., 2023), we attempt to automatically search for solver schedules with good performance under certain NFE budgets. However, the search space is extremely large, and evaluating a solver schedule is also time-consuming since the standard calculation processes of most metrics need to generate a large number of images (Heusel et al., 2017). Thus, random search with a regular metric evaluation method is inefficient in this case.4.2 PREDICTOR-BASED MULTI-STAGE SEARCH FOR SOLVER SCHEDULE  To surmount the aforementioned challenges and accelerate the search process, we propose S3, a predictor-based multi-stage evolutionary method. Specifically, we train a light predictor to predict the performance of a solver schedule and use it to guide an evolutionary search process to sample solver schedules. We can evaluate new solver schedules in the evolutionary search with negligible cost with the help of the predictor. making the search process much more efficient.\nBut training the predictor still needs a bunch of evaluated schedules, which is time consuming. To decrease this part of cost, we propose the multi-stage workflow S3. In the early stage of the evolutionary search, only the sub-space adjacent to the current population can be explored. Therefore, the predictor is not required to generalize to the whole search space and can be trained with a small amount of data. As the population expands, generalization ability to new sub-spaces is needed, and thus the predictor should be trained with more sampled schedules. Moreover, the training data of the predictor should also be selected carefully and efficiently.\nBased on the above intuition, we propose the multi-stage workflow. Our workflow is summarized in Alg. 2 and demonstrated in App. D.2, which contains three steps in every loop. (1). A set of solver schedules are sampled from the search space through evolutionary search. The performances needed to guide the evolutionary search are calculated by the predictor except for the first loop in which all performances are truly evaluated. (2). The true metrics of all sampled solver schedules are calculated. (3). All evaluated schedule-performance data pairs are used to update the weights of the predictor. Compared to the single-stage method, which evaluates the true performance of all schedules in one go and trains a predictor to search in the whole space, S3 accelerates the exploration of schedules which have to be truly evaluated.5 EXPERIMENTS  We choose several SOTA diffusion samplers, including DPM-Solver (Lu et al., 2022a), DPMSolver++ (Lu et al., 2022b), and UniPC (Zhao et al., 2023) as baseline methods. We use FID\u2193 (Heusel et al., 2017) as the evaluation metric. Our method is validated on CIFAR-10, CelebA, ImageNet-64, ImageNet-128 (guided), ImageNet-256 (guided), and LSUN-bedroom datasets and outperforms baseline methods by a large margin on all of them. We further apply S3 to text-image generation task with Stable Diffusion pre-trained models (Rombach et al., 2022) on MS-COCO2014 validation set. The models we use for all experiments are listed in App. F.1.\nWe evaluate 9 common settings in total of these SOTA methods. We list the best (Baseline-B in all tables) and the worst (Baseline-W in all tables) results of all baseline methods we evaluate under the same NFE budget. For the worst baseline, we distinguish singlestep methods (S) and multistep methods (M) since the former is usually significantly worse with meager budgets. Detailed settings and full results can be found in App. F.3 and App. G.1, correspondingly.5.1 MAIN RESULTS  We list the results on unconditional datasets and with classifier-guidance in Tab. 2 and the results with Stable Diffusion models in Tab. 3. The key takeaways are: (1) S3 can achieve much higher sample quality than baselines, especially with very few steps. Our searched schedules outperform all baselines across all budgets and datasets by a large margin. On text-to-image generation with Stable Diffusion models, S3 achieves comparable FIDs with only 5 NFE compared to baselines with 10 NFE, bringing a 2\u00d7 acceleration ratio. Remarkably, our method achieves a significant boost under very tight budgets (like 11.50 v.s. 57.52 on CIFAR-10, 33.84 v.s. 76.69 on ImageNet64), making it feasible to sample with very low NFEs. (2) Choosing proper settings for existing methods is important. We can see that in many cases, the performances of the best baseline and the worst baseline are significantly different from each other, even both of which are under the SOTA frameworks. Additionally, the best baselines on different datasets or even under different NFE budgets are not consistent. It indicates the necessity of a careful selection of solver settings and the significance of our USF since it is proposed to present all adjustable settings systematically.5.2 ABLATION STUDY: REDUCE SEARCH OVERHEAD  The main overhead of our method comes from evaluating the true performance of solver schedules since the predictor is much lighter than diffusion U-Nets and has negligible overheads. The time consumed is proportional to the number of generated images used for metric calculation. Fewer evaluation images lead to higher variance and lower generalization ability to other texts and initial noises, thus may cause worse performance. In this section, we ablate this factor to give a relationship\nbetween overhead and performance. We choose the text-to-image task because the text condition is far more flexible than label condition or no condition and thus more difficult to generalize from a small number of samples to other samples. We validate S3 with 250 and 500 generated images rather than the default setting 1000, and our results are shown in the last two rows in Tab. 3. The performance under almost all NFE budgets becomes worse as the number of generated samples decreases. Moreover, some FIDs under higher budgets are even higher compared to lower budgets due to the high variance caused by lower sample numbers. Remarkably, the search results can still outperform baseline methods, indicating the potential of S3.5.3 ANALYSIS AND INSIGHTS  We give some empirical insights in this section based on the pattern of our searched schedules. Our general observation is that most components of searched schedules show different patterns on different datasets. We analyze the patterns of different components as follows.\nTimesteps. For low resolution datasets, more time points should be placed when t is small. Unlike the default schedule \u201clogSNR uniform\u201d, we suggest putting more points at 0.2 < t < 0.5 rather than putting too much at t < 0.05. For high resolution datasets, we recommend a slightly smaller step size at 0.35 < t < 0.75 on top of the uniform timestep schedule.\nPrediction Types. We find that data prediction can outperform noise prediction by a large margin in the following cases: 1. the NFE budget is very low (i.e., 4 or 5), 2. the Taylor order of predictor or corrector is high and 3. on low resolution datasets. But for the rest cases, their performance has no significant difference. Noise prediction can even outperform data prediction sometimes.\nDerivative Estimation Methods. We find that low order derivative estimation is preferred for high Taylor order expansion when step size is very high. Oppositely, full-order estimation is more likely to outperform low-order estimation when step size is not so large. For derivative scaling functions, we find additional scaling is often used in all searched schedules. But the pattern varies among datasets.\nCorrectors. Though previous work validates the effectiveness of using correctors Zhao et al. (2023), we find that not all timesteps are suitable for correctors. For large guidance scale sampling, corrector is not usually used. But in general, corrector is more likely to bring positive impact on the performance.6 CONCLUSION AND FUTURE WORK  In this paper, we propose a new sampling framework, USF, for systematical studying of solving strategies of DPMs. We further reveal that suitable strategies at different timesteps are different, and thus propose to search solver schedules with the proposed framework in the predictor-based multistage manner. Experiments show that our proposed method can boost the sample quality under a very tight budget by a large margin, making it feasible to generate samples with very few NFE.\nAlthough we propose a general framework for sampling based on exponential integral, we prune the search space empirically to avoid excessive search overheads. Exploring the strategies not used in this work could be a valuable direction for future research. Besides, our method has additional overhead related to the evaluation speed and the number of sampled schedules. Therefore, fast evaluation methods and efficient search methods are worth studying. \\\n            \n            Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$\\assistant\n\n$$\n$$$Paper Summary:\nThe paper proposes a unified sampling framework (USF) for diffusion probabilistic models (DPMs) that aims to improve the sample quality and efficiency of DPMs. The USF is based on the exponential integral formulation and allows for the choice of different solving strategies at each timestep. The authors also propose a predictor-based multi-stage search algorithm (S3) to search for optimal solver schedules. The experimental results show that the proposed method can achieve much higher sample quality than existing methods, especially with very few steps.\n\nStrengths:\n- The paper provides a comprehensive overview of the current state-of-the-art methods for DPMs and highlights the need for a unified sampling framework.\n- The proposed USF is a significant improvement over existing methods, allowing for the choice of different solving strategies at each timestep.\n- The S3 algorithm is a useful tool for searching for optimal solver schedules.\n\nWeaknesses:\n- The paper assumes that the reader is familiar with the basics of DPMs and diffusion ODEs, which may be a barrier for non-experts.\n- Some of the mathematical derivations are complex and may require additional explanations.\n- The paper does not provide a clear comparison with other optimization methods, such as gradient-based methods.\n\nSuggestions for Improvement:\n- Add more explanations and derivations for complex mathematical concepts.\n- Provide a clear comparison with other optimization methods.\n- Consider adding more experimental results, such as the performance of the proposed method on different datasets and architectures.\n\nRecommendation:\n- The proposed USF and S3 algorithm are significant contributions to the field of DPMs, and the paper provides a clear overview of the current state-of-the-art methods.\n- With some revisions to address the weaknesses mentioned above, the paper can be considered for publication in a top-tier conference or journal.\n\n$$",
        "peer_review_with_watermark": "$$$Paper Summary:\nThe manuscript presents a new unified sampling framework (USF) aimed at enhancing the sampling efficiency and quality of diffusion probabilistic models (DPMs). This framework leverages the exponential integral formulation, permitting the choice of distinct solving strategies at each timestep. Additionally, the authors introduce a predictor-based multi-stage search algorithm (S3) designed to optimize solver schedules based on a specified function evaluation (NFE) budget. Empirical results indicate that both the USF and S3 outperform existing state-of-the-art methods, particularly with limited NFE.\n\nStrengths:\n- The framework offers a structured approach to analyzing solving strategies for DPMs.\n- The introduction of S3 represents a noteworthy advancement in search algorithms within this context.\n- Empirical findings convincingly illustrate the framework's ability to enhance sample quality and operational efficiency.\n- The analysis provides valuable insights into the characteristics of the solver schedules discovered.\n\nWeaknesses:\n- The writing may presuppose prior familiarity with DPM concepts and exponential integrals, which could alienate less informed readers.\n- The manuscript lacks a comprehensive discussion on the computational demands associated with the S3 algorithm, raising concerns about its practicality in extensive applications.\n- There is a need for a broader comparative analysis of USF and S3 against various leading methods to contextualize their performance.\n- The explanation surrounding the utilization of correctors within the USF is insufficiently explored.\n\nSuggestions for Improvement:\n- A thorough examination of the computational complexity of S3 should be included to address scalability concerns for larger datasets.\n- Incorporating a detailed comparative assessment between the proposed methods and related state-of-the-art approaches would enrich the study's relevance and applicability.\n- A more extensive elucidation about the function and importance of correctors would aid comprehension, particularly for readers less familiar with this aspect.\n- Additional experiments on alternative datasets and varying NFE allocations could lend further insight into the robustness of the proposed methods.\n\nRecommendation:\nThis work significantly enriches the realm of diffusion probabilistic models through the development of the USF and the S3 optimization algorithm. The empirical results affirm their potential to boost both sample quality and efficiency. Nonetheless, the manuscript would benefit from enhanced clarity regarding its assumptions, computational implications, and detailed comparisons with existing methodologies. Addressing these facets would further solidify the contributions of this research. $$$",
        "gr_score_with": 0.8958837772397095,
        "gr_score_without": 0.9368709972552608,
        "output_without": {
            "num_tokens_scored": 4372,
            "num_green_tokens": 4096,
            "green_fraction": 0.9368709972552608,
            "z_score": 8.126503113987262,
            "p_value": 2.2092607886974266e-16
        },
        "output_with": {
            "num_tokens_scored": 413,
            "num_green_tokens": 370,
            "green_fraction": 0.8958837772397095,
            "z_score": -0.27883843773956785,
            "p_value": 0.6098155925962652
        }
    }
]