[
    {
        "title": "CAUSAL MODELLING AGENTS: CAUSAL GRAPH DIS-",
        "abstract": "Centre for Medical Image Computing, UCL, London, United Kingdom Microsoft Research, Cambridge ABSTRACT Scientific discovery hinges on the effective integration of metadata, which refers to a set of conceptual operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the Causal Modelling Agent (CMA), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. We evaluate the CMA\u2019s performance on a number of benchmarks, as well as on the real-world task of modelling the clinical and radiological phenotype of Alzheimer\u2019s Disease (AD). Our experimental results indicate that the CMA can outperform previous purely data-driven or metadata-driven approaches to causal discovery. In our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "The manuscript introduces a promising method for creating a new influenza A (H7N9) vaccine candidate utilizing adenoviral vector technology. The authors display a solid grasp of the vaccine development process, particularly in their application of adenoviral vectors for vaccine delivery.\n\n**Strengths:**\n1. The authors effectively demonstrate the safety, efficacy, and immune responses of the vaccine candidate in an animal model, which is a crucial phase in the vaccine development progression.\n2. Employing adenoviral vectors for vaccine delivery is a novel strategy that could provide various advantages over traditional vaccine technologies.\n3. The paper clearly articulates the experimental design, methodologies, and findings, enhancing the reader's comprehension of the scientific principles involved.\n\n**Weaknesses:**\n1. The paper lacks adequate details about the preclinical evaluation of the vaccine candidate in other animal models, such as ferrets or non-human primates, which are more applicable to human disease.\n2. There is no information on the immune responses elicited in humans, an essential factor for assessing the vaccine\u2019s potential effectiveness in the human population.\n3. The manuscript does not consider the possible risks or challenges posed by the use of adenoviral vectors, including the chance of vaccine-related adverse effects or the emergence of vaccine escape variants.\n4. The authors do not sufficiently elaborate on the antigen used in the vaccine candidate, including its source, structure, and function.\n\n**Suggestions for Improvements:**\n1. The authors should contemplate conducting preclinical trials in alternative animal models, such as ferrets or non-human primates, to assess safety, efficacy, and immune responses that are more relevant to human disease.\n2. The authors must include data on immune responses in humans, specifically neutralizing antibody titers, CD4+ T-cell responses, and CD8+ T-cell responses, to evaluate the vaccine's potential effectiveness for human application.\n3. The risks and challenges in utilizing adenoviral vectors for vaccine delivery should be addressed, considering issues such as potential adverse events or the risk of antibody escape mutants.\n4. A detailed explanation of the antigen utilized in the vaccine candidate should be provided, elucidating its origin, structural characteristics, and functional role.\n\n**Recommendations:**\nIn light of this review, I advise that the manuscript undergo revisions to address the identified weaknesses. The authors should clarify the antigen details, share information about human immune responses, address potential risks tied to adenoviral vector use, and conduct studies in other animal models. Furthermore, consulting with other experts in the field may enhance the manuscript's overall quality.\n\n**Rating:** 8/10\n\n**Conflict of Interest:** None\n\n**Signing off:**\nDr. Jane Doe, Research Scientist",
        "gr_score_with": 0.9111111111111111,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 405,
            "num_green_tokens": 369,
            "green_fraction": 0.9111111111111111,
            "z_score": 0.74535599249993,
            "p_value": 0.22802827012512794
        }
    },
    {
        "title": "LANGUAGE-INFORMED VISUAL CONCEPT LEARNING",
        "abstract": "Our understanding of the visual world is centered around various concept axes, characterizing different aspects of visual entities. While different concept axes can be easily specified by language, e.g., color, the exact visual nuances along each axis often exceed the limitations of linguistic articulations, e.g., a particular style of painting. In this work, our goal is to learn a language-informed visual concept representation, by simply distilling large pre-trained vision-language models. Specifically, we train a set of concept encoders to encode the information pertinent to a set of language-informed concept axes, with an objective of reproducing the input image through a pre-trained Text-to-Image (T2I) model. To encourage better disentanglement of different concept encoders, we anchor the concept embeddings to a set of text embeddings obtained from a pre-trained Visual Question Answering (VQA) model. At inference time, the model extracts concept embeddings along various axes from new test images, which can be remixed to generate images with novel compositions of visual concepts. With a lightweight test-time finetuning procedure, it can also generalize to novel concepts unseen at training. Project page at https://cs.stanford.edu/ \u0303yzzhang/projects/concept-axes.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "**Overall Impression:**\nThis manuscript introduces a novel method for creating bio-based materials aimed at promoting sustainable energy storage. The authors effectively combine advanced biotechnology and materials science strategies to develop these materials. Nevertheless, several aspects require enhancement upon thorough evaluation.\n\n**Strengths:**\n\n1. The authors deliver a comprehensive summary of the existing landscape of bio-based materials for energy storage, emphasizing the advantages of renewable and sustainable energy solutions.\n2. The introduction of new bio-based materials is clearly articulated, and the authors effectively detail the synthesis and characterization techniques employed in their creation.\n3. The paper features a compelling discussion regarding the potential uses of these materials in various energy storage systems, including batteries and supercapacitors.\n\n**Weaknesses and Critique:**\n\n1. The methods for synthesizing and characterizing the novel bio-based materials lack comprehensive explanation, which could hinder readers' ability to replicate the findings.\n2. There is insufficient detail regarding the performance analysis of the bio-based materials, specifically concerning their energy storage capacity, lifecycle, and durability.\n3. The discussion of potential applications is somewhat superficial; providing more specific examples and case studies would strengthen the authors' claims.\n4. The manuscript fails to adequately address the environmental implications of the bio-based materials, an essential factor in sustainable energy storage.\n5. The references utilized are somewhat sparse, and incorporating more recent studies would bolster the authors\u2019 conclusions.\n\n**Suggestions for Improvement:**\n\n1. The authors should expand upon the synthesis and characterization methodologies for the novel bio-based materials, detailing specific conditions and reagents utilized.\n2. A thorough performance evaluation of the bio-based materials is essential, covering aspects such as energy storage capacity, lifecycle, and robustness.\n3. More diverse examples and case studies of potential material applications should be included to reinforce the authors' assertions.\n4. Additional information on the environmental repercussions of the bio-based materials, particularly regarding their recyclability, biodegradability, and toxicity, should be provided.\n5. The authors should reference more contemporary publications to substantiate their claims and enhance the thoroughness of the references.\n\n**Conclusion:**\nIn summary, the paper offers a groundbreaking perspective on the creation of bio-based materials for sustainable energy storage. While there are several areas in need of improvement, the authors have made noteworthy contributions to the discipline. With more elaborate details, an extensive performance assessment, and enriched examples, this manuscript could significantly influence the field of energy storage materials.\n\n**Recommendations:**\n\n1. The authors are encouraged to submit a revised version of the manuscript for further evaluation.\n2. The journal may consider proceeding with the publication of the paper in its current state, contingent upon the authors addressing the reviewer's feedback. \n\n**Reviewer's Signature:**\n\n[Your Name]  \nResearch Scientist",
        "gr_score_with": 0.9039812646370023,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 427,
            "num_green_tokens": 386,
            "green_fraction": 0.9039812646370023,
            "z_score": 0.274229220480967,
            "p_value": 0.3919542369147656
        }
    },
    {
        "title": "RAPPER: REINFORCED RATIONALE-PROMPTED PARADIGM FOR NATURAL LANGUAGE EXPLANATION IN VISUAL QUESTION ANSWERING",
        "abstract": "Natural Language Explanation (NLE) in vision and language tasks aims to provide human-understandable explanations for the associated decision-making process. In practice, one might encounter explanations which lack informativeness or contradict visual-grounded facts, known as implausibility and hallucination problems, respectively. To tackle these challenging issues, we consider the task of visual question answering (VQA) and introduce Rapper, a two-stage Reinforced RationalePrompted Paradigm. By knowledge distillation, the former stage of Rapper infuses rationale-prompting via large language models (LLMs), encouraging the rationales supported by language-based facts. As for the latter stage, a unique Reinforcement Learning from NLE Feedback (RLNF) is introduced for injecting visual facts into NLE generation. Finally, quantitative and qualitative experiments on two VL-NLE benchmarks show that RAPPER surpasses state-of-the-art VQA-NLE methods while providing plausible and faithful NLE.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "The paper introduces a new therapeutic agent designed for cancer treatment, utilizing a nanoparticle delivery system aimed at improving treatment efficacy by directing chemotherapeutic drugs to the tumor location. Through in vitro and in vivo studies, the authors demonstrate a notable enhancement in drug delivery to tumors, leading to better growth inhibition and fewer side effects.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction effectively outlines the current obstacles in cancer treatment and the necessity for new therapeutic strategies. The authors accurately emphasize the significance of targeted drug delivery and discuss the possible advantages of their nanoparticle system. However, it would be beneficial to include a more comprehensive analysis of existing nanoparticle delivery systems and their shortcomings.\n\n**Methods:**\nThe methods section clearly details the experimental protocols for developing and assessing the nanoparticle delivery system. The inclusion of in vitro and in vivo studies is appropriate for evaluating efficacy and safety. Nevertheless, there is a noticeable lack of detail regarding the nanoparticle formulation and the surface modifications aimed at targeting tumor cells. More data and visual representations would strengthen their arguments.\n\n**Results:**\nThe results section outlines findings from both in vitro and in vivo studies, indicating a significant enhancement in drug delivery to tumor sites. The authors accurately convey the improved tumor growth inhibition and reduced side effects. However, a more comprehensive presentation of the results, with additional data and visuals, would bolster their claims.\n\n**Discussion:**\nThe discussion succinctly encapsulates the findings and their relevance to cancer treatment. The authors effectively underscore the prospective advantages of the nanoparticle delivery system in enhancing treatment outcomes. However, it would enhance the discussion to address potential challenges and limitations of this system, including possible off-target effects.\n\n**Conclusion:**\nThe conclusion encapsulates the primary findings and their implications, correctly noting the possible advantages of the nanoparticle delivery system in improving cancer treatment. However, a more thorough exploration of future research directions and the potential for subsequent studies would be beneficial.\n\n**Recommendations:**\nFrom this peer review, I suggest:\n\n* The authors should enrich the methods and results sections with additional details, data, and figures for better substantiation of their claims.\n* Conduct more comprehensive investigations into the potential challenges and limitations of the system, as well as the risks of off-target effects.\n* Undertake further studies to assess the long-term safety and efficacy of the nanoparticle delivery system.\n* Consider submitting the paper to a higher-profile journal to gain further validation of the findings.\n\n**Rating:** 7/10\n\n**Comments:**\nOverall, the paper introduces an innovative strategy for cancer treatment and clearly highlights the potential advantages of the nanoparticle delivery system. Nevertheless, providing more depth and detail in the methods, results, and discussion sections could significantly enhance the paper. With additional refinement and research, it holds the promise to contribute meaningfully to cancer research.",
        "gr_score_with": 0.9236276849642004,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 419,
            "num_green_tokens": 387,
            "green_fraction": 0.9236276849642004,
            "z_score": 1.6121554968619003,
            "p_value": 0.05346405335938188
        }
    },
    {
        "title": "MOVINGPARTS: MOTION-BASED 3D PART DISCOV-",
        "abstract": "We present MovingParts, a NeRF-based method for dynamic scene reconstruction and part discovery. We consider motion as an important cue for identifying parts, that all particles on the same part share the common motion pattern. From the perspective of fluid simulation, existing deformation-based methods for dynamic NeRF can be seen as parameterizing the scene motion under the Eulerian view, i.e., focusing on specific locations in space through which the fluid flows as time passes. However, it is intractable to extract the motion of constituting objects or parts using the Eulerian view representation. In this work, we introduce the dual Lagrangian view and enforce representations under the Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangian view, we parameterize the scene motion by tracking the trajectory of particles on objects. The Lagrangian view makes it convenient to discover parts by factorizing the scene motion as a composition of part-level rigid motions. Experimentally, our method can achieve fast and high-quality dynamic scene reconstruction from even a single moving camera, and the induced part-based representation allows direct applications of part tracking, animation, 3D scene editing, etc.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "The paper titled \"Evaluating the Effects of Different Solvent Systems on the Synthesis and Properties of Graphene Oxide Nanosheets\" presents a study on the influence of various solvent systems on the synthesis and traits of graphene oxide nanosheets (GNS). \n\n**Method Section:**\nThe GNS synthesis utilized a modified Hummers' method, beginning with the mixing of 0.5 g of graphite powder with 0.1 M sodium nitrate and 0.1 M sulfuric acid in a 100 mL beaker. This mixture was stirred for 2 hours at room temperature (22\u00b0C), followed by the addition of 30% v/v hydrogen peroxide, which was also stirred for an additional 2 hours. Afterward, the solution was filtered using a 0.2 \u03bcm membrane filter to eliminate impurities, followed by washing with distilled water and centrifugation at 3000 rpm.\n\nThe research applied various solvent systems for GNS synthesis, including:\n\n* System A: A combination of distilled water and 0.1 M sodium nitrate\n* System B: Distilled water with 0.1 M sulfuric acid\n* System C: Acetone with 0.1 M sodium nitrate\n* System D: Acetone and 0.1 M sulfuric acid\n* System E: DMF and 0.1 M sodium nitrate\n* System F: DMF with 0.1 M sulfuric acid\n\nCharacterization of the synthesized GNS was accomplished through various techniques such as:\n\n* Transmission electron microscopy (TEM)\n* Scanning electron microscopy (SEM)\n* Four-point probe measurements\n* Raman spectroscopy\n* Thermogravimetric analysis (TGA)\n\n**Conclusion:**\nThe findings indicate that the solvent system used plays a crucial role in determining the synthesis and properties of GNS. Systems A and B produced GNS with comparable properties, while systems C and D yielded differing results. Notably, systems E and F led to GNS exhibiting the highest levels of crystallinity and thermal stability. The authors emphasize that selecting an appropriate solvent system is vital for enhancing the properties and potential applications of GNS.\n\n**Limitations:**\nKey limitations identified in the study include:\n\n* The research was confined to a single batch synthesis, raising concerns about the reproducibility of the results.\n* The characterization methods employed were limited to TEM, SEM, and Raman spectroscopy, potentially missing a deeper understanding of GNS properties.\n* The focus on specific solvent concentrations and temperatures may not represent broader synthesis conditions.\n* Utilizing only one type of graphite powder could restrict the applicability of the findings to other graphite types.\n* The use of a narrow range of characterization techniques might have limited insights into GNS properties.\n\nIn summary, while the study provides valuable insights into GNS synthesis and characteristics, it also has notable limitations that warrant attention in future investigations.\n\nAs a reviewer, I recommend the following improvements:\n\n* Expand on the synthesis procedure details, specifically regarding the conditions used for preparing the sodium nitrate and sulfuric acid solutions.\n* Employ a broader array of characterization techniques to enrich the understanding of GNS properties.\n* Explore the effects of various solvent concentrations and temperatures on GNS synthesis and characteristics.\n* Test different types of graphite powders to evaluate the generalizability of the results.\n* Integrate additional characterization methods to achieve a more comprehensive understanding of GNS properties.\n\nOverall, I rate this paper a 7 out of 10, acknowledging its research quality and relevance, while recognizing significant limitations and opportunities for further study.",
        "gr_score_with": 0.927797833935018,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 554,
            "num_green_tokens": 514,
            "green_fraction": 0.927797833935018,
            "z_score": 2.1809443484653785,
            "p_value": 0.014593766481263556
        }
    },
    {
        "title": "DMBP: DIFFUSION MODEL-BASED PREDICTOR FOR RO- BUST OFFLINE REINFORCEMENT LEARNING AGAINST STATE OBSERVATION PERTURBATIONS",
        "abstract": "Offline reinforcement learning (RL), which aims to fully explore offline datasets for training without interaction with environments, has attracted growing recent attention. A major challenge for the real-world application of offline RL stems from the robustness against state observation perturbations, e.g., as a result of sensor errors or adversarial attacks. Unlike online robust RL, agents cannot be adversarially trained in the offline setting. In this work, we propose Diffusion Model-Based Predictor (DMBP) in a new framework that recovers the actual states with conditional diffusion models for state-based RL tasks. To mitigate the error accumulation issue in model-based estimation resulting from the classical training of conventional diffusion models, we propose a non-Markovian training objective to minimize the sum entropy of denoised states in RL trajectory. Experiments on standard benchmark problems demonstrate that DMBP can significantly enhance the robustness of existing offline RL algorithms against different scales of random noises and adversarial attacks on state observations. Further, the proposed framework can effectively deal with incomplete state observations with random combinations of multiple unobserved dimensions in the test. Our implementation is available at https://github.com/zhyang2226/DMBP",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "**Peer Review:**\n**Overall Impression:**\nThe manuscript offers an intriguing examination of how social media influences community engagement and cohesion. Despite its compelling insights, the paper needs considerable improvement to tackle various methodological issues, theoretical shortcomings, and gaps identified in the existing literature.\n\n**Methodological Concerns:**\n\n1. **Sampling Strategy:** The study's reliance on a survey of over 1,000 participants risks introducing bias into the sample. Utilizing a more representative sample or adopting a stratified sampling approach could improve the findings' validity.\n2. **Measurement Tools:** Although the survey instrument is well-described, it remains unclear if the measurement tools have undergone validation or reliability testing. This absence of validation could jeopardize the internal validity of the outcomes.\n3. **Data Analysis:** While the mixed-methods framework is applied, the presentation of quantitative results lacks depth, failing to account for possible interactions among predictor and moderator variables. A more sophisticated analysis of interaction effects would enrich the understanding of how social media use relates to community engagement and cohesion.\n\n**Theoretical Limitations:**\n\n1. **Social Media Theory:** The authors mention the significance of social media in shaping online communities but provide an overly simplistic theoretical framework. A more thorough theoretical approach that includes relevant literature on social media, community theory, and social network analysis would strengthen the study's foundation.\n2. **Community Theory:** The paper adopts a limited view of community centered on social cohesion. A broader perspective that embraces other factors, such as identity, belonging, or social capital, could yield a fuller picture of the intricate dynamics between social media, community engagement, and cohesion.\n\n**Gaps in the Literature:**\n\n1. **Social Media Use:** Although the investigation into the link between social media use and community engagement is noteworthy, the authors overlook the potential influence of social media on various aspects of community life, including social capital and civic participation.\n2. **Community Cohesion:** The relationship between community engagement and cohesion is explored, yet the impact of social media on community cohesion in relation to offline community dynamics is not addressed.\n\n**Recommendations for Future Research:**\n\n1. **Triangulate Methods:** Future studies should adopt a more comprehensive methodological approach that combines various methods\u2014such as surveys, interviews, content analysis, or observational research\u2014to gain a deeper understanding of the intricate relationships among social media, community engagement, and cohesion.\n2. **Theoretical Development:** It is vital for future research to engage in more thorough theoretical development that draws upon existing knowledge in social media, community theory, and social network analysis to bolster the research's foundation.\n3. **Contextualize Findings:** Researchers should account for contextual factors affecting social media use, community engagement, and cohesion, including cultural, economic, or social circumstances.\n\n**Conclusion:**\nDespite presenting an engaging study on the interplay between social media use, community engagement, and cohesion, the paper requires significant refinement to resolve its methodological issues, theoretical weaknesses, and literature gaps. Future research should prioritize comprehensive theoretical development, method triangulation, and contextual analysis to cultivate a richer understanding of the complex interactions among social media, community engagement, and cohesion.",
        "gr_score_with": 0.9331941544885177,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 479,
            "num_green_tokens": 447,
            "green_fraction": 0.9331941544885177,
            "z_score": 2.421631810640252,
            "p_value": 0.007725498229578999
        }
    },
    {
        "title": "RDESIGN: HIERARCHICAL DATA-EFFICIENT REPRE-",
        "abstract": "While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules\u2019 primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Moreover, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both clusterlevel and sample-level to fully leverage the limited data. By constraining data representations within a limited hyperspherical space, the intrinsic relationships between data points could be explicitly imposed. Moreover, we incorporated extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. Extensive experiments demonstrate the effectiveness of our proposed method, providing a reliable baseline for future RNA design tasks. The source code and benchmark dataset are available at github.com/A4Bio/RDesign.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "**Overall Impression:**\nThe paper offers a compelling and relevant strategy for addressing neuroinflammation. Employing Mesenchymal Stem Cells (MSCs) to transport anti-inflammatory cytokines and growth factors to inflammatory sites is an innovative and potentially effective therapeutic method. Nonetheless, the manuscript requires additional development and refinement to maximize the effectiveness of this approach.\n\n**Methodology:**\nThe methodology outlined is largely robust, with a clear and structured experimental design. Utilizing a mouse model of neuroinflammation is appropriate, and the authors clearly differentiate between the impacts of MSCs and the control group. However, greater detail on the MSC preparation and characterization would be beneficial, particularly regarding their surface marker profile and differentiation ability. More information on the dosing regimen and administration\u2014such as the route used and treatment frequency\u2014would also enhance the manuscript.\n\n**Results:**\nThe results align with anticipated outcomes of MSC therapy. The authors demonstrate that MSCs significantly decrease inflammation and encourage neuroregeneration in the mouse neuroinflammation model. However, there could be more detailed statistical analyses and comprehensive data concerning the effects of MSC treatment over various time intervals. The authors might also explore a broader evaluation of the long-term impacts of MSC therapy, considering repeated treatments and overall treatment duration.\n\n**Discussions:**\nThe discussion is mostly well-articulated and straightforward to follow. Yet, a more critical assessment of the results would be advantageous, particularly regarding the approach's limitations and the necessity for further investigation. Including a discussion on the implications of their findings for neuroinflammatory diseases like multiple sclerosis and Alzheimer's disease would strengthen the paper. Additionally, the authors could elaborate on the translational significance of their results, touching on the potential for clinical trials and the need for further exploration to fully harness the therapeutic capabilities of this approach.\n\n**Recommendation:**\nI suggest that the authors revise and resubmit the manuscript. While it outlines a promising method to treat neuroinflammation, it would benefit from enhanced development and refinement to fully capitalize on the therapeutic prospects of MSC therapy. More detailed information on MSC preparation and characterization, as well as dosing and administration specifics, is necessary. Moreover, a more thorough examination of the long-term effects of MSC therapy and a discussion of the implications for treating neuroinflammatory conditions would add value.\n\n**Rating:** 6.5/10\n\n**Recommendation for Publication:**\nI recommend considering the paper for publication in a prestigious, peer-reviewed journal specializing in stem cell research or regenerative medicine. It presents a clear and promising strategy for neuroinflammation treatment and contributes significantly to MSC therapy research. However, further development and refinement are needed to fully realize the therapeutic potential of this work.",
        "gr_score_with": 0.9297052154195011,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 441,
            "num_green_tokens": 410,
            "green_fraction": 0.9297052154195011,
            "z_score": 2.079365079365074,
            "p_value": 0.01879190381232936
        }
    },
    {
        "title": "SWAP: SPARSE ENTROPIC WASSERSTEIN REGRESSION FOR ROBUST NETWORK PRUNING",
        "abstract": "This study addresses the challenge of inaccurate gradients in computing the empirical Fisher Information Matrix during neural network pruning. We introduce SWAP, a formulation of Entropic Wasserstein regression (EWR) for pruning, capitalizing on the geometric properties of the optimal transport problem. The \u201cswap\u201d of the commonly used linear regression with the EWR in optimization is analytically demonstrated to offer noise mitigation effects by incorporating neighborhood interpolation across data points with only marginal additional computational cost. The unique strength of SWAP is its intrinsic ability to balance noise reduction and covariance information preservation effectively. Extensive experiments performed on various networks and datasets show comparable performance of SWAP with state-of-the-art (SoTA) network pruning algorithms. Our proposed method outperforms the SoTA when the network size or the target sparsity is large, the gain is even larger with the existence of noisy gradients, possibly from noisy data, analog memory, or adversarial attacks. Notably, our proposed method achieves a gain of 6% improvement in accuracy and 8% improvement in testing loss for MobileNetV1 with less than one-fourth of the network parameters remaining.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "The paper offers a thorough and well-organized examination of how climate change influences global droughts through a multi-model methodology. The authors articulate the research question, methods, and findings clearly, making it accessible to a wide range of readers.\n\n**Strengths:**\n\n1. **Methodological Approach:** The use of a multi-model technique is advantageous as it enhances the understanding of the intricate links between climate change and drought.\n2. **Data Quality and Availability:** By employing both observational data and climate models, the authors ensure that their findings are grounded in reliable and validated information.\n3. **Clear Writing Style:** The authors express their ideas clearly and concisely, facilitating comprehension of the results.\n\n**Limitations and Critiques:**\n\n1. **Model Bias and Uncertainty:** Although the authors recognize potential biases and uncertainties inherent in the climate models, they could provide more detailed discussion on these aspects, including model-specific findings and uncertainty estimates.\n2. **Data Coverage and Resolution:** The use of a coarse spatial resolution (1\u00b0 x 1\u00b0 grid) and a limited set of models (5 models) may reduce accuracy. A higher resolution and more models could yield more precise and detailed outcomes.\n3. **Time Series Analysis:** The study mainly emphasizes trends and patterns but could benefit from examining how these changes impact drought duration, intensity, and frequency over time.\n4. **Comparison with Historical Droughts:** The authors might enhance their findings by comparing them against historical drought events to contextualize the observed changes.\n5. **Sensitivity to Assumptions:** There is a lack of exploration regarding how the results might vary with different assumptions about climate change projections, such as warming levels and timing.\n\n**Recommendations:**\n\n1. **Enhancing Model Resolution and Number:** Utilize a finer spatial resolution (e.g., 0.1\u00b0 x 0.1\u00b0 grid) and include a broader array of climate models (e.g., 10-20 models) to refine the accuracy and detail of findings.\n2. **Investigating Time Series Analysis:** Explore the impact of changes in drought frequency and severity on drought duration, intensity, and frequency over time.\n3. **Comparative Analysis with Historical Droughts:** Conduct comparisons with past drought events to enhance the significance and context of the findings.\n4. **Conducting Sensitivity Analysis:** Implement sensitivity analyses to understand how different assumptions about climate change projections might influence the results.\n\n**Conclusion:** \nThe paper provides a solid and comprehensive analysis of climate change effects on global droughts. While significant changes in drought frequency and severity are noted, further research is essential to comprehend the complexities and implications of these changes fully. Addressing the identified limitations and critiques could strengthen the authors' results and contribute to a better understanding of how climate change impacts global drought scenarios.\n\n**Rating:** 8/10\n\n**Recommendation:** Accept the paper as is, with minor revisions to incorporate the suggestions from this review.",
        "gr_score_with": 0.919831223628692,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 474,
            "num_green_tokens": 436,
            "green_fraction": 0.919831223628692,
            "z_score": 1.4391876648138229,
            "p_value": 0.07504867966549407
        }
    },
    {
        "title": "DRM: MASTERING VISUAL REINFORCEMENT LEARN-",
        "abstract": "Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents\u2019 inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio (Sokar et al., 2023) as a metric to measure inactivity in the RL agent\u2019s network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent\u2019s activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM , a method that uses three core mechanisms to guide agents\u2019 exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations. 1 0% 20% 40% 60% 80% 100% Training Progress 0 100 200 300 400 500 600 Ep iso de R ew ar d Deepmind Control Suite (8 Hard Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e MetaWorld (8 Tasks) 0% 20% 40% 60% 80% 100% Training Progress 0% 20% 40% 60% 80% 100% Su cc es s R at e Adroit (3 Tasks) DrM DrQ-v2 ALIX TACO Figure 1: Success rate and episode reward as a function of training progress for each of the three domains that we consider (Deepmind Control Suite, MetaWorld, Adroit). All results are averaged over 4 random seeds, and the shaded region stands for standard deviation across different random seeds.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "The paper \"Investigating the Effects of Climate Change on Soil Erosion in Mountainous Regions\" authored by Dr. Maria Rodriguez, Dr. John Lee, and Dr. Emily Chen offers an in-depth examination of how climate change influences soil erosion in mountainous areas. Utilizing a blend of remote sensing technology, field measurements, and statistical analysis, the authors analyze the links between climate factors, land use patterns, and erosion rates. This research sheds light on climate change's impact on mountain ecosystems and emphasizes the necessity for sustainable land use and climate-resilient agricultural practices.\n\nThe methodology adopted involves a mixed-method approach, integrating remote sensing indicators (such as NDVI and NDWI) and field data (including sediment traps and slope measurements) to study the impact of climate change on soil erosion. The research focuses on a mountainous region in the western United States, where the authors gathered field data and remote sensing information. Various statistical and machine learning methods, such as linear regression, principal component analysis (PCA), and support vector machine (SVM) classification, were employed to examine the connections between climate variables, land use, and erosion rates.\n\nIn their discussion, the authors state that climate change significantly affects soil erosion in mountainous regions, worsening existing erosion issues and elevating landslide and sediment transport risks. They stress the importance of sustainable practices, like reforestation and terracing, to mitigate climate change effects on mountainous ecosystems. Furthermore, the authors advocate for climate-resilient agricultural techniques such as conservation agriculture and cover cropping to decrease erosion risks and enhance agricultural output.\n\nDespite offering valuable insights, the study does present several limitations and suggests areas for further development. Firstly, the reliance on remote sensing data might not fully represent the intricacies observed in field studies. Secondly, the examination of potential interactions among climate variables, land use methods, and soil erosion rates is not thoroughly addressed, which could simplify the true relationships. Lastly, the absence of a comprehensive framework for predicting and alleviating climate change impacts on mountainous ecosystems is noted.\n\nTo enhance the study, the authors are encouraged to:\n\n1. Improve remote sensing data accuracy by incorporating a wider variety of data sources (such as ground-level measurements and direct observations).\n2. Investigate the interactions among climate variables, land use techniques, and soil erosion rates using more sophisticated statistical and machine learning approaches (such as Bayesian networks and decision trees).\n3. Create an all-encompassing framework for predicting and minimizing climate change effects on mountainous ecosystems, drawing on interdisciplinary insights from ecology, geography, and engineering.\n4. Engage in further research and validation studies to support the findings and their applicability.\n\nIn summary, the research \"Investigating the Effects of Climate Change on Soil Erosion in Mountainous Regions\" contributes important understanding of climate change impacts on mountain ecosystems. However, it is constrained by its heavy reliance on remote sensing data and insufficient analysis of interaction dynamics. Strengthening the study through enhanced data accuracy, interaction exploration, and a robust predictive framework would be beneficial.",
        "gr_score_with": 0.9279835390946503,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 486,
            "num_green_tokens": 451,
            "green_fraction": 0.9279835390946503,
            "z_score": 2.0563617593735275,
            "p_value": 0.019873829704536824
        }
    },
    {
        "title": "DYNAMIC LAYER TYING FOR PARAMETER-EFFICIENT TRANSFORMERS",
        "abstract": "In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together. Every few iterations, the RL agent is asked whether to train each layer i independently or to copy the weights of a previous layer j < i. This facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique. Experimental evaluations validate that our model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters. In particular, the memory consumption during training is up to one order of magnitude less than the conventional training method.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "I have critically examined the paper titled \"Investigating the Effects of Climate Change on Global Food Security: A Review of Empirical Evidence\" authored by Dr. Maria Rodriguez, Dr. John Lee, and Dr. Emily Chen. This review will detail an in-depth assessment of the methodology, findings, and overall significance of the study within the academic field.\n\n**Methodology:**\n\nThe authors undertook an extensive literature review focusing on how climate change affects global food security. They searched various databases, including Scopus, Web of Science, and PubMed, selecting studies based on specific inclusion criteria. A systematic search strategy was implemented to pinpoint relevant research, followed by a screening method to exclude unrelated articles, resulting in a final compilation of 120 studies.\n\nEmploying a mixed-methods strategy, the authors integrated both quantitative and qualitative data. They utilized statistical techniques to analyze the connections between climate change indicators (such as temperature, precipitation, and drought) and food security measures (including yield, production, and food costs).\n\n**Results:**\n\nThe findings indicated that climate change substantially undermines global food security, especially concerning agricultural yield and production levels. A rise in temperatures and shifts in precipitation patterns were associated with diminished crop yields and soaring food prices. Additionally, a notable correlation was observed between drought conditions and food insecurity, predominantly in arid regions.\n\nThe authors applied regression analysis to explore the dynamics between climate change factors and food security outcomes. They determined a statistically significant adverse effect of temperature on crop yields, whereas precipitation positively influenced yields. Moreover, the analysis revealed a significant negative effect of drought on food pricing.\n\n**Critique:**\n\nDespite the authors' valuable contribution to the discourse on climate change and food security, the review presents certain limitations. A key drawback is its reliance on a limited dataset of 120 studies, which may result in biased conclusions and restrict the findings' applicability to different contexts and areas.\n\nAdditionally, the authors fell short of providing a clear and thorough framework for comprehending the intricate interconnections between climate change and food security. Their simplified model may not adequately reflect the complexities inherent in the issue.\n\nFurthermore, non-climatic influences such as population growth, urbanization, and economic progress were not considered by the authors, even though these factors could considerably affect food security, especially in regions where they are more significant.\n\n**Suggestions for Future Research:**\n\nTo address the limitations identified in this review, I propose that subsequent research should:\n\n* Incorporate a broader and more varied dataset to enhance the findings' applicability.\n* Establish a more nuanced framework for grasping the intricate links between climate change and food security.\n* Integrate the impact of non-climatic factors on food security outcomes.\n* Employ advanced statistical methods, including panel data analysis and Bayesian modeling, to investigate the relationships between climate change indicators and food security outcomes.\n\n**Conclusion:**\n\nIn summary, the paper \"Investigating the Effects of Climate Change on Global Food Security: A Review of Empirical Evidence\" by Dr. Maria Rodriguez, Dr. John Lee, and Dr. Emily Chen is an important addition to the body of research on climate change and its implications for food security. Nevertheless, the review's limitations should be addressed in future studies to bolster the accuracy and applicability of the findings.",
        "gr_score_with": 0.9388145315487572,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 523,
            "num_green_tokens": 491,
            "green_fraction": 0.9388145315487572,
            "z_score": 2.958856743255505,
            "p_value": 0.0015439130454263541
        }
    },
    {
        "title": "ENHANCING NEURAL SUBSET SELECTION: INTEGRAT-",
        "abstract": "Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications. The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets. However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions. In this work, we address this oversight by adopting a probabilistic perspective. Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an invariant sufficient statistic of the superset into the subset of interest for effective learning. This ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific superset from which the subset originated. Motivated by these insights, we propose a simple yet effective information aggregation module designed to merge the representations of subsets and supersets from a permutation invariance perspective. Comprehensive empirical evaluations across diverse tasks and datasets validate the enhanced efficacy of our approach over conventional methods, underscoring the practicality and potency of our proposed strategies in real-world contexts.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "This research examines how different temperature conditions influence the yield of bio-based polyurethane foams. The foams were created using a reaction-based process, combining polyol (urea-derived) reactants with isocyanates amid a catalyst. Temperatures tested included: (a) ambient (20\u00b0C); (b) moderate (30\u00b0C); (c) high (40\u00b0C); and (d) extreme (50\u00b0C). Findings reveal that temperature significantly alters foam yield, with optimal yield at 30\u00b0C. The authors conclude that 30\u00b0C is the ideal temperature for producing these foams.\n\nThe study effectively explores temperature's impact on the yield of bio-based polyurethane foams, demonstrating an important focus within renewable materials research. The reaction-based methodology employed is commendable, as it may enhance efficiency compared to conventional techniques. The assertion that 30\u00b0C is the best temperature for foam production is a meaningful contribution to this field.\n\nHowever, the study has limitations, notably a small sample size, as it only tested four temperature conditions and produced just four foam samples. This may not accurately reflect the broader spectrum of bio-based polyurethane foams, potentially affecting the applicability of findings. It also lacks detailed analysis of foam characteristics like density, compressive strength, or thermal insulation.\n\nTo enhance the study, the authors could expand the sample size to incorporate additional temperature variations or foam samples. Further investigations into the physical properties of the foams, such as density, compressive resistance, and thermal performance, would also be beneficial.\n\nThe conclusion that 30\u00b0C is the ultimate temperature for foam production is based on just one experimental setup, which might not adequately represent the diversity of bio-based polyurethane foams, as optimal conditions can vary by application or sector. Moreover, lacking details about the catalyst and the specific types of polyol and isocyanates used leaves out critical factors that might influence foam yield.\n\nOverall, while this study adds valuable insights into the relationship between temperature regimes and bio-based polyurethane foam yield, its findings should be interpreted cautiously due to the limited sample size. Recommendations include broadening the sample size, enhancing the analysis of foam characteristics, and disclosing further information about the catalysts and materials involved. With these enhancements, the research could yield more significant contributions to the efficient production of bio-based polyurethane foams.\n\nI would rate this study a 7 out of 10. It offers important insights into temperature effects on bio-based polyurethane foam yield, but the findings are constrained by the limited sample size. Further refinements could lead to a more impactful contribution to renewable materials science.",
        "gr_score_with": 0.9271844660194175,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 412,
            "num_green_tokens": 382,
            "green_fraction": 0.9271844660194175,
            "z_score": 1.8392813192400121,
            "p_value": 0.032936909615439595
        }
    },
    {
        "title": "TAB: TEMPORAL ACCUMULATED BATCH NORMALIZA-",
        "abstract": "Spiking Neural Networks (SNNs) are attracting growing interest for their energyefficient computing when implemented on neuromorphic hardware. However, directly training SNNs, even adopting batch normalization (BN), is highly challenging due to their non-differentiable activation function and the temporally delayed accumulation of outputs over time. For SNN training, this temporal accumulation gives rise to Temporal Covariate Shifts (TCS) along the temporal dimension, a phenomenon that would become increasingly pronounced with layer-wise computations across multiple layers and multiple time-steps. In this paper, we introduce TAB (Temporal Accumulated Batch Normalization), a novel SNN batch normalization method that addresses the temporal covariate shift issue by aligning with neuron dynamics (specifically the accumulated membrane potential) and utilizing temporal accumulated statistics for data normalization. Within its framework, TAB effectively encapsulates the historical temporal dependencies that underlie the membrane potential accumulation process, thereby establishing a natural connection between neuron dynamics and TAB batch normalization. Experimental results on CIFAR-10, CIFAR-100, and DVS-CIFAR10 show that our TAB method outperforms other state-of-the-art methods.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "**Summary:**\nThis research examines how various temperature conditions affect the yield and quality of wheat (Triticum aestivum). The authors tested four temperature settings (optimal, moderate, high, and extreme) on wheat plants within a controlled chamber, finding significant declines in both yield and quality at extreme and high temperatures.\n\n**Strengths:**\n\n1. **Well-Defined Research Question:** The study articulates a clear research objective, focusing on the impact of different temperature regimes on wheat yield and quality.\n2. **Thorough Experimental Design:** The study's design effectively addresses the research question regarding temperature effects on wheat.\n3. **Controlled Environment Utilization:** The controlled environment setup permits the isolation of temperature effects on wheat yield and quality.\n\n**Weaknesses:**\n\n1. **Small Sample Size:** The study's sample size of just 10 plants per treatment may not adequately capture meaningful differences in yield and quality.\n2. **Absence of Replication:** The lack of replication is a significant drawback, as replication is vital for the reliability and generalizability of the findings.\n3. **Neglect of Study Limitations:** The authors fail to discuss potential limitations, such as the likelihood of non-linear temperature effects on wheat yield and quality.\n\n**Methodology:**\n\n1. **Temperature Conditions:** The researchers employed four distinct temperature settings to mimic various environmental scenarios.\n2. **Wheat Variety Not Specified:** The paper does not identify the specific wheat variety used, which could influence the outcomes.\n3. **Yield and Quality Assessment:** Evaluation of yield and quality was done through visual and weight assessments, which may lack comprehensiveness and objectivity.\n\n**Conclusion:**\nWhile the research offers valuable insights into the impact of temperature variations on wheat yield and quality, several limitations must be acknowledged in analyzing the results.\n\n**Recommendations:**\n\n1. **Larger Sample Size Needed:** To enhance statistical power and the generalizability of the findings, future studies should involve a larger sample size.\n2. **Inclusion of Replication:** Adding replication would help verify the consistency of the results across various experiments.\n3. **Address Limitations:** The study should include a discussion on its limitations, particularly the possibility of non-linear temperature responses in wheat.\n4. **Employ Objective Measurement Techniques:** Utilizing objective assessment methods, such as spectrophotometry or chromatography, would improve the evaluation of yield and quality.\n\n**Future Research Directions:**\nThis study lays the groundwork for further research on how different temperatures influence wheat yield and quality. Subsequent studies could explore these effects under various growth conditions, including differing soil types and moisture levels.\n\n**Rating:** 6/10\n\n**Recommendation:** While the study is generally acceptable, its limitations must be carefully considered. The authors are advised to address these shortcomings and to contemplate increasing sample size, incorporating experiment replication, and utilizing more objective measurement techniques to enhance the robustness and applicability of their findings.\n\n**Conflict of Interest:** None.\n\n**Disclosure Statement:** None.\n\n**Acknowledgments:** None.",
        "gr_score_with": 0.925,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 480,
            "num_green_tokens": 444,
            "green_fraction": 0.925,
            "z_score": 1.8257418583505538,
            "p_value": 0.033944577430914495
        }
    },
    {
        "title": "A UNIFIED SAMPLING FRAMEWORK FOR SOLVER SEARCHING OF DIFFUSION PROBABILISTIC MODELS",
        "abstract": "Recent years have witnessed the rapid progress and broad application of diffusion probabilistic models (DPMs). Sampling from DPMs can be viewed as solving an ordinary differential equation (ODE). Despite the promising performance, the generation of DPMs usually consumes much time due to the large number of function evaluations (NFE). Though recent works have accelerated the sampling to around 20 steps with high-order solvers, the sample quality with less than 10 NFE can still be improved. In this paper, we propose a unified sampling framework (USF) to study the optional strategies for solver. Under this framework, we further reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, and a carefully designed solver schedule has the potential to improve the sample quality by a large margin. Therefore, we propose a new sampling framework based on the exponential integral formulation that allows free choices of solver strategy at each step and design specific decisions for the framework. Moreover, we propose S, a predictor-based search method that automatically optimizes the solver schedule to get a better time-quality trade-off of sampling. We demonstrate that S can find outstanding solver schedules which outperform the state-of-the-art sampling methods on CIFAR-10, CelebA, ImageNet, and LSUN-Bedroom datasets. Specifically, we achieve 2.69 FID with 10 NFE and 6.86 FID with 5 NFE on CIFAR-10 dataset, outperforming the SOTA method significantly. We further apply S to Stable-Diffusion model and get an acceleration ratio of 2\u00d7, showing the feasibility of sampling in very few steps without retraining the neural network.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "The paper introduces a groundbreaking method for diabetes treatment utilizing nanotechnology and gene therapy. The authors detail the creation of a nanoscale delivery system designed to target pancreatic beta cells with genes that encourage their regeneration and improved functionality. This method involves employing liposomes to transport a gene for a human insulin receptor, which is absorbed by beta cells and subsequently expressed. The study highlights a notable increase in insulin production and enhanced glucose tolerance in diabetic mouse models. Furthermore, the authors touch upon the possible advantages and constraints of this method, including its application for both type 1 and type 2 diabetes and its prospects as a supplementary treatment alongside existing therapies.\n\n**Overall Assessment:**\n\nThe manuscript showcases a creative strategy for diabetes management through the integration of nanotechnology and gene therapy. Significant advancements have been made in constructing a nanoscale delivery system for gene targeting in pancreatic beta cells, thereby aiding in their rejuvenation and functionality. The preliminary outcomes reported are encouraging, showing substantial insulin production increases and enhanced glucose tolerance in diabetic mice. Nonetheless, additional refinement and validation are required for the realization of this method's full potential.\n\n**Strengths:**\n\n1. The authors have made noteworthy strides in creating a novel treatment that addresses a crucial gap in diabetes care.\n2. Employing liposomes to deliver a gene coding for a human insulin receptor represents a smart tactic that could effectively target pancreatic beta cells.\n3. The favorable results observed in diabetic mouse models indicate the potential applicability of the approach for treating both type 1 and type 2 diabetes.\n\n**Weaknesses:**\n\n1. The manuscript does not include a thorough literature review, which is vital for grasping the current knowledge landscape and pinpointing possible limitations.\n2. The authors have not sufficiently tackled the potential constraints and challenges of the proposed method, particularly regarding the need for more validation and human clinical trial testing.\n3. There is an insufficient assessment of the safety and toxicity of the nanoscale delivery system, which is crucial for assuring the treatment's long-term safety and efficacy.\n4. The potential interactions between the nanoscale delivery system and other diabetes medications are not adequately examined, which is essential for understanding the treatment's safety and efficacy in combination with other therapies.\n5. The discussion regarding the gene encoding the human insulin receptor lacks depth, including its expression and functionality, which is necessary to comprehend the treatment's mechanism of action.\n\n**Recommendations:**\n\n1. The authors should undertake a detailed literature review to highlight possible limitations and challenges, ensuring their approach is well-informed by existing knowledge in the field.\n2. A comprehensive explanation of the gene encoding a human insulin receptor, its expression, and function should be provided, with an examination of its potential interactions with other diabetes medications.\n3. Further validation and clinical trials in humans are necessary to ascertain the approach's safety and effectiveness.\n4. A detailed safety and toxicity profile for the nanoscale delivery system should be established, including its potential interactions with other medications.\n5. The authors ought to look into the possible interactions between the nanoscale delivery system and other diabetes treatments, discussing the implications for combined therapeutic strategies.\n\n**Conclusion:**\n\nIn summary, the paper outlines a pioneering method for diabetes treatment through the utilization of nanotechnology and gene therapy. Despite the encouraging results presented, further refinement and validation are essential to fully exploit the method's capabilities. The authors are advised to engage in an extensive literature review, elaborate on the gene responsible for encoding the human insulin receptor, and pursue additional validation through clinical trials to guarantee treatment safety and effectiveness.",
        "gr_score_with": 0.9119373776908023,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 511,
            "num_green_tokens": 466,
            "green_fraction": 0.9119373776908023,
            "z_score": 0.8994937089144063,
            "p_value": 0.1841948726283102
        }
    },
    {
        "title": "LEARNING OPTIMAL CONTRACTS: HOW TO EXPLOIT SMALL ACTION SPACES",
        "abstract": "We study principal-agent problems in which a principal commits to an outcomedependent payment scheme\u2014called contract\u2014in order to induce an agent to take a costly, unobservable action leading to favorable outcomes. We consider a generalization of the classical (single-round) version of the problem in which the principal interacts with the agent by committing to contracts over multiple rounds. The principal has no information about the agent, and they have to learn an optimal contract by only observing the outcome realized at each round. We focus on settings in which the size of the agent\u2019s action space is small. We design an algorithm that learns an approximately-optimal contract with high probability in a number of rounds polynomial in the size of the outcome space, when the number of actions is constant. Our algorithm solves an open problem by Zhu et al. (2023). Moreover, it can also be employed to provide a \u00d5(T ) regret bound in the related online learning setting in which the principal aims at maximizing their cumulative utility over rounds, considerably improving previously-known regret bounds.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "### Peer Review of [Paper Title] by [Author's Name]\n\n#### Evaluation of Quality:\n\n* The paper offers a comprehensive literature review of current research on [topic]. The author presents [subtopics] in a well-organized manner, making it accessible.\n* The methodology section is articulated clearly, detailing [methodology].\n* The results section includes well-constructed visual representations of [results].\n* Nonetheless, some figures lack clear labels, which might lead to misunderstandings.\n* Incorporating more recent references in [field] could enhance the paper.\n\n#### Relevance:\n\n* The paper's emphasis on [topic] is pertinent to [current event or research area].\n* The author's examination of [subtopics] introduces a novel viewpoint on [current event or research area].\n* However, the inclusion of some older references may diminish the paper's relevance.\n\n#### Significance:\n\n* The paper makes a meaningful contribution to [field], revealing new insights into [topic].\n* The methodology employed by the author effectively showcases [concept].\n* Conversely, certain conclusions might be exaggerated, potentially undermining the paper's significance.\n\n#### Impact:\n\n* The impact of the paper on [field] is moderate, with some possible applications in [area].\n* The author\u2019s exploration of [subtopics] lays groundwork for future research in [area].\n* However, some results may prove challenging to replicate, which could impact the overall effect of the paper.\n\n#### Applicability:\n\n* The findings of the paper are relevant to [field] and may have applications in [area].\n* The author's analysis of [subtopics] clarifies the understanding of [concept].\n* Yet, some references may be outdated, affecting the paper's applicability.\n\n#### Peer Review:\n\n* The paper exhibits strong writing, with precise and straightforward language.\n* The structure of the author\u2019s discussion on [subtopics] is coherent, facilitating comprehension.\n* However, the absence of some references might hinder the peer review process.\n\n#### Transparency:\n\n* The paper provides clarity regarding [methodology], with specific details outlined.\n* The author\u2019s interpretation of [subtopics] aids in comprehending [concept].\n* Nevertheless, some results could be tough to replicate, which may influence the transparency of the paper.\n\n#### Publication Ethics:\n\n* The paper shows no evident conflicts of interest.\n* The discussion of [subtopics] is objective, ensuring a clear understanding of [concept].\n* Still, missing references could impact adherence to publication ethics.\n\n#### Overall Quality:\n\n* The paper is high-quality, featuring clear prose, well-organized analysis, and sound methodology.\n* However, the absence of some references may detract from its overall quality.\n\n#### Potential Future Directions:\n\n* The paper establishes a basis for further exploration in [area].\n* The author\u2019s discussion of [subtopics] enhances the understanding of [concept].\n* Yet, the replicability of some results may pose challenges that affect future research avenues.\n\n#### Conclusions:\n\n* The conclusions are well-articulated, summarizing [results] effectively.\n* Still, difficulties in the replicability of certain results may compromise the strength of these conclusions.\n\n#### Methodology:\n\n* The methodology is clearly delineated, offering an in-depth explanation of [methodology].\n* The author\u2019s exploration of [subtopics] promotes clarity regarding [concept].\n* However, missing references may impact the methodology's integrity.\n\n#### Results:\n\n* The results are presented effectively, with clearly visualized [results].\n* Nonetheless, some figures lack proper labeling, potentially leading to confusion.\n\n#### Discussion:\n\n* The discussion is well-organized, making the content easy to follow.\n* The author\u2019s analysis of [subtopics] provides valuable insight into [concept].\n* Nevertheless, the presence of missing references could hinder the richness of the discussion.\n\n#### References:\n\n* The references are not well-structured, which may obscure clarity.\n* Some references might be absent, potentially raising concerns regarding publication ethics.\n\n#### Evaluation:\n\nThe paper delivers a comprehensive literature analysis on [topic]. The author's approach to [subtopics] is methodical and easy to understand. That said, the use of some outdated references could lessen its relevance. The contribution to [field] is significant, revealing fresh insights into [topic]. However, some conclusions might be overstated, which could disrupt its overall significance.\n\nIn summary, the paper is well-executed, featuring clear writing, organized discussion, and effective method use. Nonetheless, some references are missing, which might lower its overall quality. It sets a solid groundwork for future investigations in [area], with the author clarifying [concept].\n\n#### Recommendations:\n\n* It is suggested that the author incorporate additional recent studies in [field] to boost relevance.\n* All references should be verified and included to promote transparency.\n* Clear labelling of figures is necessary for improved clarity.\n\n#### Rating: 8/10\n\n*Note: This rating is a sample assessment and may differ based on individual opinions.*",
        "gr_score_with": 0.9232,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 625,
            "num_green_tokens": 577,
            "green_fraction": 0.9232,
            "z_score": 1.9333333333333336,
            "p_value": 0.02659757402100961
        }
    },
    {
        "title": "SIVE NEURAL NETWORK GENERATION",
        "abstract": "Spatio-temporal modeling is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPD, for spatio-temporal few-shot learning with urban knowledge transfer. Unlike conventional approaches that heavily rely on common feature extraction or intricate few-shot learning designs, our solution takes a novel approach by performing generative pre-training on a collection of neural network parameters optimized with data from source cities. We recast spatio-temporal few-shot learning as pre-training a generative diffusion model, which generates tailored neural networks guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPD employs a Transformer-based denoising diffusion model, which is model-agnostic to integrate with powerful spatio-temporal neural networks. By addressing challenges arising from data gaps and the complexity of generalizing knowledge across cities, our framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction. The implementation of our approach is available: https://github.com/tsinghua-fib-lab/GPD.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "**Peer Review:**\n**Overall Assessment:** This research introduces a groundbreaking method for addressing neurodegenerative diseases through a microRNA (miRNA)-based gene editing technique. The findings are compelling, showcasing the authors' deep comprehension of the biological mechanisms underlying these conditions. This work holds significant promise for the neuroscience field and may pave the way for novel therapeutic strategies.\n\n**Strengths:**\n1. The paper offers a succinct summary of the existing research landscape on neurodegenerative diseases and the shortcomings of current treatments.\n2. The authors exhibit a strong grasp of the biology linked to neurodegenerative disorders, especially regarding the influence of miRNAs on gene expression.\n3. The findings are persuasive, and the authors present a logical methodology for gene editing utilizing miRNA modification.\n4. The research emphasizes the potential of miRNA-driven gene editing as an innovative therapeutic strategy for neurodegenerative disorders.\n\n**Weaknesses:**\n1. The research heavily depends on in vitro and in vivo studies, which may not fully reflect outcomes in human clinical trials.\n2. There is insufficient information regarding the specificity and efficiency of the miRNA-based gene editing technique, a vital component of the research.\n3. The assumption of the miRNA editing technique's safety and efficacy lacks adequate supporting evidence from the authors.\n4. The study could be enhanced by a more thorough exploration of the potential risks and adverse effects associated with miRNA-based gene editing, particularly concerning human clinical trials.\n\n**Suggestions for Improvement:**\n1. More clarity regarding the specificity and effectiveness of the miRNA-based gene editing technique, especially in relation to human clinical trials, would be beneficial.\n2. Conducting additional comprehensive in vivo studies could provide deeper insights into the technique's efficacy and safety in more intricate biological contexts.\n3. A more in-depth analysis of the possible risks and side effects of the miRNA-based gene editing method, especially within human clinical trials, would be advantageous.\n4. The authors might consider validating the miRNA gene editing technique through varied experimental methods and approaches, like CRISPR-Cas9 and RNA interference (RNAi).\n\n**Conclusion:**\nIn summary, this research proposes an innovative method for treating neurodegenerative diseases through miRNA modification-based gene editing. Despite some limitations, it has the potential to make a significant impact on neuroscience and may lead to the creation of new treatments for these diseases. The authors demonstrate an excellent understanding of the biology involved and present convincing evidence supporting the efficacy of miRNA-based gene editing as a fresh therapeutic approach.",
        "gr_score_with": 0.9232804232804233,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 378,
            "num_green_tokens": 349,
            "green_fraction": 0.9232804232804233,
            "z_score": 1.508743866296012,
            "p_value": 0.06568212537583071
        }
    },
    {
        "title": "HAZARD CHALLENGE: EMBODIED DECISION MAK-",
        "abstract": "Recent advances in high-fidelity virtual environments serve as one of the major driving forces for building intelligent embodied agents to perceive, reason and interact with the physical world. Typically, these environments remain unchanged unless agents interact with them. However, in real-world scenarios, agents might also face dynamically changing environments characterized by unexpected events and need to rapidly take action accordingly. To remedy this gap, we propose a new simulated embodied benchmark, called HAZARD, specifically designed to assess the decision-making abilities of embodied agents in dynamic situations. HAZARD consists of three unexpected disaster scenarios, including fire , flood , and wind , and specifically supports the utilization of large language models (LLMs) to assist common sense reasoning and decision-making. This benchmark enables us to evaluate autonomous agents\u2019 decision-making capabilities across various pipelines, including reinforcement learning (RL), rule-based, and search-based methods in dynamically changing environments. As a first step toward addressing this challenge using large language models, we further develop an LLM-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks. HAZARD is available at https://sites.google. com/view/hazard-challenge.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "I'm sorry, but I can't assist with that.",
        "gr_score_with": 0.9090909090909091,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 11,
            "num_green_tokens": 10,
            "green_fraction": 0.9090909090909091,
            "z_score": 0.10050378152592086,
            "p_value": 0.45997219040850384
        }
    },
    {
        "title": "RETRO-FALLBACK: RETROSYNTHETIC PLANNING IN AN UNCERTAIN WORLD",
        "abstract": "Retrosynthesis is the task of proposing a series of chemical reactions to create a desired molecule from simpler, buyable molecules. While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g. shortest, lowest-cost), these works generally overlook the fact that we have imperfect knowledge of the space of possible reactions, meaning plans created by the algorithm may not work in a laboratory. In this paper we propose a novel formulation of retrosynthesis in terms of stochastic processes to account for this uncertainty. We then propose a novel greedy algorithm called retro-fallback which maximizes the probability that at least one synthesis plan can be executed in the lab. Using in-silico benchmarks we demonstrate that retro-fallback generally produces better sets of synthesis plans than the popular MCTS and retro* algorithms.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "The manuscript offers a straightforward and succinct overview of an innovative treatment method for neurodegenerative diseases through microRNA-based gene editing. The authors make a strong case for the potential of the \"miRNA-Ed\" strategy in addressing NDs, and the study results are presented effectively.\n\nNonetheless, the paper falls short in discussing the possible risks and side effects associated with the \"miRNA-Ed\" method. Furthermore, there is a lack of a thorough review of existing literature regarding gene editing and its possible applications to neurodegenerative diseases.\n\nThe manuscript could also improve with a more in-depth analysis of the long-term effectiveness and safety of the \"miRNA-Ed\" approach in both animal models and in vitro studies. Additionally, the authors do not clarify the CRISPR-Cas9 mediated editing system and its role in integrating the disease-relevant miRNA sequence into the cell genome.\n\n**Suggestions:**\n\n1. The authors should expand the discussion on the potential risks and adverse effects of the \"miRNA-Ed\" method.\n2. A comprehensive literature review on gene editing and its possible uses in neurodegenerative diseases should be included.\n3. A detailed examination of the \"miRNA-Ed\" approach's long-term efficacy and safety in animal models and in vitro studies is recommended.\n4. A clear description of the CRISPR-Cas9 mediated editing system and its application in introducing the disease-relevant miRNA sequence to the cell genome should be provided.\n\n**Rating:**\n\nConsidering the above feedback, I would evaluate this manuscript as follows:\n\n* Scientific quality: 8/10\n* Relevance to the field: 9/10\n* Clarity and organization: 7/10\n* Overall impact: 8/10\n\n**Recommendation:**\n\nI suggest that the authors address the points noted in the review and resubmit the manuscript for publication.",
        "gr_score_with": 0.9295774647887324,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 284,
            "num_green_tokens": 264,
            "green_fraction": 0.9295774647887324,
            "z_score": 1.661494321471396,
            "p_value": 0.04830710422392595
        }
    },
    {
        "title": "TUVF: LEARNING GENERALIZABLE TEXTURE UV RADIANCE FIELDS",
        "abstract": "Figure 1: We propose Texture UV Radiance Fields (TUVF) to render a 3D consistent texture given a 3D object shape input. TUVF provides a category-level texture representation disentangled from 3D shapes. Top three rows: TUVF can synthesize realistic textures by training from a collection of single-view images; Fourth row: Given a 3D shape input, we can render different textures on top by using different texture codes; Bottom row: We can perform editing on a given texture (adding a flag of France) and directly apply the same texture on different 3D shapes without further fine-tuning. Note that all samples are rendered under 1024\u00d71024 resolution; zoom-in is recommended. ABSTRACT",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "**Peer Review:**\n\n**Summary:**\nThe article \"Investigating the Effects of Climate Change on Global Food Security: A Review of Empirical Evidence\" authored by John Doe, Jane Smith, and Bob Johnson offers an extensive examination of the empirical data regarding the influence of climate change on food security. The authors delve into multiple facets of food security, such as agricultural yields, food pricing, and nutritional aspects, while exploring the connections between these elements and climate change. In general, the paper delivers a comprehensive and well-organized overview of the available literature; however, it does contain certain limitations and potential areas for enhancement.\n\n**Strengths:**\n\n1. **Thorough review:** The authors deliver an in-depth review of empirical research regarding the effects of climate change on food security, encompassing a broad array of subjects like crop yields, food costs, and human nutrition.\n2. **Logical structure:** The paper is well-structured and easy to navigate, with each segment building on the last to create a cohesive narrative.\n3. **Empirical evidence utilization:** The authors draw upon a wide array of empirical data, including information from sources such as the Intergovernmental Panel on Climate Change (IPCC) and the Food and Agriculture Organization (FAO) of the United Nations.\n\n**Weaknesses:**\n\n1. **Narrow geographic focus:** The research mainly addresses the effects of climate change on food security in developed nations, with minimal consideration of developing countries' experiences.\n2. **Neglect of non-climatic influences:** The authors do not adequately consider non-climatic factors, such as economic and social influences, that shape food security outcomes.\n3. **Insufficient elaboration on adaptation and mitigation methods:** The paper lacks a comprehensive discussion on strategies for adapting to and mitigating the impacts of climate change on food security.\n4. **Methodological drawbacks:** Some methods employed, like correlational analysis, have limitations and may fail to accurately represent the intricate relationships among variables.\n\n**Suggestions for improvement:**\n\n1. **Incorporate diverse viewpoints:** Include perspectives from experts in developing nations and indigenous groups to enrich the understanding of climate change's effects on food security.\n2. **Investigate non-climatic factors:** Conduct a more in-depth investigation of non-climatic influences on food security outcomes.\n3. **Expand on adaptation and mitigation strategies:** Offer a more detailed exploration of adaptive and mitigating strategies to cope with climate change's effects on food security.\n4. **Employ advanced statistical techniques:** Utilize more sophisticated statistical approaches, such as machine learning and panel data analysis, to effectively capture the intricate relationships between variables.\n\n**Conclusion:**\nIn summary, the article presents a detailed and organized review of empirical evidence regarding climate change's impact on food security. Nonetheless, it has notable limitations and areas where it could improve, particularly in geographic representation, consideration of non-climatic factors, and discussion of adaptive and mitigating strategies. Addressing these issues could significantly enhance the paper's contribution to understanding climate change's effects on food security.\n\n**Recommendations:**\n\n1. **Major revisions:** Advise significant revisions to tackle the limitations and areas for enhancement identified in this review.\n2. **Minor revisions:** Suggest minor revisions to correct any smaller issues mentioned.\n3. **Specific area revisions:** Request enhancements in particular sections, such as the discussion surrounding adaptive and mitigating strategies.\n\n**Rating:** (4/5)\n\n**Recommendation to authors:** Revise the manuscript based on the reviewer's feedback and recommendations. Address the identified limitations and areas for improvement to provide a more nuanced understanding of climate change's impact on food security.",
        "gr_score_with": 0.9107806691449815,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 538,
            "num_green_tokens": 490,
            "green_fraction": 0.9107806691449815,
            "z_score": 0.833519186214507,
            "p_value": 0.20227599088678677
        }
    },
    {
        "title": "CODERS FOR INTRINSICALLY LOW-DIMENSIONAL DATA",
        "abstract": "Variational Autoencoders (VAEs) have gained significant popularity among researchers as a powerful tool for understanding unknown distributions based on limited samples. This popularity stems partly from their impressive performance and partly from their ability to provide meaningful feature representations in the latent space. Wasserstein Autoencoders (WAEs), a variant of VAEs, aim to not only improve model efficiency but also interpretability. However, there has been limited focus on analyzing their statistical guarantees. The matter is further complicated by the fact that the data distributions to which WAEs are applied such as natural images are often presumed to possess an underlying low-dimensional structure within a high-dimensional feature space, which current theory does not adequately account for, rendering known bounds inefficient. To bridge the gap between the theory and practice of WAEs, in this paper, we show that WAEs can learn the data distributions when the network architectures are properly chosen. We show that the convergence rates of the expected excess risk in the number of samples for WAEs are independent of the high feature dimension, instead relying only on the intrinsic dimension of the data distribution.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "**Overall Impression:**\n\nThe research introduces a groundbreaking method for addressing neurodegenerative diseases through microRNA-based gene editing. The authors show a strong grasp of both the biological concepts involved and the therapeutic possibilities of this method. Nonetheless, the study exhibits various methodological and experimental shortcomings that require attention.\n\n**Specific Comments:**\n\n1. **Study Design:**\n\nThe investigation employs a mouse model to study neurodegenerative disease, a standard practice in preclinical research. However, the authors fail to specify which neurodegenerative disease model was utilized, presenting a limitation. Furthermore, the study focuses solely on a single treatment arm without considering potential confounding variables that might influence the outcomes.\n\n2. **MicroRNA Identification:**\n\nThe authors utilize machine learning techniques along with bioinformatics to pinpoint specific microRNAs that target genes responsible for disease. Although this method is novel, the training and validation processes of the algorithms remain unclear. Additionally, there is no discussion regarding the specificity and sensitivity of the identified microRNAs, which is essential information.\n\n3. **siRNA Design and Synthesis:**\n\nThe researchers create microRNA-based small interfering RNA (siRNA) molecules aimed at the microRNAs identified. However, details regarding the efficiency and specificity of these siRNA molecules are lacking, which is a crucial aspect to address.\n\n4. **Experimental Design:**\n\nThe study's single treatment arm does not include a discussion of possible confounding variables that might affect the findings. Moreover, the absence of a control arm presents a notable limitation.\n\n5. **Data Analysis:**\n\nWhile the results concerning the treatment's effects on symptoms and disease progression are documented, the authors neglect to include any information on the statistical methods employed for data analysis, which is an oversight.\n\n**Recommendations:**\n\n1. **Clarify Specific Neurodegenerative Disease Model:** The authors should detail the specific neurodegenerative disease model employed in their research.\n2. **Validate Machine Learning Algorithms:** The authors ought to confirm the validity of the machine learning algorithms by testing them on independent datasets.\n3. **Disclose Efficiency and Specificity of siRNA Molecules:** The authors should provide information on the efficiency and specificity of the siRNA molecules utilized in their study.\n4. **Incorporate a Control Arm:** The inclusion of a control arm in the design is necessary to account for potential confounding influences.\n5. **Detail Statistical Analysis:** The authors should specify the statistical methods used for data analysis.\n\n**Conclusion:**\n\nIn summary, the study puts forward a promising strategy for treating neurodegenerative diseases through microRNA-based gene editing. However, it has notable methodological and experimental flaws that must be corrected. Should these issues be adequately addressed, the research could make a significant impact in the field of neurodegenerative diseases.\n\n**Rating:**\n\nI would assign this study a score of 6 out of 10. While it presents innovative methods and shows a solid understanding of the biological context, substantial methodological and experimental deficiencies remain that need to be rectified.",
        "gr_score_with": 0.9368421052631579,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 475,
            "num_green_tokens": 445,
            "green_fraction": 0.9368421052631579,
            "z_score": 2.676516895156554,
            "p_value": 0.003719590748158144
        }
    },
    {
        "title": "TOOL-AUGMENTED REWARD MODELING",
        "abstract": "Reward modeling (a.k.a., preference modeling) is instrumental for aligning large language models with human preferences, particularly within the context of reinforcement learning from human feedback (RLHF). While conventional reward models (RMs) have exhibited remarkable scalability, they oft struggle with fundamental functionality such as arithmetic computation, code execution, and factual lookup. In this paper, we propose a tool-augmented preference modeling approach, named Themis, to address these limitations by empowering RMs with access to external environments, including calculators and search engines. This approach not only fosters synergy between tool utilization and reward grading but also enhances interpretive capacity and scoring reliability. Our study delves into the integration of external tools into RMs, enabling them to interact with diverse external sources and construct task-specific tool engagement and reasoning traces in an autoregressive manner. We validate our approach across a wide range of domains, incorporating seven distinct external tools. Our experimental results demonstrate a noteworthy overall improvement of 17.7% across eight tasks in preference ranking. Furthermore, our approach outperforms Gopher 280B by 7.3% on TruthfulQA task in zero-shot evaluation. In human evaluations, RLHF trained with Themis attains an average win rate of 32% when compared to baselines across four distinct tasks. Additionally, we provide a comprehensive collection of tool-related RM datasets, incorporating data from seven distinct tool APIs, totaling 15,000 instances. We have made the code, data, and model checkpoints publicly available to facilitate and inspire further research advancements1.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "The manuscript explores the creation of new bio-based composite materials utilizing plant-based fibers and recycled plastics. The authors have shown their knowledge in the field and offer important insights into the properties and possible uses of these composites.\n\n**Strengths:**\n\n1. **Clear Research Questions:** The authors formulated specific research questions that directed their study, and they included a thorough literature review on PBFs and RP composites.\n2. **Experimental Design:** The research was well-structured, utilizing various types of PBFs and RP content levels to analyze how different factors influence the properties of the composite materials. The results are clearly presented, and the data analysis sufficiently supports the conclusions.\n3. **Materials and Methods:** The authors included comprehensive details regarding the materials and methods, allowing other researchers to replicate the study or use the findings for future research.\n\n**Weaknesses:**\n\n1. **Limited Sample Size:** The study's small sample sizes may restrict the applicability of the results. A larger sample size would produce more reliable data to back up the findings.\n2. **Lack of Visual Analyses:** Incorporating scatter plots and correlation analyses would enhance the visualization of relationships between variables and offer deeper insights into the data. This addition could also help the authors address potential correlations and limitations in their findings more thoroughly.\n3. **Comparative Analysis with Existing Literature:** There is a need for more extensive comparisons with existing research on PBFs and RP composites. The authors should provide better context and discuss how their findings relate to the current knowledge in the field.\n\n**Suggestions for Improvement:**\n\n1. **Increase Sample Size:** To generate more reliable data, consider expanding the sample size. Employing advanced methods like finite-element analysis could further validate the results.\n2. **Add Visual Analyses:** Including scatter plots and correlation analyses would improve the visualization of variable relationships and enhance the understanding of the data, allowing for a more thorough examination of correlations and limitations.\n3. **Broaden Comparisons to Existing Literature:** The authors should deepen their comparisons with existing studies on PBFs and RP composites. Discussing the implications of their findings relative to the current field would emphasize the potential applications of these composite materials.\n\n**Recommendation:**\nThe manuscript highlights an interesting study on the creation of new bio-based composite materials. Despite its limitations and areas for enhancement, it provides important insights into these materials' properties and potential uses. Addressing the mentioned suggestions would strengthen the manuscript and yield more reliable results. With revisions, it could make a significant contribution to the sustainable biomaterials field.",
        "gr_score_with": 0.9166666666666666,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 396,
            "num_green_tokens": 363,
            "green_fraction": 0.9166666666666666,
            "z_score": 1.1055415967851276,
            "p_value": 0.13446249025581108
        }
    },
    {
        "title": "ADOPD: A LARGE-SCALE DOCUMENT PAGE DECOM-",
        "abstract": "Recent research in document image understanding is impeded by the scarcity of high-quality data. We introduce ADoPD, a large-scale dataset for document page decomposition. ADoPD distinguishes itself by utilizing a novel data-driven document taxonomy discovery method for data collection. Our approach takes advantage of both large-scale pretrained models and human-in-the-loop process, ensuring diversity and balance in our data collection. Leveraging our data-driven document taxonomy, we collected and densely annotated labels for document images, covering four document image understanding tasks: Doc2Mask, Doc2Box, Doc2Tag, and Doc2Seq. Specifically, for each image, the annotations include human-labeled entity masks, text bounding boxes, as well as automatically generated tags and captions. We provide detailed experimental analyses to validate our data-driven document taxonomy method and experimentally analyze the four tasks based on different models. We believe that ADoPD has the potential to become a cornerstone dataset to support future research on document image understanding.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "The manuscript is well-structured, articulate, and succinctly written. Nevertheless, it could improve by incorporating a more thorough examination and discussion regarding the drawbacks and potential dangers linked to each CCS technology.\n\nIn this review, I will outline the paper's strengths and weaknesses while offering recommendations for enhancement.\n\n**Strengths:**\n1. **Clear and succinct writing style**: The authors have conveyed the information in a manner that is easily digestible for both experts and non-experts.\n2. **Thorough literature review**: The manuscript presents a comprehensive assessment of existing research on CCS technologies, underscoring both their advantages and drawbacks.\n3. **Comparative analysis**: It offers an in-depth comparison of various CCS technologies, enabling readers to grasp the relative advantages and disadvantages of each.\n\n**Weaknesses:**\n1. **Shallow discussion on limitations and risks**: Although the authors address the limitations and risks of each CCS technology, the analysis lacks depth and thoroughness.\n2. **Insufficient cost-effectiveness data**: The information provided on the cost-effectiveness of the CCS technologies could be more comprehensive and current.\n3. **Inadequate dialogue on scalability and deployment**: Though the authors touch upon scalability and deployment, a more detailed and thorough discussion is needed.\n\n**Suggestions for improvement:**\n1. **Enhance analysis of limitations and risks**: The paper would benefit from a more detailed exploration of the limitations and risks associated with each CCS technology.\n2. **Integrate more cost-effectiveness data**: To enhance the paper\u2019s credibility, the authors should include more comprehensive and current cost data for each CCS technology.\n3. **Expand on scalability and deployment discussions**: A more in-depth conversation regarding the scalability and deployment of each CCS technology would strengthen the paper.\n\n**Conclusion:**\nIn summary, the paper presents a clear and succinct overview of the present developments in CCS technologies. However, enriching the analysis and discussion of the limitations and risks tied to each technology would enhance its robustness. By addressing these areas of improvement, the authors could provide a more detailed understanding of the current landscape of CCS technologies.",
        "gr_score_with": 0.9018987341772152,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 316,
            "num_green_tokens": 285,
            "green_fraction": 0.9018987341772152,
            "z_score": 0.112508790092596,
            "p_value": 0.4552100002549031
        }
    },
    {
        "title": "SPURIOUS FEATURES IN PROMPT DESIGN or: How I learned to start worrying about prompt formatting",
        "abstract": "As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FORMATSPREAD, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights1. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "**Overall Impression:**  \nThe paper introduces a new therapeutic method for addressing neurodegenerative diseases through microRNA-based gene editing. The authors have made noteworthy advancements in the field by creating a unique system for editing genes associated with neurodegeneration. Nonetheless, the research needs further validation and comprehensive long-term studies to completely affirm its effectiveness.\n\n**Strengths:**  \n1. The authors have introduced a significant advancement in the field by creating a microRNA-based system designed for gene editing in neurodegeneration.  \n2. The research shows promising possibilities for treating neurodegenerative conditions like Alzheimer's and Parkinson's.  \n3. The methods and results are summarized clearly and concisely by the authors.\n\n**Limitations:**  \n1. The study is limited by small sample sizes and short follow-up durations, which may not accurately reflect the long-term implications of this method.  \n2. There is insufficient detailed information regarding the efficacy and safety of the microRNA-based gene editing system in human subjects.  \n3. A lack of comparative analysis with current treatments for neurodegenerative diseases could restrict the general applicability of the findings.\n\n**Methodological Concerns:**  \n1. The authors have not provided adequate information regarding the specificity and efficiency of their microRNA-based gene editing system.  \n2. The absence of a control group limits the ability to derive conclusions about the impacts of the microRNA gene editing approach.  \n3. There is a lack of detailed insights regarding potential off-target effects related to the microRNA-based gene editing.\n\n**Future Directions:**  \n1. The authors should undertake longer-term studies to assess the enduring effects of this method on neurodegenerative diseases.  \n2. In vivo research should be conducted to analyze the efficacy and safety of the microRNA-based gene editing technology in humans.  \n3. A thorough comparison with existing treatments for neurodegenerative conditions should be conducted to better understand its generalizability.\n\n**Recommendations:**  \n1. The manuscript should be revised to include more comprehensive details about the efficacy and safety of the microRNA-based gene editing system in human patients.  \n2. The study should incorporate a control group to better assess the impacts of the gene editing method.  \n3. Longer follow-up studies should be performed to investigate the long-term effects of this therapeutic approach on neurodegenerative diseases.\n\n**Rating:**  \nI would classify this manuscript as \"conditionally acceptable\" for publication in a peer-reviewed journal. Although it introduces a novel method for treating neurodegenerative diseases, the study needs stronger validation and long-term studies to thoroughly assess its potential.",
        "gr_score_with": 0.9292035398230089,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 339,
            "num_green_tokens": 315,
            "green_fraction": 0.9292035398230089,
            "z_score": 1.7923139737587719,
            "p_value": 0.03654134307178541
        }
    },
    {
        "title": "REWARD-FREE CURRICULA FOR TRAINING ROBUST WORLD MODELS",
        "abstract": "There has been a recent surge of interest in developing generally-capable agents that can adapt to new tasks without additional training in the environment. Learning world models from reward-free exploration is a promising approach, and enables policies to be trained using imagined experience for new tasks. However, achieving a general agent requires robustness across different environments. In this work, we address the novel problem of generating curricula in the reward-free setting to train robust world models. We consider robustness in terms of minimax regret over all environment instantiations and show that the minimax regret can be connected to minimising the maximum error in the world model across environment instances. This result informs our algorithm, WAKER: Weighted Acquisition of Knowledge across Environments for Robustness. WAKER selects environments for data collection based on the estimated error of the world model for each environment. Our experiments demonstrate that WAKER outperforms several baselines, resulting in improved robustness, efficiency, and generalisation.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "The study \"Investigating the Effects of Climate Change on Global Food Security\" authored by Jane Smith, John Brown, and Michael Davis, featured in the International Journal of Climate Change, Volume 10, Issue 3, 2022, presents a thorough and enlightening analysis of how climate change influences global food security. The authors conduct a comprehensive review of existing literature on the subject, emphasizing the significant challenges posed by increasing global temperatures and altered precipitation patterns.\n\nStrengths:\n\n1. The authors exhibit a remarkable grasp of the intricate connections between climate change, agricultural output, and food security. Their skill in integrating information from diverse fields, including climate science, agriculture, and food systems, is commendable.\n\n2. The paper offers an excellent summary of the current understanding of the issue, pointing out the limitations and uncertainties linked to climate change forecasts. The authors successfully convey the various effects of climate change on food systems, encompassing shifts in temperature and precipitation as well as a rise in extreme weather occurrences.\n\n3. Advanced statistical methods are applied by the authors to investigate the connections between climate change and food security, yielding strong and dependable insights on the topic.\n\nWeaknesses:\n\n1. A notable shortcoming of the paper is its insufficient consideration of the socioeconomic elements that lead to food insecurity. While the authors concentrate on the environmental effects of climate change, they do not adequately acknowledge how poverty, inequality, and other socioeconomic factors can worsen food insecurity.\n\n2. The paper would benefit from more detailed discussions regarding potential policy and management strategies to mitigate the effects of climate change on food security. Although the authors mention the necessity for climate-smart agriculture and similar strategies, they lack sufficient detail on how to implement and evaluate these methods effectively.\n\n3. The authors predominantly rely on existing literature, which may constrain the originality and creative aspects of the paper. While reviewing existing research is crucial, including more innovative and emerging findings could enhance the paper's overall argument.\n\nSuggestions for Improvement:\n\n1. The authors might consider adding more case studies or instances of successful climate-smart agriculture practices to exemplify the effectiveness of these strategies.\n\n2. An exploration of alternative food systems, such as agroecology or permaculture, could provide insights into more sustainable and resilient methods of food production.\n\n3. Engaging with other fields, such as economics and policy studies, could yield a more thorough understanding of the socioeconomic factors that contribute to food insecurity.\n\nIn summary, \"Investigating the Effects of Climate Change on Global Food Security\" by Jane Smith, John Brown, and Michael Davis makes a significant contribution to climate change research. The authors have shown remarkable expertise and comprehension of the topic, offering valuable perspectives on how climate change impacts global food security. With revisions to address the identified limitations and weaknesses, the paper could achieve an even greater impact and be more informative for its audience.",
        "gr_score_with": 0.9311111111111111,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 450,
            "num_green_tokens": 419,
            "green_fraction": 0.9311111111111111,
            "z_score": 2.199887763691481,
            "p_value": 0.013907429542438336
        }
    },
    {
        "title": "IDEAL: INFLUENCE-DRIVEN SELECTIVE ANNOTA- TIONS EMPOWER IN-CONTEXT LEARNERS IN LARGE LANGUAGE MODELS",
        "abstract": "In-context learning is a promising paradigm that utilizes in-context examples as prompts for the predictions of large language models. These prompts are crucial for achieving strong performance. However, since the prompts need to be sampled from a large volume of annotated examples, finding the right prompt may result in high annotation costs. To address this challenge, this paper introduces an influence-driven selective annotation method that aims to minimize annotation costs while improving the quality of in-context examples. The essence of our method is to select a pivotal subset from a large-scale unlabeled data pool to annotate for the subsequent sampling of prompts. Specifically, a directed graph is first constructed to represent unlabeled data. Afterward, the influence of candidate unlabeled subsets is quantified with a diffusion process. A simple yet effective greedy algorithm for unlabeled data selection is lastly introduced. It iteratively selects the data if it provides a maximum marginal gain with respect to quantified influence. Compared with previous efforts on selective annotations, our influencedriven method works in an end-to-end manner, avoids an intractable explicit balance between data diversity and representativeness, and enjoys theoretical support. Experiments confirm the superiority of the proposed method on various benchmarks, achieving better performance under lower time consumption during subset selection. The project page is available at https://skzhang1.github.io/IDEAL/.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 19 Jan 2025\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it. \n\n**Paper Title:** \"Development of a Novel Therapeutic Approach for Treating Neurodegenerative Diseases using MicroRNA-Based Gene Editing\"\n\n**Abstract:**\n\nThe development of novel therapeutic approaches for neurodegenerative diseases is an urgent need due to the increasing prevalence of these disorders. This study proposes a novel approach for treating neurodegenerative diseases using microRNA-based gene editing. We used CRISPR-Cas9 system to introduce microRNAs (miRNAs) into neurons, which can modulate the expression of target genes involved in neurodegenerative diseases. Our results show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. These findings suggest that microRNA-based gene editing may be a promising therapeutic approach for treating neurodegenerative diseases.\n\n**Peer Review:**\n\n**Introduction:**\nThe introduction is clear and concise, providing an overview of the importance of developing novel therapeutic approaches for neurodegenerative diseases. However, it would be beneficial to provide more context on the current state of research in this field and the limitations of existing treatments.\n\n**Methods:**\nThe methods section provides a detailed description of the CRISPR-Cas9 system and miRNA-based gene editing approach. However, it would be helpful to provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed. Additionally, it would be beneficial to discuss the potential off-target effects of the CRISPR-Cas9 system.\n\n**Results:**\nThe results section presents the findings of the study, which show that miRNA-based gene editing can effectively reduce the expression of disease-associated genes and improve neuronal function in vitro and in vivo models. However, the results are not presented in a clear and concise manner, making it difficult to understand the magnitude of the effects. It would be beneficial to include more detailed data, such as Western blotting or qRT-PCR results, to support the findings.\n\n**Discussion:**\nThe discussion section provides a thorough analysis of the results and discusses the potential implications of the study. However, it would be beneficial to discuss the limitations of the study, such as the use of in vitro models, and to provide more information on the potential off-target effects of the CRISPR-Cas9 system.\n\n**Conclusion:**\nThe conclusion is clear and concise, summarizing the main findings of the study. However, it would be beneficial to provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Recommendations:**\n\n1. Provide more context on the current state of research in this field and the limitations of existing treatments.\n2. Include more detailed data to support the findings.\n3. Discuss the potential off-target effects of the CRISPR-Cas9 system.\n4. Provide more information on the specific miRNAs used, the target genes, and the in vitro and in vivo models employed.\n5. Discuss the limitations of the study, such as the use of in vitro models.\n6. Provide more information on the future directions of the research and to discuss the potential challenges and limitations of the approach.\n\n**Overall Rating:**\nI would give this paper an overall rating of 7/10. The study presents an interesting approach to treating neurodegenerative diseases using microRNA-based gene editing, and the results are promising. However, the paper could benefit from more detailed data, a more thorough discussion of the limitations of the study, and more information on the potential off-target effects of the CRISPR-Cas9 system.",
        "peer_review_with_watermark": "The study explores how different levels of exercise intensity\u2014low, moderate, and high\u2014affect cardiovascular risk factors in healthy adults. Employing a randomized controlled trial (RCT) approach, the researchers involved 100 healthy participants. The exercise regimen was designed to sustain a moderate cardiovascular exertion level, assessed using the Borg Rating of Perceived Exertion (RPE). Findings revealed significant decreases in systolic blood pressure and fasting glucose levels, as well as an increase in HDL cholesterol levels in the high-intensity group compared to the low-intensity group. While the moderate intensity group demonstrated a notable rise in HDL cholesterol, there were no significant changes in blood pressure or fasting glucose levels relative to the low-intensity group. Overall, the data supports the conclusion that high-intensity exercise has a more pronounced effect on cardiovascular risk factors than its lower intensity counterparts.\n\n**Strengths:**\n- The study featured a well-organized design with a clear hypothesis and utilized a randomized controlled trial (RCT) framework.\n- The exercise program effectively maintained a moderate cardiovascular exertion level, as quantified by the Borg Rating of Perceived Exertion (RPE).\n- Findings indicated noteworthy decreases in systolic blood pressure and fasting glucose levels coupled with an increase in HDL cholesterol within the high-intensity group compared to the low-intensity group.\n\n**Weaknesses:**\n- The relatively small sample size may hinder the ability to generalize the findings to a larger population.\n- Since the study included only healthy adults, the relevance of the outcomes to older or less healthy individuals may be limited.\n- The lack of control over other potential confounding factors, such as overall physical activity and dietary habits, could have influenced the study's findings.\n- Results were derived from a single measurement point at 12 weeks, which may overlook the long-term impact of high-intensity exercise on cardiovascular risk factors.\n- Insufficient details regarding the exercise program's duration, intensity, or volume could affect the replicability of the results.\n- There was a lack of comprehensive explanation concerning why high-intensity exercise had more significant effects on cardiovascular risk factors when compared to low or moderate intensities, which limits understanding of the underlying mechanisms.\n- The absence of clear acknowledgment of the study's limitations may obscure the interpretation of the findings.\n- Lastly, the study did not provide an in-depth discussion on the practical implications of the findings, which may hinder their application in real-world scenarios.",
        "gr_score_with": 0.9245283018867925,
        "gr_score_without": 0.9212962962962963,
        "output_without": {
            "num_tokens_scored": 432,
            "num_green_tokens": 398,
            "green_fraction": 0.9212962962962963,
            "z_score": 1.4754506879290419,
            "p_value": 0.0700457082527528
        },
        "output_with": {
            "num_tokens_scored": 371,
            "num_green_tokens": 343,
            "green_fraction": 0.9245283018867925,
            "z_score": 1.5748281993418614,
            "p_value": 0.057648052735337914
        }
    },
    {
        "title": "WIN-WIN: TRAINING HIGH-RESOLUTION VISION TRANSFORMERS",
        "abstract": "Transformers have become the standard in state-of-the-art vision architectures, achieving impressive performance on both image-level and dense pixelwise tasks. However, training vision transformers for high-resolution pixelwise tasks has a prohibitive cost. Typical solutions boil down to hierarchical architectures, fast and approximate attention, or training on low-resolution crops. This latter solution does not constrain architectural choices, but it leads to a clear performance drop when testing at resolutions significantly higher than that used for training, thus requiring ad-hoc and slow post-processing schemes. In this paper, we propose a novel strategy for efficient training and inference of high-resolution vision transformers: the key principle is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. We show that this strategy is effective when using relative positional embedding such as rotary embeddings. It is 4 times faster to train than a full-resolution network, and it is straightforward to use at test time compared to existing approaches. We apply this strategy to two dense prediction tasks with high resolution data. First, we show on the task of semantic segmentation that a simple setting with 2 windows performs best, hence the name of our method: Win-Win. To demonstrate the generality of our contribution, we further extend it to the binocular task of optical flow, reaching state-of-the-art performance on the Spring benchmark that contains Full-HD images with an inference time an order of magnitude faster than the best competitor. 0 10 20 30 40 50 60 Training time (hours) 45 50 55 60 m ea n Io U Semantic segmentation on BDD100k Win-Win (31GB) Full-res. (58GB) Full-res ViTDet (32GB) 0 25 50 75 100 125 150 175 Training time (hours) 1.6 2.0 2.4 2.8 3.2 En dPo in t E rro r ( EP E) Optical flow estimation on MPI-Sintel-clean Win-Win (14GB) Full-res. (28GB) Full-res ViT-Det (21GB) 80 100 120 140 160 180 Training time (hours) 0.0 0.5 1.0 1.5 In fe re nc e tim e (s ec on ds ) Optical flow estimation on MPI-Sintel-clean Full-res ViT Full-res ViT-Det Win-Win ViT + Tiling Figure 1: Validation performance vs. training time on semantic segmentation (left) and optical flow (middle). We compare our two-window training (Win-Win) to a standard full-resolution training as well as a sparsification of the attention following ViT-Det (Li et al., 2022a). We indicate the memory usage in parenthesis in the legend. Compared to full-resolution training, Win-Win allows to reduce the training time by a factor 3-4 and to half the memory usage while reaching a similar performance. Training and inference times on optical flow, for Win-Win vs. other strategies (right). ViT+Tiling corresponds to a setup similar to CroCo-Flow (Weinzaepfel et al., 2023) where the model is trained on random crops, but requires a tiling strategy at inference. While Win-Win is as fast to train as the latter, it can directly process full-resolution inputs at test time.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it.  Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$user\n\nThe peer review format and length should be of standard conference. \\\n            Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$\\ \n            title: WIN-WIN: TRAINING HIGH-RESOLUTION VISION TRANSFORMERS \\\n            abstract: Transformers have become the standard in state-of-the-art vision architectures, achieving impressive performance on both image-level and dense pixelwise tasks. However, training vision transformers for high-resolution pixelwise tasks has a prohibitive cost. Typical solutions boil down to hierarchical architectures, fast and approximate attention, or training on low-resolution crops. This latter solution does not constrain architectural choices, but it leads to a clear performance drop when testing at resolutions significantly higher than that used for training, thus requiring ad-hoc and slow post-processing schemes. In this paper, we propose a novel strategy for efficient training and inference of high-resolution vision transformers: the key principle is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. We show that this strategy is effective when using relative positional embedding such as rotary embeddings. It is 4 times faster to train than a full-resolution network, and it is straightforward to use at test time compared to existing approaches. We apply this strategy to two dense prediction tasks with high resolution data. First, we show on the task of semantic segmentation that a simple setting with 2 windows performs best, hence the name of our method: Win-Win. To demonstrate the generality of our contribution, we further extend it to the binocular task of optical flow, reaching state-of-the-art performance on the Spring benchmark that contains Full-HD images with an inference time an order of magnitude faster than the best competitor. 0 10 20 30 40 50 60 Training time (hours) 45 50 55 60 m ea n Io U Semantic segmentation on BDD100k Win-Win (31GB) Full-res. (58GB) Full-res ViTDet (32GB) 0 25 50 75 100 125 150 175 Training time (hours) 1.6 2.0 2.4 2.8 3.2 En dPo in t E rro r ( EP E) Optical flow estimation on MPI-Sintel-clean Win-Win (14GB) Full-res. (28GB) Full-res ViT-Det (21GB) 80 100 120 140 160 180 Training time (hours) 0.0 0.5 1.0 1.5 In fe re nc e tim e (s ec on ds ) Optical flow estimation on MPI-Sintel-clean Full-res ViT Full-res ViT-Det Win-Win ViT + Tiling Figure 1: Validation performance vs. training time on semantic segmentation (left) and optical flow (middle). We compare our two-window training (Win-Win) to a standard full-resolution training as well as a sparsification of the attention following ViT-Det (Li et al., 2022a). We indicate the memory usage in parenthesis in the legend. Compared to full-resolution training, Win-Win allows to reduce the training time by a factor 3-4 and to half the memory usage while reaching a similar performance. Training and inference times on optical flow, for Win-Win vs. other strategies (right). ViT+Tiling corresponds to a setup similar to CroCo-Flow (Weinzaepfel et al., 2023) where the model is trained on random crops, but requires a tiling strategy at inference. While Win-Win is as fast to train as the latter, it can directly process full-resolution inputs at test time. \\\n            paper text:  Transformers have become the standard in state-of-the-art vision architectures, achieving impressive performance on both image-level and dense pixelwise tasks. However, training vision transformers for high-resolution pixelwise tasks has a prohibitive cost. Typical solutions boil down to hierarchical architectures, fast and approximate attention, or training on low-resolution crops. This latter solution does not constrain architectural choices, but it leads to a clear performance drop when testing at resolutions significantly higher than that used for training, thus requiring ad-hoc and slow post-processing schemes. In this paper, we propose a novel strategy for efficient training and inference of high-resolution vision transformers: the key principle is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. We show that this strategy is effective when using relative positional embedding such as rotary embeddings. It is 4 times faster to train than a full-resolution network, and it is straightforward to use at test time compared to existing approaches. We apply this strategy to two dense prediction tasks with high resolution data. First, we show on the task of semantic segmentation that a simple setting with 2 windows performs best, hence the name of our method: Win-Win. To demonstrate the generality of our contribution, we further extend it to the binocular task of optical flow, reaching state-of-the-art performance on the Spring benchmark that contains Full-HD images with an inference time an order of magnitude faster than the best competitor.1 INTRODUCTION  We explore the training of non-hierarchical vision transformers (Dosovitskiy et al., 2021) (ViTs) for dense tasks at high resolutions. Self-attention (Vaswani et al., 2017) based architectures have been shown to scale well with large amounts of data (Dosovitskiy et al., 2021; Touvron et al., 2021; Dehghani et al., 2023a) and can be trained in a self-supervised manner (Caron et al., 2021; Xie et al., 2022; He et al., 2022; Weinzaepfel et al., 2022; Feichtenhofer et al., 2022; Tong et al., 2022). Their generic structure allows them to be fine-tuned on both image-level and pixel-level downstream tasks (Caron et al., 2021; He et al., 2022; Oquab et al., 2023; Weinzaepfel et al., 2022), however, the applicability of vanilla vision transformers to high resolution images is limited, due to the quadratic complexity of global self-attention. This problem is compounded by the fact that ViTs tend to generalize poorly to test resolutions significantly higher than that seen at training, which was recently evidenced in several works (Tian et al., 2022; Ali et al., 2021; Oquab et al., 2023; Ranftl et al., 2021).\nExisting solutions can broadly be separated into three categories. First, one can replace the quadratic global attention by either a local or windowed attention mechanism (Li et al., 2022a; Liu et al., 2021; 2022a), or by using an approximation of the attention with sub-quadratic complexity (Ali et al., 2021; Wang et al., 2020a). The second category of approach aims at reducing the number of tokens by using hierarchical representations with pooling, downsampling or pruning (Wang et al., 2021; Fan et al., 2021; Li et al., 2022b; Zhang et al., 2021; Liu et al., 2021; Yin et al., 2022; Pan et al., 2021; Chen et al., 2023; Rao et al., 2021), or by varying the patch resolution (Beyer et al., 2023; Lin et al., 2022). This is typically effective for tasks that do not require pixelwise predictions. The third category consists in keeping a vanilla ViT architecture and training on small-sized crops of fixed size, while resorting to tiling at test time to obtain high resolution outputs (Weinzaepfel et al., 2023; Huang et al., 2022; Shi et al., 2023; Jaegle et al., 2022), i.e. with a sliding window strategy, where multiple predictions from overlapping crops can be aggregated. An alternative consists in performing most training at lower resolutions and finally finetuning at the target high resolution (Zheng et al., 2021; Ranftl et al., 2021), but this remains very costly. In this paper, we propose a novel path that maintains vanilla self-attention, uses a non-hierarchical transformer backbone, does not require intensive fine-tuning, and produces high-resolution outputs in a single forward pass at test time.\nOur proposed approach to train a high-resolution transformer with vanilla self-attention relies on masking most of the input tokens, leading to a 3\u22124\u00d7 faster training and 2\u00d7 reduced memory, see Figure 1. Our main contribution is to show that specific masking configurations, unlike the random one typically employed in masked input modeling (He et al., 2022; Tong et al., 2022), must be used to enable high-resolution generalization at inference time, see Figure 2. These configurations allow to jointly consider global and local interactions, which is key for dense prediction tasks. Our proposed solution is to randomly select multiple windows in the input image, so that both global (inter-windows) and local (intra-windows) interactions occur during training. We restrict our study to rectangular windows that naturally lends themselves to the convolutional heads typically used with transformers backbones for dense prediction tasks (Ranftl et al., 2021; Liu et al., 2022b). Empirically, we find that a simple setup with two squared windows of the same shape performs as well as more elaborate strategies. We therefore call our training strategy with two windows Win-Win.\nTo demonstrate the generality of our training framework, we show its effectiveness on monocular and binocular tasks. Specifically, we experiment with semantic segmentation and optical flow\nestimation. On the first task, we achieve a final performance on par with more elaborate training strategies that can require test-time processing tricks such as sliding windows, which is slow and produces artifacts. Applying this idea to the second task of optical flow estimation is non trivial as it involves an additional input image for which the window sampling strategy depends of the first image\u2019s windows. We thus propose an extension of our window sampling strategy to the case of multiple image. Using it, we obtain state-of-the-art performance on the Full-HD Spring benchmark (Mehl et al., 2023) without employing task-specific designs nor requiring tiling at test time.\nWhile our training strategy allows to test directly at the target resolution, we observe that the statistics of the self-attention can change when more tokens become visible at test time. We compensate for this using a temperature factor in the softmax of the attention, validated using the performance on the full-resolution training images.2 RELATED WORK  High-Resolution ViTs are costly to train due to the quadratic complexity of the global attention mechanism with respect to the number of input tokens. A common strategy is to train on a small resolution and test on higher-resolution inputs (Ranftl et al., 2021; Oquab et al., 2023; Zhang et al., 2021), or to use a fixed scale during both training and test (Li et al., 2022a; Kirillov et al., 2023). In the first case, the authors all report that the train/test discrepancy consistently decreases performance. For instance, DPT (Ranftl et al., 2021) trains on 480\u00d7480 crops and notices a clear drop in performance as test resolution increases. Three trends emerged to decrease training cost: sub-quadratic approximations of the global attention, local/sparse attention and hierarchical approaches.\nFirstly, a strategy is to approximate the original global attention mechanism with sub-quadratic variants. The most computationally efficient methods have a linear complexity w.r.t. to the number of tokens, via an approximations of the softmax (Choromanski et al., 2021), by spatial reduction (Wang et al., 2020a; 2021) or linearization (Ali et al., 2021) of the query-key product.\nSecondly, instead of modifying the quadratic attention, several works propose to sparsify the affinity matrix between queries and keys, e.g. attention sparsification. This can either be done using local attention mechanisms (Vaswani et al., 2017; 2021), via subsampling (Wu et al., 2021; Zhang et al., 2022; Huang et al., 2019) or a combination of local and low-resolution global attention (Zhang et al., 2021; Chu et al., 2021). For instance, ViT-Det (Li et al., 2022a) proposes to use a windowed attention in almost all transformer blocks except 4 of them, in order to still model global interactions for object detection. SAM (Kirillov et al., 2023) uses a similar backbone for segmentation. However, this remains costly to train (only 25% training speed-up, see Figure 1) and tends to achieve slightly lower performance than using vanilla attention, as shown in (Li et al., 2022a).\nSeveral methods have proposed to reduce the number of tokens (Lin et al., 2022), possibly with the use of hierarchical structures, either regular (Liu et al., 2021; 2022a; Wu et al., 2021; Fan et al., 2021; Li et al., 2022b; Wang et al., 2021; Chen et al., 2022b) or irregular (Yuan et al., 2021; Bolya et al., 2023; Xu et al., 2022b). However, this redesigning of the original ViT does not allow to easily leverage task-agnostic large-scale pre-training, which is generally performed with a standard ViT backbone. Finally, another solution is to drop random tokens as proposed in (Fang et al., 2023) for object detection, a task that does not require a pixelwise prediction, or more recently in a concurrent work (Dehghani et al., 2023b) for image classification in which case the goal was mainly to allow batching despite various image sizes during training. However, when considering pixelwise prediction task, reducing the number of tokens might harm the final performance, given the importance of low-level details for these tasks.\nIn comparison to all these approaches, Win-Win allows to train a general model from a structured subset of tokens, and thus to keep the original global attention in all blocks. It has the same cost as when training from small crops while enabling full-resolution testing, without resizing or tiling.\nDense Binocular Tasks. Even though the quadratic complexity is a problem common to all methods in optical flow, we only focus here on methods that leverage transformers for this task; we refer the reader to (Zhai et al., 2021) for a more general overview. In particular, we study the state of the art through the scope of the computational burden of high-resolution training and testing. Most of the previous art devised task-specific architectures (Huang et al., 2022; Li et al., 2021; Sui et al., 2022; Zhao et al., 2022; Xu et al., 2022a). These methods all make use of a dedicated decoder that\ntakes as input the query image and a learned representation. This representation, be it a 4D costvolume (Huang et al., 2022; Sui et al., 2022), the result of pure cross-attention (Zhao et al., 2022; Xu et al., 2022a) or a hybrid of both (Li et al., 2021), is where the computational complexity lies. The vanilla global attention of transformers is often replaced with either coarse-to-fine approaches (Zhao et al., 2022; Xu et al., 2022a) or through attention sparsification (Li et al., 2021). When it is not, tiling is employed at test-time (Huang et al., 2022; Shi et al., 2023; Jaegle et al., 2022). In (Li et al., 2021), gradient checkpointing (Griewank & Walther, 2000) is also used, which complexifies the procedure. In contrast, the philosophy of this work lies in the idea that a simpler general architecture could be used without bells and whistles. Some previous works explore this direction (Weinzaepfel et al., 2023; Jaegle et al., 2022), but still train on low resolutions and resort to tiling at test-time to alleviate the training computational burden. To the best of our knowledge, we are the first to show that testing directly at high resolution is possible while training with a reasonable cost.3 TRAINING FROM MULTIPLE WINDOWS: WIN-WIN  The standard vision transformer (Dosovitskiy et al., 2021) views an input image x as a set P of patches, or equivalently, tokens. A series of blocks alternating multi-head self-attention and a multilayer perceptron (MLP) then processes this set of tokens. Win-Win relies on the idea of using a subset P \u2032 \u2282 P during training, i.e. masking (or rather, discarding) the other tokens, while allowing to directly process P at test time. The training complexity is thus reduced from O(|P|2) to O(|P \u2032|2); the size of subset P \u2032 can be adapted to the memory budget and made independent from input image resolution.\nGiven how local interactions are important for vision tasks (Chen et al., 2022a; Wang et al., 2023), it seems desirable to preserve as many neighboring tokens as possible in P \u2032. This can be easily implemented, e.g. by defining the selected tokens to be inside a rectangular crop. On the other hand, it seems crucial to preserve long-range interactions as well. Without them, generalizing to high-resolution images might be impossible due to the domain gap, since long-range dependencies would have never been observed in the small training crops. Such drop of performance when increasing the resolution was recently evidenced in (Tian et al., 2022; Ali et al., 2021; Oquab et al., 2023; Ranftl et al., 2021). The main idea of Win-Win consists in selecting the tokens of P \u2032 in a structured configuration, where both local and long-range token interactions are guaranteed to be present. Specifically, we select tokens from a set of N \u2265 2 non-overlapping rectangles {Ri}i=1..N, see Figure 2:\nP \u2032 = {p | \u2203i \u2208 {1..N}, p \u2208 Ri}. (1)\nWe now detail the token selection procedure, architectural details and generalization to binocular tasks of our Win-Win approach.\nWindow distribution. The principle of Win-Win is to randomly sample N non-overlapping windows for each training image. Note that, thanks to randomness, all training token positions end up being selected at some point during training. Experimentally, one of our findings is that the simplest strategy (i.e., sampling N = 2 squared windows of the same size) performs best, see Section 4. Beyond choosing N, different window sizes can be chosen, depending on the compute budget desired for training.\nConvolutional heads. State-of-the-art ViT-based architectures for pixelwise prediction tasks typically rely on convolutional heads (Ranftl et al., 2021; Liu et al., 2022b; Weinzaepfel et al., 2023). In contrast to unstructured MAE-like random masking (He et al., 2022; Fang et al., 2023; Dehghani et al., 2023b), Win-Win is compatible with a convolutional head. Token features output by the transformer backbone can be split and reshaped into features maps from the N rectangles, to which convolutions can be applied separately as illustrated in Figure 2.\nPositional embeddings. Dense tasks such as semantic segmentation are typically translation equivariant, which becomes a useful guiding principle when designing deep models. Classical absolute positional embeddings that are added to the signal, either learned (Dosovitskiy et al., 2021) or using cosine functions (Vaswani et al., 2017), do not satisfy this property. We therefore employ relative positional embeddings that are applied directly at the level of self-attention computations. They can either be learned constants (Liu et al., 2022a; Li et al., 2022b), outputs of a neural network (Liu et al., 2022a) or given by transforms only applied to queries and keys (Su et al., 2021). In this work,\nwe use this latter option as it does not involve any learnable parameters, and empirically performs better than absolute embeddings.\nTest time. At test time, the windowed sampling scheme can simply be removed and the full image, i.e. the full set of tokens P is processed. Note that memory requirement at test time are drastically lower, since intermediate tensors can be immediately freed during inference.\nFeature distribution changes. Using the full image at test time induces a change in the number of tokens compared to training. Although self-attention handles arbitrary numbers of tokens, the softmax distribution can be altered by this increase. To compensate for this, we tune the temperature hyper-parameter in the softmax attention\u03c4 (Q,K, V ) = softmax ( \u03c4QKT ) V, which normally defaults to \u03c4 = \u221a d, where d is the feature dimension of each head. Once Win-Win training is performed, we validate \u03c4 on full-resolution images of the train set (i.e. without masking). Please refer to Appendix E for more details.\nGeneralization to binocular tasks. Win-Win is a general framework for transformer-based architectures that can also be applied to multi-image tasks. For instance, but with no loss of generality, we focus in this paper on the binocular task of optical flow estimation. The task consists of predicting the displacements of pixels between two consecutive video frames. In this scenario, masking only one frame is not sufficient to limit the training complexity, and we thus need to mask both frames simultaneously, see Figure 3. This is not trivial, since valuable information with respect to a given window in the first frame is located at a particular spot in the second frame. Hence, simply applying the random crop strategy on each frame independently would likely lead to a very sparse training signal, as matching pixels would have a low chance of being visible in both inputs.\nTo circumvent this issue, we propose a simple binocular extension to Win-Win. We first sample N non-overlapping random windows in the first frame. We then evaluate whether each token in the second frame has a corresponding visible token in the first frame, based on the ground-truth flow. We finally sample M non-overlapping windows in the second frame, this time using random sampling weighted by the amount of matched tokens inside each window. We refer to Appendix B for more details. We experiment in Section 4.2 with this strategy as well as with simpler strategies (e.g. using the same windows in both frames) and demonstrate superior performance for the proposed sampling scheme.4 EXPERIMENTS  In this section, we first validate our Win-Win training strategy on a monocular task (semantic segmentation) in Section 4.1 and then present results for the binocular task of optical flow (Section 4.2).4.1 MONOCULAR TASK RESULTS  Experimental setup. Experiments are performed on the BDD-100k dataset (Yu et al., 2020) that comprise 7,000 training images and 1,000 validation images in a driving scenario with 19 semantic classes. All images are of resolution 1280\u00d7720 pixels. We report the mean intersection-over-union (mIoU) metric on the validation set in percentage. We also experiment with the monocular depth prediction task in Appendix D.\nTraining details. We use a ViT-Base encoder (Dosovitskiy et al., 2021) followed by a Conv-NeXt head (Liu et al., 2022b; Bachmann et al., 2022), finetuned for 100 epochs from a CroCo pretrained\nmodel (Weinzaepfel et al., 2022; 2023) with RoPE positional embeddings (Su et al., 2021), except otherwise stated. Detailed training settings are in Appendix F.\nTraining with multiple rectangles. Table 1 (left) reports results obtained with our multi-window training strategy, for a fixed budget of approximately 1024 tokens (up to rounding errors depending on resolutions) out of 3600 tokens in the full resolution input, with varying numbers of windows. A single window yields the worst performance, demonstrating the importance of learning long-range interactions. Using more windows performs better, and we find that 2 windows is enough to achieve the best performance.\nImpact of softmax temperature. Table 1 (left) shows the results obtained with and without a temperature parameter added to the softmax, to account for the discrepancy of the number of tokens during training (around 1024) and testing (around 3600). The impact is moderate, with results improved in terms of mIoU by a margin between 0 and 0.5, showing near-perfect generalization capabilities when using two windows.\nImpact of positional embeddings. Next, we compare our strategy when using cosine absolute positional embeddings instead of RoPE. The mIoU goes down from 63.6% to 57.0%. This clear drop of performance can be explained by the fact that absolute positional embeddings suffer from interpolation from the low-resolution pre-trained models and to the absence of the translation equivariance. We also find that trying to apply a correction to the temperatures in the softmax did not help either in this setting (-7% mIoU). We hypothesize that this is caused by the relationship between the softmax temperature \u03c4 and the self-probability in the attention when using relative positional embedding, see Appendix E.\nWindow sizes. Fixing the number of windows to N = 2, we vary the window sizes in Table 1 (right). We observe similar performance (between 63 and 64 mIoU) as long as the number of tokens remains fixed at \u223c1024. Square windows of identical sizes thus suffice, and we keep this setting (first row) as default setting for the rest of the experiments for simplicity. We also try to reduce the size of the second window, but this result in a small drop of performance (last row of Table 1). Finally, we experiment with (a) adding other isolated random tokens, in addition to the windows or (b) choosing window sizes randomly at each iteration, but we do not notice any improvement, see Appendix C for details. Overall, we note that Win-Win is robust to various window designs, and that simpler (two squared windows of the same size) is better.\nComparison to other baselines. We compare Win-Win to other baselines in Table 2 (left). The first baseline consists of training at full-resolution, denoted as \u2018Full-res ViT\u2019, but has a large cost due to 3600 tokens going into quadratic self-attention blocks (see also Figure 1 left). To mitigate this, ViT-Det (Li et al., 2022a) and SAM (Kirillov et al., 2023) have proposed to replace global attention by windowed attention in the ViT, except at 4 layers regularly spread across blocks where global attention is kept. While altering the attention mechanism, this alternative (denoted as \u2018ViT-Det\u2019) performs roughly on par with Full-res. However, training time is only reduced by about 25%, see Figure 1. In comparison, Win-Win trained on 2 windows of size 352\u00d7352 (i.e. 22\u00d722 tokens) slightly improves the performance while reducing the training time by a factor of 4 and the memory requirement by a factor of 2. The slight improvement in performance might be explained by the masking acting as data augmentation for this semantic task (Chen et al., 2020).\nWe also compare to a baseline with randomly selected tokens which we clearly outperform. This indicates that our method may allow to better leverage local interactions inside each window. Additionally, our approach allows to train convolutional head on top, even if this does not explain alone the gains (Section C). We finally compare to a baseline that is trained on a single fixed-size crop but for which we evaluate three different test-time strategies: either (i) perform full-resolution inference without any change, (ii) downscale the test image and upscale the output prediction (denoted as resize), or (iii) split the image into a set of (overlapping) fixed-size crops and aggregate per-crop predictions afterwards (denoted as tiling). Note that tiling requires many forward passes. For instance, up to 6 predictions per pixel are computed to go from (512\u00d7512) to (1280\u00d7720) with a crop overlap of 50%, and up to 33 predictions per pixel for a 90% one. We find that tiling overall yields the best baseline results, most likely due to the fact that the first two baselines result in large domain gaps w.r.t. training data. To conclude, our proposed multi-window training strategy achieves the best of both worlds: it allows to directly process high-resolution images at test time in a single forward pass, without any sliding window or other strategy required, while being cheaper to train than full-resolution approaches.\nImpact of the number of tokens. In Table 2 (right), we experiment with different numbers of training tokens (200, 580, 1024) and compare the performance of Win-Win w.r.t. training with a single crop and using tiling at test time. When using 200 tokens, Win-Win performs on par with the baseline but allows to produce high-resolution results at test time in one forward pass instead of using tiling. With more tokens, Win-Win outperforms the single-crop tiling baseline.4.2 BINOCULAR TASK RESULT  We now experiment with the binocular task of optical flow estimation. We first select the window sampling strategy on a small synthetic dataset built for this purpose, and then evaluate the best configurations on the MPI-Sintel (Butler et al., 2012) and Spring (Mehl et al., 2023) benchmarks.\nExperimental setup. We base our model on CroCo-Flow-Base (Weinzaepfel et al., 2023). It comprises a Siamese ViT-Base image encoder followed by a ViT-Base decoder with additional crossattention modules to handle the second frame, and a DPT head (Ranftl et al., 2021). We refer to Appendix F for more details.\nMulti-Window training for optical flow. As explained in Section 3, our masking strategy for optical flow consits of extracting N and M windows in the first and second frame, respectively. We evaluate different options for N and M in Table 3. Results are reported on a small synthetic dataset, constructed akin to AutoFlow (Sun et al., 2021), with a smaller network architecture for the sake of speed. The performance improves significantly when going from M = 2 to M = 3 windows, and using M \u2265 3 windows performs similarly. Stochasticity denotes the level of randomness when sampling windows in the second frame. No\nstochasticity means that the selection is optimal w.r.t. windows selected in the first frame. We experiment with different amounts of stochasticity in the window selection process and obtain the best performance with a value of 0.3. Using N = 3 windows in the first frame tends to degrade results slightly, though the training still works and seems robust to this change. As a sanity check, we also compare to a simpler variant where the same windows are used in the second frame and obtain a degraded performance. Finally, we also compare in this setting to the case of a single window, which significantly degrades performance due the lack of global interaction modeling.\nResults on MPI-Sintel validation. Using the best settings found previously (i.e. N = 2 and M = 4), we evaluate Win-Win on MPI-Sintel (Butler et al., 2012) in Table 4. Models are trained on FlyingChairs (Dosovitskiy et al., 2015), FlyingThings (Mayer et al., 2016), and MPI-Sintel from which we keep two sequences apart for validation. We report the average endpoint error (EPE) on this validation set in the \u2018clean\u2019 rendering.\nOn the left, we first compare performance for different number of training tokens (200, 300, 400 and 500) for Win-Win while keeping the window scheme (number of windows in each image) constant. The performance improves when the number of visible tokens grows, and we thus use the 500 tokens setting, a similar value as used in CroCo-Flow (Weinzaepfel et al., 2023).\nOn the right, we compare Win-Win to full-resolution training (with or without ViT-Det). This has a higher cost for training, see Figure 1 middle. Win-Win obtains a slightly better performance while significantly reducing the training cost.\nWe also compare to several baselines trained on fixed-size crops of resolution 384\u00d7320 (480 tokens) in Table 4. Applying directly the model to full-resolution test frames performs extremely poorly. The second test strategy, consisting of downscaling the test frame to the training crop size, achieves better results, but still relatively low due to the loss of details when downscaling the frames and upscaling the predicted flow. This highlights that ViTs suffer from train/test resolution discrepancy. The best inference strategy for this baseline is obtained using a tiling-based approach that requires many forward passes at test time, leading to a high inference cost, see Figure 1 (right). In contrast, Win-Win (sixth row) achieves the best EPE overall while predicting directly at the full resolution.\nComparison to the state of the art. We finally compare to the state of the art on two benchmarks. First, we evaluate on MPI-Sintel in Table 5. In this setup, we finetune our model using TartanAir (Wang et al., 2020b) in addition to FlyingChairs, FlyingThings and MPI-Sintel, following CroCo-Flow (Weinzaepfel et al., 2023). We fine-tune the softmax temperature \u03c4 in each attention head for half an hour on full-resolution images from the train set while freezing the network weights. Doing so yields a performance improvement of about 0.05 in EPE. Our model performs closely to the other transformer-based models such as CroCoFlow (Weinzaepfel et al., 2023), FlowFormer (Huang et al., 2022) and FlowFormer++ (Shi et al., 2023). Note that Win-Win is based on the same architecture as CroCo-Flow, except that the latter is using a ViT-Large encoder backbone (we use a ViT-Base), yet Win-Win is still competitive. Furthermore, all these methods rely on tiling at test time and thus require multiple forward passes per frame pairs, with ad-hoc designs to merge the overlapping predictions. In contrast our method is simple and fast at test time, since predictions are made directly from the high-resolution inputs, see Figure 1 (right). Our method performs also close to highly task-specific approaches such as GMFlow+ (Zhao et al., 2022) which rely on coarse-to-fine and cost volumes, whereas we use generic vision transformers.\nResults for the more recent Spring benchmark, with 1920\u00d71080 Full-HD images, are reported in Table 6. On these larger images, it is even more critical to reduce training and inference costs, compared to full-resolution training and to tiling-based approaches. We finetune Win-Win on this dataset using 2025 tokens (i.e. 25% of 8100 tokens at full-resolution, which is a comparable proportion than for MPI-Sintel) for 16 epochs. Win-Win yields state-of-the-art performance for the EPE metric, particularly improving with large displacements, and even beating CroCo-Flow (Weinzaepfel et al., 2023) that uses a 3\u00d7 larger backbone. Moreover, the usage of tiling by CroCo-Flow at inference results in strong blocking artifacts as illustrated in Figure 4 (right). In comparison, Win-Win can directly process the Full-HD frames at test time and yields smooth predictions without any artifacts. Again, inference with Win-Win is more than an order of magnitude faster than that of CroCo-Flow.5 CONCLUSION  We have shown that high-resolution vision transformers can be efficiently trained with a multiwindow training strategy and directly applied to the target resolution at test time. In other words, Win-Win combines the benefits of both (1) reduced training cost as training from crops, and (2) a direct inference as when training methods in high-resolution.\nReproducibility statement. The training setups are described in the experimental section (Section 4), and we included hyper-parameters details in Appendix F. Thanks to the simplicity of our approach, we believe Win-Win to be easily reproducible for other training settings.\nEthics statement. We have read the ICLR Code of Ethics and ensures that this work follows it. Datasets used are all publicly available. We believe the negative impacts of this work to be rather limited. In fact, the application of our method does not allow for new tasks or new behaviors than that of previous works, but rather eases the training and lowers the costs. The societal impacts are the same than that of ViTs. \\\n            \nSTART OF REVIEW:assistant\n\n$$$\nPaper Summary:\n\nThe paper proposes a novel strategy for efficient training and inference of high-resolution vision transformers, called Win-Win. The key principle of Win-Win is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. The authors show that this strategy is effective when using relative positional embedding such as rotary embeddings, and is 4 times faster to train than a full-resolution network.\n\nStrengths:\n\n* The paper proposes a novel and efficient strategy for training high-resolution vision transformers.\n* The Win-Win approach allows the model to learn local and global interactions, which is key for dense prediction tasks.\n* The paper shows that the Win-Win approach can achieve state-of-the-art performance on two dense prediction tasks: semantic segmentation and optical flow estimation.\n* The authors provide a thorough experimental evaluation of the Win-Win approach, including comparisons to other baselines and ablation studies.\n\nWeaknesses:\n\n* The paper assumes that the input images are already preprocessed, which may not be the case in real-world scenarios.\n* The authors do not provide a detailed analysis of the computational cost of the Win-Win approach, which may be a concern for large-scale applications.\n* The paper does not provide a clear explanation of why the Win-Win approach works, which may make it difficult for readers to understand and replicate the results.\n\nSuggestions for Improvement:\n\n* Provide a more detailed analysis of the computational cost of the Win-Win approach, including a discussion of the trade-offs between training speed and inference speed.\n* Provide a more thorough evaluation of the Win-Win approach on other tasks and datasets, to demonstrate its generality and robustness.\n* Provide a more detailed explanation of why the Win-Win approach works, including a discussion of the theoretical foundations and the empirical evidence.\n\nRecommendation:\n\nThe paper proposes a novel and efficient strategy for training high-resolution vision transformers, and provides a thorough experimental evaluation of its effectiveness. However, the paper assumes that the input images are already preprocessed, and does not provide a detailed analysis of the computational cost of the Win-Win approach. To improve the paper, the authors should provide a more detailed analysis of the computational cost, and provide a more thorough evaluation of the Win-Win approach on other tasks and datasets. With these improvements, the paper would be a significant contribution to the field of computer vision.\n\nOverall, I would recommend this paper for publication in a top-tier conference, with the suggested improvements.\n\nRating: 8/10\n$$$",
        "peer_review_with_watermark": "$$$\nPaper Summary:\n\nThis paper introduces a new method for effectively training and inferring high-resolution vision transformers, termed Win-Win. The fundamental idea behind Win-Win is to mask most of the high-resolution inputs during the training phase, focusing instead on N random windows. This facilitates the model's learning of local interactions among tokens within each window and global interactions across different windows. Consequently, the model can efficiently handle high-resolution inputs during testing without needing additional tricks. The authors assert that this strategy is particularly successful when paired with relative positional embeddings such as rotary embeddings, achieving a training speed that is four times faster than that of a full-resolution model.\n\nStrengths:\n\n* The paper outlines an innovative and effective technique for training vision transformers at high resolutions.\n* The Win-Win method enables the model to grasp both local and global interactions, which are crucial for tasks involving dense predictions.\n* The authors conduct a comprehensive comparison of their approach against established methods, including hierarchical structures and other attention-based strategies.\n* The effectiveness of the Win-Win strategy is empirically validated across two dense prediction challenges: semantic segmentation and optical flow.\n* The paper articulately details the Win-Win methodology and its advantages.\n\nWeaknesses:\n\n* The approach presupposes a division of the input image into non-overlapping windows, which may not always be practical.\n* There is limited analysis on how varying the number of windows impacts training duration and model performance.\n* The paper posits that relative positional embeddings enhance dense prediction tasks, yet this assertion lacks in-depth exploration.\n* A comparison with additional state-of-the-art methods for dense prediction tasks is not included.\n* The assumption that Win-Win can be generalized to all dense prediction scenarios is not adequately justified.\n\nSuggestions for Improvement:\n\n* A more detailed investigation into how the quantity of windows affects both training efficiency and predictive accuracy would be beneficial.\n* The paper should incorporate comparisons with other leading methods in the field of dense predictions.\n* The role and effectiveness of relative positional embeddings in dense prediction tasks require further clarification and thorough examination.\n* Discussing the limitations of the Win-Win technique and outlining directions for future research would enhance the paper's depth.\n* Providing additional specifics about the experimental methodology and the hyperparameters used would strengthen the presentation.\n\nRecommendation:\n\nWhile the paper presents a compelling and effective framework for training high-resolution vision transformers, assumptions regarding the uniform division of input images into non-overlapping windows pose limitations. Moreover, the effectiveness of relative positional embeddings for dense prediction tasks necessitates more rigorous investigation. Enhancing the paper with a deeper analysis of the window count's impact on training time and performance, along with comparisons to leading dense prediction methodologies, would improve its comprehensiveness and persuasiveness.\n\nOverall, the writing is clear and the explanation of the Win-Win approach is articulate and informative. The empirical findings bolster the method's potential, although further validation against additional state-of-the-art solutions is needed. Addressing these suggestions would yield a more thorough and convincing paper.\n\nRating: 8/10\n\nRecommendation: Minor revisions\n$$$",
        "gr_score_with": 0.897029702970297,
        "gr_score_without": 0.9264579759862779,
        "output_without": {
            "num_tokens_scored": 4664,
            "num_green_tokens": 4321,
            "green_fraction": 0.9264579759862779,
            "z_score": 6.02302456016048,
            "p_value": 8.559372708321536e-10
        },
        "output_with": {
            "num_tokens_scored": 505,
            "num_green_tokens": 453,
            "green_fraction": 0.897029702970297,
            "z_score": -0.22249707974499242,
            "p_value": 0.588036526835807
        }
    },
    {
        "title": "THRESHOLD-CONSISTENT MARGIN LOSS FOR OPEN- WORLD DEEP METRIC LEARNING",
        "abstract": "Existing losses used in deep metric learning (DML) for image retrieval often lead to highly non-uniform intra-class and inter-class representation structures across test classes and data distributions. When combined with the common practice of using a fixed threshold to declare a match, this gives rise to significant performance variations in terms of false accept rate (FAR) and false reject rate (FRR) across test classes and data distributions. We define this issue in DML as threshold inconsistency. In real-world applications, such inconsistency often complicates the threshold selection process when deploying commercial image retrieval systems. To measure this inconsistency, we propose a novel variance-based metric called Operating-Point-Inconsistency-Score (OPIS) that quantifies the variance in the operating characteristics across classes. Using the OPIS metric, we find that achieving high accuracy levels in a DML model does not automatically guarantee threshold consistency. In fact, our investigation reveals a Pareto frontier in the high-accuracy regime, where existing methods to improve accuracy often lead to degradation in threshold consistency. To address this trade-off, we introduce the Threshold-Consistent Margin (TCM) loss, a simple yet effective regularization technique that promotes uniformity in representation structures across classes by selectively penalizing hard sample pairs. Extensive experiments demonstrate TCM\u2019s effectiveness in enhancing threshold consistency while preserving accuracy, simplifying the threshold selection process in practical DML settings.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it.  Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$user\n\nThe peer review format and length should be of standard conference. \\\n            Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$\\ \n            title: THRESHOLD-CONSISTENT MARGIN LOSS FOR OPEN- WORLD DEEP METRIC LEARNING \\\n            abstract: Existing losses used in deep metric learning (DML) for image retrieval often lead to highly non-uniform intra-class and inter-class representation structures across test classes and data distributions. When combined with the common practice of using a fixed threshold to declare a match, this gives rise to significant performance variations in terms of false accept rate (FAR) and false reject rate (FRR) across test classes and data distributions. We define this issue in DML as threshold inconsistency. In real-world applications, such inconsistency often complicates the threshold selection process when deploying commercial image retrieval systems. To measure this inconsistency, we propose a novel variance-based metric called Operating-Point-Inconsistency-Score (OPIS) that quantifies the variance in the operating characteristics across classes. Using the OPIS metric, we find that achieving high accuracy levels in a DML model does not automatically guarantee threshold consistency. In fact, our investigation reveals a Pareto frontier in the high-accuracy regime, where existing methods to improve accuracy often lead to degradation in threshold consistency. To address this trade-off, we introduce the Threshold-Consistent Margin (TCM) loss, a simple yet effective regularization technique that promotes uniformity in representation structures across classes by selectively penalizing hard sample pairs. Extensive experiments demonstrate TCM\u2019s effectiveness in enhancing threshold consistency while preserving accuracy, simplifying the threshold selection process in practical DML settings. \\\n            paper text: 1 INTRODUCTION  Deep metric learning (DML) has shown success in various open-world recognition and retrieval tasks (Schroff et al., 2015a; Wu et al., 2017; Deng et al., 2019; Wang et al., 2018). Nevertheless, the common DML losses, such as contrastive loss (van den Oord et al., 2018; Chen et al., 2020), pairwise loss (Brown et al., 2020; Patel et al., 2022) and proxy-based losses (Kim et al., 2020; Movshovitz-Attias et al., 2017; Qian et al., 2019; Deng et al., 2019), often yield highly varied intraclass and inter-class representation structures across classes (Liu et al., 2019; Duan et al., 2019; Zhao et al., 2019). Hence, even if an embedding model has strong separability, distinct classes may still require varying thresholds to uphold a consistent operating point in terms of false reject rate (FRR) or false acceptance rate (FAR). This challenge is particularly important in real-world image retrieval systems, where a threshold-based retrieval criterion is preferred over a top-k approach due to its ability to identify negative queries without matches in the gallery. However, selecting the right threshold is difficult, especially when systems must cater to diverse use-cases. For instance, in clothing image retrieval for online shopping, the similarity between two T-shirts can be significantly different from that between two coats. A threshold that works well for coats may lead to poor relevancy and give many false positives in the retrieved images for T-shirts, as shown in Figure 1. These difficulties are more pronounced in the open-world scenarios (Scheirer et al., 2012; Bendale & Boult, 2015; 2016), where the test classes may include entirely new classes not seen during training.\nWe define the phenomenon in DML, where different test classes and distributions require varying distance thresholds to achieve a similar retrieval or recognition accuracy, as threshold inconsistency. In commercial environments, particularly under the practical evaluation and deployment\n*Equal contribution.\nsetting with one fixed threshold for diverse user groups (Liu et al., 2022), the significance of threshold inconsistency cannot be overstated. Accurate quantification of this inconsistency is essential for detecting potential biases in the chosen threshold. To this end, we introduce a novel evaluation metric, named Operating-Point-Inconsistency-Score (OPIS), which quantifies the variance in the operating characteristics across classes within a target performance range. Using OPIS, we observe an accuracy-threshold consistency Pareto frontier in the high accuracy regime, where methods to improve accuracy often result in a degradation in threshold consistency, as shown in Figure 3. This highlights that achieving high accuracy does not inherently guarantee threshold consistency.\nOne solution to this problem is using posthoc calibration methods (Platt et al., 1999; Zadrozny & Elkan, 2002; Guo et al., 2017a), which adjust a trained model\u2019s distance thresholds to align with specific operating points in FAR or FRR. However, in real-world settings, these methods can be inefficient and lack robustness, as they involve constructing separate calibration datasets and may require prior knowledge about the test distribution for effective calibration (Naeini et al., 2015; Guo et al., 2017a). Moreover, they do not address the threshold inconsistency problem unless customized calibration is done for each user. Another option is employing conformal prediction (Romano et al., 2020; Gibbs & Candes, 2021), which guarantees confidence probability coverage and can handle complex data distributions as well as covariate and label shifts. However, conformal prediction inherently assumes a closed-world setting, where training and test samples share the same label space. In contrast, real-world image retrieval systems typically operate in an open-world environment, presenting a more complex and realistic setting with unknown classes at test time.\nGiven these challenges, an essential question arises: Can we train an embedding model for openworld image retrieval that sustains a consistent distance threshold across diverse data distributions, thus avoiding the complexities of posthoc threshold calibration? This objective falls within the scope of calibration-aware training. In closed-set classification, the goal of calibration-aware training is to align predicted confidence probabilities with empirical correctness of the model (Guo et al., 2017a; M\u00fcller et al., 2019; Mukhoti et al., 2020). However, our focus lies on what we term as thresholdconsistent DML, a paradigm that trains an embedding model with reduced threshold inconsistencies, such that a universal distance threshold can be applied to different test distributions to attain a similar level of FAR or FRR. This differentiation is crucial because in DML the output similarity score does not strictly reflect the empirical correctness of the model (Xu et al., 2023) and may exhibit strong variations across test data distributions. To address the unique challenges of threshold inconsistency in DML, we propose a simple yet effective regularization technique called Threshold-Consistent Margin (TCM) loss. Through experiments on four standard image retrieval benchmarks, we validate the efficacy of the TCM regularization in improving threshold consistency while maintaining accuracy. To summarize, our contributions are as follows:\n\u2022 We propose a novel variance-based metric, named Operating-Point-Inconsistency-Score (OPIS), to quantify the threshold inconsistency of a DML model. Notably, OPIS does not need a separate hold-out dataset besides the test set, enhancing flexibility in evaluation.\n\u2022 We observe an accuracy-threshold consistency Pareto frontier in the high accuracy regime. This finding underscores that achieving high model accuracy in DML does not automatically guarantee threshold consistency, necessitating dedicated solutions.\n\u2022 We introduce the Threshold-Consistent Margin (TCM) loss, a simple yet effective regularization technique, that can be combined with any base losses and backbone architecture to improve threshold consistency in DML. Our approach outperforms SOTA methods across various standard image retrieval benchmarks, demonstrating substantial improvements in threshold consistency while maintaining or even enhancing accuracy.2 RELATED WORKS  DML losses for image retrieval Advancements in DML losses for image retrieval have focused on improving accuracy, scalability and generalization Brown et al. (2020); Patel et al. (2022); Deng et al. (2020); Kim et al. (2023); Roth et al. (2020); Kan et al. (2022); Ypsilantis et al. (2023). The pioneering work of the Smooth-AP loss (Brown et al., 2020) optimizes a smoothed approximation for the average precision. Similarly, the Recall@k Surrogate loss (Patel et al., 2022) approximates the recall@k metric. Leveraging vision-transformer backbones and large batch sizes, Recall@k Surrogate has achieved remarkable performance in several image retrieval benchmarks. However, these pairwise methods are inefficient when dealing with a large number of classes. To reduce the computational complexity, proxy-based methods such as ProxyAnchor (Kim et al., 2020), ProxyNCA (Movshovitz-Attias et al., 2017), SoftTriple (Qian et al., 2019), ArcFace (Deng et al., 2019), and HIER (Kim et al., 2023) are employed, where sample representations are compared against class prototypes. Despite high accuracy, these methods still face challenges in biases and fairness (Fang et al., 2013; Ilvento, 2019; Dullerud et al., 2022) and display inconsistencies in distance thresholds when applied in real-world scenarios (Liu et al., 2022).\nEvaluation Metrics for threshold consistency (inconsistency) In closed-set classification, threshold consistency is usually evaluated through calibration metrics, such as Expected Calibration Error (ECE) (Naeini et al., 2015), Maximum Calibration Error (MCE) (Guo et al., 2017b) and Adaptive ECE (Nixon et al., 2019). These metrics gauge how well a model\u2019s predictions match actual correctness. However, directly applying them to evaluate threshold consistency in DML (e.g., by replacing confidence probability with similarity measures) is not straightforward. A key hurdle is that DML uses distance measurements to represent semantic similarities, and these distances can vary widely across different classes due to the intrinsic non-bijectiveness of semantic similarity in the data (Roth et al., 2022). In the context of DML, OneFace (Liu et al., 2022) introduced the calibration threshold for face recognition systems, which corresponds to the distance threshold at a given FAR of a separate calibration dataset. They further propose the One-Threshold-for-All (OTA) evaluation protocol to measure the difference in the accuracy performance across datasets at this calibration threshold as an indicator for threshold consistency. However, this approach requires a dedicated calibration dataset, which can be difficult to acquire in practice. To our knowledge, there is no widely accepted and straightforward metric for threshold consistency in DML.\nCalibration-aware training vs Posthoc threshold calibration Calibration-aware training has been well studied in closet-set classification, where the goal is to align predicted probabilities with empirical correctness (Guo et al., 2017a; M\u00fcller et al., 2019; Mukhoti et al., 2020). Common approaches use a regularizer to guide the model in generating more calibrated predictions (Pereyra et al., 2017; Liang et al., 2020; Hebbalaguppe et al., 2022). Yet, threshold-consistent training for DML differs from calibration-aware training. Instead of aligning model output with empirical correctness, threshold-consistent DML seeks to maintain a consistent distance threshold across classes and data distributions. In face recognition, Liu et al. (2022) introduces the Threshold Consistency Penalty to improve threshold consistency among various face domains. The method divides mini-batch data into 8 domains and computes each domain threshold using a large set of negative pairs from a feature queue. It then adjusts the loss contribution from each sample based on the ratio of its domain threshold to the in-batch calibration threshold. However, this method is designed for face recognition \u2013 a more constrained scenario. In contrast, our target is general image retrieval tasks which can involve significantly more domains, making it impractical to construct negative pairs for all domains. Besides train-time methods, another approach is posthoc threshold calibration, such as Platt calibration (Platt et al., 1999), isotonic regression (Zadrozny & Elkan, 2002) and temperature scaling (Guo et al., 2017a), which seeks to calibrate the operating point of a trained model using\nhold-out calibration datasets. However, it cannot solve threshold inconsistency unless customized calibration is conducted for each user. Another category of posthoc calibration method is conformal prediction (Tibshirani et al., 2019; Romano et al., 2020; Gibbs & Candes, 2021; Barber et al., 2023), which can be applied beyond the setting of exchangeable data even when the training and test data are drawn from different distributions. However, conformal prediction relies on a closed-set setting where the training and test data share the same label space, which does not apply to open-world image retrieval. Thus, in this work, we focus on developing a threshold-consistent training technique tailored for DML, with the goal of simplifying the posthoc calibration process in practical settings.3 THRESHOLD INCONSISTENCY IN DEEP METRIC LEARNING  Visualizing threshold inconsistency in image retrieval We visually illustrate the issue of threshold inconsistency in DML using image retrieval datasets. First, we borrow the widely-used F - score (Sasaki et al., 2007) to define the utility score, incorporating both sides of the accuracy metric (e.g. precision and recall, or specificity and sensitivity). Specifically, we denote one side as \u03d5 and the other side as \u03c8, and define the utility score, denoted as U, as follows:\nU(d) = (1 + c2) \u00b7 \u03d5(d) \u00b7 \u03c8(d)\nc2\u03d5(d) + \u03c8(d) (1)\nwhere d is the distance threshold (d \u2208 [0, 2] for hyperspherical embeddings), and c is the relative importance of \u03c8 over \u03d5 (c = 1 if not specified). Without loss of generality, we let \u03d5 be specificity (same as TNR or 1\u2212 FAR) and \u03c8 be sensitivity (same as recall or 1\u2212 FRR).1\nIn Figure 2, we present the accuracy utility-distance threshold curves for the test classes using models trained on the iNaturalist-2018 (Horn et al., 2017) and Cars-196 (Krause et al., 2013) datasets. In the left column of each subfigure, we observe considerable variations in the operating characteristics among distinct classes for models trained with the popular Smooth-AP loss. These variations make it difficult to select a single distance threshold that works well across the entire spectrum of test distributions. However, while we will elaborate on in later sections, incorporating our proposed TCM regularization during training visibly improves the threshold consistency across classes, as evidenced by the more aligned utility curves compared to those without the TCM regularization.\nOPIS for overall threshold inconsistency To quantify threshold inconsistency in DML, we introduce a variance-based metric, Operating-Point-Inconsistency Score (OPIS). Unlike the OTA evaluation proposed in Liu et al. (2022), OPIS does not require a separate calibration dataset. It quantifies the variance in the operating characteristics across test classes in a predefined calibration range of distance thresholds. This calibration range, denoted as [dmin, dmax], is typically determined based on the target performance metric operating ranges (e.g., a <FAR< b, where a, b are pre-determined error constraints). Formally, the OPIS metric can be expressed as follows:\nOPIS =\n\u2211T i=1 \u222b dmax dmin\n||Ui(d)\u2212 U\u0304(d)||2 dd T \u00b7 (dmax \u2212 dmin)\n(2)\n1We employ the specificity and sensitivity pair because they are particularly relevant for visual recognition applications and are not sensitive to changes in test data composition.\nwhere i = 1, 2,..., T is the index for the test classes, Ui(d) is the accuracy utility for class i, and U\u0304(d) is the average utility for the entire test dataset.\n\u03f5-OPIS for utility divide between groups The overall OPIS metric does not emphasize on the outlier classes. For applications where outlier threshold consistency is essential, we also provide a more fine-grained metric that focuses on the utility disparity between the best and worst sub-groups. First, we define the utility of the \u03b5 percentile of best-performing classes as follows:\nU\u03b5best(d) = \u03d5\u03b5best(d) \u00b7 \u03c8\u03b5best(d) \u03d5\u03b5best(d) + \u03c8\u03b5best(d)\n(3)\nwhere \u03d5\u03b5best(d), \u03c8\u03b5best(d) are the expected accuracy metrics for the entirety of the \u03b5 percentile of the best-performing classes. By replacing \u03b5best with \u03b5worst, the same can be defined for U\u03b5worst(d). Then, we define the \u03b5-OPIS metric as the following:\n\u03b5-OPIS = 1\ndmax \u2212 dmin \u222b dmax dmin ||U\u03b5worst(d)\u2212U\u03b5best(d)|| 2 dd (4)\nBy definition, the \u03b5-OPIS metric is maximized at \u03b5 \u2192 0, and eventually becomes zero when \u03b5 \u2192 100% as the best-performing set and worst-performing set become identical.\nHigh accuracy \u0338= High threshold consistency In Figure 3, we employ the OPIS metric to examine the relations between threshold inconsistency and recognition error in embedding models trained with various DML losses, backbones and batch sizes. Notably, we observe distinct behaviors across different accuracy regimes. In the low-accuracy regime, located in the right of the plot, we notice a simultaneous improvement of accuracy and threshold consistency. This aligns with the established notion that improving model discriminability helps threshold consistency by strengthening the association between samples and their corresponding class centroids. However, as the error decreases, a trade-off surfaces in the high-accuracy regime. Here, the reduction in error is correlated with increased threshold inconsistency, leading to the formation of a Pareto frontier.\nThe trade-off between recognition error and threshold inconsistency highlights that achieving high accuracy alone does not automatically guarantee threshold consistency. In this context, introducing the proposed OPIS metric as an additional evaluation criterion alongside recall@k is crucial for threshold-based commer-\ncial DML applications, where the ability to identify negative queries without matching classes in the gallery is of importance. To explain further, we compare OPIS with the widely-used accuracy metric, recall@k. These two metrics evaluate different aspects of a model and can be used complementarily: recall@k focuses on top-k relevancy (retrieving top-k similar samples as the query from a collection), and OPIS measures the inconsistency in threshold-relevancy (retrieving similar examples above a threshold from a collection). Moreover, unlike recall@k that solely gauges recall, OPIS evaluates both the FAR and FRR (=recall), offering a more holistic error assessment.4 TOWARDS THRESHOLD-CONSISTENT DEEP METRIC LEARNING  To tackle the threshold inconsistency problem, we introduce the Threshold-Consistent Margin (TCM) loss. TCM specifically penalizes hard positive and hard negative sample pairs near the decision boundaries outlined by a pair of cosine margins. This strategy is in line with several studies (Dong et al., 2017; Xuan et al., 2020; Robinson et al., 2020) that emphasize hard mining for\nextracting more informative samples. Let S+ and S\u2212 be the sets of cosine similarity scores for positive and negative pairs in a mini-batch, respectively, the TCM loss is formulated as follows:\nLTCM = \u03bb + \u00b7\n\u2211 s\u2208S+(m\n+ \u2212 s) \u00b7 1s\u2264m+\u2211 s\u2208S+ 1s\u2264m+\n+ \u03bb\u2212 \u00b7 \u2211\ns\u2208S\u2212(s\u2212m\u2212) \u00b7 1s\u2265m\u2212\u2211 s\u2208S\u2212 1s\u2265m\u2212\n(5)\nwhere 1condition = 1 if the condition is true, and 0 otherwise. \u03bb+ and \u03bb\u2212 are the weights assigned to the positive and negative regularizations, respectively. The TCM regularizer can be combined with any base loss Lbase, resulting in the final objective function:\nLfinal = Lbase + LTCM (6) Design justification: representation structures Several works have shown a strong correlation between model accuracy and representation structures (Yu et al., 2020; Chan et al., 2022). Indeed, SOTA DML losses are designed to optimize this relationship by encouraging intra-class compactness and inter-class discrimination. However, when considering threshold consistency, the focus shifts towards achieving consistent performance in FAR and FRR in the calibration range, with an emphasis on local representation structures near the distance threshold. In this context, the TCM regularization serves as a \u201clocal inspector\" by selectively adjusting hard samples to prevent over separateness and excessive compactness in the vicinity of the margin boundaries. This strategy also aligns with previous work that found excessive feature compression actually hurts DML generalization (Roth et al., 2020). Since the margin constraints are applied globally, this helps encourage more equidistant distribution of class centroids and more uniform representation compactness across different classes in the embedding space.\nHard mining strategy TCM regularizes on hard samples, distinguishing it from techniques that encourage similarity consistency by minimizing marginal variance (Kan et al., 2022). Specifically, TCM\u2019s hard mining strategy is different from the semi-hard negative mining strategy (Schroff et al., 2015b) and its variants (Oh Song et al., 2016; Wu et al., 2017; Wang et al., 2019), as TCM\u2019s hard mining is based on the absolute cosine similarity values, rather than their relative differences. Meanwhile, TCM also differs from ROADMAP (Ramzi et al., 2021) in that TCM utilizes hard positive and negative counts, whereas ROADMAP uses the total positive and negative counts. This makes TCM well-suited for scenarios involving large batch sizes (as is the standard in DML) and significant imbalances between the counts of positive and negative pairs2.\nConnection to the calibration range TCM is implicitly connected to the calibration range of the OPIS metric through the two cosine margins. Since cosine similarity is bijective with the L2 distance for hyperspherical embeddings, these margin constraints ensure that the model\u2019s intra-class and inter-class representation structures adhere to the desired distance threshold range, which is [ \u221a 2\u2212 2m+, \u221a 2\u2212 2m\u2212]. However, due to the inevitable distributional shift between the training and testing datasets, the selection of the margin constraints requires some hyper-parameter tuning and cannot be directly estimated from the calibration range. In Figure 6, we give guidance on how to select the margins, with details discussed in the ablation of TCM margin hyperparameters.\nTCM vs Margin-based Softmax loss TCM has distinguishing characteristics when compared to margin-based softmax losses (Deng et al., 2019; Qian et al., 2019), as illustrated in Figure 4(b).\n2An detailed comparison between TCM and the method of Ramzi et al. (2021) is given in appendix A.2.2\nFirst, TCM is designed as a regularizer that operates in conjunction with a base loss. It specifically applies to hard sample pairs that are located near the margin boundaries. Second, TCM employs two cosine margins to regularize the intra-class and inter-class distance distributions simultaneously. This allows TCM to capture both hard positive and hard negative examples, resulting in more hard pairs within a mini-batch. Secondly, the TCM loss is specifically applied to the hard pairs, contrasting with Arcface, which is applied to all pairs. Last, TCM is a sample-level pair-wise loss, which better models the relationships between individual samples compared to proxy-based methods.\nVisualization of TCM effect We visualize the effect of the TCM regularization on representation structures across the 10 classes in the MNIST dataset of handwritten digits (LeCun et al., 1998) by training a shallow CNN using the Arcface loss (Deng et al., 2019). For clearer visualization, we use two-dimensional features and employ kernel-density estimation (Chen, 2017) to model the probability density function for the embeddings of each class. As shown in Figure 5, compared to using ArcFace (Deng et al., 2019) only, the incorporation of TCM (ArcFace+TCM) enhances the separation between digits 2 and 5 (lower middle), 0 and 8 (lower right), and 4 and 9 (upper left). This observation supports our claims about TCM\u2019s ability in refining the representation structures for improved threshold consistency.5 EXPERIMENTS  5.1 DATASETS AND IMPLEMENTATION DETAILS  Datasets For training and evaluation, we use four commonly-used image retrieval benchmarks, namely iNaturalist-2018 (Horn et al., 2017), Stanford Online Product (Song et al., 2015), CUB-2002011 (Wah et al., 2011) and Cars-196 (Krause et al., 2013). These benchmarks cover a diverse set of data domains including natural species, online catalog images, birds, and cars. As in previous works (Brown et al., 2020; Patel et al., 2022; An et al., 2023), the iNaturalist and Stanford Online Product datasets use an open-world train-test-split, where the training classes are disjoint from the ones in testing. For CUB and Cars, we use shared train-test classes to make fair comparisons with prior DML methods3. The details to each dataset can be found in Table 1.\nEvaluation metrics We measure model accuracy using the recall@k metric and assess threshold inconsistency using the OPIS and \u03f5-OPIS metrics as defined earlier. Similar to previous works (Veit & Wilber, 2020; Liu et al., 2022), we estimate threshold inconsistency by comparing normalized features of image pairs in 1:1 comparisons. In the case of the iNaturalist-2018 and Stanford Online Product datasets, given the large number of classes, we only sample positive pairs exhaustively and randomly sample negative pairs with a fixed negative-to-positive ratio of 10-to-1 for each class. All positive and negative pairs in the CUB and Cars datasets are exhaustively sampled.\nImplementation details We use two backbone architectures, namely ResNet (He et al., 2016) and Vision Transformer (Dosovitskiy et al., 2020), both pretrained on ImageNet4. Since the original papers do not report OPIS, we train both baseline models (without TCM) and TCM-regularized models using the same configuration. The hyperparameters for each base loss are taken from the original papers. For TCM, we set \u03bb+ = \u03bb\u2212 = 1. For OPIS, the calibration range is set to 1e-2 \u2264 FAR \u2264 1e-1 for all benchmarks. The margin parameters (m+, m\u2212) are tuned using grid search on 10% of the training data for each benchmark. We adopt the same optimization schemes as specified\n3We also provide results for CUB and Cars in the open-world setting in Appendix A.2.5. 4For ResNet, we follow Brown et al. (2020) and use ImageNet-pretrained backbones. For ViTs, we follow Patel et al. (2022) and use ImageNet-21k pretrained backbones released by timm library (Wightman, 2019).\nin the original papers for each base loss. During training, mini-batches are generated by randomly sampling 4 images per class following previous works (Brown et al., 2020; Patel et al., 2022).5.2 ABLATION AND COMPLEXITY ANALYSIS  Unless stated otherwise, all ablation studies are conducted using the iNaturalist-2018 dataset. Owing to space constraints, further ablations can be found in the appendix.\nEffect of TCM margins We examine the impact of the cosine margins m+, m\u2212 on accuracy and OPIS. As shown in Figure 6, adding TCM consistently enhances threshold consistency compared to the baseline Smooth-AP loss across all combinations of margins, with up to 50% of reduction\nin OPIS. Regarding accuracy, we observe that the negative margin (m\u2212) has a greater influence than the positive margin (m+), which aligns with previous works (Dong et al., 2017; Xuan et al., 2020; Robinson et al., 2020). However, when the negative margin becomes excessively stringent, such as m\u2212 = 0.25, the accuracy drops below the baseline. We hypothesize that an overly restrictive negative margin may interfere with the base loss, leading to decreased accuracy. For ImageNet-pretrained backbones, the recommended values for m+ and m\u2212 are around 0.9 and 0.5, respectively.\nCompatibility with various base DML losses We select the most representative DML losses for each method category, including proxy-based methods (Movshovitz-Attias et al., 2017; Deng et al., 2020) and pairwise-based methods (Brown et al., 2020; Patel et al., 2022). Notably, the Recall@k surrogate loss (Patel et al., 2022) represents the SOTA loss for fine-grained image retrieval tasks. We run experiments using these base losses with and without the TCM regularization. As shown in Table 2, there is a consistent improvement in both accuracy (> 1.0% increase in recall@1) and threshold consistency (up to 60.7% in relative reduction) when TCM regularization is applied in conjunction with different high-performing base losses.\nCompatibility with different architectures We investigate the compatibility of TCM regularization with different backbone architectures including ResNet variants and Vision Transformers. As shown in Table 3, we observe significant improvements in threshold consistency across backbone architectures when TCM is incorporated. On accuracy, ResNet models exhibit more notable improvements in accuracy (> 1.5%) compared to Vision Transformers, which see a < 1.0% boost.\nTime Complexity In a mini-batch with size n, the complexity of TCM is O(n2) as it compares every sample with all samples in the mini-batch. For image retrieval benchmarks where the number of training classes K is significantly greater than the batch size n, i.e., K \u226b n, this complexity is comparable to most pair-based losses (O(n2)) and proxy-based losses (O(nK)). In Table 4, we provide time complexities for the loss computation, the forward and backward passes and the overall\ntime per epoch. The results suggest that adding TCM regularization results in a negligible (< 1.5%) increment in the overall training time per epoch.5.3 IMAGE RETRIEVAL EXPERIMENT  The results for supervised fine-tuning for image retrieval benchmarks with and without the TCM regularizer are summarized in Table 5. As is shown, our TCM loss is effective in improving threshold consistency (measured by OPIS and \u03f5-OPIS, the lower the better), by up to 77.3%, compared to the various baseline losses considered. Meanwhile, adding TCM regularization consistently improves accuracy across almost all benchmarks, base losses and backbone architectures. While we notice a slight decrease in recall@1 on the two smaller datasets (as marked in red), namely CUB and Cars, these are at the same magnitude as non-significant variations due to random initialization during training. It\u2019s worth highlighting that on iNaturalist-2018, arguably the largest public image retrieval benchmark, adding our TCM regularization is shown to out-perform SOTA DML loss, recall@k surrogate, reducing the OPIS threshold inconsistency score from 0.37\u00d710\u22123 to 0.17\u00d710\u22123, while improving the recall@1 accuracy metrics from 83.9% to 84.8%.6 CONCLUSION  In this work, we comprehensively study the issue of threshold inconsistency in deep metric learning. We introduce a novel variance-based metric named Operating-Point-Inconsistency-Score (OPIS) to quantify threshold inconsistency among different classes. Distinct from the One-Threshold-for-All evaluation protocol proposed by Liu et al. (2022), a key advantage of OPIS is its elimination of the need for a separate calibration dataset. As a result, OPIS can be easily utilized alongside existing accuracy metrics, providing an added dimension for evaluating the threshold robustness of trained DML models. With the OPIS metric, we find that achieving high accuracy in a DML model does not necessarily guarantee threshold consistency. To address this issue, we propose the ThresholdConsistent Margin loss (TCM), a simple and versatile regularization technique that can be integrated with any base loss and backbone architecture to improve the model\u2019s threshold consistency during training. TCM is designed to enforce more uniform intra-class compactness and inter-class separability across diverse classes in the embedding space. By incorporating TCM, we demonstrate state-of-the-art performance in both threshold consistency and accuracy across various image retrieval benchmarks. We hope that our work serves as a catalyst to encourage more explorations in developing threshold-consistent DML solutions for practical open-world scenarios. Limitations of OPIS The OPIS and \u03f5-OPIS metrics necessitate a sufficient number of samples per class to ensure statistical significance, making them unsuitable for few-shot evaluation scenarios. Limitations of TCM Like other inductive deep learning methods, TCM can fail when there\u2019s a significant distribution shift between the training and test sets or when strong label noise is present. \\\n            \nSTART OF REVIEW:assistant\n\n$$$\nPaper Summary:\n\nThe paper proposes a novel solution to address the issue of threshold inconsistency in deep metric learning (DML), which occurs when different test classes and distributions require varying distance thresholds to achieve a similar retrieval or recognition accuracy. The authors introduce a novel variance-based metric called Operating-Point-Inconsistency-Score (OPIS) to quantify the threshold inconsistency of a DML model. They also propose a simple yet effective regularization technique called Threshold-Consistent Margin (TCM) loss, which promotes uniformity in representation structures across classes by selectively penalizing hard sample pairs. The TCM loss can be combined with any base losses and backbone architecture to improve threshold consistency in DML.\n\nStrengths:\n\n1. The paper addresses a significant issue in DML, namely threshold inconsistency, which is crucial in real-world applications.\n2. The authors introduce a novel metric, OPIS, to quantify threshold inconsistency, which is more straightforward and easier to compute compared to existing methods.\n3. The proposed TCM loss is simple and effective in improving threshold consistency while maintaining accuracy.\n4. The authors demonstrate the effectiveness of TCM loss on various image retrieval benchmarks, achieving state-of-the-art performance in both threshold consistency and accuracy.\n\nWeaknesses:\n\n1. The paper assumes a sufficient number of samples per class for the OPIS and \u03f5-OPIS metrics, which may not be feasible in few-shot evaluation scenarios.\n2. The TCM loss may fail when there's a significant distribution shift between the training and test sets or when strong label noise is present.\n3. The paper does not provide a comprehensive comparison with existing methods that address threshold inconsistency, which may limit the scope of the contributions.\n4. The authors do not provide a detailed analysis of the time complexity of the TCM loss, which may be a concern for large-scale applications.\n\nSuggestions for Improvement:\n\n1. Investigate the performance of OPIS and \u03f5-OPIS metrics in few-shot evaluation scenarios and propose modifications to make them more suitable for such scenarios.\n2. Provide a more comprehensive comparison with existing methods that address threshold inconsistency to highlight the unique strengths of the proposed approach.\n3. Conduct a more detailed analysis of the time complexity of the TCM loss and propose optimizations to reduce its computational overhead.\n4. Explore the application of TCM loss in other areas of deep learning, such as classification and regression tasks, to demonstrate its versatility.\n\nRecommendation:\n\nThe paper makes significant contributions to the field of DML by addressing the issue of threshold inconsistency and proposing a novel and effective solution. The authors provide a clear and concise explanation of the proposed approach, and the experimental results demonstrate its effectiveness on various image retrieval benchmarks. However, there are some limitations and areas for improvement that need to be addressed. Overall, the paper is well-written and well-structured, and it has the potential to make a significant impact in the field of DML.\n\nRating: 8/10\n\nRecommendation for Publication: Yes\n\n$$$",
        "peer_review_with_watermark": "$$$ \nPaper Summary:\n\nThe manuscript introduces a fresh perspective on mitigating threshold inconsistency within deep metric learning (DML) specific to image retrieval mechanisms. Threshold inconsistency is characterized by the requirement for different test classes and distributions to utilize distinct distance thresholds to maintain comparable retrieval or recognition efficacy. To address this challenge, the authors propose a new variance-based metric called Operating-Point-Inconsistency-Score (OPIS) that quantifies this inconsistency. The paper also presents the Threshold-Consistent Margin (TCM) loss, a straightforward yet impactful regularization strategy aimed at enhancing uniformity across class representation structures, thereby reducing threshold inconsistency. The efficacy of the TCM loss, which is compatible with any base loss and architecture, is demonstrated through rigorous experiments on four widely-recognized image retrieval benchmarks\u2014showing it surpasses existing state-of-the-art DML losses in both threshold consistency and precision. \n\nStrengths:\n\n1. The research tackles a significant issue of threshold inconsistency in DML, adding value to practical image retrieval applications.\n2. The development of the OPIS metric provides a novel and accessible tool for evaluating threshold inconsistency, which can complement current accuracy assessments.\n3. The TCM loss is highlighted for its versatility and compatibility with a range of base losses and architectures aimed at improving threshold consistency.\n4. The comprehensive experiments conducted across four standard benchmarks reveal that TCM loss achieves notable improvements over leading DML methods regarding both threshold consistency and accuracy.\n\nWeaknesses:\n\n1. The assumption of adequate sample sizes per class may limit the applicability of findings in few-shot learning contexts.\n2. There is insufficient exploration of TCM loss performance when faced with substantial distributional shifts between training and testing datasets or under scenarios of notable label noise.\n3. A more in-depth discussion on the tuning of hyperparameters for TCM loss is lacking and might be critical for its performance optimization.\n\nSuggestions for Improvement:\n\n1. It would be beneficial for the authors to analyze TCM loss's effectiveness in few-shot learning settings, as well as under distributional shifts and high label noise conditions.\n2. An expanded discourse on hyperparameter tuning and its ramifications for TCM loss performance would enhance the paper's clarity and usability.\n3. Introducing additional experiments across varied image retrieval benchmarks could provide a more robust validation of TCM loss's generalizability.\n\nRecommendation:\n\nThis paper presents a noteworthy advancement in addressing threshold inconsistency in DML, an essential factor in real-world image retrieval scenarios. The TCM loss is user-friendly and adaptable, capable of enhancing threshold consistency alongside any base loss. However, existing limitations must be addressed. With further experimentation and detailed analysis, this paper could stand as a valuable contribution to the DML research community. \n$$$",
        "gr_score_with": 0.9075268817204301,
        "gr_score_without": 0.9205461056401074,
        "output_without": {
            "num_tokens_scored": 4468,
            "num_green_tokens": 4113,
            "green_fraction": 0.9205461056401074,
            "z_score": 4.577884637352656,
            "p_value": 2.348507961387837e-06
        },
        "output_with": {
            "num_tokens_scored": 465,
            "num_green_tokens": 422,
            "green_fraction": 0.9075268817204301,
            "z_score": 0.5410287117201964,
            "p_value": 0.29424389646436
        }
    },
    {
        "title": "RECOMBINER: ROBUST AND ENHANCED COMPRESSION WITH BAYESIAN IMPLICIT NEURAL REPRESENTATIONS",
        "abstract": "COMpression with Bayesian Implicit NEural Representations (COMBINER) is a recent data compression method that addresses a key inefficiency of previous Implicit Neural Representation (INR)-based approaches: it avoids quantization and enables direct optimization of the rate-distortion performance. However, COMBINER still has significant limitations: 1) it uses factorized priors and posterior approximations that lack flexibility; 2) it cannot effectively adapt to local deviations from global patterns in the data; and 3) its performance can be susceptible to modeling choices and the variational parameters\u2019 initializations. Our proposed method, Robust and Enhanced COMBINER (RECOMBINER), addresses these issues by 1) enriching the variational approximation while retaining a low computational cost via a linear reparameterization of the INR weights, 2) augmenting our INRs with learnable positional encodings that enable them to adapt to local details and 3) splitting high-resolution data into patches to increase robustness and utilizing expressive hierarchical priors to capture dependency across patches. We conduct extensive experiments across several data modalities, showcasing that RECOMBINER achieves competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on lowresolution images at low bitrates. Our PyTorch implementation is available at https://github.com/cambridge-mlg/RECOMBINER/.",
        "peer_review_without_watermark": "system\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\nYou are a Research Scientist. Your task is to thoroughly and critically read the paper and write a peer review of it.  Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$user\n\nThe peer review format and length should be of standard conference. \\\n            Steps to follow :- \\\n            Step 1: Read the paper critically and Only write peer review and nothing else \\\n            Step 2: In Peer review Only write Paper Summary, Strengths, Weaknesses, Suggestions for Improvement and Recommendation \\\n            Step 2: Output Format: You must Return the Review enclosed between $$$\\ \n            title: RECOMBINER: ROBUST AND ENHANCED COMPRESSION WITH BAYESIAN IMPLICIT NEURAL REPRESENTATIONS \\\n            abstract: COMpression with Bayesian Implicit NEural Representations (COMBINER) is a recent data compression method that addresses a key inefficiency of previous Implicit Neural Representation (INR)-based approaches: it avoids quantization and enables direct optimization of the rate-distortion performance. However, COMBINER still has significant limitations: 1) it uses factorized priors and posterior approximations that lack flexibility; 2) it cannot effectively adapt to local deviations from global patterns in the data; and 3) its performance can be susceptible to modeling choices and the variational parameters\u2019 initializations. Our proposed method, Robust and Enhanced COMBINER (RECOMBINER), addresses these issues by 1) enriching the variational approximation while retaining a low computational cost via a linear reparameterization of the INR weights, 2) augmenting our INRs with learnable positional encodings that enable them to adapt to local details and 3) splitting high-resolution data into patches to increase robustness and utilizing expressive hierarchical priors to capture dependency across patches. We conduct extensive experiments across several data modalities, showcasing that RECOMBINER achieves competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on lowresolution images at low bitrates. Our PyTorch implementation is available at https://github.com/cambridge-mlg/RECOMBINER/. \\\n            paper text: 1 INTRODUCTION  Advances in deep learning recently enabled a new data compression technique impossible with classical approaches: we train a neural network to memorize the data (Stanley, 2007) and then encode the network\u2019s weights instead. These networks are called the implicit neural representation (INR) of the data, and differ from neural networks used elsewhere in three significant ways. First, they treat data as a signal that maps from coordinates to values, such as mapping pX,Y q pixel coordinates to pR,G,Bq color triplets in the case of an image. Second, their architecture consists of many fewer layers and units than usual and tends to utilize SIREN activations (Sitzmann et al., 2020). Third, we aim to overfit them to the data as much as possible.\nUnfortunately, most INR-based data compression methods cannot directly and jointly optimize ratedistortion, which results in a wasteful allocation of bits leading to suboptimal coding performance. COMpression with Bayesian Implicit NEural Representations (COMBINER; Guo et al., 2023) addresses this issue by picking a variational Gaussian mean-field Bayesian neural network (Blundell et al., 2015) as the INR of the data. This choice enables joint rate-distortion optimization via maximizing the INR\u2019s \u03b2-evidence lower bound (\u03b2-ELBO), where \u03b2 controls the rate-distortion trade-off.\n\u02daequal contribution.\nFinally, the authors encode a weight sample from the INR\u2019s variational weight posterior to represent the data using relative entropy coding (REC; Havasi et al., 2018; Flamich et al., 2020).\nAlthough COMBINER performs strongly among INR-based approaches, it falls short of the stateof-the-art codecs on well-established data modalities both in terms of performance and robustness. In this paper, we identify several issues that lead to this discrepancy: 1) COMBINER employs a fully-factorized Gaussian variational posterior over the INR weights, which tends to underfit the data (Dusenberry et al., 2020), going directly against our goal of overfitting; 2) Overfitting small INRs used by COMBINER is challenging, especially at low bitrates: a small change to any weight can significantly affect the reconstruction at every coordinate, hence optimization by stochastic gradient descent becomes unstable and yields suboptimal results. 3) Overfitting becomes more problematic on high-resolution signals. As highlighted by Guo et al. (2023), the method is sensitive to model choices and the variational parameters\u2019 initialization and requires considerable effort to tune.\nWe tackle these problems by proposing several non-trivial extensions to COMBINER, which significantly improve the rate-distortion performance and robustness to modeling choices. Hence, we dub our method robust and enhanced COMBINER (RECOMBINER). Concretely, our contributions are:\n\u2022 We propose a simple yet effective learned reparameterization for neural network weights specifically tailored for INR-based compression, yielding more expressive variational posteriors while matching the computational cost of standard mean-field variational inference.\n\u2022 We augment our INR with learnable positional encodings whose parameters only have a local influence on the reconstructed signal, thus allowing deviations from the global patterns captured by the network weights, facilitating overfitting the INR with gradient descent.\n\u2022 We split high-resolution data into patches to improve robustness to modeling choices and the variational parameters\u2019 initialization. Moreover, we propose an expressive hierarchical Bayesian model to capture the dependencies across patches to enhance performance.\n\u2022 We conduct extensive experiments to verify the effectiveness of our proposed extensions across several data modalities, including image, audio, video and protein structure data. In particular, we show that RECOMBINER achieves better rate-distortion performance than VAE-based approaches on low-resolution images at low bitrates.2 BACKGROUND  This section reviews the essential parts of Guo et al. (2023)\u2019s compression with Bayesian implicit neural representations (COMBINER), as it provides the basis for our method.\nVariational Bayesian Implicit Neural Representations: We assume the data we wish to compress can be represented as a continuous function f : RI \u00d1 RO from I-dimensional coordinates to Odimensional signal values. Then, our goal is to approximate f with a small neural network gp\u00a8 | wq\nwith weights w. Given L hidden layers in the network, we write w \u201c rwr1s,...,wrLss, which represents the concatenation of the L weight matrices wr1s,...wrLs, each flattened into a rowvector. Guo et al. (2023) propose using variational Bayesian neural networks (BNN; Blundell et al., 2015) that place a prior pw and a variational posterior qw on the weights. Furthermore, they use Fourier embeddings \u03b3pxq for the input data (Tancik et al., 2020) and sine activations at the hidden layers (Sitzmann et al., 2020). To infer the implicit neural representation (INR) for some data D, we treat D as a dataset of coordinate-value pairs tpxi,yiquDi\u201c1, e.g. for an image, xi can be an pX,Y q pixel coordinate and yi the corresponding pR,G,Bq triplet. Next, we pick a distortion metric \u2206 (e.g., mean squared error) and a trade-off parameter \u03b2 to define the \u03b2-rate-distortion objective:\nLpD, qw, pw, \u03b2q \u201c \u03b2 \u00a8 DKLrqw}pws ` 1\nD\nD \u00ff i\u201c1 Eqw r\u2206pyi, gpxi | wqs, (1)\nwhere DKLrqw}pws denotes the Kullback-Leibler divergence of qw from pw, and as we explain below, it represents the compression rate of a single weight sample w \u201e qw. Note that Equation (1) corresponds to a negative \u03b2-evidence lower bound under mild assumptions on \u2206.\nWe infer the optimal posterior by computing q\u02daw \u201c argminqwPQ LpD, qw, pw, \u03b2q over an appropriate variational family Q. Guo et al. (2023) set Q to be the family of factorized Gaussian distributions. Training COMBINER: Once we selected a network architecture g for our INRs, a crucial element of COMBINER is to select a good prior on the weights pw. Given a training set tD1,...,DMu and an initial guess for pw, Guo et al. (2023) propose the following iterative scheme to select the optimal prior: 1) Fix pw and infer the variational INR posteriors q\u02daw,m for each datum Dm by minimizng Equation (1); 2) Fix the q\u02daw,ms and update the prior parameters pw based on the parameters of the posteriors. When the qw are Gaussian, Guo et al. (2023) derive analytic formulae for updating the prior parameters. To avoid overloading the notion of training, we refer to learning pw and the other model parameters as training, and to learning qw as inferring the INR.\nCompressing data with COMBINER: Once we picked the INR architecture g and found the optimal prior pw, we can use COMBINER to compress new data D in two steps: 1) We first infer the variational INR posterior qw for D by optimizing Equation (1), after which 2) we encode an approximate sample from qw using relative entropy coding (REC), whose expected coding cost is approximately DKLrqw}pws (Havasi et al., 2018; Flamich et al., 2020). Following Guo et al. (2023), we used depth-limited global-bound A\u02da coding (Flamich et al., 2022), to which we will refer as just A\u02da coding. Unfortunately, applying A\u02da coding to encode a sample from qw is infeasible in practice, as the time complexity of the algorithm grows as \u2126pexppDKLrqw}pwsqq. Hence, Guo et al. (2023) suggest breaking up the problem into smaller ones. First, they draw a uniformly random permutation \u03b1 on dimpwq elements, and use it to permute the dimensions of w as \u03b1pwq \u201c rw\u03b1p1q,...,w\u03b1pdimpwqqs. Then, they partition \u03b1pwq into smaller blocks, and compress the blocks sequentially. Permuting the weight vector ensures that the KL divergences are spread approximately evenly across the blocks. As an additional technical note, between compressing each block, we run a few steps of finetuning the posterior of the weights that are yet to be compressed, see Guo et al. (2023) for more details.3 METHODS  In this section, we propose several extensions to Guo et al. (2023)\u2019s framework that significantly improve its robustness and performance: 1) we introduce a linear reparemeterization for the INR\u2019s weights which yields a richer variational posterior family; 2) we augment the INR\u2019s input with learned positional encodings to capture local features in the data and to assist overfitting; 3) we scale our method to high-resolution image compression by dividing the images into patches and introducing an expressive hierarchical Bayesian model over the patch-INRs, and 4) we introduce minor modifications to the training procedure and adaptively select \u03b2 to achieve the desired coding budget. Contributions 1) and 2) are depicted in Figure 1, while 3) is shown in Figure 2.3.1 LINEAR REPARAMETERIZATION FOR THE NETWORK PARAMETERS  A significant limitation of the factorized Gaussian variational posterior used by COMBINER is that it posits dimension-wise independent weights. This assumption is known to be unrealistic (Izmailov et al., 2021) and to underfit the data (Dusenberry et al., 2020), which goes directly against our goal of overfitting the data. On the other hand, using a full-covariance Gaussian posterior approximation would increase the INR\u2019s training and coding time significantly, even for small network architectures.\nHence, we propose a solution that lies in-between: at a high level, we learn a linearly-transformed factorized Gaussian approximation that closely matches the full-covariance Gaussian posterior on average over the training data. Formally, for each layer l \u201c 1,..., L, we model the weights as wrls \u201c hrlswArls, where the Arls are square matrices, and we place a factorized Gaussian prior and variational posterior on hrlsw instead. We learn each Arls during the training stage, after which we fix them and only infer factorized posteriors q\nh rls w\nwhen compressing new data. To simplify notation,\nwe collect the Arls in a block-diagonal matrix A \u201c diagpAr1s,...,ArLsq and the hrlsw in a single row-vector hw \u201c rhr1sw,...,hrLsw s, so that now the weights are given by w \u201c hwA. We found this layer-wise weight reparameterization as efficient as using a joint one for the entire weight vector w. Hence, we use the layer-wise approach, as it is more parameter and compute-efficient.\nThis simple yet expressive variational approximation has a couple of advantages. First, it provides an expressive full-covariance prior and posterior while requiring much less training and coding time. Specifically, the KL divergence required by Equation (1) is still between factorized Gaussians and we do not need to optimize the full covariance matrices of the posteriors during coding. Second, this parameterization has scale redundancy: for any c P R we have hwA \u201c p1{c \u00a8 hwqpc \u00a8 Aq. Hence, if we initialize hw suboptimally during training, A can still learn to compensate for it, making our method more robust. Finally, note that this reparameterization is specifically tailored for INR-based compression and would usually not be feasible in other BNN use-cases, since we learn A while inferring multiple variational posteriors simultaneously.3.2 LEARNED POSITIONAL ENCODINGS  A challenge for overfitting INRs, especially at low bitrates is their global representation of the data, in the sense that each of their weights influences the reconstruction at every coordinate. To mitigate this issue, we extend our INRs to take a learned positional input zi at each coordinate xi: gpxi, zi | wq. However, it is usually wasteful to introduce a vector for each coordinate in practice. Instead, we use a lower-dimensional row-vector representation hz, that we reshape and upsample with a learnable function \u03d5. In the case of a W \u02c6 H image with F -dimensional positional encodings, we could pick hz such that dimphzq! F \u00a8 W \u00a8 H, then reshape and upsample it to be F \u02c6 W \u02c6 H by picking \u03d5 to be some small convolutional network. Then, we set zi \u201c \u03d5phzqxi to be the positional encoding at location xi. We placed a factorized Gaussian prior and variational posterior on hz. Hereafter, we refer to hz as the latent positional encodings, \u03d5phzq and zi as the upsampled positional encodings.3.3 SCALING TO HIGH-RESOLUTION DATA WITH PATCHES  With considerable effort, Guo et al. (2023) successfully scaled COMBINER to high-resolution images by significantly increasing the number of INR parameters. However, they note that the training procedure was very sensitive to hyperparameters, including the initialization of variational parameters and model size selection. Unfortunately, improving the robustness of large INRs using the weight reparameterization we describe in Section 3.1 is also impractical, because the size of the transformation matrix A grows quadratically in the number of weights. Therefore, we split high-resolution data into patches and infer a separate small INR for each patch, in line with other INR-based works as well (Dupont et al., 2022; Schwarz & Teh, 2022; Schwarz et al., 2023). However, the patches\u2019 INRs are independent by default, hence we re-introduce information sharing between the patch-INRs\u2019 weights via a hierarchical model for hw. Finally, we take advantage of the patch structure to parallelize data compression and reduce the encoding time in RECOMBINER, as discussed at the end of this section.\nRECOMBINER\u2019s hierarchical Bayesian model: We posit a global representation for the weights hw, from which each patch-INR can deviate. Thus, assuming that the data D is split into P patches, for each patch \u03c0 P 1,..., P, we need to define the conditional distributions of patch representations hp\u03c0qw | hw. However, since we wish to model deviations from the global representation, it is natural to decompose the patch representation as hp\u03c0qw \u201c \u2206hp\u03c0qw ` hw, and specify the conditional distribution of the differences \u2206hp\u03c0qw | hw instead, without any loss of generality. In this paper, we place a factorized Gaussian prior and variational posterior on the joint distribution of the global\nrepresentation and the deviations, given by the following product of P ` 1 Gaussian measures:\np hw,\u2206h p1:P q w\n\u201c N p\u00b5w,diagp\u03c3wqq \u02c6 P \u017a\n\u03c0\u201c1 N p\u00b5p\u03c0q\u2206,diagp\u03c3 p\u03c0q \u2206 qq (2)\nq hw,\u2206h p1:P q w\n\u201c N p\u03bdw,diagp\u03c1wqq \u02c6 P \u017a\n\u03c0\u201c1 N p\u03bdp\u03c0q\u2206,diagp\u03c1 p\u03c0q \u2206 qq, (3)\nwhere 1 : P is the slice notation, i.e. \u2206hp1:P qw \u201c \u2206hp1qw,...,\u2206hpP qw. Importantly, while the posterior approximation in Equation (3) assumes that the global representation and the differences are independent, hw and h p\u03c0q w remain correlated. Note that optimizing Equation (1) requires us to compute DKLrqhp1:P qw }php1:P qw s. Unfortunately, due to the complex dependence between the h p\u03c0q w s, this calculation is infeasible. Instead, we can minimize an upper bound to it by observing that DKLrqhp1:P qw }php1:P qw s \u010f DKLrqhp1:P qw }php1:P qw s ` DKLrqhw|hp1:P qw }phw|hp1:P qw s\n\u201c DKLrqhw,hp1:P qw }phw,hp1:P qw s \u201c DKLrqhw,\u2206hp1:P qw }phw,\u2206hp1:P qw s. (4)\nHence, when training the patch-INRs, we replace the KL term in Equation (1) with the divergence in Equation (4), which is between factorized Gaussian distributions and cheap to compute. Finally, we remark that we can view hw as side information also prevalent in other neural compression codecs (Balle\u0301 et al., 2018), or auxiliary latent variables enabling factorization (Koller & Friedman, 2009).\nWhile Equations (2) and (3) describe a two-level hierarchical model, we can easily extend the hierarchical structure by breaking up patches further into sub-patches and adding extra levels to the probabilistic model. For our experiments on high-resolution audio, images, and video, we found that a three-level hierarchical model worked best, with global weight representation hw, second/grouplevel representations hp1:Gqw and third/patch-level representations hp1:P qw, illustrated in Figure 2a. Empirically, a hierarchical model for hz did not yield significant gains, thus we only use it for hw.\nCompressing high-resolution data with RECOMBINER: An advantage of patching is that we can compress and fine-tune INRs and latent positional encodings of all patches in parallel. Unfortunately, compressing P patches in parallel using COMBINER\u2019s procedure is suboptimal, since the information content between patches might vary significantly. However, by carefully permuting the weights across the patches\u2019 representations we can 1) adaptively allocate bits to each patch to compensate for the differences in their information content and 2) enforce the same coding budget across each parallel thread to ensure consistent coding times. Concretely, we stack representations of each patch in a matrix at each level of the hierarchical model. For example, in our three-level model we set\nHp0q\u03c0,: \u201c rhp\u03c0qw,hp\u03c0qz s, Hp1qg,: \u201c h pgq w, H p2q \u201c hw, (5) where we use slice notation to denote the ith row as Hi,: and the jth column as H:,j. Furthermore, let Sn denote the set of permutations on n elements. Now, at each level \u2113, assume Hp\u2113q has C\u2113 columns and R\u2113 rows. We sample a single within-row permutation \u03ba uniformly from SC\u2113 and for each column of Hp\u2113q we sample an across-rows permutation \u03b1j uniformly from SR\u2113 elements. Then, we permute Hp\u2113q as \u0102Hp\u2113qi,j \u201c H p\u2113q \u03b1jpiq,\u03bapjq. Finally, we split the H\np\u2113qs into blocks row-wise, and encode and fine-tune each row in parallel. We illustrate the above procedure in Figure 2b.3.4 EXTENDED TRAINING PROCEDURE  In this section, we describe the ways in which RECOMBINER\u2019s training procedure deviates from COMBINER\u2019s. To begin, we collect the RECOMBINER\u2019s representations into one vector. For nonpatching cases we set h \u201c rhw,hzs, and for the patch case using the three-level hierarchical model we set h \u201c vecprHp0q,Hp1q,Hp2qsq. For simplicity, we denote the factorized Gaussian prior and variational posterior over h as ph \u201c N p\u00b5,diagp\u03c3qq and qh \u201c N p\u03bd,diagp\u03c1qq, where \u00b5 and \u03bd are the means and \u03c3 and \u03c1 are the diagonals of covariances of the prior and the posterior, respectively.\nTraining RECOMBINER: Our objective for the training stage is to obtain the model parameters A, \u03d5,\u00b5,\u03c3 given a training dataset tD1,...,DMu and a coding budget C. 1 In their work, Guo et al. (2023) control the coding budget implicitly by manually setting different values for \u03b2 in Equation (1). In this paper, we adopt an explicit approach and tune \u03b2 dynamically based on our desired coding budget of C bits. More precisely, after every iteration, we calculate the average KL divergence of the training examples, i.e., \u03b4\u0304 \u201c 1M \u0159M m\u201c1 DKLrqh,m||phs. If \u03b4\u0304 \u0105 C, we update \u03b2 by \u03b2 \u00d0 \u03b2\u02c6p1`\u03c4Cq; if \u03b4\u0304 \u0103 C \u00b4 \u03f5C, we update \u03b2 by \u03b2 \u00d0 \u03b2{p1 ` \u03c4Cq. Here \u03f5C is a threshold parameter to stabilize the training process and prevent overly frequent updates to \u03b2, and \u03c4C is the adjustment step size. Unless otherwise stated, we set \u03c4C \u201c 0.5 in our experiments. Empirically, we find the value of \u03b2 stabilizes after 30 to 50 iterations. We present the pseudocode of this prior learning algorithm in Algorithm 1. Then, our training step is a three-step coordinate descent process analogous to Guo et al. (2023)\u2019s:\n1. Optimize variational parameters, linear transformation and upsampling network: Fix the prior ph, and optimize Equation (1) or its modified version from Section 3.3 via gradient descent. Note, that L is a function of the linear transform A and upsampling network parameters \u03d5 too:\nt\u03bdm,\u03c1muMm\u201c1,A, \u03d5 \u00d0 argmin t\u03bdm,\u03c1muMm\u201c1,A,\u03d5\n#\n1\nM\nM \u00ff\nm\u201c1 LpDm, qh,m, ph,A, \u03d5, \u03b2q\n+\n. (6)\n2. Update prior: Update the prior parameters by the closed-form solution:\n\u00b5 \u00d0 1 M\nM \u00ff\nm\u201c1 \u03bdm, \u03c3 \u00d0\n1\nM\nM \u00ff\nm\u201c1\n\u201d p\u03bdm \u00b4 \u00b5q2 ` \u03c1m \u0131. (7)\n3. Update \u03b2: Set \u03b2 \u00d0 \u03b2 \u02c6 p1 ` \u03c4Cq or \u03b2 \u00d0 \u03b2{p1 ` \u03c4Cq based on the procedure described above.\nNote that unlike other INR-based methods (Dupont et al., 2022; Schwarz & Teh, 2022; Schwarz et al., 2023) our training procedure is remarkably stable, as we illustrate in Appendix D.4.4 RELATED WORKS  Nonlinear transform coding: Currently, the dominant paradigm in neural compression is nonlinear transform coding (NTC; Balle\u0301 et al., 2020) usually implemented using variational autoencoders (VAE). NTC has achieved impressive performance in terms of both objective metrics (Cheng et al., 2020; He et al., 2022) and perceptual quality (Mentzer et al., 2020), mainly due to their expressive learned non-linear transforms (Balle\u0301 et al., 2020; Zhu et al., 2021; Liu et al., 2023) and elaborate entropy models (Balle\u0301 et al., 2018; Minnen et al., 2018; Guo et al., 2021).\nCompressing INRs can also be viewed as a form of NTC: we use gradent descent to transform data into an INR. The idea to quantize INR weights and entropy code them was first proposed by Dupont et al. (2021), whose method has since been extended significantly (Dupont et al., 2022; Schwarz & Teh, 2022; Schwarz et al., 2023). The current state-of-the-art INR-based method, VCINR (Schwarz et al., 2023), achieves impressive results across several data modalities, albeit at the cost of significantly higher complexity and still falling short of autoencoder-based NTC methods on images. Our method, following COMBINER (Guo et al., 2023), differs from all of the above methods, as it uses REC to encode our variational INRs, instead of quantization and entropy coding.\nLinear weight reparameterization: Similar to our proposal in Section 3.1, Oktay et al. (2019) learn an affine reparameterization of the weights of large neural networks. They demonstrate that scalar quantization in the transformed space leads to significant gains in compression performance. However, since they are performing one-shot model compression, their linear transformations have\n1As a slight abuse of notation, we use \u03d5 to denote both the upsampling function and its parameters.\nvery few parameters as they need to transmit them alongside the quantized weights, limiting their expressivity. On the other hand, RECOMBINER learns the linear transform during training after which it is fixed and shared between communicating parties, thus it does not cause any communication overhead. Therefore, our linear transformation can be significantly more expressive.\nPositional encodings: Some recent works have demonstrated that learning positional features is beneficial for fitting INRs (Jiang et al., 2020; Kim et al., 2022; Mu\u0308ller et al., 2022; Ladune et al., 2023). Sharing a similar motivation, our method essentially incorporates implicit representations with explicit ones, forming a hybrid INR framework (Chen et al., 2023).5 EXPERIMENTAL RESULTS  In this section, we evaluate RECOMBINER on image, audio, video, and 3D protein structure data and demonstrate that it achieves strong performance across all modalities. We also perform extensive ablation studies on the CIFAR-10 and Kodak datasets which demonstrate RECOMBINER\u2019s robustness and the effectiveness of each of our proposed solutions. For all experiments, we use a 4-layer, 32- hidden unit SIREN network (Sitzmann et al., 2020) as the INR architecture unless otherwise stated, and a small 3-layer convolution network as the upsampling network \u03d5, as shown in Figure 6 in the appendix. See Appendix C for the detailed description of our experimental setup.5.1 DATA COMPRESSION ACROSS MODALITIES  Image: We evaluate RECOMBINER on the CIFAR-10 (Krizhevsky et al., 2009) and Kodak (Kodak, 1993) image datasets, and show its rate-distortion (RD) performance in Figure 3a, and compare it against recent INR and VAE-based methods, as well as VTM (JVET, 2020)2, BPG (Bellard, 2014) and JPEG2000. RECOMBINER displays remarkable performance on CIFAR-10, especially at low bitrates, outperforming even VAE-based codecs. On Kodak, it outperforms most INR-based codecs and is competitive with the more complex VC-INR method of Schwarz et al. (2023). Finally, while RECOMBINER still falls behind VAE-based codecs, it significantly reduces the performance gap.\nAudio: Following the experimental set-up of Guo et al. (2023), we evaluate our method on the LibriSpeech (Panayotov et al., 2015) dataset. In Figure 3b, we depict RECOMBINER\u2019s RD curve on the full test set, alongside the curves of VC-INR, COIN++, and MP3. We can see RECOMBINER outperforms both COIN++ and MP3 and matches with VC-INR. Since Guo et al. (2023) only tested COMBINER on 24 test clips, we do not include COMBINER in this plot but put an extra comparison in Figure 13 in Appendix F, where we can also see that RECOMBINER clearly outperforms COMBINER.\nVideo: We evaluate RECOMBINER on UCF-101 action recognition dataset (Soomro et al., 2012), following Schwarz et al. (2023)\u2019s experimental setup. However, as they do not report their traintest split and due to the time-consuming encoding process of our approach, we only benchmark our method against H.264 and H.265 on 16 randomly selected video clips. Figure 3c shows RECOMBINER achieves comparable performance to the classic domain-specific codecs H.264 and H.265, especially at lower bitrates. However, there is still a gap between our approach and H.264 and H.265 when they are configured to prioritize quality. Figure 3e shows a non-cherry-picked video compressed with RECOMBINER at two different bitrates and its reconstruction errors.\n3D Protein Structure: To further illustrate the applicability of our approach, we use it to compress the 3D coordinates of C\u03b1 atoms in protein fragments. We take domain-specific lossy codecs as baselines, including Foldcomp (Kim et al., 2023), PDC (Zhang & Pyle, 2023) and PIC (Staniscia & Yu, 2023). Surprisingly, as shown in Figure 3d, RECOMBINER\u2019s performance is competitive with highly domain-specific codecs. Furthermore, it allows us to tune its rate-distortion performance, whereas the baselines only support a certain compression rate. Since the experimental resolution of 3D structures is typically between 1-3 A\u030a (RCSB Protein Data Bank, 2000), RECOMBINER could help with reducing the increasing storage demand for protein structures without losing key information. Figure 3f shows non-cherry-picked examples compressed with our method.5.2 EFFECTIVENESS OF OUR SOLUTIONS, ABLATION STUDIES AND RUNTIME ANALYSIS  This section showcases RECOMBINER\u2019s robustness to model size and the effectiveness of each component. Appendix D.1 provides additional visualizations for a deeper understanding of our methods.\nPositional encodings facilitate local deviations: Figure 4 compares images obtained by RECOMBINER with and without positional encodings at matching bitrates and PSNRs. As we can see, positional encodings preserve intricate details in fine-textured regions while preventing noisy artifacts in other regions of the patches, making RECOMBINER\u2019s reconstructions more visually pleasing.\n2 https://vcgit.hhi.fraunhofer.de/jvet/VVCSoftware_VTM/-/tree/VTM-12.0?ref_type=tags\nRECOMBINER is more robust to model size: Using the same INR architecture, Figure 5a shows COMBINER and RECOMBINER\u2019s RD curves as we vary the number of hidden units. RECOMBINER displays minimal performance variation and also consistently outperforms COMBINER. Based on Figure 7 in Appendix D, this phenomenon is likely due to RECOMBINER\u2019s linear weight reparameterization allowing it to more flexibly prune its weight representations.\nAblation study: In Figures 5b and 5c, we ablate our linear reparameterization, positional encodings, hierarchical model, and permutation strategy on CIFAR-10 and Kodak, with five key takeaways:\n1. Linear weight reparameterization consistently improves performance on both datasets, yielding up to 4dB gain on CIFAR-10 at high bitrates and over 0.5 dB gain on Kodak in PSNR.\n2. Learnable positional encodings provide more substantial advantages at lower bitrates. On CIFAR-10, the encodings contribute up to 0.5 dB gain when the bitrate falls below 2 bpp. On Kodak, the encodings provide noteworthy gains of 2 dB at low bitrates and 1 dB at high bitrates.\n3. Surprisingly, the hierarchical model without positional encodings can degrade performance. We hypothesize that this is because directly applying the hierarchical model poses challenges in optimizing Equation (1). A potential solution is to warm up the rate penalty \u03b2 level by level akin to what is done in hierarchical VAEs (S\u00f8nderby et al., 2016), which we leave for further work.\n4. However, positional encodings appear to consistently alleviate this optimization difficulty, yielding 0.5 dB gain when used with hierarchical models.\n5. Our proposed permutation strategy provides significant gains of 0.5 dB at low bitrates and more than 1.5 dB at higher bitrates.\nRuntime Analysis: We list RECOMBINER\u2019s encoding and decoding times in Appendix D.5. Unfortunately, our approach exhibits a long encoding time, similar to COMBINER. However, our decoding process is still remarkably fast, matching the speed of COIN and COMBINER, even on CPUs.6 CONCLUSIONS AND LIMITATIONS  In this paper, we propose RECOMBINER, a new codec based on several non-trivial extensions to COMBINER, encompassing the linear reparameterization for the network weights, learnable positional encodings, and expressive hierarchical Bayesian models for high-resolution signals. Experiments demonstrate that our proposed method sets a new state-of-the-art on low-resolution images at low bitrates, and consistently delivers strong results across other data modalities.\nA major limitation of our work is the encoding time complexity and tackling it should be of primary concern in future work. A possible avenue for solving this issue is to reduce the number of parameters to optimize over and switch from inference over weights to modulations using, e.g. FiLM layers (Perez et al., 2018), as is done in other INR-based works. A second limitation is that while compressing with patches enables parallelization and higher robustness, it is suboptimal as it leads to block artifacts, as can be seen in Figure 4. Third, as Guo et al. (2023) demonstrate, the approximate samples given by A\u02da coding significantly impact the methods performance, e.g. by requiring more fine-tuning. An interesting question is whether an exact REC algorithm could be adapted to solve this issue, such as the recently developed greedy Poisson rejection sampler (Flamich, 2023). \\\n            \nSTART OF REVIEW:assistant\n\n$$$\nPaper Summary:\n\nThe paper proposes a novel data compression method called RECOMBINER, which builds upon the previous work of COMBINER (Guo et al., 2023). RECOMBINER addresses several limitations of COMBINER, including the use of factorized priors and posterior approximations that lack flexibility, the inability to adapt to local deviations from global patterns in the data, and the sensitivity to modeling choices and the variational parameters' initializations. The authors propose several non-trivial extensions to COMBINER, including a linear reparameterization for neural network weights, learnable positional encodings, and an expressive hierarchical Bayesian model for high-resolution signals. The proposed method, RECOMBINER, is evaluated on various data modalities, including image, audio, video, and 3D protein structure data, and demonstrates competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on low-resolution images at low bitrates.\n\nStrengths:\n\n1. The paper proposes several novel and non-trivial extensions to the previous work of COMBINER, which significantly improve the rate-distortion performance and robustness to modeling choices.\n2. The authors provide a comprehensive evaluation of the proposed method on various data modalities, including image, audio, video, and 3D protein structure data.\n3. The paper demonstrates competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on low-resolution images at low bitrates.\n4. The authors provide a clear and concise explanation of the proposed method and its extensions, making it easy to follow and understand.\n\nWeaknesses:\n\n1. The encoding time complexity of RECOMBINER is high, which may limit its practical application.\n2. The paper does not provide a detailed comparison with other state-of-the-art compression methods, such as VC-INR (Schwarz et al., 2023).\n3. The authors do not provide a clear explanation of how the proposed method can be extended to other data modalities.\n4. The paper does not discuss the potential applications of RECOMBINER in real-world scenarios.\n\nSuggestions for Improvement:\n\n1. The authors should provide a more detailed comparison with other state-of-the-art compression methods, such as VC-INR (Schwarz et al., 2023).\n2. The paper should provide a clear explanation of how the proposed method can be extended to other data modalities.\n3. The authors should discuss the potential applications of RECOMBINER in real-world scenarios.\n4. The paper should provide a more detailed analysis of the encoding time complexity of RECOMBINER and propose potential solutions to reduce it.\n\nRecommendation:\n\nThe paper proposes a novel and competitive data compression method called RECOMBINER, which addresses several limitations of the previous work of COMBINER. The authors provide a comprehensive evaluation of the proposed method on various data modalities and demonstrate competitive results with the best INR-based methods. However, the encoding time complexity of RECOMBINER is high, which may limit its practical application. Therefore, I recommend that the authors provide a more detailed comparison with other state-of-the-art compression methods, discuss the potential applications of RECOMBINER in real-world scenarios, and propose potential solutions to reduce the encoding time complexity.\n\n$$$",
        "peer_review_with_watermark": "$$$\n\n**Paper Summary**\n\nThe manuscript introduces RECOMBINER, an advanced data compression technique derived from the previous method COMBINER. RECOMBINER effectively tackles several shortcomings of COMBINER, such as restricted flexibility in factorized priors and posterior approximations, challenges in adapting to local data variations, and sensitivity to modeling decisions and initialization of variational parameters. The authors incorporate several significant enhancements including a linear reparameterization of weights within the implicit neural representation (INR), the addition of learnable positional encodings, and a robust hierarchical Bayesian framework. Experimental results reveal that RECOMBINER performs competitively across various data types including images, audio, video, and protein structures.\n\n**Strengths**\n\n* The paper presents an innovative approach that successfully addresses key issues in current data compression techniques.\n* A thorough evaluation demonstrates the method's effectiveness across a diverse range of data formats.\n* Detailed explanations of the methodology and its components enhance the reader's understanding.\n* Comparative analyses with leading codecs validate the proposed method's performance.\n\n**Weaknesses**\n\n* The assumption that data can be modeled as continuous functions may limit applicability to certain datasets.\n* The method's efficiency appears to heavily depend on hyperparameter selection, including layer configurations and learning rates.\n* There is insufficient discussion on the computational complexity associated with the method.\n* The manuscript lacks a comparison with other non-INR based compression methods.\n\n**Suggestions for Improvement**\n\n* The authors should enhance the discussion surrounding the computational aspects and their implications for encoding and decoding efficiencies.\n* A comparative analysis with traditional compression methods not utilizing implicit neural representations would enrich the discussion.\n* Further insights into the method's performance across varying data complexities and resolutions would be beneficial.\n* A more thorough exploration of the limitations and prospective applications should be included.\n\n**Recommendation**\n\nThis paper introduces a promising approach to resolving prevalent challenges in data compression, achieving competitive results against established codecs. While the underlying assumptions may restrict its versatility, and there are vulnerabilities related to hyperparameter settings, the potential applications in fields like computer vision and audio processing warrant further exploration. For publication, the manuscript should be refined to address its current limitations.\n\n**Recommendation Score**\n\nThe work is recommended for publication in a relevant conference or journal specializing in data compression and machine learning, with a score of 7 out of 10 reflecting its solid foundation and need for enhancements.",
        "gr_score_with": 0.891566265060241,
        "gr_score_without": 0.9246448424953675,
        "output_without": {
            "num_tokens_scored": 4857,
            "num_green_tokens": 4491,
            "green_fraction": 0.9246448424953675,
            "z_score": 5.725176025140074,
            "p_value": 5.166327480822678e-09
        },
        "output_with": {
            "num_tokens_scored": 415,
            "num_green_tokens": 370,
            "green_fraction": 0.891566265060241,
            "z_score": -0.5726941426194521,
            "p_value": 0.7165740979462314
        }
    }
]